[
  {
    "url": "https://www.python.org/Algorithms",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now, Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/Natural_language_processing",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now, Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/WebReact",
    "title": "ð¤·ð½ââï¸ Page not found | Page not found | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Go back to the home page Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.bbc.com/newsSoftware_engineering",
    "title": "BBC - 404: Not Found",
    "content": "404\n              \n                Not Found Check the page address or search for it below. BBC Homepage Copyright © BBC.\n              \n              The BBC is not responsible for the content of external sites.\n              \n                Read about our approach to external\n                linking."
  },
  {
    "url": "https://www.bbc.com/newsLinux",
    "title": "BBC - 404: Not Found",
    "content": "404\n              \n                Not Found Check the page address or search for it below. BBC Homepage Copyright © BBC.\n              \n              The BBC is not responsible for the content of external sites.\n              \n                Read about our approach to external\n                linking."
  },
  {
    "url": "https://www.python.org/Flask",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now, Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://towardsdatascience.com/JavaScript",
    "title": "JavaScript for Data Analysis | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. With the web opening new frontiers in collaboration, the web's native language of JavaScript is the best choice for thinking with data. On the demise of cameras in favor of camera-equipped mobile phones, Chase Jarvis once quipped, \"The best camera is the one that’s with you.\" In other words, portability and convenience outweigh technical differences in resolution, color, bokeh, etc. If you want to take a picture, a camera in your hand is better than a camera left at home. I think of this when asked about JavaScript versus Python, R, or Julia for data analysis. Those other languages are all great. They have powerful features such as operator overloading and multidimensional arrays, robust open-source libraries for math and visualization, and active communities. It’s easy to argue why any of them might be better suited than JavaScript for data analysis. Yet in my view, the language differences are beside the point. In part this is because JavaScript – the language, the web platform, and the open-source ecosystem – grows more capable every year. A tremendous amount of work goes into JavaScript by browser vendors, standards groups, researchers, and a vast population of global contributors. When we say JavaScript is fast, it isn’t by luck; it’s because people made it fast. We all benefit from this consolidated effort. And because JavaScript is the language of the web, it runs everywhere. Like the camera phone in your pocket, nearly everyone has a browser that runs JavaScript – the very browser you’re using to read this article. JavaScript enables interactive visualizations, explorable explanations, live analyses, model simulations, computational art, games, quizzes, you name it… When someone shares an analysis implemented in JavaScript, you’re not just seeing a static snapshot of their work; you’re running it live in your browser. You can go beyond passive reading by querying data, tweaking assumptions, and asking questions. JavaScript is the richest medium we’ve ever had for communication, but thanks to the open nature of the web it’s also something more: a medium amenable to inspection and tinkering, for learning and collaboration. Browser developer tools allow you to view, step into, and even modify, running code. The web got me hooked on programming. I first learned JavaScript through View Source: if a web page did something cool, I’d view its source and tinker with it to understand how it worked and to see if I could repurpose it to make something interesting. The immediacy and accessibility of JavaScript – and the endless possibilities – made it intoxicating. What has motivated me to keep building software over the last twenty-odd years is the idea that when you build something on the web, you’re teaching others how to build something too. We’re all swimming in this primordial soup of ideas, giving and receiving creative inspiration. Collaboration and communication are why the web exists. The web is how we work. It’s how we learn. It’s how we share ideas. It’s the web what makes JavaScript great for data analysis (and much else besides). On the web, we can edit and run code together in realtime, share exploratory views of data to answer questions, and explain concepts, with little friction. We can do almost anything we can imagine. So I admire the many powerful features and libraries available in other languages, but JavaScript is my first choice for its portability and convenience. It’s the easiest way to share live code anyone can edit. Of course, there’s still much to do. JavaScript may not have been conceived as a language for data analysis, but the language can be extended to better support it. I’m optimistic. JavaScript has improved dramatically over the last ten years, adding async/await, arrow functions, promises, iterators, generators, and more. Thanks to ES modules and services like Skypack, we may finally see easier library interoperability between Node.js and browsers (or at least fewer headaches with bundlers, transpilers, and loaders). With WebGPU, WebAssembly, and other standards under development, JavaScript’s future is bright. (See Ben Schmidt’s JavaScript and the next decade of data programming for more.) We also need new libraries and abstractions that let us spend more time thinking with data and less wrestling with the intricacies of programming. Open-source efforts such as Apache Arrow, Arquero, tidy.js, Observable Plot (which I contribute to), and Vega-Lite are helping. As the author of D3.js, another open-source JavaScript library for visualization, I’ve heard from folks who were inspired to learn visualization by the work others shared openly on the web. Some of these folks even made it a career. I’m hopeful that with better tools – and specifically tools that embrace collaboration on the web – even more folks can unlock the power of thinking with data, together. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. This article explores the relationship between a movie’s dialogue and its genre, leveraging domain-driven data… How to build a modern, scalable data platform to power your analytics and data science… How to make graphs using Matplotlib, Pandas and Seaborn Create Visualizations at the Level of Leading Newspapers To what extent are countries adopting data policies and systems for the public good? From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://www.bbc.com/newsGitHub",
    "title": "BBC - 404: Not Found",
    "content": "404\n              \n                Not Found Check the page address or search for it below. BBC Homepage Copyright © BBC.\n              \n              The BBC is not responsible for the content of external sites.\n              \n                Read about our approach to external\n                linking."
  },
  {
    "url": "https://www.bbc.com/newsDjango",
    "title": "BBC - 404: Not Found",
    "content": "404\n              \n                Not Found Check the page address or search for it below. BBC Homepage Copyright © BBC.\n              \n              The BBC is not responsible for the content of external sites.\n              \n                Read about our approach to external\n                linking."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Wikipedia",
    "content": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.[2] Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5] Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6] Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7] Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2] The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8] Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12] The term deep learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16] Deep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24] The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26] The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27] There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model[28][29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was republished by John Hopfield in 1982.[32] Other early recurrent neural networks were published by Kaoru Nakano in 1971.[33][34] Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime,[35] containing \"ideas related to artificial evolution and learning RNNs\".[31] Frank Rosenblatt (1958)[36] proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]: section 16  The book cites an earlier network by R. D. Joseph (1960)[38] \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,[39] or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships.[40] A 1971 paper described a deep network with eight layers trained by this method,[41] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".[31] The first deep learning multilayer perceptron trained by stochastic gradient descent[42] was published in 1967 by Shun'ichi Amari.[43] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[25][31] The rectifier has become the most popular activation function for deep learning.[44] Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[45][46] Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[47] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[37] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[48] The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).[49][50][31] G.M. Ostrovski et al. republished it in 1971.[51][52] Paul Werbos applied backpropagation to neural networks in 1982[53] (his 1974 PhD thesis, reprinted in a 1994 book,[54] did not yet describe the algorithm[52]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[55][56] The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58]  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59] \nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[60] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[61] In 1991, a CNN was applied to medical image object segmentation[62] and breast cancer detection in mammograms.[63] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.[64] Recurrent neural networks (RNN)[28][30] were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[65] and the Elman network (1990),[66] which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68] This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network.[67][68][31] In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[69] The \"P\" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991)[70] implemented the neural history compressor,[67] and identified and analyzed the vanishing gradient problem.[70][71]  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[72] LSTM can learn \"very deep learning\" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[73] which became the standard RNN architecture. In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[74][75] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).[76] During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[77] restricted Boltzmann machine,[78] Helmholtz machine,[79] and the wake-sleep algorithm.[80] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 [81]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[82] Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[83][84][85] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[86] Key difficulties have been analyzed, including gradient diminishing[70] and weak temporal correlation structure in neural predictive models.[87][88] Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[89][90] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91] The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[90] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[92] Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed] In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93] In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[94] in stacks of LSTMs.[95] In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.[96][9] In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[97][98] deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.[99] They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.[100][101][102] The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103] Industrial applications of deep learning to large-scale speech recognition started around 2010. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106] The deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112] including CNNs,[113] faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114] A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113] In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115] In 2011, a CNN named DanNet[116][117] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9] It then won more contests.[118][119] They also showed how max-pooling CNNs on GPU improved performance significantly.[3] In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[120] In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[121] and Google's Inceptionv3.[122] The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[123][124][125] In 2014, the state of the art was training “very deep neural network” with 20 to 30 layers.[126] Stacking too many layers led to a steep reduction in training accuracy,[127] known as the \"degradation\" problem.[128] In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet)[129] in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015),[130] both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014)[131] (based on  Jürgen Schmidhuber's principle of artificial curiosity[74][76])\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[132] based on the Progressive GAN by Tero Karras et al.[133] Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[134] Diffusion models (2015)[135] eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.[136][137] Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][138] Convolutional neural networks were superseded for ASR by LSTM.[137][139][140][141] but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".[142] Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"[144]). A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[145] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed] For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, [146] and complex DNN have many layers, hence the name \"deep\" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[147] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[148] Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[146] DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[149] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[150][151][152][153][154] Long short-term memory is particularly effective for this use.[155][156] Convolutional neural networks (CNNs) are used in computer vision.[157] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[158] As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[41] or weight decay (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularization) or sparsity (\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularization) can be applied during training to combat overfitting.[159] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[160] Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[161] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[162] DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[163] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[164][165] Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[166][167] Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[168] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[169] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[170][171] Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[172] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[173] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[174][175] Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[176] In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[177] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[177] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[177] Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[9] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[156] is competitive with traditional speech recognizers on certain tasks.[93] The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[178] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[23][108][106] More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications.[183][184][185] All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[23][186][187] A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[188] Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[189][190] Deep learning-trained vehicles now interpret 360° camera views.[191] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of Neural networks have been used for implementing language models since the early 2000s.[150] LSTM helped to improve machine translation and language modeling.[151][152][153] Other key techniques in this field are negative sampling[194] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[195] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[195] Deep neural architectures provide the best results for constituency parsing,[196] sentiment analysis,[197] information retrieval,[198][199] spoken language understanding,[200] machine translation,[151][201] contextual entity linking,[201] writing style recognition,[202] named-entity recognition (token classification),[203] text classification, and others.[204] Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[205][206][207][208] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\".[206] It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.[206] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[206][209] GT uses English as an intermediate between most language pairs.[209] A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[210][211] Research has explored use of deep learning to predict the biomolecular targets,[212][213] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[214][215][216] AtomNet is a deep learning system for structure-based rational drug design.[217] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[218] and multiple sclerosis.[219][218] In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[220] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[221][222] Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[223] Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[224][225] Multi-view deep learning has been applied for learning user preferences from multiple domains.[226] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[227] In medical informatics, deep learning was used to predict sleep quality based on data from wearables[228] and predictions of health complications from electronic health record data.[229] Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[230][231] Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).[232] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.[232] Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[233][234] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[235][236] Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[237] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[238] These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[239] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration. Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[240] and anti-money laundering.[241] In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[242][243][244] The United States Department of Defense applied deep learning to train robots in new tasks through observation.[245] Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[246] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.[247][248] Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[249] In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems. Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [250] and ultrasound imaging.[251] Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[252][253] An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[254] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[255][256][257][258] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".[259] A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[260][261] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[262][263] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[264] Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[265] and neural populations.[266] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[267] both at the single-unit[268] and at the population[269] levels. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[270] Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[271][272][273] Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[274] As of 2008,[275] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[245] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[245] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".[276] Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. A main criticism concerns the lack of theory surrounding some methods.[277] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[278] In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[279] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[280] website. Furthermore, some researchers have argued that standard loss functions and differentiable architectures in deep learning may limit the discovery of deeper causal or generative mechanisms.[281] Building on Algorithmic information theory (AIT), Hernández-Orozco et al. (2021)[282] proposed an algorithmic loss function to measure the discrepancy between predicted and observed system behavior. Their approach integrates AIT with Machine learning to formulate a framework for learning generative rules in non-differentiable spaces, bridging discrete algorithmic theory with continuous optimization techniques. This framework provides a new perspective on generalization and model interpretability by grounding learning dynamics in algorithmic complexity. [283] [284] Some deep learning architectures display problematic behaviors,[285] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[286] and misclassifying minuscule perturbations of correctly classified images (2013).[287] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[285] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[288] decompositions of observed entities and events.[285] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[289] and artificial intelligence (AI).[290] As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[291] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".[292] In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[293] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[294] Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[293] ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[293] In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".[293] In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[293] The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both.[295] It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[296] The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[296]"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/WebUbuntu",
    "title": "ð¤·ð½ââï¸ Page not found | Page not found | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Go back to the home page Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Algorithms",
    "title": "Algorithm - Wikipedia",
    "content": "In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ ⓘ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation.[1] Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning). In contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results.[2] For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation. As an effective method, an algorithm can be expressed within a finite amount of space and time[3] and in a well-defined formal language[4] for calculating a function.[5] Starting from an initial state and initial input (perhaps empty),[6] the instructions describe a computation that, when executed, proceeds through a finite[7] number of well-defined successive states, eventually producing \"output\"[8] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[9] Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). In the early 12th century, Latin translations of these texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath.[10] Here, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name;[1] the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\".[2] The word algorism in English came to mean the use of place-value notation in calculations; it occurs in the Ancrene Wisse from circa 1225.[11] By the time Geoffrey Chaucer wrote The Canterbury Tales in the late 14th century, he used a variant of the same word in describing augrym stones, stones used for place-value calculation.[12][13] In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.[14] By 1596, this form of the word was used in English, as algorithm, by Thomas Hood.[15] One informal definition is \"a set of rules that precisely defines a sequence of operations\",[16] which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure[17]\nor cook-book recipe.[18] In general, a program is an algorithm only if it stops eventually[19]—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.[20] Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device. Step-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC),[21] Egyptian mathematics (around 1550 BC),[21] Indian mathematics (around 800 BC and later),[22][23] the Ifa Oracle (around 500 BC),[24] Greek mathematics (around 240 BC),[25] Chinese mathematics (around 200 BC and later),[26] and Arabic mathematics (around 800 AD).[27] The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm.[21] During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas.[28] Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[29] Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC.[21] Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,[30][25]: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).[25]: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.[22] The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[27] Bolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the Middle Ages],\" specifically the verge escapement mechanism[31] producing the tick and tock of a mechanical clock. \"The accurate automatic machine\"[32] led immediately to \"mechanical automata\" in the 13th century and \"computational machines\"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century.[33] Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called \"history's first programmer\". Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and \"telephone switching technologies\" led to the development of the first computers.[34] By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape. Telephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".[35][36] In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\"[37] or \"effective method\".[38] Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939. Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms. There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called \"sets of quadruples\", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description.[39] A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine.[39] An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states.[39] In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.[39] The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠\n\n\n\nO\n(\nn\n)\n\n\n{\\displaystyle O(n)}\n\n⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠\n\n\n\nO\n(\n1\n)\n\n\n{\\displaystyle O(1)}\n\n⁠, otherwise ⁠\n\n\n\nO\n(\nn\n)\n\n\n{\\displaystyle O(n)}\n\n⁠ is required. Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠\n\n\n\nO\n(\nlog\n⁡\nn\n)\n\n\n{\\displaystyle O(\\log n)}\n\n⁠) outperforms a sequential search (cost ⁠\n\n\n\nO\n(\nn\n)\n\n\n{\\displaystyle O(n)}\n\n⁠ ) when used for table lookups on sorted lists or arrays. The analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign. Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.[40] To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[41] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[42] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power. The best case of an algorithm refers to the scenario or input for which the algorithm or data structure takes the least time and resources to complete its tasks.[43]  The worst case of an algorithm is the case that causes the algorithm or data structure to consume the maximum period of time and computational resources.[44] Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[45] with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.[46] Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\".[47] Tausworthe augments the three Böhm-Jacopini canonical structures:[48] SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE.[49] An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.[50] By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial,[51] and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography). Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are: For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following: One of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as: High-level description: (Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/HTML",
    "title": "HTML - Wikipedia",
    "content": "Hypertext Markup Language (HTML) is the standard markup language[a] for documents designed to be displayed in a web browser. It defines the content and structure of web content. It is often assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for its appearance. HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects such as interactive forms may be embedded into the rendered page. HTML provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes, and other items. HTML elements are delineated by tags, written using angle brackets. Tags such as <img> and <input> directly introduce content into the page. Other tags such as <p> and </p> surround and provide information about document text and may include sub-element tags. Browsers do not display the HTML tags, but use them to interpret the content of the page. HTML can embed programs written in a scripting language such as JavaScript, which affects the behavior and content of web pages. The inclusion of CSS defines the look and layout of content. The World Wide Web Consortium (W3C), former maintainer of the HTML and current maintainer of the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997.[update][3] A form of HTML, known as HTML5, is used to display video and audio, primarily using the <canvas> element, together with JavaScript. In 1980, physicist Tim Berners-Lee, a contractor at CERN, proposed and prototyped ENQUIRE, a system for CERN researchers to use and share documents. In 1989, Berners-Lee wrote a memo proposing an Internet-based hypertext system.[4] Berners-Lee specified HTML and wrote the browser and server software in late 1990. That year, Berners-Lee and CERN data systems engineer Robert Cailliau collaborated on a joint request for funding, but the project was not formally adopted by CERN. In his personal notes of 1990, Berners-Lee listed \"some of the many areas in which hypertext is used\"; an encyclopedia is the first entry.[5] The first publicly available description of HTML was a document called \"HTML Tags\",[6] first mentioned on the Internet by Tim Berners-Lee in late 1991.[7][8] It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by CERN SGML, an in-house Standard Generalized Markup Language (SGML)-based documentation format at CERN. Eleven of these elements still exist in HTML 4.[9] HTML is a markup language that web browsers use to interpret and compose text, images, and other material into visible or audible web pages. Default characteristics for every item of HTML markup are defined in the browser, and these characteristics can be altered or enhanced by the web page designer's additional use of CSS. Many of the text elements are mentioned in the 1988 ISO technical report TR 9537 Techniques for using SGML, which describes the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from the commands used by typesetters to manually format documents. However, the SGML concept of generalized markup is based on elements (nested annotated ranges with attributes) rather than merely print effects, with separate structure and markup. HTML has been progressively moved in this direction with CSS. Berners-Lee considered HTML to be an application of SGML. It was formally defined as such by the Internet Engineering Task Force (IETF) with the mid-1993 publication of the first proposal for an HTML specification, the \"Hypertext Markup Language (HTML)\" Internet Draft by Berners-Lee and Dan Connolly, which included an SGML Document type definition to define the syntax.[10][11] The draft expired after six months, but was notable for its acknowledgment of the NCSA Mosaic browser's custom tag for embedding in-line images, reflecting the IETF's philosophy of basing standards on successful prototypes. Similarly, Dave Raggett's competing Internet Draft, \"HTML+ (Hypertext Markup Format)\", from late 1993, suggested standardizing already-implemented features like tables and fill-out forms.[12] After the HTML and HTML+ drafts expired in early 1994, the IETF created an HTML Working Group. In 1995, this working group completed \"HTML 2.0\", the first HTML specification intended to be treated as a standard against which future implementations should be based.[13] Further development under the auspices of the IETF was stalled by competing interests. Since 1996,[update] the HTML specifications have been maintained, with input from commercial software vendors, by the World Wide Web Consortium (W3C).[14] In 2000, HTML became an international standard (ISO/IEC 15445:2000). HTML 4.01 was published in late 1999, with further errata published through 2001. In 2004, development began on HTML5 in the Web Hypertext Application Technology Working Group (WHATWG), which became a joint deliverable with the W3C in 2008, and was completed and standardized on 28 October 2014.[15] XHTML is a separate language that began as a reformulation of HTML 4.01 using XML 1.0. It is now referred to as the XML syntax for HTML and is no longer being developed as a separate standard.[59] On 28 May 2019, the W3C announced that WHATWG would be the sole publisher of the HTML and DOM standards.[66][67][68][69] The W3C and WHATWG had been publishing competing standards since 2012. While the W3C standard was identical to the WHATWG in 2007 the standards have since progressively diverged due to different design decisions.[70] The WHATWG \"Living Standard\" had been the de facto web standard for some time.[71] HTML markup consists of several key components, including those called tags (and their attributes), character-based data types, character references and entity references. HTML tags most commonly come in pairs like <h1> and </h1>, although some represent empty elements and so are unpaired, for example <img>. The first tag in such a pair is the start tag, and the second is the end tag (they are also called opening tags and closing tags). Another important component is the HTML document type declaration, which triggers standards mode rendering. The following is an example of the classic \"Hello, World!\" program: The text between <html> and </html> describes the web page, and the text between <body> and </body> is the visible page content. The markup text <title>This is a title</title> defines the browser page title shown on browser tabs and window titles and the tag <div> defines a division of the page used for easy styling. Between <head> and </head>, a <meta> element can be used to define webpage metadata. The Document Type Declaration <!DOCTYPE html> is for HTML5. If a declaration is not included, various browsers will revert to \"quirks mode\" for rendering.[72] HTML documents imply a structure of nested HTML elements. These are indicated in the document by HTML tags, enclosed in angle brackets.[73][better source needed] In the simple, general case, the extent of an element is indicated by a pair of tags: a \"start tag\" <p> and \"end tag\" </p>. The text content of the element, if any, is placed between these tags. Tags may also enclose further tag markup between the start and end, including a mixture of tags and text. This indicates further (nested) elements, as children of the parent element. The start tag may also include the element's attributes within the tag. These indicate other information, such as identifiers for sections within the document, identifiers used to bind style information to the presentation of the document, and for some tags such as the <img> used to embed images, the reference to the image resource in the format like this: <img src=\"example.com/example.jpg\"> Some elements, such as the line break <br> do not permit any embedded content, either text or further tags. These require only a single empty tag (akin to a start tag) and do not use an end tag. Many tags, particularly the closing end tag for the very commonly used paragraph element <p>, are optional. An HTML browser or other agent can infer the closure for the end of an element from the context and the structural rules defined by the HTML standard. These rules are complex and not widely understood by most HTML authors. The general form of an HTML element is therefore: <tag attribute1=\"value1\" attribute2=\"value2\">''content''</tag>. Some HTML elements are defined as empty elements and take the form <tag attribute1=\"value1\" attribute2=\"value2\">. Empty elements may enclose no content, for instance, the <br> tag or the inline <img> tag.\nThe name of an HTML element is the name used in the tags.\nThe end tag's name is preceded by a slash character /. If a tag has no content, an end tag is not allowed. If attributes are not mentioned, default values are used in each case. Header of the HTML document: <head>...</head>. The title is included in the head, for example: HTML headings are defined with the <h1> to <h6> tags with H1 being the highest (or most important) level and H6 the least: The effects are: CSS can substantially change the rendering. Paragraphs: <br>. The difference between <br> and <p> is that <br> breaks a line without altering the semantic structure of the page, whereas <p> sections the page into paragraphs. The element <br> is an empty element in that, although it may have attributes, it can take no content and it must not have an end tag. This is a link in HTML. To create a link the <a> tag is used. The href attribute holds the URL address of the link. There are many possible ways a user can give inputs like: Comments: Comments can help in the understanding of the markup and do not display in the webpage. There are several types of markup elements used in HTML: Most of the attributes of an element are name–value pairs, separated by = and written within the start tag of an element after the element's name. The value may be enclosed in single or double quotes, although values consisting of certain characters can be left unquoted in HTML (but not XHTML).[75][76] Leaving attribute values unquoted is considered unsafe.[77] In contrast with name-value pair attributes, there are some attributes that affect the element simply by their presence in the start tag of the element,[7] like the ismap attribute for the img element.[78] There are several common attributes that may appear in many elements : The abbreviation element, abbr, can be used to demonstrate some of these attributes: This example displays as HTML; in most browsers, pointing the cursor at the abbreviation should display the title text \"Hypertext Markup Language.\" Most elements take the language-related attribute dir to specify text direction, such as with \"rtl\" for right-to-left text in, for example, Arabic, Persian or Hebrew.[79] As of version 4.0, HTML defines a set of 252 character entity references and a set of 1,114,050 numeric character references, both of which allow individual characters to be written via simple markup, rather than literally. A literal character and its markup counterpart are considered equivalent and are rendered identically. The ability to \"escape\" characters in this way allows for the characters < and & (when written as &lt; and &amp;, respectively) to be interpreted as character data, rather than markup. For example, a literal < normally indicates the start of a tag, and & normally indicates the start of a character entity reference or numeric character reference; writing it as &amp; or &#x26; or &#38; allows & to be included in the content of an element or in the value of an attribute. The double-quote character (\"), when not used to quote an attribute value, must also be escaped as &quot; or &#x22; or &#34; when it appears within the attribute value itself. Equivalently, the single-quote character ('), when not used to quote an attribute value, must also be escaped as &#x27; or &#39; (or as &apos; in HTML5 or XHTML documents[80][81]) when it appears within the attribute value itself. If document authors overlook the need to escape such characters, some browsers can be very forgiving and try to use context to guess their intent. The result is still invalid markup, which makes the document less accessible to other browsers and to other user agents that may try to parse the document for search and indexing purposes for example. Escaping also allows for characters that are not easily typed, or that are not available in the document's character encoding, to be represented within the element and attribute content. For example, the acute-accented e (é), a character typically found only on Western European and South American keyboards, can be written in any HTML document as the entity reference &eacute; or as the numeric references &#xE9; or &#233;, using characters that are available on all keyboards and are supported in all character encodings. Unicode character encodings such as UTF-8 are compatible with all modern browsers and allow direct access to almost all the characters of the world's writing systems.[82] HTML defines several data types for element content, such as script data and stylesheet data, and a plethora of types for attribute values, including IDs, names, URIs, numbers, units of length, languages, media descriptors, colors, character encodings, dates and times, and so on. All of these data types are specializations of character data. HTML documents are required to start with a document type declaration (informally, a \"doctype\"). In browsers, the doctype helps to define the rendering mode—particularly whether to use quirks mode. The original purpose of the doctype was to enable the parsing and validation of HTML documents by SGML tools based on the document type definition (DTD). The DTD to which the DOCTYPE refers contains a machine-readable grammar specifying the permitted and prohibited content for a document conforming to such a DTD. Browsers, on the other hand, do not implement HTML as an application of SGML and as consequence do not read the DTD. HTML5 does not define a DTD; therefore, in HTML5 the doctype declaration is simpler and shorter:[83] An example of an HTML 4 doctype This declaration references the DTD for the \"strict\" version of HTML 4.01. SGML-based validators read the DTD in order to properly parse the document and to perform validation. In modern browsers, a valid doctype activates standards mode as opposed to quirks mode. In addition, HTML 4.01 provides Transitional and Frameset DTDs, as explained below. The transitional type is the most inclusive, incorporating current tags as well as older or \"deprecated\" tags, with the Strict DTD excluding deprecated tags. The frameset has all tags necessary to make frames on a page along with the tags included in transitional type.[84] Semantic HTML is a way of writing HTML that emphasizes the meaning of the encoded information over its presentation (look). HTML has included semantic markup from its inception,[85] but has also included presentational markup, such as <font>, <i> and <center> tags. There are also the semantically neutral div and span tags. Since the late 1990s, when Cascading Style Sheets were beginning to work in most browsers, web authors have been encouraged to avoid the use of presentational HTML markup with a view to the separation of content and presentation.[86] In a 2001 discussion of the Semantic Web, Tim Berners-Lee and others gave examples of ways in which intelligent software \"agents\" may one day automatically crawl the web and find, filter, and correlate previously unrelated, published facts for the benefit of human users.[87] Such agents are not commonplace even now, but some of the ideas of Web 2.0, mashups and price comparison websites may be coming close[citation needed]. The main difference between these web application hybrids and Berners-Lee's semantic agents lies in the fact that the current aggregation and hybridization of information is usually designed by web developers, who already know the web locations and the API semantics of the specific data they wish to mash, compare and combine. An important type of web agent that does crawl and read web pages automatically, without prior knowledge of what it might find, is the web crawler or search-engine spider. These software agents are dependent on the semantic clarity of web pages they find as they use various techniques and algorithms to read and index millions of web pages a day and provide web users with search facilities without which the World Wide Web's usefulness would be greatly reduced. In order for search engine spiders to be able to rate the significance of pieces of text they find in HTML documents, and also for those creating mashups and other hybrids as well as for more automated agents as they are developed, the semantic structures that exist in HTML need to be widely and uniformly applied to bring out the meaning of the published text.[88] Presentational markup tags are deprecated in current HTML and XHTML recommendations. The majority of presentational features from previous versions of HTML are no longer allowed as they lead to poorer accessibility, higher cost of site maintenance, and larger document sizes.[89] Good semantic HTML also improves the accessibility of web documents (see also Web Content Accessibility Guidelines). For example, when a screen reader or audio browser can correctly ascertain the structure of a document, it will not waste the visually impaired user's time by reading out repeated or irrelevant information when it has been marked up correctly. HTML documents can be delivered by the same means as any other computer file. However, they are most often delivered either by HTTP from a web server or by email. The World Wide Web is composed primarily of HTML documents transmitted from web servers to web browsers using the Hypertext Transfer Protocol (HTTP). However, HTTP is used to serve images, sound, and other content, in addition to HTML. To allow the web browser to know how to handle each document it receives, other information is transmitted along with the document. This meta data usually includes the MIME type (e.g., text/html or application/xhtml+xml) and the character encoding (see Character encodings in HTML). In modern browsers, the MIME type that is sent with the HTML document may affect how the document is initially interpreted. A document sent with the XHTML MIME type is expected to be well-formed XML; syntax errors may cause the browser to fail to render it. The same document sent with the HTML MIME type might be displayed successfully since some browsers are more lenient with HTML. The W3C recommendations state that XHTML 1.0 documents that follow guidelines set forth in the recommendation's Appendix C may be labeled with either MIME Type.[90] XHTML 1.1 also states that XHTML 1.1 documents should[91] be labeled with either MIME type.[92] Most graphical email clients allow the use of a subset of HTML (often ill-defined) to provide formatting and semantic markup not available with plain text. This may include typographic information like colored headings, emphasized and quoted text, inline images and diagrams. Many such clients include both a GUI editor for composing HTML e-mail messages and a rendering engine for displaying them. Use of HTML in e-mail is criticized by some because of compatibility issues, because it can help disguise phishing attacks, because of accessibility issues for blind or visually impaired people, because it can confuse spam filters and because the message size is larger than plain text. The most common filename extension for files containing HTML is .html. A common abbreviation of this is .htm, which originated because some early operating systems and file systems, such as DOS and the limitations imposed by FAT data structure, limited file extensions to three letters.[93] An HTML Application (HTA; file extension .hta) is a Microsoft Windows application that uses HTML and Dynamic HTML in a browser to provide the application's graphical interface. A regular HTML file is confined to the security model of the web browser's security, communicating only to web servers and manipulating only web page objects and site cookies. An HTA runs as a fully trusted application and therefore has more privileges, like creation/editing/removal of files and Windows Registry entries. Because they operate outside the browser's security model, HTAs cannot be executed via HTTP, but must be downloaded (just like an EXE file) and executed from local file system. Since its inception, HTML and its associated protocols gained acceptance relatively quickly. However, no clear standards existed in the early years of the language. Though its creators originally conceived of HTML as a semantic language devoid of presentation details,[94] practical uses pushed many presentational elements and attributes into the language, driven largely by the various browser vendors. The latest standards surrounding HTML reflect efforts to overcome the sometimes chaotic development of the language[95] and to create a rational foundation for building both meaningful and well-presented documents. To return HTML to its role as a semantic language, the W3C has developed style languages such as CSS and XSL to shoulder the burden of presentation. In conjunction, the HTML specification has slowly reined in the presentational elements. There are two axes differentiating various variations of HTML as currently specified: SGML-based HTML versus XML-based HTML (referred to as XHTML) on one axis, and strict versus transitional (loose) versus frameset on the other axis. One difference in the latest[when?] HTML specifications lies in the distinction between the SGML-based specification and the XML-based specification. The XML-based specification is usually called XHTML to distinguish it clearly from the more traditional definition. However, the root element name continues to be \"html\" even in the XHTML-specified HTML. The W3C intended XHTML 1.0 to be identical to HTML 4.01 except where limitations of XML over the more complex SGML require workarounds. Because XHTML and HTML are closely related, they are sometimes documented in parallel. In such circumstances, some authors conflate the two names as (X)HTML or X(HTML). Like HTML 4.01, XHTML 1.0 has three sub-specifications: strict, transitional, and frameset. Aside from the different opening declarations for a document, the differences between an HTML 4.01 and XHTML 1.0 document—in each of the corresponding DTDs—are largely syntactic. The underlying syntax of HTML allows many shortcuts that XHTML does not, such as elements with optional opening or closing tags, and even empty elements which must not have an end tag. By contrast, XHTML requires all elements to have an opening tag and a closing tag. XHTML, however, also introduces a new shortcut: an XHTML tag may be opened and closed within the same tag, by including a slash before the end of the tag like this: <br/>. The introduction of this shorthand, which is not used in the SGML declaration for HTML 4.01, may confuse earlier software unfamiliar with this new convention. A fix for this is remove the slash preceding the closing angle bracket, as such: <br>.[96] To understand the subtle differences between HTML and XHTML, consider the transformation of a valid and well-formed XHTML 1.0 document that adheres to Appendix C (see below) into a valid HTML 4.01 document. Making this translation requires the following steps: Those are the main changes necessary to translate a document from XHTML 1.0 to HTML 4.01. To translate from HTML to XHTML would also require the addition of any omitted opening or closing tags. Whether coding in HTML or XHTML it may just be best to always include the optional tags within an HTML document rather than remembering which tags can be omitted. A well-formed XHTML document adheres to all the syntax requirements of XML. A valid document adheres to the content specification for XHTML, which describes the document structure. The W3C recommends several conventions to ensure an easy migration between HTML and XHTML (see HTML Compatibility Guidelines). The following steps can be applied to XHTML 1.0 documents only: By carefully following the W3C's compatibility guidelines, a user agent should be able to interpret the document equally as HTML or XHTML. For documents that are XHTML 1.0 and have been made compatible in this way, the W3C permits them to be served either as HTML (with a text/html MIME type), or as XHTML (with an application/xhtml+xml or application/xml MIME type). When delivered as XHTML, browsers should use an XML parser, which adheres strictly to the XML specifications for parsing the document's contents. HTML 4 defined three different versions of the language: Strict, Transitional (once called Loose), and Frameset. The Strict version is intended for new documents and is considered best practice, while the Transitional and Frameset versions were developed to make it easier to transition documents that conformed to older HTML specifications or did not conform to any specification to a version of HTML 4. The Transitional and Frameset versions allow for presentational markup, which is omitted in the Strict version. Instead, cascading style sheets are encouraged to improve the presentation of HTML documents. Because XHTML 1 only defines an XML syntax for the language defined by HTML 4, the same differences apply to XHTML 1 as well. The Transitional version allows the following parts of the vocabulary, which are not included in the Strict version: The Frameset version includes everything in the Transitional version, as well as the frameset element (used instead of body) and the frame element. In addition to the above transitional differences, the frameset specifications (whether XHTML 1.0 or HTML 4.01) specify a different content model, with frameset replacing body, that contains either frame elements, or optionally noframes with a body. As this list demonstrates, the loose versions of the specification are maintained for legacy support. However, contrary to popular misconceptions, the move to XHTML does not imply a removal of this legacy support. Rather the X in XML stands for extensible and the W3C is modularizing the entire specification and opens it up to independent extensions. The primary achievement in the move from XHTML 1.0 to XHTML 1.1 is the modularization of the entire specification. The strict version of HTML is deployed in XHTML 1.1 through a set of modular extensions to the base XHTML 1.1 specification. Likewise, someone looking for the loose (transitional) or frameset specifications will find similar extended XHTML 1.1 support (much of it is contained in the legacy or frame modules). Modularization also allows for separate features to develop on their own timetable. So for example, XHTML 1.1 will allow quicker migration to emerging XML standards such as MathML (a presentational and semantic math language based on XML) and XForms—a new highly advanced web-form technology to replace the existing HTML forms. In summary, the HTML 4 specification primarily reined in all the various HTML implementations into a single clearly written specification based on SGML. XHTML 1.0, ported this specification, as is, to the new XML-defined specification. Next, XHTML 1.1 takes advantage of the extensible nature of XML and modularizes the whole specification. XHTML 2.0 was intended to be the first step in adding new features to the specification in a standards-body-based approach. The HTML Living Standard, which is developed by WHATWG, is the official version, while W3C HTML5 is no longer separate from WHATWG. There are some WYSIWYG editors (what you see is what you get), in which the user lays out everything as it is to appear in the HTML document using a graphical user interface (GUI), often similar to word processors. The editor renders the document rather than showing the code, so authors do not require extensive knowledge of HTML. The WYSIWYG editing model has been criticized,[97][98] primarily because of the low quality of the generated code; there are voices[who?] advocating a change to the WYSIWYM model (what you see is what you mean). WYSIWYG editors remain a controversial topic because of their perceived flaws such as:"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/WebRobotics",
    "title": "ð¤·ð½ââï¸ Page not found | Page not found | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Go back to the home page Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.bbc.com/newsUbuntu",
    "title": "BBC - 404: Not Found",
    "content": "404\n              \n                Not Found Check the page address or search for it below. BBC Homepage Copyright © BBC.\n              \n              The BBC is not responsible for the content of external sites.\n              \n                Read about our approach to external\n                linking."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/WebMachine_learning",
    "title": "ð¤·ð½ââï¸ Page not found | Page not found | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Go back to the home page Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.python.org/Robotics",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now, Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://towardsdatascience.com/React",
    "title": "React & D3: Adding A Bar Chart | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. So the last component to refactor in Streetball Mecca is the bar chart. In my last article, React & D3: Preparing The Data With D3.Nest, I… So the last component to refactor in Streetball Mecca is the bar chart. In my last article, React & D3: Preparing The Data With D3.Nest, I discussed how the data for the chart was organized and formatted using d3.nest. This **** involved grouping the parks based on their neighborhood location and calculating the overall average of all parks associated with that neighborhood. Here is the current working version used in this article and the CodeSandbox as well. As I started to contemplate how best to refactor the D3 bar chart code into the React ecosystem, I leveraged the lessons learned from all the previous refactoring. One of the most important changes was to allow React to manage state, render/update DOM elements, and then allow D3 to do what it does best, such as creating scales and nesting data. For this refactor I decided to take it one step further and opted to take a non-svg approach to rendering the bar and circle elements. This decision was born initially from many previous frustrations having to manually position g elements. It was also influenced by the fact that I just taught a class on Flexbox and as part of that prep I came across a bookmark I saved over a year ago to the article \"Making Data Vis Without SVG Using D3 & Flexbox\" by Amber Thomas. I felt like the stars had aligned and all the signs pointed to Flexbox. Initially, I had ported over the D3 code and refactored it just slightly to work within a single React BarChart component. This D3 within React approach was meant as a first step to understanding how to coerce D3 to work within React. I also kept much of the D3 code intact in order to get something up and running. It however ended up being close to 200 lines of code and started to feel unmanageable. Now that I was ready to move forward with the second refactor and take a \"D3 for the math and React for everything else\" approach, the first thing I needed to plan out was the component hierarchy. I used an app called excalidraw to draw out the initial mockup and help work through the though and design process. The BarChart is the first component in this hierarchy so it makes sense to start there. Since both the XAxis and Neighborhoods components would require the same D3 scale, I decided to create it here and then pass it down to both as a prop. D3 has several methods that can be used for creating scales. Scales take an input (usually a number, date, or category) and return a value (such as a coordinate, color, length, or radius). They come in several categories that range from continuous input/output (d3.scaleLinear()) to discrete input/output (d3.scaleOridinal()) and somewhere in-between (d3.scaleQuantize()). Scales themselves could be an entire lecture in their own right so if you’re interested in learning more I highly suggest you read d3indepth.com. The scale I needed for this chart would be a dynamic, continuous input/output scale so I went with d3.scaleLinear(). All ** D3 scales require that you define a .domain() and .range(). The domain of values used for this scale is based on the overall rating of the parks. They could potentially be in total disrepair and assigned 0 (lowest value possible) or the creme de la creme of parks with a rating of 100 (highest value possible). Here, for example, is the 100% Playground park located in Canarsi**e Brooklyn which falls somewhere in between with an overall rating of 56. The average of those parks was also needed as this would be assigned to the parent neighborhood and I had used D3.nest() to rollup those values as seen in the example below. If you’re curious about working with d3.nest (which I hope you are), you can read more about how I nested the data in my previous article: Preparing The Data With D3.Nest. The .range() is based on the pixel space provided to the .bar-group element which is assigned a width of 1100px. I did need to subtract 25px in order to provide a bit of margin between both, the last tick value(100) and the circle and the right border of the .bar-group element. As you continue to examine more and more D3 code in general, you will see a common pattern of defining a set of margins that are used to define this additional space which is then referenced by the scale but quite possibly throughout the code in key places. Almost any D3 tutorial on scales will provide a similar visual in order to convey the relationship between the domain and range so I thought I’d do the same as well. A Little Bit About Axes If you have ever worked with D3 in the past, using it to create any one of the more standard chart types such as bar, line, or scatterplot, then it’s safe to say you have some familiarity with adding an axis. If, however, you are new to D3, then know that axes require the following steps: Creating a scale (we did so using d3.scaleLinear()) Setting its orientation: the orientation of an axis is fixed, however, it can be changed using one of the ** four orientation methods – .axisBottom(), .axisTop(), .axisLeft(), and .axisRight()**. Appending the Axis: once the axis has been created it is then appended using .call(xAxis). The axis also has additional configurations for setting tick values, sizes, and format. Being that I’ll be using React for the DOM and D3 for where it’s best suited, I decided that for this refactor I would only make use of d3.scaleLinear() and **** render the ticks using React. The only prop being passed to the XAxis component was the xScale so I’ll just object de-structure that value to make it clear that this is all that the component needs as it pertains to props. For much of the XAxis refactor I had used as a reference a beautifully written tutorial on React & D3 by Amelia Wattenberger, She wrote the article in order to promote her recent book \"Full Stack D3 and Data Visualization\". I was, at one point, tempted to buy it and would have done so had I not already owned the following books on D3: In Amelia’s article she looped over the array of individual ticks (0, 10, 20, etc.) using .ticks() and assigned each of them an xOffset value based on the returned value from the xScale. It’s been a while since I’ve worked on such a low level with scales and forgot that they included a .ticks() method that **** returns an array of the ticks. For the axis I decided to stay in line with Amelia’s article rendering an svg that uses a nested g element to position the axis to the right 200px. It then maps over the ticks array and positions each one of those g elements which, itself, renders the tick value. Side Note: I was able to render the ticks as divs using Flexbox, but that solution required creating an additional xScale and removing not only an additional 200px but also another 8px for half the font size. Since I felt only one scale was need, I opted to put that solution on the shelf for now. This component is responsible for mapping over the nestedData array and rendering individual Neighborhood components. It passes down several props to the child component, including the dispatch function which will be passed as an action and which the useReducer uses to update state. If you’re curious about useReducer, take a look at this previous article I wrote: Managing Complex State Transitions With useReducer. This component is responsible for rendering the following elements: It also makes use of the following hooks: The next step is to map over the parks array and create a Circle component. One thing to note is that the element determines how many pixels it should position itself left using the xScale but it also needs to subtract 5px. These additional pixels were needed in order to move the circle left, to center it directly under the axis value. Let’s break down the return statement in the map of the Neighborhood component as there are a few things happening here. First, the neighborhood element will reference the activeNeighborhood value in state to determine if it needs to highlight its background color in order to indicate it is the active neighborhood as seen below. I opted to allow the user to click anywhere in on the element in order to trigger the onClick event which calls the dispatch function and passes it an object containing a type and payload as required by useReducer. You can read more about working with useReducer and actions in this article: React: Managing Complex State Transitions With useReducer. Next, we have to render the neighborhood title and assign it a specific color based on which borough it belongs to. And finally, the Bars and Circles needed to be rendered. The Bar component requires the width value ** which was calculated using the xScale** along with the entire neighborhood object as key data points are needed by the tooltip. Flexbox was enabled on the This component would require the logic needed to render a tooltip when the user hovered over the colored bar. That logic involves adding both onMouseMove and onMouseOut event listeners and updating a local version of state in order to trigger the re-render. The first step to take was create the following state values: Now we add the handler functions for the event listeners. For this I had to do a bit of research as the standard event object’s e.pageY, e.clientY, and e.screenY were returning values that were much greater than the actual position of the mouse. I ended up using e.nativeEvent.offsetX and e.nativeEvent.offsetY and pulled some of that insight right from the React docs themselves: If you find that you need the underlying browser event for some reason, simply use the nativeEvent attribute to get it. The synthetic events are different from, and do not map directly to, the browser’s native events. For example in onMouseLeave event.nativeEvent will point to a mouseout event. All that was left was to return the DOM elements configured, assign the event listeners and add some conditional logic to render the tooltip only when isActive was true. This component required a much of the same logic for the tooltip as was needed for the Bar component along with one additional onClick event listener that filtered the map for the selected park. The handleClick function required the use of e.stopPropagation() in order ** to prevent the event from bubbling up to the Neighborhood component where an additional onClick event was configured. I found that without e.stopPropagation(**), all parks, from that neighborhood, would be rendered within the map instead of just that single park. I had looked forward to using fancy Flexbox properties for this particular layout but it turned out it was only needed enable it on the neighborhood and title elements. Here are is the actual html layout of the .neighborhood element and its children. Those elements, along with their corresponding CSS rendered as follows: As for Flexbox I first enabled it on the .neighborhood element which positioned the title and bar-group horizontally to one another. The .title element was also assigned a display of flex and used align-items to to vertically center the title text This is the 7th article of my Streetball Mecca series, where I document refactoring the project from its humble beginnings as a D3 visualization over to a React build to manage state and the DOM and use D3 for its unique helper functions. Thus far, I’ve replaced all instances of D3’s Enter/Update/Exit with either React’s useReducer or useState hooks. useReducer is being used to manage the global state and overall business logic of the app, and useState is used for more localized state as in the tooltips. Although React took the reins here, D3 was used for what it does best; creating scales (d3.scaleLinear), rendering geo-projections (d3.geoMercator/d3.geoPath), and nesting data (d3.nest). This project has certainly helped validate my opinion that React is by far the better choice for managing state and the overall flow of the application logic. Besides all the native React hooks used, such as useReducer, useState, useRef, and useCallback, I also leveraged a few custom hooks such as useDataApi, useOnClickOutside, and useLocalStorage. My favorite part of the refactor has been working with the React-Spring animation library **** in order to replace all the D3 transitions. A close runner up was building a bar chart using DIV’s and Flexbox instead of SVGs and G elements, something I’ve wanted to try for some time. I certainly have a new appreciation for both libraries and look forward to building more interactive dashboards. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. This article explores the relationship between a movie’s dialogue and its genre, leveraging domain-driven data… How to build a modern, scalable data platform to power your analytics and data science… How to make graphs using Matplotlib, Pandas and Seaborn Create Visualizations at the Level of Leading Newspapers To what extent are countries adopting data policies and systems for the public good? From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://www.python.org/Cloud_computing",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now, Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/WebLinux",
    "title": "ð¤·ð½ââï¸ Page not found | Page not found | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Go back to the home page Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Data_science",
    "title": "Data science - Wikipedia",
    "content": "Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge from potentially noisy, structured, or unstructured data.[2] Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4] Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8] A data scientist is a professional who creates programming code and combines it with statistical knowledge to summarize data.[9] Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge from that data to solve problems in other application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, and summarizing these findings. As such, it incorporates skills from computer science, mathematics, data visualization, graphic design, communication, and business.[11] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[12] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[13] Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[14] In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[14] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C. F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[15] Later, attendees at a 1992 statistics symposium at the University of Montpellier  II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[16][17] The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C. F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[18] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[17] In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[19] a catchphrase that was picked up even by major-city newspapers like the New York Times[20] and the Boston Globe.[21] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[22] The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[23] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[24] The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[25] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[26] Data analysis typically involves working with structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning and data visualization to summarize data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data.[27] Data science involves working with larger datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models. Data science often uses statistical analysis, data preprocessing, and supervised learning.[28][29] Cloud computing can offer access to large amounts of computational power and storage.[30] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[31] Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reduce processing times.[32] Data science involves collecting, processing, and analyzing data which often includes personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts.[33][34] Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[35][36]"
  },
  {
    "url": "https://www.python.org/",
    "title": "Welcome to Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The core of extensible programming is defining functions. Python allows mandatory and optional arguments, keyword arguments, and even arbitrary argument lists. More about defining functions in Python 3 Lists (known as arrays in other languages) are one of the compound data types that Python understands. Lists can be indexed, sliced and manipulated with other built-in functions. More about lists in Python 3 Calculations are simple with Python, and expression syntax is straightforward: the operators +, -, * and / work as expected; parentheses () can be used for grouping. More about simple math functions in Python 3. Python knows the usual control flow statements that other languages speak — if, for, while and range — with some of its own twists, of course. More control flow tools in Python 3 Experienced programmers in any other language can pick up Python very quickly, and beginners find the clean syntax and indentation structure easy to learn. Whet your appetite with our Python 3 overview. Python is a programming language that lets you work quickly and integrate systems more effectively. Learn More Whether you're new to programming or an experienced developer, it's easy to learn and use Python. Start with our Beginner’s Guide Python source code and installers are available for download for all versions! Latest: Python 3.13.5 Documentation for Python's standard library, along with tutorials and guides, are available online. docs.python.org Looking for work or have a Python related position that you're trying to hire for? Our relaunched community-run job board is the place to go. jobs.python.org More More More Using Python scripts to analyse SEO and broken links on your site by Marnix de Munck More The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. Learn more Become a Member\nDonate to the PSF Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/psf/",
    "title": "Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. In 2024, the PSF awarded $655,000 USD to 257 groups or individuals in 61 countries around the world. We support and maintain python.org,\r\n          The Python Package Index,\r\n          Python Documentation,\r\n          and many other services the Python Community relies on. We produce and underwrite the\r\n          PyCon US Conference,\r\n          the largest annual gathering for the Python community.\r\n         Support from sponsors, attendees, PyLadies, and CPython enabled us to award more than $384,000 USD in travel grants to 254 attendees for PyCon US 2025. Help the PSF promote, protect, and advance the Python programming language and community! Membership FAQ Assist the foundation's goals with a donation. The PSF is a recognized 501(c)(3) non-profit organization. How to Contribute Learn how you can help the PSF and the greater Python community! How to Volunteer Without our sponsors we wouldn't be able to help the Python community grow and prosper. Sponsorship Possibilities The Python Software Foundation welcomes grant proposals for projects related to the development of Python, Python-related technology, and educational resources. Proposal Guidelines, FAQ and Examples Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://docs.python.org",
    "title": "3.13.5 Documentation",
    "content": "Download these documents Welcome! This is the official documentation for Python 3.13.5. Documentation sections: What's new in Python 3.13?\n Or all \"What's new\" documents since Python 2.0 Tutorial\nStart here: a tour of Python's syntax and features Library reference\nStandard library and builtins Language reference\nSyntax and language elements Python setup and usage\nHow to install, configure, and use Python Python HOWTOs\nIn-depth topic manuals Installing Python modules\nThird-party modules and PyPI.org Distributing Python modules\nPublishing modules for use by other people Extending and embedding\nFor C/C++ programmers Python's C API\nC API reference FAQs\nFrequently asked questions (with answers!) Deprecations\nDeprecated functionality Indices, glossary, and search: Global module index\nAll modules and libraries General index\nAll functions, classes, and terms Glossary\nTerms explained Search page\nSearch this documentation Complete table of contents\nLists all sections and subsections Project information: Reporting issues Contributing to docs Download the documentation History and license of Python Copyright About the documentation Download these documents"
  },
  {
    "url": "https://pypi.org/",
    "title": "PyPI · The Python Package Index",
    "content": "Or browse projects 664,098 projects 7,245,673 releases 15,051,685 files 952,633 users The Python Package Index (PyPI) is a repository of software for the Python programming language. PyPI helps you find and install software developed and shared by the Python community.        Learn about installing packages. Package authors use PyPI to distribute their software.        Learn how to package your Python code for PyPI. Status:\nall systems operational Developed and maintained by the Python community, for the Python community.    \nDonate today! \"PyPI\", \"Python Package Index\", and the blocks logos are registered trademarks of the Python Software Foundation. © 2025 Python Software Foundation\n\nSite map Supported by"
  },
  {
    "url": "https://www.python.org/jobs/",
    "title": "Python Job Board | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Have a job that our community would be interested in? \n    Please check our job submission how-to for details on how\n    to file a job posting. After you have reviewed our how-to document,\n    please login\n    and use this form to create a new job posting If you have submitted jobs previously under your login, you can view them by\n    logging in now. In case of questions, please contact the \n    PSF Python Job Board team. Thank you. Subscribe via RSS Follow The PSF via Twitter Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/",
    "title": "Our Community | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Great software is supported by great people, and Python is no exception. Our user base is enthusiastic and dedicated to spreading use of the language far and wide. Our community can help support the beginner, the expert, and adds to the ever-increasing open-source knowledgebase. New to the community? Here are some great places to get started: We want to be open about how we can improve transparency, provide the community with opportunities to interact with us, and be responsive to raised suggestions. Contribute by filling out the Python Software Foundation Community Survey here. Python Weekly is a free weekly email newsletter featuring curated news, articles, new releases, jobs, and more. Curated by Rahul Chaudhary every Thursday. Go to pythonweekly.com to sign up. PySlackers is a community of Python enthusiasts centered around an open Slack team. Go to pyslackers.com for more information and to join. Python Discord is a large community focused around the Python programming language. Go to pythondiscord.com for more information and to join. This is the place where Python Engineers level up their knowledge, skills and network. Exchange technical publications, coding tutorials and other learning resources. Go to the Python Developers Community on LinkedIn. Libera.Chat hosts several channels. Select an IRC client, register your nickname with Libera.Chat, and you can be off and running! Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://psfmember.org/civicrm/contribute/transact?reset=1&id=2",
    "title": "Donation for the PSF – Python Software Foundation",
    "content": "Python Software Foundation The Python Software Foundation is a 501(c)(3) non-profit organization dedicated to the Python programming language. Your donation helps sustain a vibrant and diverse Python community and shows your support for Open Source. We rely on memberships, individual donations, and corporate sponsorships to award grants for regional Python conferences, sprints, local Meetups, and other community events, as well as to support Python documentation, fiscal sponsorships, and community projects such as: Python Developer's Guide Issue Tracker PyPI - the Python Package Index Contact information is required for tax reporting purposes and will be shared only with the US government. Thank you for supporting the Python Software Foundation!\nPayments are processed by PayPal, but may be made either with your credit card or your existing PayPal account.  If you wish to use a credit card, once on the PayPal site click the link \"Don't have a PayPal account?\", which will take you to a credit card form.  A PayPal account is not required.\nFor your security, this website and the PSF do not acquire or retain any of your credit card data.\nFollowing the transaction, you will receive both a PayPal receipt and/or our formal acknowledgment of your contribution.\nPlease whitelist psfmember.org and check your spam bins if you don't receive the acknowledgment from the PSF. Payments are processed by PayPal, but may be made either with your credit card or your existing PayPal account.  If you wish to use a credit card, once on the PayPal site click the link \"Don't have a PayPal account?\", which will take you to a credit card form.  A PayPal account is not required. For your security, this website and the PSF do not acquire or retain any of your credit card data. Following the transaction, you will receive both a PayPal receipt and/or our formal acknowledgment of your contribution. Please whitelist psfmember.org and check your spam bins if you don't receive the acknowledgment from the PSF."
  },
  {
    "url": "https://www.python.org/community/irc/",
    "title": "Internet Relay Chat | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. There are several Python-related channels on the libera IRC network.\nAll channels are available by connecting to Internet Relay Chat server\nLibera.Chat. The #python channel is for all discussion about the Python language,\necosystem, and community. You can get immediate help with programming\nquestions. You will need to first register your nickname with Libera,\nusing the nickname setup instructions (https://libera.chat/guides/registration). Spanish speakers can use the #pyar channel, from the Python Argentina user group. French speakers can join the #python-fr channel. Finnish speakers can join the #python.fi channel on a different network, IRCnet. (Note: prior to May 2021, these channels existed on Freenode. Some of them\nwere forcibly removed by Freenode operators, after a change in management and network policy. The channels on Freenode are no longer under the PSF umbrella.) #python-dev is for CPython developers, where they can\ncoordinate their work or discuss problems.  Bots post updates to the channel based on\nactivity in the CPython source tree and bug tracker. #python-infra is for Python infrastructure discussion. #pydotorg is for discussion of this website, python.org. #distutils and #pypa are for Python packaging discussion. IRC clients for many platforms can be found in the Internet Relay Chat (IRC)\nHelp Archive. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/",
    "title": "About Python™ | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. These are some of the reasons people who use Python would rather not use anything else. Python can be easy to pick up whether you're a first time programmer or you're experienced with other languages. The following pages are a useful first step to get on your way writing programs with Python! The community hosts conferences and meetups, collaborates on code, and much more. Python's documentation will help you along the way, and the mailing lists will keep you in touch. The Python Package Index (PyPI) hosts thousands of third-party modules for Python. Both Python's standard library and the community-contributed modules allow for endless possibilities. Python is developed under an OSI-approved open source license, making it freely usable and distributable, even for commercial use. Python's license is administered by the Python Software Foundation. Can’t find what you’re looking for? Try our comprehensive Help section More More The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers. Learn more Become a Member\nDonate to the PSF Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/apps/",
    "title": "Applications for Python | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Python is used in many application domains.  Here's\na sampling. Python offers many choices for web development: Python's standard library supports many Internet protocols: And the Package Index has yet more libraries: Python is widely used in scientific and numeric computing: Python is a superb language for teaching programming, both at the introductory\nlevel and in more advanced courses. The Tk GUI library\nis included with most binary distributions of Python. Some toolkits that are usable on several platforms are available\nseparately: Platform-specific toolkits are also available: Python is often used as a support language for software developers,\nfor build control and management, testing, and in many other ways. Python is also used to build ERP and e-commerce systems: The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/quotes/",
    "title": "Quotes about Python | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Python is used successfully in thousands of real-world business\napplications around the world, including many large and mission\ncritical systems.  Here are some quotes from happy Python users: \"Python is fast enough for our site and allows us to produce\nmaintainable features in record times, with a minimum of developers,\"\nsaid Cuong Do, Software Architect, YouTube.com. \"Python plays a key role in our production pipeline.  Without it a\nproject the size of Star Wars: Episode II would have been very\ndifficult to pull off. From crowd rendering to batch processing to\ncompositing, Python binds all things together,\" said Tommy Burnette,\nSenior Technical Director, Industrial Light & Magic. \"Python is everywhere at ILM. It's used to extend the capabilities\nof our applications, as well as providing the glue between them. Every\nCG image we create has involved Python somewhere in the process,\" said\nPhilip Peterson, Principal Engineer, Research & Development, Industrial Light & Magic. \"Python has been an important part of Google since the beginning,\nand remains so as the system grows and evolves. Today dozens of Google\nengineers use Python, and we're looking for more people with skills in\nthis language.\" said Peter Norvig, director of search quality at Google, Inc. \"Journyx technology, from the source code of our software to the code that\nmaintains our Web site and ASP sites, is entirely based on Python. It\nincreases our speed of development and keeps us several steps ahead of\ncompetitors while remaining easy to read and use.  It's as high level of a\nlanguage as you can have without running into functionality problems.  I\nestimate that Python makes our coders 10 times more productive than Java\nprogrammers, and 100 times more than C programmers.\" -- Curt Finch, CEO,\nJournyx \"IronPort email gateway appliances are used by the largest\ncorporations and ISPs in the world,\" said Mark Peek, Sr. Director of\nEngineering at IronPort Systems.  \"Python\nis a critical ingredient in this high performance system. IronPort's\nsuite of products contains over a million lines of Python. The PSF is\nan invaluable resource that helps keep Python on the cutting edge.\" \"Python enabled us to create EVE Online,\na massive multiplayer game, in record\ntime. The EVE Online server cluster runs over 50,000 simultaneous players\nin a shared space simulation, most of which is created in Python. The\nflexibilities of Python have enabled us to quickly improve the game\nexperience based on player feedback\" said\nHilmar Veigar Petursson of CCP Games. \"HomeGain maintains its commitment to continual improvement through\nrapid turnaround of new features and enhancements.  Python supports\nthis short time-to-market philosophy with concise, clear syntax and a\npowerful standard library.  New development proceeds rapidly, and\nmaintenance of existing code is straightforward and fast,\" said Geoff\nGerrietts, Software Engineer, HomeGain.com. \"Python makes us extremely productive, and makes\nmaintaining a large and rapidly evolving codebase relatively\nsimple,\" said Mark Shuttleworth. \"I have the students learn Python in our undergraduate and graduate\nSemantic Web courses.  Why?  Because basically there's nothing else\nwith the flexibility and as many web libraries,\" said Prof. James\nA. Hendler. \"The travel industry is made up of a myriad supplier data feeds all of\nwhich are proprietary in some way and are constantly changing.   Python\nrepeatedly has allowed us to access, build and test our in-house\ncommunications with hundreds of travel suppliers around the world in a\nmatter of days rather then the months it would have taken using other\nlanguages.  Since adopting Python 2 years ago, Python has provided us\nwith a measurable productivity gain that allows us to stay competitive\nin the online travel space,\" said Michael Engelhart, CTO of\nEZTrip.com. \"Python in conjunction with PHP has repeatedly allowed us to develop\nfast and proficient applications that permit Real Estate Agent .com to operate with minimal\nresources. Python is a critical part of our dynamically growing\ncluster directory of real estate agents.\" said Gadi Hus, Webmaster,\nVolico Web Consulting \"Like XML, scripting was extremely useful as both a mod tool and an\ninternal development tool.  If you don't have any need to expose code\nand algorithms in a simple and safe way to others, you can argue that\nproviding a scripting language is not worth the effort.  However, if\nyou do have that need, as we did, scripting is a no brainer, and it\nmakes complete sense to use a powerful, documented, cross-platform\nstandard such as Python.\"  -- Mustafa Thamer of Firaxis Games, talking\nabout Civilization IV.  Quoted on page 18 of the August 2005\nGame Developer Magazine. \"Python, like many good technologies, soon spreads virally throughout\nyour development team and finds its way into all sorts of applications\nand tools.  In other words, Python begins to feel like a big hammer and\ncoding tasks look like nails.\"\n-- Mustafa Thamer of Firaxis Games, talking about Civilization IV.\nQuoted on page 18 of the August 2005 Game Developer Magazine. \"We chose to use python because we wanted a well-supported scripting\nlanguage that could extend our core code. Indeed, we wrote much more\ncode in python than we were expecting, including all in-game screens\nand the main interface. It was a huge win for the project because\nwriting code in a language with garbage collection simply goes faster\nthan writing code in C++. The fact that users will be able to easily\nmod the interface is a nice plus as well. The downside of python was\nthat it significantly increased our build times, mostly from linking\nwith Boost.\" -- Soren Johnson, lead designer, Civilization IV.  Quoted\nin a Slashdot interview. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/gettingstarted/",
    "title": "Python For Beginners | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Welcome! Are you completely new to programming?\nIf not then we presume you will be looking for information about\nwhy and how to get started with Python.\nFortunately an\nexperienced programmer in any programming language (whatever it may be)\ncan pick up Python very quickly.\nIt's also easy for beginners to use and learn, so\njump in! Installing Python is generally easy, and nowadays\nmany Linux and UNIX distributions include a recent Python.\nEven\nsome Windows computers (notably those from HP) now come with Python\nalready installed.\nIf you do need to install Python and aren't confident about the\ntask you can find\na few notes on the\nBeginnersGuide/Download\nwiki page, but installation is unremarkable on most platforms. Before getting started, you may want to find out which IDEs and text\neditors are tailored to make\nPython editing easy, browse the list of introductory books, or look at code samples that you might find\nhelpful. There is a list of tutorials suitable for experienced programmers on the\nBeginnersGuide/Tutorials\npage. There is also a list of\nresources in other languages\nwhich might be useful if English is not your first language. The online documentation\nis your first port of call for definitive information.\nThere is a fairly brief\ntutorial\nthat gives you basic information about the language and\ngets you started. You can follow this by looking at the\nlibrary reference\nfor a full description of Python's many libraries and the\nlanguage reference for\na complete (though somewhat dry) explanation of Python's syntax.\nIf you are looking for common Python recipes and patterns, you\ncan browse the ActiveState Python Cookbook If you want to know whether a particular application, or a library\nwith particular functionality, is available in Python there are a\nnumber of possible sources of information. The Python web site\nprovides a\nPython Package Index\n(also known as the Cheese Shop, a reference to the Monty Python\nscript of that name).\nThere is also a\nsearch page for a number of sources of Python-related\ninformation. Failing that, just\nGoogle for a phrase including the word ''python''\nand you may well get the result you need.\nIf all else fails, ask on the\npython newsgroup\nand there's a good chance someone will put you on the right track. If you have a question, it's a good idea to try the\nFAQ, which answers the most commonly\nasked questions about Python. If you want to help to develop Python, take a look at the\ndeveloper area for further information.\nPlease note that you don't have to be an expert programmer\nto help.  The documentation is just as important as the\ncompiler, and still needs plenty of work! The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/help/",
    "title": "Help | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "http://brochure.getpython.info/",
    "title": "Get the Python Brochure Vol.1 as download! —",
    "content": "Skip to content. |\n\n  Skip to navigation Personal tools Navigation The first run of the printed edition was successfully released in 2014 at the PyCon in Montreal. After that we distributed this excellent showcase of Python to major computing conferences around the world. We were proud to offer to ship the second print run until end of 2016. We have send all our copies to your local events and shows around Python over the whole world. Since then it is still a growing technology that powers the web and science & engineering in both education and production. You can download the Python brochure as the latest updated screen resolution PDF suitable for electronic distribution and is well suited for low resolution printing on A4 as a A5 folded brochure booklet. The original printed issue was 32 pages, A4, full-color. Quick Read of License and redistribution rules of this PDF: Get the PDF file The Python Software Foundation (PSF) has collected success stories and case studies over the last two years and we've created a beautiful, professional quality printed brochure to promote the usage of Python to audiences which we are currently not reaching well. We cover business, science, industry, education, media, government, public sector and charity stories. Our target groups are CIOs and chief developers, scientists and programmers, university lecturers, teachers and students, customers, clients, managers and employees. Until a new edition is planned, you can still get the PDF file as download. RSS news feed Twitter @pythonbrochure or subscribe to our email newsletter"
  },
  {
    "url": "https://www.python.org/downloads/",
    "title": "Download Python | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Download Python 3.13.5 Download Python 3.13.5 Download Python 3.13.5 Download Python 3.13.5 Looking for Python with a different OS? Python for\r\n  Windows,\r\n  Linux/Unix,\r\n  macOS,\r\n  other Want to help test development versions of Python 3.14?\r\n  Pre-releases,\r\n  Docker images For more information visit the Python Developer's Guide. Python releases by version number: View older releases Visionary sponsors help to host Python downloads. All Python releases are Open Source. Historically, most, but not all, Python releases have also been GPL-compatible. The Licenses page details GPL-compatibility and Terms and Conditions. Read more For most Unix systems, you must download and compile the source code. The same source code archive can also be used to build the Windows and Mac versions, and is the starting point for ports to all other platforms. Download the latest Python 3 source. Read more This site hosts the \"traditional\" implementation of Python (nicknamed CPython). A number of alternative implementations are available as well. Read more Python was created in the early 1990s by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands as a successor of a language called ABC. Guido remains Python’s principal author, although it includes many contributions from others. Read more See Status of Python Versions for all an overview of all versions, including unsupported. Starting with the Python 3.11.0, Python 3.10.7, and Python 3.9.14 releases, CPython release artifacts are signed with Sigstore. See our dedicated Sigstore Information page for how it works. Python versions before 3.14 are also signed using OpenPGP private keys of the respective release manager. In this case, verification through the release manager's public key is also possible.\r\nSee our dedicated OpenPGP Verification page for how it works. See PEP 761 for why OpenPGP key verification was dropped in Python 3.14. (Updated for Azure Trusted Signing, which applies for all releases chronologically from 3.14.0a1)\r\n\r\nThe Windows installers and all binaries produced as part of each Python release are signed using an Authenticode signing certificate issued to the Python Software Foundation. This can be verified by viewing the properties of any executable file, looking at the Digital Signatures tab, and confirming the name of the signer. Our full certificate subject is CN = Python Software Foundation, O = Python Software Foundation, L = Beaverton, S = Oregon, C = US and as of 14th October 2024 the certificate authority is Microsoft Identity Verification Root Certificate Authority. Our previous certificates were issued by DigiCert.\r\n\r\nNote that some executables may not be signed, notably, the default pip command. These are not built as part of Python, but are included from third-party libraries. Files that are intended to be modified before use cannot be signed and so will not have a signature.\r\n\r\nmacOS Installer Packages\n\r\nInstaller packages for Python on macOS downloadable from python.org are signed with\r\nwith an Apple Developer ID Installer certificate.\r\n\n\r\nAs of Python 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the Python Software Foundation (Apple Developer ID BMM5U3QVKW)).\r\n\n\r\nInstaller packages for previous releases were signed with certificates issued to Ned Deily (DJ3H93M7VJ).\r\n\nOther Useful Items\n\nLooking for third-party Python modules?  The\r\nPython Package Index has many of them.\nYou can view the standard documentation\r\nonline, or you can download it\r\nin HTML, PostScript, PDF and other formats.  See the main\r\nDocumentation page.\nInformation on tools for unpacking archive files\r\nprovided on python.org is available.\nTip: even if you download a ready-made binary for your\r\nplatform, it makes sense to also download the source.\r\nThis lets you browse the standard library (the subdirectory Lib)\r\nand the standard collections of tools\r\n(Tools) that come with it.  There's a lot you can learn from the\r\nsource!\n\nWant to contribute?\nWant to contribute?  See the Python Developer's Guide\r\nto learn about how Python development is managed. Installer packages for Python on macOS downloadable from python.org are signed with\r\nwith an Apple Developer ID Installer certificate. As of Python 3.11.4 and 3.12.0b1 (2023-05-23), release installer packages are signed with certificates issued to the Python Software Foundation (Apple Developer ID BMM5U3QVKW)). Installer packages for previous releases were signed with certificates issued to Ned Deily (DJ3H93M7VJ). Want to contribute?  See the Python Developer's Guide\r\nto learn about how Python development is managed. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/downloads/source/",
    "title": "Python Source Releases | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/downloads/windows/",
    "title": "Python Releases for Windows | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Note that Python 3.13.5 cannot be used on Windows 7 or earlier. Note that Python 3.13.4 cannot be used on Windows 7 or earlier. Note that Python 3.13.3 cannot be used on Windows 7 or earlier. Note that Python 3.12.10 cannot be used on Windows 7 or earlier. Note that Python 3.13.2 cannot be used on Windows 7 or earlier. Note that Python 3.12.9 cannot be used on Windows 7 or earlier. Note that Python 3.12.8 cannot be used on Windows 7 or earlier. Note that Python 3.13.1 cannot be used on Windows 7 or earlier. Note that Python 3.13.0 cannot be used on Windows 7 or earlier. Note that Python 3.12.7 cannot be used on Windows 7 or earlier. Note that Python 3.12.6 cannot be used on Windows 7 or earlier. Note that Python 3.12.5 cannot be used on Windows 7 or earlier. Note that Python 3.12.4 cannot be used on Windows 7 or earlier. Note that Python 3.12.3 cannot be used on Windows 7 or earlier. Note that Python 3.11.9 cannot be used on Windows 7 or earlier. Note that Python 3.11.8 cannot be used on Windows 7 or earlier. Note that Python 3.12.2 cannot be used on Windows 7 or earlier. Note that Python 3.12.1 cannot be used on Windows 7 or earlier. Note that Python 3.11.7 cannot be used on Windows 7 or earlier. Note that Python 3.12.0 cannot be used on Windows 7 or earlier. Note that Python 3.11.6 cannot be used on Windows 7 or earlier. Note that Python 3.11.5 cannot be used on Windows 7 or earlier. Note that Python 3.11.4 cannot be used on Windows 7 or earlier. Note that Python 3.10.11 cannot be used on Windows 7 or earlier. Note that Python 3.11.3 cannot be used on Windows 7 or earlier. Note that Python 3.10.10 cannot be used on Windows 7 or earlier. Note that Python 3.11.2 cannot be used on Windows 7 or earlier. Note that Python 3.11.1 cannot be used on Windows 7 or earlier. Note that Python 3.10.9 cannot be used on Windows 7 or earlier. Note that Python 3.11.0 cannot be used on Windows 7 or earlier. Note that Python 3.10.8 cannot be used on Windows 7 or earlier. Note that Python 3.10.7 cannot be used on Windows 7 or earlier. Note that Python 3.10.6 cannot be used on Windows 7 or earlier. Note that Python 3.10.5 cannot be used on Windows 7 or earlier. Note that Python 3.9.13 cannot be used on Windows 7 or earlier. Note that Python 3.10.4 cannot be used on Windows 7 or earlier. Note that Python 3.9.12 cannot be used on Windows 7 or earlier. Note that Python 3.10.3 cannot be used on Windows 7 or earlier. Note that Python 3.9.11 cannot be used on Windows 7 or earlier. Note that Python 3.9.10 cannot be used on Windows 7 or earlier. Note that Python 3.10.2 cannot be used on Windows 7 or earlier. Note that Python 3.10.1 cannot be used on Windows 7 or earlier. Note that Python 3.9.9 cannot be used on Windows 7 or earlier. Note that Python 3.9.8 cannot be used on Windows 7 or earlier. Note that Python 3.10.0 cannot be used on Windows 7 or earlier. Note that Python 3.9.7 cannot be used on Windows 7 or earlier. Note that Python 3.9.6 cannot be used on Windows 7 or earlier. Note that Python 3.9.5 cannot be used on Windows 7 or earlier. Note that Python 3.8.10 cannot be used on Windows XP or earlier. Note that Python 3.9.4 cannot be used on Windows 7 or earlier. Note that Python 3.8.9 cannot be used on Windows XP or earlier. Note that Python 3.9.2 cannot be used on Windows 7 or earlier. Note that Python 3.8.8 cannot be used on Windows XP or earlier. Note that Python 3.8.7 cannot be used on Windows XP or earlier. Note that Python 3.9.1 cannot be used on Windows 7 or earlier. Note that Python 3.9.0 cannot be used on Windows 7 or earlier. Note that Python 3.8.6 cannot be used on Windows XP or earlier. Note that Python 3.8.6rc1 cannot be used on Windows XP or earlier. Note that Python 3.7.9 cannot be used on Windows XP or earlier. Note that Python 3.8.5 cannot be used on Windows XP or earlier. Note that Python 3.8.4 cannot be used on Windows XP or earlier. Note that Python 3.8.4rc1 cannot be used on Windows XP or earlier. Note that Python 3.7.8 cannot be used on Windows XP or earlier. Note that Python 3.8.3 cannot be used on Windows XP or earlier. Note that Python 3.8.3rc1 cannot be used on Windows XP or earlier. Note that Python 3.7.7 cannot be used on Windows XP or earlier. Note that Python 3.8.2 cannot be used on Windows XP or earlier. Note that Python 3.8.1 cannot be used on Windows XP or earlier. Note that Python 3.7.6 cannot be used on Windows XP or earlier. Note that Python 3.7.5 cannot be used on Windows XP or earlier. Note that Python 3.8.0 cannot be used on Windows XP or earlier. Note that Python 3.7.4 cannot be used on Windows XP or earlier. Note that Python 3.7.3 cannot be used on Windows XP or earlier. Note that Python 3.7.2 cannot be used on Windows XP or earlier. Note that Python 3.6.8 cannot be used on Windows XP or earlier. Note that Python 3.7.1 cannot be used on Windows XP or earlier. Note that Python 3.6.7 cannot be used on Windows XP or earlier. Note that Python 3.7.0 cannot be used on Windows XP or earlier. Note that Python 3.6.6 cannot be used on Windows XP or earlier. Note that Python 3.6.5 cannot be used on Windows XP or earlier. Note that Python 3.6.4 cannot be used on Windows XP or earlier. Note that Python 3.6.3 cannot be used on Windows XP or earlier. Note that Python 3.5.4 cannot be used on Windows XP or earlier. Note that Python 3.6.2 cannot be used on Windows XP or earlier. Note that Python 3.6.1 cannot be used on Windows XP or earlier. Note that Python 3.5.3 cannot be used on Windows XP or earlier. Note that Python 3.6.0 cannot be used on Windows XP or earlier. Note that Python 3.5.2 cannot be used on Windows XP or earlier. Note that Python 3.5.1 cannot be used on Windows XP or earlier. Note that Python 3.5.0 cannot be used on Windows XP or earlier. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/downloads/macos/",
    "title": "Python Releases for macOS | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/download/other/",
    "title": "Download Python for other platforms | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Python has been ported to a number of specialized and/or older platforms,\nlisted below in alphabetical order.  Note that these ports often lag\nwell behind the latest Python release. Briefcase (from the BeeWare project) and Buildozer (from the Kivy project) are two tools that can be used to deploy Python code as an Android app. Chaquopy is a project that can be used to embed Python into an existing Android Gradle project. The Chaquopy binaries are used by Briefcase when deploying to Android. pyqtdeploy can be used to convert a Qt project into an Android app. Termux is a terminal emulator and Linux environment that provides a Python binary. AIX binary packages for Python are available from IBM AIX Toolbox in RPM format. They can be installed using dnf package manager. Visit the Get Started page for more details. You can purchase ActivePython\n(commercial and community versions, including scientific computing modules, not open source) Both Python 2 and Python 3 are available from IBM in RPM form. They can be installed with the yum package manager or with the IBM i Access Client Solutions product. To get started with RPM-based open source packages for IBM i, visit http://ibm.biz/ibmi-rpms.  These RPM packages require a version of IBM i in active (not extended) support. Briefcase (from the BeeWare project) and Buildozer (from the Kivy project) are two tools that can be used to deploy Python code as an iOS app. Python-Apple-support is a project that provides pre-compiled Python frameworks that can be embedded into an Xcode project. PythonKit can be used to provide Swift integration with Python. Pythonista is a complete development environment for writing Python scripts including third-party libraries and system integration on your iPad or iPhone. Pyto  also provides a complete development environment for running Python 3 including many third-party libraries and system integration on an iPad or iPhone. You can purchase ActivePython\n(commercial and community versions, including scientific computing modules, not open source) Python is available for RISC OS, and can be obtained using the PackMan package manager. You can purchase ActivePython\n(commercial and community versions, including scientific computing modules, not open source), or build from\nsource if you have a C compiler. UNIX Packages has a variety\nof Python versions for a variety of Solaris versions.  These use the\nstandard Sun pkgadd. Standard CPython version 3.6.8 port for the Unified Extensible Firmware Interface (UEFI)\nshell environment is available through the Tianocore open source project.\nThis provides the standard Python scripting capabilities on UEFI environment,\nhelping the UEFI based firmware and platform developer community to use it for platform,\nfirmware validation, debug and the like.\nPython for UEFI source code and build instructions are available here. Currently build support is enabled using VS2019 and GCC5 tool chains for x86 and x64 bit platforms. Rocket Software provides a port of Python for z/OS. Python for z/OS is available from IBM for no license charge. It is available in PAX format from\nEarly Programs Web Tool\nor SMP/E format from Shopz.\nOptional no-cost Subscription and Support (S&S) is available in the Shopz ordering process.\nPlease visit the IBM Open Enterprise SDK for Python product page for more information. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://docs.python.org/3/license.html",
    "title": "History and License — Python 3.13.5 documentation",
    "content": "Copyright Python was created in the early 1990s by Guido van Rossum at Stichting\nMathematisch Centrum (CWI, see https://www.cwi.nl) in the Netherlands as a\nsuccessor of a language called ABC.  Guido remains Pythonâs principal author,\nalthough it includes many contributions from others. In 1995, Guido continued his work on Python at the Corporation for National\nResearch Initiatives (CNRI, see https://www.cnri.reston.va.us) in Reston,\nVirginia where he released several versions of the software. In May 2000, Guido and the Python core development team moved to BeOpen.com to\nform the BeOpen PythonLabs team.  In October of the same year, the PythonLabs\nteam moved to Digital Creations, which became\nZope Corporation.  In 2001, the Python Software Foundation (PSF, see\nhttps://www.python.org/psf/) was formed, a non-profit organization created\nspecifically to own Python-related Intellectual Property.  Zope Corporation was a\nsponsoring member of the PSF. All Python releases are Open Source (see https://opensource.org for the Open\nSource Definition). Historically, most, but not all, Python releases have also\nbeen GPL-compatible; the table below summarizes the various releases. Release Derived from Year Owner GPL-compatible? (1) 0.9.0 thru 1.2 n/a 1991-1995 CWI yes 1.3 thru 1.5.2 1.2 1995-1999 CNRI yes 1.6 1.5.2 2000 CNRI no 2.0 1.6 2000 BeOpen.com no 1.6.1 1.6 2001 CNRI yes (2) 2.1 2.0+1.6.1 2001 PSF no 2.0.1 2.0+1.6.1 2001 PSF yes 2.1.1 2.1+2.0.1 2001 PSF yes 2.1.2 2.1.1 2002 PSF yes 2.1.3 2.1.2 2002 PSF yes 2.2 and above 2.1.1 2001-now PSF yes Note GPL-compatible doesnât mean that weâre distributing Python under the GPL.\nAll Python licenses, unlike the GPL, let you distribute a modified version\nwithout making your changes open source. The GPL-compatible licenses make\nit possible to combine Python with other software that is released under\nthe GPL; the others donât. According to Richard Stallman, 1.6.1 is not GPL-compatible, because its license\nhas a choice of law clause. According to CNRI, however, Stallmanâs lawyer has\ntold CNRIâs lawyer that 1.6.1 is ânot incompatibleâ with the GPL. Thanks to the many outside volunteers who have worked under Guidoâs direction to\nmake these releases possible. Python software and documentation are licensed under the\nPython Software Foundation License Version 2. Starting with Python 3.8.6, examples, recipes, and other code in\nthe documentation are dual licensed under the PSF License Version 2\nand the Zero-Clause BSD license. Some software incorporated into Python is under different licenses.\nThe licenses are listed with code falling under that license.\nSee Licenses and Acknowledgements for Incorporated Software for an incomplete list of these licenses. BEOPEN PYTHON OPEN SOURCE LICENSE AGREEMENT VERSION 1 This section is an incomplete, but growing list of licenses and acknowledgements\nfor third-party software incorporated in the Python distribution. The _random C extension underlying the random module\nincludes code based on a download from\nhttp://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/emt19937ar.html. The following are\nthe verbatim comments from the original code: The socket module uses the functions, getaddrinfo(), and\ngetnameinfo(), which are coded in separate source files from the WIDE\nProject, https://www.wide.ad.jp/. The test.support.asynchat and test.support.asyncore\nmodules contain the following notice: The http.cookies module contains the following notice: The trace module contains the following notice: The uu codec contains the following notice: The xmlrpc.client module contains the following notice: The test.test_epoll module contains the following notice: The select module contains the following notice for the kqueue\ninterface: The file Python/pyhash.c contains Marek Majkowskiâ implementation of\nDan Bernsteinâs SipHash24 algorithm. It contains the following note: The file Python/dtoa.c, which supplies C functions dtoa and\nstrtod for conversion of C doubles to and from strings, is derived\nfrom the file of the same name by David M. Gay, currently available\nfrom https://web.archive.org/web/20220517033456/http://www.netlib.org/fp/dtoa.c.\nThe original file, as retrieved on March 16, 2009, contains the following\ncopyright and licensing notice: The modules hashlib, posix and ssl use\nthe OpenSSL library for added performance if made available by the\noperating system. Additionally, the Windows and macOS installers for\nPython may include a copy of the OpenSSL libraries, so we include a copy\nof the OpenSSL license here. For the OpenSSL 3.0 release,\nand later releases derived from that, the Apache License v2 applies: The pyexpat extension is built using an included copy of the expat\nsources unless the build is configured --with-system-expat: The _ctypes C extension underlying the ctypes module\nis built using an included copy of the libffi\nsources unless the build is configured --with-system-libffi: The zlib extension is built using an included copy of the zlib\nsources if the zlib version found on the system is too old to be\nused for the build: The implementation of the hash table used by the tracemalloc is based\non the cfuhash project: The _decimal C extension underlying the decimal module\nis built using an included copy of the libmpdec\nlibrary unless the build is configured --with-system-libmpdec: The C14N 2.0 test suite in the test package\n(Lib/test/xmltestdata/c14n-20/) was retrieved from the W3C website at\nhttps://www.w3.org/TR/xml-c14n2-testcases/ and is distributed under the\n3-clause BSD license: MIT License: Parts of the asyncio module are incorporated from\nuvloop 0.16,\nwhich is distributed under the MIT license: The file Python/qsbr.c is adapted from FreeBSDâs âGlobal Unbounded\nSequencesâ safe memory reclamation scheme in\nsubr_smr.c.\nThe file is distributed under the 2-Clause BSD License: Copyright"
  },
  {
    "url": "https://www.python.org/download/alternatives",
    "title": "Alternative Python Implementations | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. This site hosts the \"traditional\" implementation of Python (nicknamed CPython).\nA number of alternative implementations are available as well, namely Other parties have re-packaged CPython.  These re-packagings often\ninclude more libraries or are specialized for a particular application: If you want to host and run Python in the cloud, these implementations may be right for you: The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/doc/",
    "title": "Our Documentation | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Python's documentation, tutorials, and guides are constantly evolving. Get started here, or scroll down for documentation broken out by type and subject. Python Docs See also Documentation Releases by Version Can’t find what you’re looking for? Try our comprehensive Help section Open source software is made better when users can easily contribute code and documentation to fix bugs and add features. Python strongly encourages community involvement in improving the software. Learn more about how to make Python better for everyone. Contribute to Python\nIssue Tracker Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/doc/av",
    "title": "Audio/Video Instructional Materials for Python | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. There is a growing body of podcasts, screencasts and video presentations for\nthe Python community.  This page collects some of the best. Python411 is a series of podcasts about Python presented by Ron Stephens,\naimed at hobbyists and others who are learning Python.  Each episode\nfocuses on one aspect of learning Python, or one kind of Python\nprogramming, and points to online tools and tutorials.  Python related news\nand events will also be reported upon as well as interviews with key\nPython contributors. Ron has built up quite a collection of podcasts since he started in May\n2005 - over fifty as of April 2007.  They are great for listening to on\nthe train or in traffic. The site provides an XML/RSS feed to which you can subscribe with your\nfavorite reader or make a live bookmark of dropdown podcast titles\nusing Mozilla Firefox. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://wiki.python.org/moin/BeginnersGuide",
    "title": "BeginnersGuide - Python Wiki",
    "content": "Beginner's Guide to Python\nNew to programming? Python is free and easy to learn if you know where to start! This guide will help you to get started quickly. Chinese Translation/中文版入门 \nNew to Python?\nRead BeginnersGuide/Overview for a short explanation of what Python is. \nGetting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation New to programming? Python is free and easy to learn if you know where to start! This guide will help you to get started quickly. Chinese Translation/中文版入门 \nNew to Python?\nRead BeginnersGuide/Overview for a short explanation of what Python is. \nGetting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Chinese Translation/中文版入门 \nNew to Python?\nRead BeginnersGuide/Overview for a short explanation of what Python is. \nGetting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation New to Python?\nRead BeginnersGuide/Overview for a short explanation of what Python is. \nGetting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Read BeginnersGuide/Overview for a short explanation of what Python is. \nGetting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Getting Python\nNext, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Next, install the Python 3 interpreter on your computer. This is the program that reads Python programs and carries out their instructions; you need it before you can do any Python programming. Mac and Linux distributions may include an outdated version of Python (Python 2), but you should install an updated one (Python 3). See BeginnersGuide/Download for instructions to download the correct version of Python. There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation There are also Python interpreter and IDE bundles available, such as Thonny. Other options can be found at IntegratedDevelopmentEnvironments. At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation At some stage, you'll want to edit and save your program code. Take a look at HowToEditPythonCode for some advice and recommendations. \nLearning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Learning Python\nNext, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Next, read a tutorial and try some simple experiments with your new Python interpreter. If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation If you have never programmed before, see BeginnersGuide/NonProgrammers for a list of suitable tutorials. If you have previous programming experience, consult BeginnersGuide/Programmers, which lists more advanced tutorials. If English isn't your first language, you might be more comfortable with a tutorial that's been translated into your language. Consult python.org's list of Non-English resources. Most tutorials assume you know how to run a program on your computer. If you are using Windows and need help with this, see How do I Run a Program Under Windows. Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Here are some sites that focus on beginners and offer in-browser coding: Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Please keep these links sorted alphabetically Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Beginners Python tutorial at Python Land (free) Codédex (non-free) Outdated, Python 2! * Codecademy  Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Coding Bootcamps (non-free) DataCamp (non-free) Dataquest for Python for data science. (free) Genepy interactive exercises (free, open source, no ads) High School Technology Services for general Python (non-free) LabEx Python Hands-on Labs (freemium) Once you have read a tutorial, you can browse through Python's online documentation. It includes a tutorial that might come in handy, a Library Reference that lists all of the modules that come standard with Python, and the Language Reference for a complete (if rather dry) explanation of Python's syntax. When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation When you are ready to write your first program, you will need a text editor or an IDE. If you don't want to use Thonny or something more advanced, then you can use IDLE, which is bundled with Python and supports extensions. This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation This Python wiki also contains a page about Python One-Liners -- an obscure but interesting subculture in Python. \nNeed Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Need Help?\nNeed help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Need help with any of this? Read BeginnersGuide/Help for mailing lists and newsgroups. An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation An interactive and free way to learn Python is to ask ChatGPT or another generative AI model for help. The Python code quality generated by AI agents has become pretty good. State-of-the-art AI models work especially well for small projects or code understanding questions. Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Most Python books will include an introduction to the language; see IntroductoryBooks for suggested titles. Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Consult BeginnersGuide/Examples for small programs and little snippets of code that can help you learn. Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Or, if you prefer to learn Python through listening to a lecture, you can attend a training course or even hire a trainer to come to your company. Consult the PythonEvents page to see if any training courses are scheduled in your area and the PythonTraining page for a list of trainers. Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Teachers can join the EDU-SIG, a mailing list for discussion of Python's use in teaching at any level ranging from K-12 up to university. \nComplete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Complete list of Beginner's Guide pages\n\nBeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation BeginnersGuide/DownloadBeginnersGuide/ExamplesBeginnersGuide/HelpBeginnersGuide/MathematicsBeginnersGuide/NonProgrammersBeginnersGuide/NonProgrammersChineseBeginnersGuide/OverviewBeginnersGuide/OverviewChineseBeginnersGuide/ProgrammersBeginnersGuide/Programmers (Cpp2Python.pdf)BeginnersGuide/Programmers/SimpleExamples\n \nQuiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Quiz and Exercises\nAfter Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform \nPython Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation After Hours Programming - Python Quiz CheckIO - Online learning, testing and improving your python skills CS Circles - Online lessons and graded exercises Finxter - How good are your Python skills? Test and Training with a Daily Python Puzzle LabEx - 1000+ Python Interactive Challenges PyGUI - Collection of python quiz answers, Examples And GUI Tkinter Tutorials For Beginners Pythonspot - Python Quiz Python Challenge - A Python Quiz App on Android Platform Python Based AI and Prompt Engineering\nAI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) \nLooking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation AI Engineering Academy - Become a Certified AI Engineer with OpenAI API, HuggingFace, and Llama Generative AI Tutorial - Generative AI Tutorial GitHub Prompt Engineering Guide - List of helpful resources Google Gemini (Python) - Python Library to access Google's Gemini Model OpenAI API (Python) - Quickstart to Access OpenAI's API (Python, Javascript, curl) Looking for a particular Python module or application?\nThe first place to look is the Python Package Index. If you can't find anything relevant in the Package Index, try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Finally, you can try posting a query to the comp.lang.python Usenet group. \nPython-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation The first place to look is the Python Package Index. try searching python.org - you can find anything mentioned on the Python site, in the FAQs, or in the newsgroup. More info: where to search. Next, try Google or another search engine of your choice. Searching for \"python\" and some relevant keywords will usually find something helpful. Python-Related Cheat Sheets\nPython: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets \nWant to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Python: Collection of 11 Best Python Cheat Sheets NumPy: Collection of 10 Best NumPy Cheat Sheets Pandas: Collection of 7 Beautiful Pandas Cheat Sheets Machine Learning: Collection of 15 Machine Learning Cheat Sheets Want to contribute?\nPython is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page.  CategoryDocumentation Python is a product of the Python Software Foundation, a non-profit organization that holds the copyright. Donations to the PSF are tax-deductible in the USA, and you can donate via credit card or PayPal. To report a bug in the Python core, use the Python Bug Tracker. To contribute a bug fix or other patch to the Python core, read the Python Developer's Guide for more information about Python's development process. To contribute to the official Python documentation, join the Documentation SIG, write to docs@python.org , or use the Issue Tracker to contribute a documentation patch. To announce your module or application to the Python community, use comp.lang.python.announce. See the guide to Python mailing lists for more information. To propose changes to the Python core, post your thoughts to comp.lang.python. If you have an implementation, follow the Python Patch Guidelines. If you have a question are not sure where to report it, check out the WhereDoIReportThis? page. CategoryDocumentation CategoryDocumentation BeginnersGuide  (last edited 2025-03-07 01:13:11 by MaciejOlko) Unable to edit the page? See the FrontPage for instructions."
  },
  {
    "url": "https://devguide.python.org/",
    "title": "Python Developer’s Guide",
    "content": "This guide is a comprehensive resource for contributing\nto Python – for both new and experienced contributors. It is\nmaintained by the same\ncommunity that maintains Python.  We welcome your contributions to Python! Here are the basic steps needed to get set up and contribute a pull request.\nThis is meant as a checklist, once you know the basics. For complete\ninstructions please see the setup guide. Install and set up Git and other dependencies\n(see the Git Setup page for detailed information). Fork the CPython repository\nto your GitHub account and get the source code using: Build Python: See also more detailed instructions,\nhow to install and build dependencies,\nand the platform-specific pages for Unix,\nmacOS, and Windows. Run the tests: Note: Most macOS systems use\n./python.exe in order to avoid filename conflicts with\nthe Python directory. Create a new branch where your work for the issue will go, for example: If an issue does not already exist, please create it.  Trivial issues (for example, typo fixes) do\nnot require any issue to be created. Once you fixed the issue, run the tests, and the patchcheck: If everything is ok, commit. Push the branch on your fork on GitHub and create a pull request.  Include the issue number using gh-NNNN in the\npull request description.  For example: Add a News entry into the Misc/NEWS.d directory as individual file. The\nnews entry can be created by using blurb-it,\nor the blurb tool and its blurb add\ncommand. Please read more about blurb in its\nrepository. Note First time contributors will need to sign the Contributor Licensing\nAgreement (CLA) as described in the Licensing section of\nthis guide. Here are some links that you probably will reference frequently while\ncontributing to Python: Issue tracker Buildbot status Where to get help PEPs (Python Enhancement Proposals) Git bootcamp and cheat sheet We encourage everyone to contribute to Python and that’s why we have put up this\ndeveloper’s guide.  If you still have questions after reviewing the material in\nthis guide, then the Core Python Mentorship group is available to help guide new\ncontributors through the process. A number of individuals from the Python community have contributed to a series\nof excellent guides at Open Source Guides. Core developers and contributors alike will find the following guides useful: How to Contribute to Open Source Building Welcoming Communities Guide for contributing to Python: Contributors Documentarians Triagers Core team Setup and building Helping with documentation Issue tracker Responsibilities Where to get help Getting started Triaging an issue Team log Lifecycle of a pull request Style guide Helping triage issues Accepting pull requests Running and writing tests reStructuredText primer Experts index Development cycle Fixing “easy” issues (and beyond) Translating GitHub labels Motivations and affiliations Following Python’s development Helping with the Developer’s Guide GitHub issues for BPO users Experts index Git bootcamp and cheat sheet Triage Team Development cycle We recommend that the documents in this guide be read as needed. You\ncan stop where you feel comfortable and begin contributing immediately without\nreading and understanding these documents all at once.  If you do choose to skip\naround within the documentation, be aware that it is written assuming preceding\ndocumentation has been read so you may find it necessary to backtrack to fill in\nmissing concepts and terminology. Improving Python’s code, documentation and tests are ongoing tasks that are\nnever going to be “finished”, as Python operates as part of an ever-evolving\nsystem of technology.  An even more challenging ongoing task than these\nnecessary maintenance activities is finding ways to make Python, in the form of\nthe standard library and the language definition, an even better tool in a\ndeveloper’s toolkit. While these kinds of change are much rarer than those described above, they do\nhappen and that process is also described as part of this guide: Adding to the stdlib Changing the Python language This guide is specifically for contributing to the Python reference interpreter,\nalso known as CPython (while most of the standard library is written in Python,\nthe interpreter core is written in C and integrates most easily with the C and\nC++ ecosystems). There are other Python implementations, each with a different focus.  Like\nCPython, they always have more things they would like to do than they have\ndevelopers to work on them.  Some major examples that may be of interest are: PyPy: A Python interpreter focused on high speed (JIT-compiled) operation\non major platforms. GraalPy: A Python interpreter which has first-class support for\nembedding in Java, built on GraalVM. Jython: A Python interpreter focused on good integration with the Java\nVirtual Machine (JVM) environment. IronPython: A Python interpreter focused on good integration with the\nCommon Language Runtime (CLR) provided by .NET and Mono. Stackless: A Python interpreter focused on providing lightweight\nmicrothreads while remaining largely compatible with CPython specific\nextension modules. MicroPython: A tiny Python interpreter with small subset of the Python\nstandard library that is optimised to run on microcontrollers and in\nconstrained environments. CircuitPython: A fork of MicroPython designed to simplify experimenting\nand learning to code on low-cost microcontroller boards. Coding style guides PEP 7 (Style Guide for C Code) PEP 8 (Style Guide for Python Code) Issue tracker Experts index Buildbot status Source code Browse online Snapshot of the *main* branch PEPs (Python Enhancement Proposals) Where to get help Team log Anyone can clone the sources for this guide.  See Helping with the Developer’s Guide. Help with … CPython source code Changing CPython’s grammar Guide to the parser Compiler design Garbage collector design Tool support GDB support Dynamic analysis with Clang Various tools with configuration files as found in the Misc directory Information about editors and their configurations can be found in the\nwiki python.org maintenance Search this guide Please note that all interactions on\nPython Software Foundation-supported\ninfrastructure is covered\nby the PSF Code of Conduct,\nwhich includes all infrastructure used in the development of Python itself\n(for example, mailing lists, issue trackers, GitHub, etc.).\nIn general this means everyone is expected to be open, considerate, and\nrespectful of others no matter what their position is within the project. Moved to Status of Python versions"
  },
  {
    "url": "http://wiki.python.org/moin/Languages",
    "title": "Languages - Python Wiki",
    "content": "Attempt to have languages and links listed in the native tongue of the user.  2009-09-01 This page's links go to the various languages' pages, most of which have been copied from the other set of language pages, with a few updates.  Ideally, all the pages should be like the Polish or Turkish pages - all native language, only the necessary English. There are some ground rules, some are laid down by the site admins, some are my suggestions: Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) (my suggestion) We probably want to limit invites to edit the pages to people we know well, or Pythonistas with a track record.  Hopefully, this is inclusive enough without opening the site up to a spam flood and vandalismfest. Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). \nLanguages\nAfrikaansLanguage Afrikaans AlbanianLanguage Shqip AmharicLanguage አማርኛ ArabicLanguage العربية ArmenianLanguage Հայերեն AssameseLanguage অসমীয়া AzerbaijaniLanguage Azərbaycan dili BelarusianLanguage Беларуская мова BengaliLanguage বাংলা BodoLanguage बड़ो BosnianLanguage bosanski BulgarianLanguage български език BurmeseLanguage မြန်မာဘာသာ CatalanLanguage català ChineseLanguage 中文 CroatianLanguage hrvatski CzechLanguage čeština DanishLanguage dansk DogriLanguage डोगरी  Devanagari script DutchLanguage Nederlands EsperantoLanguage Esperanto EstonianLanguage eesti keel FinnishLanguage suomi FrenchLanguage français GeorgianLanguage ქართული ენა GermanLanguage Deutsch GreekLanguage Νέα Ελληνικά GujaratiLanguage ગુજરાતી HausaLanguage Hausa HebrewLanguage עִבְרִית HindiLanguage हिन्दी HungarianLanguage magyar nyelv IndonesianLanguage Bahasa Indonesia IcelandicLanguage íslenska IgboLanguage Asụsụ Igbo ItalianLanguage italiano JapaneseLanguage 日本語 KannadaLanguage ಕನ್ನಡ KashmiriLanguage कॉशुर (Koshur) KazakhLanguage Қазақ тілі KhmerLanguage ភាសាខ្មែរ KonkaniLanguage कोंकणी Devangari script KoreanLanguage 한국어/조선말 LaoLanguage ພາສາລາວ LatvianLanguage latviešu valoda LithuanianLanguage lietuvių kalba MalayLanguage Bahasa Melayu MalayalamLanguage മലയാളം MarathiLanguage मराठी MongolianLanguage Монгол хэл NepaliLanguage नेपाली NorwegianLanguage norsk OriyaLanguage ଓଡ଼ିଆ OromoLanguage Afaan Oromoo PersianLanguage فارسی PolishLanguage język polski PortugueseLanguage português PunjabiLanguage ਪੰਜਾਬੀ WesternPunjabiLanguage پنجابی RomanianLanguage limba română RussianLanguage русский язык SanskritLanguage संस्कृत SlovakLanguage slovenský jazyk SloveneLanguage slovenščina SerbianLanguage Српски SinhalaLanguage සිංහල SpanishLanguage español SwahiliLanguage Kiswahili SwedishLanguage svenska TagalogLanguage Wikang Tagalog TamilLanguage தமிழ TeluguLanguage తెలుగు ThaiLanguage ภาษาไทย TigrinyaLanguage ትግርኛ TurkishLanguage Türkçe UkranianLanguage украї́нська мо́ва UrduLanguage اُردوُ UzbekLanguage O‘zbek tili VietnameseLanguage tiếng Việt XhosaLanguage isiXhosa ZuluLanguage isiZulu CategoryLanguage CategoryUnicode 2009-09-01 This page's links go to the various languages' pages, most of which have been copied from the other set of language pages, with a few updates.  Ideally, all the pages should be like the Polish or Turkish pages - all native language, only the necessary English. There are some ground rules, some are laid down by the site admins, some are my suggestions: Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) (my suggestion) We probably want to limit invites to edit the pages to people we know well, or Pythonistas with a track record.  Hopefully, this is inclusive enough without opening the site up to a spam flood and vandalismfest. Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). \nLanguages\nAfrikaansLanguage Afrikaans AlbanianLanguage Shqip AmharicLanguage አማርኛ ArabicLanguage العربية ArmenianLanguage Հայերեն AssameseLanguage অসমীয়া AzerbaijaniLanguage Azərbaycan dili BelarusianLanguage Беларуская мова BengaliLanguage বাংলা BodoLanguage बड़ो BosnianLanguage bosanski BulgarianLanguage български език BurmeseLanguage မြန်မာဘာသာ CatalanLanguage català ChineseLanguage 中文 CroatianLanguage hrvatski CzechLanguage čeština DanishLanguage dansk DogriLanguage डोगरी  Devanagari script DutchLanguage Nederlands EsperantoLanguage Esperanto EstonianLanguage eesti keel FinnishLanguage suomi FrenchLanguage français GeorgianLanguage ქართული ენა GermanLanguage Deutsch GreekLanguage Νέα Ελληνικά GujaratiLanguage ગુજરાતી HausaLanguage Hausa HebrewLanguage עִבְרִית HindiLanguage हिन्दी HungarianLanguage magyar nyelv IndonesianLanguage Bahasa Indonesia IcelandicLanguage íslenska IgboLanguage Asụsụ Igbo ItalianLanguage italiano JapaneseLanguage 日本語 KannadaLanguage ಕನ್ನಡ KashmiriLanguage कॉशुर (Koshur) KazakhLanguage Қазақ тілі KhmerLanguage ភាសាខ្មែរ KonkaniLanguage कोंकणी Devangari script KoreanLanguage 한국어/조선말 LaoLanguage ພາສາລາວ LatvianLanguage latviešu valoda LithuanianLanguage lietuvių kalba MalayLanguage Bahasa Melayu MalayalamLanguage മലയാളം MarathiLanguage मराठी MongolianLanguage Монгол хэл NepaliLanguage नेपाली NorwegianLanguage norsk OriyaLanguage ଓଡ଼ିଆ OromoLanguage Afaan Oromoo PersianLanguage فارسی PolishLanguage język polski PortugueseLanguage português PunjabiLanguage ਪੰਜਾਬੀ WesternPunjabiLanguage پنجابی RomanianLanguage limba română RussianLanguage русский язык SanskritLanguage संस्कृत SlovakLanguage slovenský jazyk SloveneLanguage slovenščina SerbianLanguage Српски SinhalaLanguage සිංහල SpanishLanguage español SwahiliLanguage Kiswahili SwedishLanguage svenska TagalogLanguage Wikang Tagalog TamilLanguage தமிழ TeluguLanguage తెలుగు ThaiLanguage ภาษาไทย TigrinyaLanguage ትግርኛ TurkishLanguage Türkçe UkranianLanguage украї́нська мо́ва UrduLanguage اُردوُ UzbekLanguage O‘zbek tili VietnameseLanguage tiếng Việt XhosaLanguage isiXhosa ZuluLanguage isiZulu CategoryLanguage CategoryUnicode Ideally, all the pages should be like the Polish or Turkish pages - all native language, only the necessary English. There are some ground rules, some are laid down by the site admins, some are my suggestions: Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) (my suggestion) We probably want to limit invites to edit the pages to people we know well, or Pythonistas with a track record.  Hopefully, this is inclusive enough without opening the site up to a spam flood and vandalismfest. Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). \nLanguages\nAfrikaansLanguage Afrikaans AlbanianLanguage Shqip AmharicLanguage አማርኛ ArabicLanguage العربية ArmenianLanguage Հայերեն AssameseLanguage অসমীয়া AzerbaijaniLanguage Azərbaycan dili BelarusianLanguage Беларуская мова BengaliLanguage বাংলা BodoLanguage बड़ो BosnianLanguage bosanski BulgarianLanguage български език BurmeseLanguage မြန်မာဘာသာ CatalanLanguage català ChineseLanguage 中文 CroatianLanguage hrvatski CzechLanguage čeština DanishLanguage dansk DogriLanguage डोगरी  Devanagari script DutchLanguage Nederlands EsperantoLanguage Esperanto EstonianLanguage eesti keel FinnishLanguage suomi FrenchLanguage français GeorgianLanguage ქართული ენა GermanLanguage Deutsch GreekLanguage Νέα Ελληνικά GujaratiLanguage ગુજરાતી HausaLanguage Hausa HebrewLanguage עִבְרִית HindiLanguage हिन्दी HungarianLanguage magyar nyelv IndonesianLanguage Bahasa Indonesia IcelandicLanguage íslenska IgboLanguage Asụsụ Igbo ItalianLanguage italiano JapaneseLanguage 日本語 KannadaLanguage ಕನ್ನಡ KashmiriLanguage कॉशुर (Koshur) KazakhLanguage Қазақ тілі KhmerLanguage ភាសាខ្មែរ KonkaniLanguage कोंकणी Devangari script KoreanLanguage 한국어/조선말 LaoLanguage ພາສາລາວ LatvianLanguage latviešu valoda LithuanianLanguage lietuvių kalba MalayLanguage Bahasa Melayu MalayalamLanguage മലയാളം MarathiLanguage मराठी MongolianLanguage Монгол хэл NepaliLanguage नेपाली NorwegianLanguage norsk OriyaLanguage ଓଡ଼ିଆ OromoLanguage Afaan Oromoo PersianLanguage فارسی PolishLanguage język polski PortugueseLanguage português PunjabiLanguage ਪੰਜਾਬੀ WesternPunjabiLanguage پنجابی RomanianLanguage limba română RussianLanguage русский язык SanskritLanguage संस्कृत SlovakLanguage slovenský jazyk SloveneLanguage slovenščina SerbianLanguage Српски SinhalaLanguage සිංහල SpanishLanguage español SwahiliLanguage Kiswahili SwedishLanguage svenska TagalogLanguage Wikang Tagalog TamilLanguage தமிழ TeluguLanguage తెలుగు ThaiLanguage ภาษาไทย TigrinyaLanguage ትግርኛ TurkishLanguage Türkçe UkranianLanguage украї́нська мо́ва UrduLanguage اُردوُ UzbekLanguage O‘zbek tili VietnameseLanguage tiếng Việt XhosaLanguage isiXhosa ZuluLanguage isiZulu CategoryLanguage CategoryUnicode Ideally, all the pages should be like the Polish or Turkish pages - all native language, only the necessary English. There are some ground rules, some are laid down by the site admins, some are my suggestions: Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) (my suggestion) We probably want to limit invites to edit the pages to people we know well, or Pythonistas with a track record.  Hopefully, this is inclusive enough without opening the site up to a spam flood and vandalismfest. Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). There are some ground rules, some are laid down by the site admins, some are my suggestions: Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) (my suggestion) We probably want to limit invites to edit the pages to people we know well, or Pythonistas with a track record.  Hopefully, this is inclusive enough without opening the site up to a spam flood and vandalismfest. Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). Pages must be named in ASCII and English (PolishLanguage) Pages must have an explanation in English at the top (Links to Python information in <language X>) Where these pages really need help: check links, remove broken ones. add new links that are quality Python information and active. some care for languages that have next to nothing, but do have people in the Python community - even a link to the Wikipedia page for Python, in that language, is a start (Some are pretty complete and of high quality - the Russian language Wikipedia page for Python, for instance, packs a lot in). Languages\nAfrikaansLanguage Afrikaans AlbanianLanguage Shqip AmharicLanguage አማርኛ ArabicLanguage العربية ArmenianLanguage Հայերեն AssameseLanguage অসমীয়া AzerbaijaniLanguage Azərbaycan dili BelarusianLanguage Беларуская мова BengaliLanguage বাংলা BodoLanguage बड़ो BosnianLanguage bosanski BulgarianLanguage български език BurmeseLanguage မြန်မာဘာသာ CatalanLanguage català ChineseLanguage 中文 CroatianLanguage hrvatski CzechLanguage čeština DanishLanguage dansk DogriLanguage डोगरी  Devanagari script DutchLanguage Nederlands EsperantoLanguage Esperanto EstonianLanguage eesti keel FinnishLanguage suomi FrenchLanguage français GeorgianLanguage ქართული ენა GermanLanguage Deutsch GreekLanguage Νέα Ελληνικά GujaratiLanguage ગુજરાતી HausaLanguage Hausa HebrewLanguage עִבְרִית HindiLanguage हिन्दी HungarianLanguage magyar nyelv IndonesianLanguage Bahasa Indonesia IcelandicLanguage íslenska IgboLanguage Asụsụ Igbo ItalianLanguage italiano JapaneseLanguage 日本語 KannadaLanguage ಕನ್ನಡ KashmiriLanguage कॉशुर (Koshur) KazakhLanguage Қазақ тілі KhmerLanguage ភាសាខ្មែរ KonkaniLanguage कोंकणी Devangari script KoreanLanguage 한국어/조선말 LaoLanguage ພາສາລາວ LatvianLanguage latviešu valoda LithuanianLanguage lietuvių kalba MalayLanguage Bahasa Melayu MalayalamLanguage മലയാളം MarathiLanguage मराठी MongolianLanguage Монгол хэл NepaliLanguage नेपाली NorwegianLanguage norsk OriyaLanguage ଓଡ଼ିଆ OromoLanguage Afaan Oromoo PersianLanguage فارسی PolishLanguage język polski PortugueseLanguage português PunjabiLanguage ਪੰਜਾਬੀ WesternPunjabiLanguage پنجابی RomanianLanguage limba română RussianLanguage русский язык SanskritLanguage संस्कृत SlovakLanguage slovenský jazyk SloveneLanguage slovenščina SerbianLanguage Српски SinhalaLanguage සිංහල SpanishLanguage español SwahiliLanguage Kiswahili SwedishLanguage svenska TagalogLanguage Wikang Tagalog TamilLanguage தமிழ TeluguLanguage తెలుగు ThaiLanguage ภาษาไทย TigrinyaLanguage ትግርኛ TurkishLanguage Türkçe UkranianLanguage украї́нська мо́ва UrduLanguage اُردوُ UzbekLanguage O‘zbek tili VietnameseLanguage tiếng Việt XhosaLanguage isiXhosa ZuluLanguage isiZulu CategoryLanguage CategoryUnicode AfrikaansLanguage Afrikaans AlbanianLanguage Shqip AmharicLanguage አማርኛ ArabicLanguage العربية ArmenianLanguage Հայերեն AssameseLanguage অসমীয়া AzerbaijaniLanguage Azərbaycan dili BelarusianLanguage Беларуская мова BengaliLanguage বাংলা BodoLanguage बड़ो BosnianLanguage bosanski BulgarianLanguage български език BurmeseLanguage မြန်မာဘာသာ CatalanLanguage català ChineseLanguage 中文 CroatianLanguage hrvatski CzechLanguage čeština DanishLanguage dansk DogriLanguage डोगरी  Devanagari script DutchLanguage Nederlands EsperantoLanguage Esperanto EstonianLanguage eesti keel FinnishLanguage suomi FrenchLanguage français GeorgianLanguage ქართული ენა GermanLanguage Deutsch GreekLanguage Νέα Ελληνικά GujaratiLanguage ગુજરાતી HausaLanguage Hausa HebrewLanguage עִבְרִית HindiLanguage हिन्दी HungarianLanguage magyar nyelv IndonesianLanguage Bahasa Indonesia IcelandicLanguage íslenska IgboLanguage Asụsụ Igbo ItalianLanguage italiano JapaneseLanguage 日本語 KannadaLanguage ಕನ್ನಡ KashmiriLanguage कॉशुर (Koshur) KazakhLanguage Қазақ тілі KhmerLanguage ភាសាខ្មែរ KonkaniLanguage कोंकणी Devangari script KoreanLanguage 한국어/조선말 LaoLanguage ພາສາລາວ LatvianLanguage latviešu valoda LithuanianLanguage lietuvių kalba MalayLanguage Bahasa Melayu MalayalamLanguage മലയാളം MarathiLanguage मराठी MongolianLanguage Монгол хэл NepaliLanguage नेपाली NorwegianLanguage norsk OriyaLanguage ଓଡ଼ିଆ OromoLanguage Afaan Oromoo PersianLanguage فارسی PolishLanguage język polski PortugueseLanguage português PunjabiLanguage ਪੰਜਾਬੀ WesternPunjabiLanguage پنجابی RomanianLanguage limba română RussianLanguage русский язык SanskritLanguage संस्कृत SlovakLanguage slovenský jazyk SloveneLanguage slovenščina SerbianLanguage Српски SinhalaLanguage සිංහල SpanishLanguage español SwahiliLanguage Kiswahili SwedishLanguage svenska TagalogLanguage Wikang Tagalog TamilLanguage தமிழ TeluguLanguage తెలుగు ThaiLanguage ภาษาไทย TigrinyaLanguage ትግርኛ TurkishLanguage Türkçe UkranianLanguage украї́нська мо́ва UrduLanguage اُردوُ UzbekLanguage O‘zbek tili VietnameseLanguage tiếng Việt XhosaLanguage isiXhosa ZuluLanguage isiZulu CategoryLanguage CategoryUnicode Languages  (last edited 2025-05-01 12:20:09 by MaciejOlko) Unable to edit the page? See the FrontPage for instructions."
  },
  {
    "url": "https://peps.python.org",
    "title": "PEP 0 – Index of Python Enhancement Proposals (PEPs) | peps.python.org",
    "content": "This PEP contains the index of all Python Enhancement Proposals,\nknown as PEPs.  PEP numbers are assigned\nby the PEP editors, and once assigned are never changed.  The\nversion control history of\nthe PEP texts represent their historical record. PEPs for specialist subjects are indexed by topic. The PEPS API is a JSON file of metadata about all the published PEPs. Read more here. The numerical index contains a table of all PEPs, ordered by number. More info in PEP 1. More info in PEP 1."
  },
  {
    "url": "https://wiki.python.org/moin/PythonBooks",
    "title": "PythonBooks - Python Wiki",
    "content": "There are a variety of books about Python.  Here's a guide to them: IntroductoryBooks (gentle overviews of the language) AdvancedBooks (for when you don't want gentle) ReferenceBooks (much information in a small space) Specific applications: GameProgrammingBooks NetworkProgrammingBooks GuiBooks JythonBooks ScientificProgrammingBooks SystemAdministrationBooks WebProgrammingBooks WindowsBooks XmlBooks ZopeBooks Books in languages other than English: UkrainianPythonBooks DutchPythonBooks FrenchPythonBooks GermanPythonBooks GreekPythonBooks HungarianPythonBooks JapanesePythonBooks KoreanPythonBooks RussianPythonBooks PersianPythonBooks PolishPythonBooks PortuguesePythonBooks ArabicPythonBooks SpanishPythonBooks When you see a book you like, don't hesitate to add your recommendation! There's also a Courses page, filled with links to video courses. External sources of information about (Python) books: PythonBooks.org - A collection of Python books featuring popularity based ranking. 101+ Free Python Books - You don't have to pay money to read great Python books. Many are free! Python Kindle & Paperback Collection - A good collection of Kindle and Paperback books on Python 3, Django, Flask, FastAPI, and Scientific Computing (NumPy, SciPy, and Pandas) There is also a list of OutOfPrintBooks. IntroductoryBooks (gentle overviews of the language) AdvancedBooks (for when you don't want gentle) ReferenceBooks (much information in a small space) GameProgrammingBooks NetworkProgrammingBooks GuiBooks JythonBooks ScientificProgrammingBooks SystemAdministrationBooks WebProgrammingBooks WindowsBooks XmlBooks ZopeBooks UkrainianPythonBooks DutchPythonBooks FrenchPythonBooks GermanPythonBooks GreekPythonBooks HungarianPythonBooks JapanesePythonBooks KoreanPythonBooks RussianPythonBooks PersianPythonBooks PolishPythonBooks PortuguesePythonBooks ArabicPythonBooks SpanishPythonBooks When you see a book you like, don't hesitate to add your recommendation! There's also a Courses page, filled with links to video courses. External sources of information about (Python) books: PythonBooks.org - A collection of Python books featuring popularity based ranking. 101+ Free Python Books - You don't have to pay money to read great Python books. Many are free! Python Kindle & Paperback Collection - A good collection of Kindle and Paperback books on Python 3, Django, Flask, FastAPI, and Scientific Computing (NumPy, SciPy, and Pandas) There is also a list of OutOfPrintBooks. There's also a Courses page, filled with links to video courses. External sources of information about (Python) books: PythonBooks.org - A collection of Python books featuring popularity based ranking. 101+ Free Python Books - You don't have to pay money to read great Python books. Many are free! Python Kindle & Paperback Collection - A good collection of Kindle and Paperback books on Python 3, Django, Flask, FastAPI, and Scientific Computing (NumPy, SciPy, and Pandas) There is also a list of OutOfPrintBooks. External sources of information about (Python) books: PythonBooks.org - A collection of Python books featuring popularity based ranking. 101+ Free Python Books - You don't have to pay money to read great Python books. Many are free! Python Kindle & Paperback Collection - A good collection of Kindle and Paperback books on Python 3, Django, Flask, FastAPI, and Scientific Computing (NumPy, SciPy, and Pandas) There is also a list of OutOfPrintBooks. PythonBooks.org - A collection of Python books featuring popularity based ranking. 101+ Free Python Books - You don't have to pay money to read great Python books. Many are free! Python Kindle & Paperback Collection - A good collection of Kindle and Paperback books on Python 3, Django, Flask, FastAPI, and Scientific Computing (NumPy, SciPy, and Pandas) There is also a list of OutOfPrintBooks. PythonBooks  (last edited 2025-06-12 13:27:03 by MatsWichmann) Unable to edit the page? See the FrontPage for instructions."
  },
  {
    "url": "https://www.python.org/doc/essays/",
    "title": "Python Documentation Index | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. In this directory I place short essays (anything from 500 to 5000\nwords) on various Python subjects.  See also a collection of presentations I have given.  See also my\nblog at blogspot.com and my\nprevious blog at\nartima.com. --Guido van Rossum Unifying types and classes in Python 2.2 (See below) Foreword for \"Programming Python\" (1st ed.) Written in 1996, this gives an overview of the early history and\nbackground of Python and some of my philosophy about software design\nand project management.  See also my foreword to the 2nd edition. Implementing Graphs An elegant and perhaps not obvious way to represent graphs using\nPython's most fundamental and versatile data types, lists and\ndictionaries. An Optimization Anecdote Required reading if you find that your Python code runs too slow. Metaclass Programming in Python 1.5 Warning: reading this document may cause your brain to explode. Built-in Package Support in Python 1.5 The most official documentation for the new package features in\nPython 1.5. Standard Exception Classes in Python 1.5 The most official documentation for the new exception features in\nPython 1.5.  (Updated for Python 1.5.2 by Barry Warsaw.) Glue It All Together With Python A position paper I wrote for and presented at the OMG-DARPA-MCC workshop on compositional software architectures in Monterey,\nCalifornia, January 6-8, 1998. What Is Python? Executive Summary A short (two paragraphs) high-level presentation of Python's virtues. Comparing Python to Other Languages Activist ammo or flame fodder?  (Note: I received lots of feedback\nwhen I posted this to comp.lang.python.  Unfortunately the feedback\nwas diverted to a separate mailbox that I didn't know I had -- when I\nfinally found it, I was a bit overwhelmed and so far have not yet\nfound the time to update the article.) Proposed Improvements to Module Cleanup A revised version of the proposal I posted to comp.lang.python on\nFeb 6, 1998.  This has been adopted in Python 1.5.1. Open Source Summit Trip Report My trip report of O'Reilly's Open Source Summit, essentially as\nposted on c.l.p.  This version published in the Linux Gazette (it's an\nexternal link), in their May 1998 issue. Debugging Reference Count Problems An edited version of a posting to c.l.p on this subject, on May 27, 1998. Computer Programming for Everybody (old) A funding proposal that was accepted by DARPA in March 1999. Computer Programming for Everybody Revised, extended version of the previous proposal. Interview in Linux Journal Not quite an essay, but a stream of consciousness penned down by a\njournalist...  More thoughts about CP4E. Interview in by Frank Willison for \"Frankly Speaking\" column Another stream of consciousness transcribed by a friendly\nlistener...  Not just on CP4E. Foreword for \"Programming Python\" (2nd ed.) The story continues... Unifying types and classes in Python 2.2 An introduction to the type/class unification effort in Python\n2.2.  Note that the unification work is officially labeled\nexperimental and is subject to change in future releases of\nPython. Parade of the PEPs To start off Developer's Day at the Python10 conference I gave a\nkeynote ending in what I dubbed \"the parade of the PEPs\". It was a\nbrief overview of all open PEPs, where I gave my highly personal and subjective opinion\nfor each PEP.  Later, I realized that this might have been of interest\nto other developers.  I didn't take notes at the conference, so below\nis a different set of comments that I created from scratch during a\nsingle two-hour sitting on March 7, 2002.  I intend to occasionally\nupdate this with new comments and new PEPs. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/diversity/",
    "title": "Diversity | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The Python Software Foundation and the global Python community welcome\nand encourage participation by everyone. Our community is based on mutual\nrespect, tolerance, and encouragement, and we are working to help each\nother live up to these principles. We want our community to be more\ndiverse: whoever you are, and whatever your background, we welcome you. We have created this diversity statement because we believe that a\ndiverse Python community is stronger and more vibrant.  A diverse\ncommunity where people treat each other with respect has more potential\ncontributors and more sources for ideas. Although we have phrased the formal diversity statement generically to\nmake it all-inclusive, we recognize that there are specific attributes\nthat are used to discriminate against people.  In alphabetical order,\nsome of these attributes include (but are not limited to): age, culture,\nethnicity, gender identity or expression, national origin, physical or\nmental difference, politics, race, religion, sex, sexual orientation,\nsocio-economic status, and subculture.  We welcome people regardless of\nthe values of these or other attributes. The Python community welcomes people no matter what languages they are\nfluent in.  (Although core Python development is done in English.)  The\nPython community encourages the creation of user groups in all locales,\nand many of them are listed at\nhttp://wiki.python.org/moin/LocalUserGroups\nMany of these user groups also have mailing lists in the locally\npreferred language. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/lists/",
    "title": "Mailing Lists | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Here's an overview of the mail and news resources for python. For a\ncomplete listing of python.org's public mailing lists you can view them on\nMailman 3. To request a new list, send e-mail to postmaster @ python.org; please check first to make sure a similar list does not already exist. Mailing lists for users speaking languages other than\nEnglish are listed in the non-English python resources guide, which includes mailing\nlists, translated and original non-English documentation, and other\nresources. comp.lang.python is a high-volume\nUsenet open (not moderated) newsgroup for general discussions and\nquestions about Python.  You can also access it as a mailing list through\npython-list. Pretty much anything Python-related is fair game for discussion, and the group is even fairly tolerant of off-topic digressions; there have been entertaining discussions of topics such as floating point, good software design, and other programming languages such as Lisp and Forth. Most discussion on comp.lang.python is about developing with Python, not about development of the Python interpreter itself. Some of the core developers still read the list, but most of them don't. Occasionally comp.lang.python suggestions have resulted in an enhancement proposal being written, leading to a new Python feature. If you find a bug in Python, don't send it to comp.lang.python; file a bug report in the issue tracker. Items posted on the Usenet group appear on the mailing list, and vice\nversa (bidirectional gateway). Due to the mysteries of Usenet, the\norder in which items show up may vary. Rudeness and personal attacks, even in reaction to blatant flamebait, are strongly frowned upon. People may strongly disagree on an issue, but usually discussion remains civil. In case of an actual flamebait posting, you can ignore it, quietly plonk the offending poster in your killfile or mail filters, or write a sharp but still-polite response, but at all costs resist the urge to flame back. Generally comp.lang.python is a high-signal, low-noise group. It's also a high-traffic group, running at around 200 posts per day. An archive of the list is available: There are some non-English language versions of this newsgroup. The links provided here are to the Google Groups archive for each: de.comp.lang.python (German), it.comp.lang.python (Italian), pl.comp.lang.python (Polish), fr.comp.lang.python (French), and cz.comp.lang.python (Czech). comp.lang.python.announce is a\nlow-volume moderated forum for Python-related announcements. New\nmodules and programs are announced here, and it's where\nPEPs are posted to get comments from the community.\nYou'll also see announcements for conferences. This is a moderated newsgroup carrying at most perhaps 10 to 20 messages per week, so it's an easy way to be keep up-to-date on what's new in the Python world. See the comp.lang.python.announce posting guidelines for guidelines on submitting announcements. It is also available as a moderated mailing list, python-announce. Subscribing can be done via the python-announce list information page. comp.lang.python.announce is moderated by a team of people. If you need to contact them directly, e.g. to ask why a particular message was rejected, write to clpa-moderators-owner @ python.org. There are several archives for comp.lang.python.announce: The tutor mailing list is for users who want to ask questions about learning computer programming with Python. An archive of the list is available. People interested in learning about programming with Python are\nencouraged to join, as are experienced users interested in helping\nothers learn -- teaching other people is one of the best ways to learn\nmore yourself! python-dev used to be used as the main mailing list for developing Python, with practically all core developers subscribed to it. It has since been put into read-only mode. The archive is still available at Mailman 3 Python-Dev Archive. Discussion has moved on to our Discourse instance. The Core Development category has taken up the purpose of the python-dev mailing list. The python-ideas list was for discussing more speculative design ideas. Just like for python-dev, discussions has moved on to the Discourse instance in form of the Ideas category and the mailing list has been archived. The python-checkins mailing list receives an automatically generated message for each change committed to the Python Subversion tree. python-checkins makes it easy for developers to know what is happening in the repository. The volume of traffic on this list varies widely based on developer activity. The python-help mailing list is python.org's help desk. You can ask a group of knowledgeable volunteers questions about all your Python problems. You can send email to python-help by writing to help @ python.org for individual support. Mail sent there lands in the mailbox of a small group of volunteers who may reply to reasonable requests for help, depending on their area of expertise. Using it is much preferred to sending mail directly to Guido or some other individual, but less preferable than posting to comp.lang.python. In all cases, try searching the various archives first. When you ask a question, be sure to give your configuration: what hardware platform, what OS (and version), what Python version, and (when using Tkinter) what Tcl/Tk version you are using. If you're using an older Python version, try upgrading to the latest version first -- things often get better! You can't subscribe to python-help -- it is not for bystanders, only for questioners to submit questions and for helpers to receive and field them. The archives are not accessible, to protect the questioners' privacy. If you would like to help answer questions, send your qualifications to webmaster @ python.org. When you send a message to python-help, you will get an automated response. Your message is still delivered to the volunteers, and you will only receive this automated response once every approximately three months. Special Interest Groups (SIGs) are smaller communities focused on a particular topic or application such as databases, Python on macOS, etc. Every SIG has a mailing list of its own. See the SIG page for more information. SIGs vary in their success. Some, such as the XML, Database, and Distutils SIGs, have produced specifications and software that are now used throughout the Python community. Not all SIGs are as productive, though, and some sputter along for years without ever finalizing an implementation or a document. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/forums/",
    "title": "Forums | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The official Python Community forums are hosted at discuss.python.org. If you're looking for additional forums or forums in your native language, please check out the local user groups page at the Python Wiki. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/psf/annual-report/2024/",
    "title": "2024 PSF Annual Impact Report | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. In 2024, the Python community and language continued to grow! The PSF celebrated a year of remarkable growth, with Python becoming the most popular language on GitHub and worldwide community engagement at an all-time high. We expanded our impact by welcoming our inaugural PyPI Support Specialist, Maria Ashna, the revival of the User Success and Education and Outreach Workgroups, and continued investment in grants, infrastructure, and accessibility. Weâd love for you to take a look at the 2024 Annual Impact Report that we put together to share more highlights from the year, financial reporting, and some previews of whatâs to come in the next year. Download and read the report today! The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/workshops/",
    "title": "Conferences and Workshops | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. There are quite a number of Python conferences happening all year around and in many parts of the world. Many of them are taking place yearly or even more frequent: Subsets of this list are also available on other sites: Several of these conferences record the talk sessions on video. pyvideo.org provides an index to a large set these videos. If you would like to announce a Python related event, please see Submitting an event to the Python events calendars. You can also ask on pydotorg-www at python dot org for help. If you have an event to add, please see the instructions on how to edit Python Wiki for details. If you are organizing a Python conference or thinking of organizing one, please subscribe to the Python conferences mailing list. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/sigs/",
    "title": "Python Special Interest Groups | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. There are a number of Special\nInterest Groups (SIGs) for focused collaborative efforts to develop,\nimprove, or maintain specific Python resources.  Each SIG has a\ncharter, a coordinator, a mailing list, and a directory on the Python\nwebsite.  SIG membership is  informal, defined by subscription to the SIG's\nmailing list.  Anyone can join a SIG, and participate in the\ndevelopment discussions via the SIG's mailing list. Below is the list of currently active Python SIGs, with links to\ntheir resources.  The link in the first column directs you to the\nSIG's home page: a page with more information about the SIG.  The\nlinks in the \"Info\" column direct you to the SIG's archives, and to\nthe SIG's Mailman page, which you can use to subscribe or unsubscribe\nyourself and to change your subscription options. The SIG mailing lists are managed by GNU Mailman, a web-based interface for\nmailing lists written in Python. There is also a list of retired SIGs;\nthese SIGs existed in the past but are no longer active.\nTheir archives and home pages are retained.  A retired SIG can be\nrevived, using the same criteria as for\ncreating a new SIG. There are also local Python User\nGroups, organized by region rather than by special interest. All SIG mailing lists are archived. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/community/logos/",
    "title": "The Python Logo | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Projects and companies that use Python are encouraged to incorporate the Python\nlogo on their websites, brochures, packaging, and elsewhere to indicate suitability\nfor use with Python or implementation in Python.  Use of the \"two snakes\" logo element alone (the logo device), without the accompanying wordmark is permitted on the same terms as the combined logo. Combined logo: Logo device only: Currently, the following larger sized and vector variants of the logo are available: The font used in the logo is called \"Flux Regular\".  The PSF owns a copy\nbut we cannot distribute it, except for work on the PSF's behalf. The official Python Powered logo is available in two forms, wide and tall: This logo available in sizes 200x80,\n140x56, 100x40,\nand 70x28.   Also as\nSVG format source file. This logo available in sizes 140x182,\n100x130, 70x91,\nand 50x65.   Also as\nSVG format source file. The Python logo is a trademark of the Python Software Foundation, which is\nresponsible for defending against any damaging or confusing uses of the\ntrademark. See the PSF Trademark Usage Policy. In general, we want the logo to be used as widely as possible to indicate\nuse of Python or suitability for Python.  However, please ask first when using a derived version of the\nlogo or when in doubt. Making your own shirts and other items featuring the Python logo is OK, but please seek permission from the PSF if you are planning to sell merchandise that shows the Python logo. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://wiki.python.org/moin/",
    "title": "FrontPage - Python Wiki",
    "content": "The Python Wiki\nWelcome to the Python Wiki, a user-editable compendium of knowledge based around the Python programming language. Some pages are protected against casual editing - see WikiEditingGuidelines for more information about editing content. Python is a great object-oriented, interpreted, and interactive programming language. It is often compared (favorably of course  ) to Lisp, Tcl, Perl, Ruby, C#, Visual Basic, Visual Fox Pro, Scheme or Java... and it's much more fun. Python combines remarkable power with very clear syntax. It has modules, classes, exceptions, very high level dynamic data types, and dynamic typing. There are interfaces to many system calls and libraries, as well as to various windowing systems. New built-in modules are easily written in C or C++ (or other languages, depending on the chosen implementation). Python is also usable as an extension language for applications written in other languages that need easy-to-use scripting or automation interfaces.  Getting Started \nEvents and Community \nSoftware \nCore Development \nUsing and Editing the Wiki \n\n \nGetting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Welcome to the Python Wiki, a user-editable compendium of knowledge based around the Python programming language. Some pages are protected against casual editing - see WikiEditingGuidelines for more information about editing content. Python is a great object-oriented, interpreted, and interactive programming language. It is often compared (favorably of course  ) to Lisp, Tcl, Perl, Ruby, C#, Visual Basic, Visual Fox Pro, Scheme or Java... and it's much more fun. Python combines remarkable power with very clear syntax. It has modules, classes, exceptions, very high level dynamic data types, and dynamic typing. There are interfaces to many system calls and libraries, as well as to various windowing systems. New built-in modules are easily written in C or C++ (or other languages, depending on the chosen implementation). Python is also usable as an extension language for applications written in other languages that need easy-to-use scripting or automation interfaces.  Getting Started \nEvents and Community \nSoftware \nCore Development \nUsing and Editing the Wiki \n\n \nGetting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python is a great object-oriented, interpreted, and interactive programming language. It is often compared (favorably of course  ) to Lisp, Tcl, Perl, Ruby, C#, Visual Basic, Visual Fox Pro, Scheme or Java... and it's much more fun. Python combines remarkable power with very clear syntax. It has modules, classes, exceptions, very high level dynamic data types, and dynamic typing. There are interfaces to many system calls and libraries, as well as to various windowing systems. New built-in modules are easily written in C or C++ (or other languages, depending on the chosen implementation). Python is also usable as an extension language for applications written in other languages that need easy-to-use scripting or automation interfaces.  Getting Started \nEvents and Community \nSoftware \nCore Development \nUsing and Editing the Wiki \n\n \nGetting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python combines remarkable power with very clear syntax. It has modules, classes, exceptions, very high level dynamic data types, and dynamic typing. There are interfaces to many system calls and libraries, as well as to various windowing systems. New built-in modules are easily written in C or C++ (or other languages, depending on the chosen implementation). Python is also usable as an extension language for applications written in other languages that need easy-to-use scripting or automation interfaces.  Getting Started \nEvents and Community \nSoftware \nCore Development \nUsing and Editing the Wiki \n\n \nGetting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Getting Started Events and Community Software Core Development Using and Editing the Wiki Getting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Getting Started\n  \n \n\n Beginners Guide \nDocumentation \n\n Links to tutorials, courses and resources \nLearning materials, topic guides and links to central resources \n\n Beginner Errors \nPython Books \n\n Some common pitfalls of beginners \nBooks about Python plus reviews \n\n Asking for Help \nPython Audio Materials \n\n Questions asked by beginners, answered here \nA mixture of introductory and topical material \n\n Languages \nPython Implementations \n\n Resources written in languages other than English \nDifferent software which runs programs in the Python language \n\n See also the documentation category for all known documentation-related pages. \n\n \nEvents, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Beginners Guide Documentation Links to tutorials, courses and resources Learning materials, topic guides and links to central resources Beginner Errors Python Books Some common pitfalls of beginners Books about Python plus reviews Asking for Help Python Audio Materials Questions asked by beginners, answered here A mixture of introductory and topical material Languages Python Implementations Resources written in languages other than English Different software which runs programs in the Python language See also the documentation category for all known documentation-related pages. Events, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Events, Courses, Conferences, Community\nPython Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world  \nPython Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python Discussion Forums - if you want to meet people online, ask questions or discuss new ideas Python Conferences - information about the Python conference scene Local User Groups - find a Python group near you Python Training - Python training courses Python Events - event listing for conferences, training courses and more Python Event Calendars - calendars for Python conferences and user groups Participating in the Community - where people using and producing Python get together Python Software Foundation - show your support by joining the Foundation behind Python Find a job where you can use Python - Python job boards around the world Python Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python Software\n  \n \n\n Python Projects \nDevelopment Tools \n\n Information on finding software projects written in Python, including... \nManaging your code more effectively \n\n Applications \nPython Editors \n\n Ready-to-run applications which use Python \nEditing your code more effectively \n\n Useful Modules \nPublishing Python Modules \n\n Some building blocks for your own projects (including frameworks for database, GUI, Web programming) \nHow to make others aware of your own works \n\n \nPython Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python Projects Development Tools Information on finding software projects written in Python, including... Managing your code more effectively Applications Python Editors Ready-to-run applications which use Python Editing your code more effectively Useful Modules Publishing Python Modules Some building blocks for your own projects (including frameworks for database, GUI, Web programming) How to make others aware of your own works Python Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Python Core Development Tools\n  \n \n\n The Python Web Site \nBug Tracker \n\n Maintaining the official Python online resources \nRoundup and the code review services used by the Python project \n\n Package Index \nOther Resources \n\n The infrastructure behind Python package hosting \nA list of all development process-related resources on this Wiki \n\n \nUsing this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. The Python Web Site Bug Tracker Maintaining the official Python online resources Roundup and the code review services used by the Python project Package Index Other Resources The infrastructure behind Python package hosting A list of all development process-related resources on this Wiki Using this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Using this Wiki\nThis Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. This Wiki is a community place to gather and organize all things about Python. Feel free to exercise your editorial skills and expertise to make it a useful knowledge base and up-to-date reference on all Python-related topics. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. There are some guidelines describing the policies and rules governing this Wiki and how you can most effectively contribute to it. A list of site improvements describes various tasks where your help would be appreciated. To keep up with changes on this site, check RecentChanges frequently or follow it using RSS: RSS feed. \nCreating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Creating a Wiki account\nIn order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. In order to sign up for a wiki account, please go to the Create new account form, enter your account name (using the format FirstnameLastname to avoid issues - please don't use spaces in the name) and provide a password, plus email address (for password recovery). \nEditing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Editing pages\nSince spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Since spamming and vandalism on this wiki had reached a level that required constant intervention, unfamiliar users are no longer allowed to edit pages. However all you need to do is introduce yourself to the wiki admin group to become an editor. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. If you want to edit a page and have just signed up, or find that you can no longer edit a page that you could edit before, please write to the pydotorg-www mailing list describing what you would like to edit, and we'll add you to the EditorsGroup. Please include your account name (wiki name) in this message. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Sorry for any inconvenience, but we want to keep this wiki a useful tool for the community, while at the same time preventing the wiki admins from burning out cleaning up junk. \nReporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Reporting problems\nIn case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. In case of emergency, please contact the python.org maintainers, or if experiencing difficulties, contact the pydotorg-www mailing list to say \"help\".  \nWiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Wiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Wiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. Wiki Attack in January 2013\nThe wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. The wiki was subject to an attack on January 5 2013. Since it was not clear whether user account data was stolen, all passwords were subsequently reset, so you will have to use the password recovery function to get a new password. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. See the wiki attack description page for more details. If you find problems, please report them to the pydotorg-www mailing list <pydotorg-www@python.org>. \nHTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. HTTPS access to the Wiki\nWe have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. We have enabled HTTPS access to the wiki to further enhance security and avoid having to send clear text passwords over the network in order to log in to the wikis. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. If you have not been using HTTPS links to the wiki login page, please be advised that your password may have been sniffed on the network at e.g. a conference. It is best to change it and stop using HTTP links to the wiki login page. FrontPage  (last edited 2025-05-14 15:24:44 by MarcAndreLemburg) Unable to edit the page? See the FrontPage for instructions."
  },
  {
    "url": "https://www.python.org/psf/conduct/",
    "title": "Python Software Foundation Code of Conduct - Python Software Foundation Policies",
    "content": "The Python community is made up of members from around the globe with a diverse set of skills, personalities, and experiences. It is through these differences that our community experiences great successes and continued growth. When you're working with members of the community, this Code of Conduct will help steer your interactions and keep Python a positive, successful, and growing community. Members of the Python community are open, considerate, and respectful. Behaviours that reinforce these values contribute to a positive environment, and include: Every member of our community has the right to have their identity respected. The Python community is dedicated to providing a positive experience for everyone, regardless of age, gender identity and expression, sexual orientation, disability, physical appearance, body size, ethnicity, nationality, race, or religion (or lack thereof), education, or socio-economic status. Examples of unacceptable behavior by participants include: Community members asked to stop any inappropriate behavior are expected to comply immediately. No weapons are allowed at Python Software Foundation events. Weapons include but are not limited to explosives (including fireworks), guns, and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. If a participant engages in behavior that violates this code of conduct, the Python community Code of Conduct team may take any action they deem appropriate, including warning the offender or expulsion from the community and community events with no refund of event tickets. The full list of consequences for inappropriate behavior is listed in the Enforcement Procedures. Thank you for helping make this a welcoming, friendly community for everyone. This Code of Conduct applies to the following people at events hosted by the Python Software Foundation, and events hosted by projects under the PSF's fiscal sponsorship: The Code of Conduct applies in official venue event spaces, including: The Code of Conduct applies to interactions with official event accounts on social media spaces and phone applications, including: Event organizers will enforce this code throughout the event. Each event is required to provide a Code of Conduct committee that receives, evaluates, and acts on incident reports. Each event is required to provide contact information for the committee to attendees. The event Code of Conduct committee may (but is not required to) ask for advice from the Python Software Foundation Code of Conduct work group. The Python Software Foundation Code of Conduct work group can be reached by emailing conduct-wg@python.org. This Code of Conduct applies to the following online spaces: This Code of Conduct applies to the following people in official Python Software Foundation online spaces: Each online space listed above is required to provide the following information to the Python Software Foundation Code of Conduct work group: Each online space listed above is encouraged to provide the following information to community members: The Python Software Foundation Code of Conduct work group will receive and evaluate incident reports from the online communities listed above. The Python Software Foundation Code of Conduct work group will work with online community administrators/moderators to suggest actions to take in response to a report. In cases where the administrators/moderators disagree on the suggested resolution for a report, the Python Software Foundation Code of Conduct work group may choose to notify the Python Software Foundation board. If you believe that someone is violating the code of conduct, or have any other concerns, please contact a member of the Python Software Foundation Code of Conduct work group immediately. They can be reached by emailing conduct-wg@python.org Python Software Foundation Community Member Procedure For Reporting Code of Conduct Incidents Python Software Foundation Code of Conduct Working Group Enforcement Procedures This Code of Conduct is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. This Code of Conduct was forked from the example policy from the Geek Feminism wiki, created by the Ada Initiative and other volunteers, which is under a Creative Commons Zero license. Additional new language and modifications were created by Sage Sharp of Otter Tech. Language was incorporated from the following Codes of Conduct:"
  },
  {
    "url": "https://www.python.org/community/awards",
    "title": "Python Community Awards | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/psf/get-involved/",
    "title": "ð Hey Community Members! | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. More than 20 ways to get involved & stay informed! The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/psf/community-stories/",
    "title": "Community Stories | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. People who need to install Python packages get confused a lot. That's not their fault; that's because the volunteers who maintain the packaging tools don't have enough time to get everything organized so that it's clear and easy to use. In the last few years, the PSF has gotten grants and used that money to massively improve packaging. We overhauled PyPI and we're making pip more consistent, with genuine attention to user experience. With more funding, we can finally make managing packages pleasant as programming in Python. I started using Python in my PhD for performing different simulations of rotating black holes, black hole jets and their properties. I quickly fell in love with the language and its community and that led me to become a core dev some years ago. Since I became a core dev I have worked towards making Python faster and more versatile. Although most of my work is in the parser, the compiler pipeline and the garbage collector I like to work all over the place fixing bugs and trying to spot performance improvement opportunities. Hopefully, in the following years, we can push Python forward to the future by improving its general speed, making it more compatible with other implementations and improving the multi-core experience. Although this will be a challenging path and will require drastic changes in Python itself and its C-API, I am very excited about the things to come and what awaits for the Python Language and its community :) A huge turning point in my career was a Python workshop. I programmed a bit as a kid, and took a single CS class in college, and tried to poke along doing some self-study so I could get better. And then I attended a Boston Python Workshop for Women And Their Friends http://bostonpythonworkshop.com/, led by Jessica McKellar. We worked through well-designed exercises and I got hands-on practice that helped me get through that transition, from painstakingly copying individual lines and functions into the interpreter, to thinking in terms of Python's program flow. Years later, my Python and software management skills have been invaluable. I run a small business, where I've been able to hire, train, and mentor contract workers who then go on to get full-time programming jobs. We've helped overhaul the Python Package Index, mentored and\nrecruited contributors to Zulip, helped journalists open-source their code, and more. Right now I'm leading pip's work replacing its dependency resolver, which is a game-changer for future work making Python packaging easier to deal with. People who care about Python invested in running that workshop, and it's paid off very well in my case. Investing in Python events, and the organization that supports them, is a good bet. In 2015, as I was figuring out what I wanted to do next with my career, I attended PyCon North America in Montreal. I volunteered with the GNU Mailman team to expedite the release of Mailman 3.0. At the end of the sprint, I thought, \"maybe I could charge for this.\" Later that year, I started Changeset Consulting, where I expedite long-awaited releases for open source software projects. Through Changeset, I helped the PSF finish and deploy the new PyPI, and I'm working on pip's dependency resolver overhaul and a fresh release of GNU\nAutoconf. If it weren't for the PSF, my life would be very different, and probably a lot less interesting. An important community event that the PSF produces is PyCon US, where community members get a chance to receive training, share ideas, or even be mentored. At PyCon US 2019, Sumana Harihareswara helped Brian Rutledge make his 1st contribution to Python Packaging. Months later he became a co-maintainer of Twine, the upload utility for PyPI. As of May 2020, he successfully worked with a newer contributor to finish adding PEP 484 type annotations to Twine's codebase. This will make it easier for us to keep this code bug-free in the future, which means open source maintainers can have a smooth experience sharing their code with us on PyPI! My name is Iqbal and I have been using Python for 18 years. On a professional level Python as a programming tool has given me a career, and on a personal level as an ideology has helped me make friends and given me satisfaction by giving me the opportunity to be part of something bigger than myself. As member or lead of the different conference committee and also as conference chair, I have worked hard to make sure anyone can participate and contribute, regardless of personal situation, among others: Grants program were started for those that are financially constrained, quotas were setup for first time speakers, opportunities to present talks were initiated for non-english speakers, partnership with other groups such as Women Who Code and PyLadies to bolster underrepresented women groups in the conferences. Last but not least, in the beginning I also helped introduce a Code Of Conduct based on PSF's CoC to introduce a safe and welcoming environment for everyone. At that time, it wasn't common yet to have CoCs for conferences. The PSF has done a wonderful job by taking the lead and acting as a standard bearer for us in the rest of the world to follow. Initiatives such as their grants program that helps monetarily and also gives exposure to less known events, knowledge and know-how on how to manage conferences and meetups, and most importantly an access to other people within the community to share and source out ideas have been invaluable to us. Going forward, with the new normal, supporting virtual events is definitely an important thing which the PSF can help with. A step-by-step guide on how to manage virtual events, how tos for tools and a platform to expose events to the worldwide community would be very helpful. In a world that is widening in terms of inequality where we tend to take away more and more, I am happy to be part of a community led by the PSF that continues to share and give: through code, through knowledge, through money and through friendship. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/",
    "title": "Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Cuong Do, Software Architect YouTube.com Python programmability on Algorand makes the entire development lifecycle easier and means more affordable and efficient maintenance and upgrades going forward. Read more To simplify the adoption of FHE, which involves a complex and resource-intensive technological stack, Zama developed tools that streamline the integration of FHE into applications. Since Python is the de facto standard for building machine learning (ML) applications, it was an obvious choice to create an open-source FHE library in Python. Read more Maintaining our ever-evolving Python codebase poses an intricate challenge: how do we make updates to reflect the changing rules and regulations of 200+ global markets without compromising access to the systems that our engineers and traders use on a daily basis? While an inner layer of shared business logic enables coherency in our codebase performance, it also means small regulatory changes can impact many systems.\r\n\r\nIn this article, Python Engineer John Lekberg details how we use Python type annotations to minimize the time and risk involved in manual verification. Read more Prioritizing cutting-edge speed and supporting the rapid growth of Hudson River Trading’s codebase can have unintended effects that require innovative solutions. For those working on our Python codebase, this means addressing “code tangling,” the coupling of unrelated code through unintuitive import cycles. In this article, George Farcasiu, Noah Kim, Jacob Brugh, and Jiahao Li discuss how they mitigate the cost and time burden of this issue by creating new tools to analyze and untangle dependencies efficiently. Read more See All Using Python to build a solution for instant tokenized real estate redemptions Lincoln Loop: Building a sustainable business inspired by Python’s ethos Using Python for commercial cloud backup Using Python to make unstable APIs reliable Python for Financial Machine Learning at Union Investment See All How HyperFinity Is Streamlining Its Serverless Architecture with Snowflake's Snowpark for Python Reimagining data science with Python-based operators in Einblick’s visual canvas Using Python with Gretel.ai to Generate Synthetic Location Data See All Elementary school education: Is it love or just Python? Using Python to Automate Tedious Tasks Python in the Blind Audio Tactile Mapping System See All Python for Collaborative Robots Abridging clinical conversations using Python Getting to Know Python See All Python Powered CrossCompute Report Automation for eReliability Tracker Leads to Cost and Time Savings for the American Public Power Association Saving the world with Open Data and Python Frequentis TAPtools® - Python in Air Traffic Control See All Why Python Matters for the VR Community Python for Collaborative Drug Discovery Python To Help Meteorologists Python for Scientific Data Visualization Simulating Biomolecules with Python See All Zama Concrete ML: Simplifying Homomorphic Encryption for Python Machine Learning Building Robust Codebases with Python's Type Annotations Building a Dependency Graph of Our Python Codebase Bleeding Edge Dependency Testing Using Python How We Created a Python Development Framework for Non-Developers See All Python users want to know more about Python in the wild. Tell us your story Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/arts/",
    "title": "Arts | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/business/",
    "title": "Business | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/education/",
    "title": "Education | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/engineering/",
    "title": "Engineering | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/government/",
    "title": "Government | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/scientific/",
    "title": "Scientific | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/success-stories/category/software-development/",
    "title": "Software Development | Our Success Stories | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/blogs/",
    "title": "Our Blogs | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. July 24, 2025, 1:55 p.m. This year’s PSF Board Election nomination period opens next week on Tuesday, July 29th, 2:00 pm UTC and closes on Tuesday, August 12th, 2:00 pm UTC. Who runs for the board? People who care about the Python community, who want to see it flourish and grow, and also have a …\n                    Read more More July 22, 2025 July 16, 2025 July 9, 2025 July 8, 2025 July 8, 2025 Subscribe to Python Insider via: Also check out the Discussions on Python.org Python Insider by the Python Core Team is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Based on a work at blog.python.org. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/psf/newsletter/",
    "title": "PSF Newsletter Signup | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Please select all the ways you would like to hear from Python Software Foundation: You can unsubscribe at any time by clicking the link in the footer of our emails. For information about our privacy practices, please visit our website. We use Mailchimp as our marketing platform. By clicking below to subscribe, you acknowledge that your information will be transferred to Mailchimp for processing. Learn more about Mailchimp's privacy practices here. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "http://pyfound.blogspot.com/",
    "title": "Python Software Foundation News",
    "content": "News from the Python Software Foundation This year’s PSF Board Election nomination period opens next week on Tuesday, July 29th, 2:00 pm UTC and closes on Tuesday, August 12th, 2:00 pm UTC. Who runs for the board? People who care about the Python community, who want to see it flourish and grow, and also have a few hours a month to attend regular meetings, serve on committees, participate in conversations, and promote the Python community. Check out the following resources to learn more about the PSF, as well as what being a part of the PSF Board entails: Not sure what UTC is for you locally? Check this UTC time converter! You can nominate yourself or someone else. We encourage you to reach out to people before you nominate them to ensure they are enthusiastic about the potential of joining the Board. To submit a nomination for yourself or someone else, use the 2025 PSF Board Election Nomination Form on our website. The nomination form opens on Tuesday, July 29th, 2:00 pm UTC and closes on Tuesday, August 12th, 2:00 pm UTC.To support potential candidates and nominators, the 2025 PSF Board Election team has created a nomination resource (embedded below). It includes tips, formatting instructions, and guidance on what to include in a nomination. The goal is to help nominees understand what to expect and ensure that all candidates are provided the same clear and consistent standards. Every PSF Voting Member (Supporting, Contributing, and Fellow) needs to affirm their membership to vote in this year’s election. You should have received an email from \"psf@psfmember.org <Python Software Foundation>\" with the subject \"[Action Required] Affirm your PSF Membership voting intention for 2025 PSF Board Election\" that contains information on how to affirm your voting status. You can see your membership record and status on your PSF Member User Information page. If you are a voting-eligible member and do not already have a login, please create an account on psfmember.org first and then email psf-elections@python.org so we can link your membership to your account. Every PSF voting-eligible Member (Supporting, Contributing, and Fellow) needs to affirm their membership to vote in this year’s election. If you wish to vote in this year’s PSF Board election, you must affirm your intention to vote no later than Tuesday, August 26th, 2:00 pm UTC. This year’s Board Election vote begins Tuesday, September 2nd, 2:00 pm UTC, and closes on Tuesday, September 16th, 2:00 pm UTC. You should have received an email from \"psf@psfmember.org <Python Software Foundation>\" with the subject \"[Action Required] Affirm your PSF Membership voting intention for 2025 PSF Board Election\" that contains information on how to affirm your voting status. If you were expecting to receive the email but have not (make sure to check your spam!), please email psf-elections@pyfound.org, and we’ll assist you. Please note: If you opted out of emails related to your membership, you did not receive this email. Log on to psfmember.org and visit your PSF Member User Information page\n to see your membership record and status. If you are a voting-eligible \nmember (active Supporting, Contributing, and Fellow members of the PSF) \nand do not already have a login, please create an account on psfmember.org\n and then email psf-elections@pyfound.org so we can link your membership\n to your account. Please ensure you have an account linked to your \nmembership so that we can have the most up-to-date contact information \nfor you in the future. You can affirm your voting intention by following the steps in our video tutorial: Section 4.2 of the PSF Bylaws requires that “Members of any membership class with voting rights must affirm each year to the corporation in writing that such member intends to be a voting member for such year.” Our motivation is to ensure that our elections can meet quorum as required by Section 3.9 of our bylaws. As our membership has grown, we have seen that an increasing number of Contributing and Fellow members with indefinite membership do not engage with our annual election, making quorum difficult to reach. An election that does not reach quorum is invalid. This would cause the whole voting process to be re-held, resulting in fewer voters and an undue amount of effort on the part of PSF Staff. If you were formerly a Managing member, your membership has been updated to Contributing as of June 25th, 2025, per last year’s Bylaw change that merged Managing and Contributing memberships. Per another recent Bylaw change that allows for simplifying the voter affirmation process by treating past voting activity as intent to continue voting, if you voted last year, you will automatically be added to the 2025 voter roll. Please note: If you removed or changed your email on psfmember.org, you may not automatically be added to this year's voter roll. You’ll get an email from OpaVote with a ballot on or right before September 2nd, and then you can vote! Check out our PSF Membership page to learn more. If you have questions about membership, nominations, or this year’s Board election, please email psf-elections@pyfound.org or join the PSF Discord for the upcoming Board Office Hours on August 12th, 9 PM UTC. You are also welcome to join the discussion about the PSF Board election on our forum."
  },
  {
    "url": "http://planetpython.org/",
    "title": "Planet Python",
    "content": "Last update: August 04, 2025 07:42 PM UTC\n\n\n\n\n\nAugust 04, 2025\nReal Python\nSkip Ahead in Loops With Python's Continue Keyword\n\nPythonâs continue keyword functions as a statement that controls the flow of a loop. It allows you to skip code in a loop for the current iteration and jump immediately to the next one. Itâs used exclusively in for and while loops, letting you control the flow of execution, bypass specific conditions, and continue processing in a structured and predictable way.\nBy the end of this tutorial, youâll understand that:\n\nExecuting continue doesnât affect the else clause of a loop.\nUsing continue incorrectly may result in skipping necessary code.\nYou canât use continue in a function or class thatâs nested in a loop.\nOn a bytecode level, continue executes the same instructions as reaching the end of a loop.\n\nArmed with this knowledge, youâll be able to confidently write loops using continue and expand your skills as a Python programmer.\n\nGet Your Code: Click here to download the free sample code that shows you how to skip ahead in loops with Pythonâs continue keyword .\n\n\n Take the Quiz: Test your knowledge with our interactive âSkip Ahead in Loops With Python's Continue Keywordâ quiz. Youâll receive a score upon completion to help you track your learning progress:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Quiz\nSkip Ahead in Loops With Python's Continue Keyword\nTest your understanding of Python's continue keyword, which allows you to skip code in a loop for the current iteration and jump immediately to the next one.\n\n\n\nPythonâs continue Keyword\nLoops are control flow statements used to perform operations repeatedly a certain number of times. In a normal loop, the loop body runs from start to finish, with the number of iterations controlled by the type of loop:\n\nA for loop runs a specific number of times and is usually used to process a collection of data.\nA while loop runs as long as a specific condition evaluates to True. When the condition evaluates to False, the loop ends.\n\nIn both cases, you may find it useful to stop the execution of the loop body and move to the next iteration. The way to do that is with the continue keyword.\nIn any loop, continue stops the code currently executing, and jumps immediately back to the top of the loop, skipping to the next iteration.\nUnderstanding Its Behavior in for and while Loops\nIn a for loop, continue moves the iterator to the next item to be processed. If no other items are available, then the loop ends.\nAssume you have the following for loop that computes the sum of all numbers in a list:\n\n\nPython\n\n\n\n\ntotal = 0\n\nfor number in range(-10, 10):\n    total += number\n\nprint(total)\n\n\nCopied!\n\n\nThis works fine, but what if you want to add only the positive numbers, ignoring all the negative ones? You can modify this loop to add only positive numbers using continue:\n\n\nPython\n\n\n\n\ntotal = 0\n\nfor number in range(-10, 10):\n    if number < 0:\n        continue\n    total += number\n\nprint(total)\n\n\nCopied!\n\n\nIn this case, since Python executes continue only when the number is less than zero, it doesnât add those numbers to total.\nYouâve seen how the continue statement works in a for loopânow youâll see it working similarly in a while loop.\nIn a while loop, continue transfers control back to the condition at the top of the loop. If that condition is True, then the loop body will run again. If itâs False, then the loop ends.\nConsider the following while loop. It leverages Pythonâs walrus operator to get user input, casts it to an int, and adds the number to a running total. The loop stops when the user enters 0:\n\n\nPython\nsum_whole_numbers.py\n\n\n\n\nprint(\"Enter one whole number per input.\")\nprint(\"Type 0 to stop and display their sum:\")\n\ntotal = 0\n\nwhile (user_int := int(input(\"+ \"))) != 0:\n    total += user_int\n\nprint(f\"{total=}\")\n\n\nCopied!\n\n\nAgain, you only want to add the positive numbers that your users enter, so you modify the loop using continue:\n\n\nPython\nsum_whole_numbers.py\n\n\n\n\nprint(\"Enter one whole number per input.\")\nprint(\"Type 0 to stop and display their sum:\")\n\ntotal = 0\n\nwhile (user_int := int(input(\"+ \"))) != 0:\n    if user_int < 0:\n        continue\n    total += user_int\n\nprint(f\"{total=}\")\n\n\nCopied!\n\n\nYou can copy the code and try it out. When you run the script, Python keeps prompting you for input until you enter 0:\nRead the full article at https://realpython.com/python-continue/ Â»\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 02:00 PM UTC\n\nQuiz: First Steps With LangChain\n\nIn this quiz, you can test your knowledge on the fundamentals of LangChain, such as creating reusable instructions with prompt templates, LangChain chains, and LangChain’s debug mode.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Build a Scalable Flask Web Project From Scratch\n\nAre you ready to revisit the essentials of building Flask applications? In this quiz, you’ll practice Flask concepts such as views, blueprints, application factory patterns, and template usage.\nCheck your understanding of Flask fundamentals by exploring concepts covered in the tutorial Build a Scalable Flask Web Project From Scratch.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Intro to Object-Oriented Programming (OOP) in Python\n\nIn this quiz, you’ll revisit the fundamentals of object-oriented programming (OOP) in Python and how to work with classes, objects, and constructors.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Introduction to Web Scraping With Python\n\nIn this quiz, you’ll practice core concepts from Introduction to Web Scraping With Python. You’ll revisit what web scraping is, when to use Beautiful Soup, and how to install and configure your environment.\nYou’ll also try out key syntax for creating BeautifulSoup objects, explore common HTML parsers, and check your understanding of tools like MechanicalSoup.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Skip Ahead in Loops With Python's Continue Keyword\n\nIn this quiz, you’ll test your understanding of\nPython’s continue statement.\nThe continue statement allows you to skip code in a loop for the current iteration,\njumping immediately to the next iteration. It’s used exclusively in for and while loops,\nallowing you to control the flow of execution, bypass specific conditions,\nand continue processing in a structured and predictable manner.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Working With Python's Built-in Exceptions\n\nIn this quiz, you’ll revisit Python’s built-in exceptions, exploring their hierarchy and how to handle them gracefully. You’ll practice distinguishing between errors and exceptions, using try...except blocks, and identifying specific exceptions like IndexError and GeneratorExit.\nBrush up your skills by reviewing the Working With Python’s Built-in Exceptions course before you start!\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Using the \"or\" Boolean Operator in Python\n\nIn this quiz, you’ll test your understanding of the Python or Operator.\nYou’ll learn how to use it in both Boolean and non-Boolean contexts,\nand understand how it can be used to solve programming problems.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nQuiz: Python Namespace Packages\n\nIn this quiz, you’ll practice your knowledge about Python’s namespace packages.\nWhat are they used for? How do you set up a namespace package? How could you create one before PEP 420? Complete this quick quiz to test your knowledge.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 04, 2025 12:00 PM UTC\n\nEuroPython Society\nEuroPython 2025 Code of Conduct Transparency Report\n\nThe 2025 version of the EuroPython conference took place both online and in person in July 2025. This was the third conference under our current Code of Conduct (CoC), and we had Code of Conduct working group members continuously available both online and in person.ReportsOver the course of the conference, the Code of Conduct team was made aware of the following issue:One person was uncomfortable with certain phrases being used in one of the poster sessions. The author was informed, and the phrases reported were removed by the author from their poster presentation promptly.Thank you the Code of Conduct team responded to the issue reported.\n\nAugust 04, 2025 07:00 AM UTC\n\nTryton News\nAnnouncing âCode to Careâ: A Hands-On Tryton Technical Workshop (Aug 15â17)\n\nCode to Care â A Hands-On Tryton Technical Workshop\nWe are excited to announce âCode to Careâ, a 3-day live hands-on workshop on Tryton ERP customization, with a special focus on GNUHealth.\n\nDates: August 15â17, 2025\nMode: Online (Live Interactive Sessions)\nFee: â¹20,000 per participant\nLanguage: English\nRegister Interest: Expression of Interest â Code to Care: Tryton x GNUHealth Customisation Workshop\n\n About the Trainer\nWith over 10 years of hands-on experience with Tryton, the trainer has successfully implemented Tryton at Indiaâs largest medical institute, managing key domains such as HR, Payroll, Procurement, and clinical modules.\nWhat Youâll Explore\n\nHow to customise the existing modules of Tryton for your systemâs needs\nHow to build new modules to support or enhance your workflows and functionalities\nDeployment and maintenance best practices\nContributing to the Tryton and GNUHealth ecosystems\n\nGiving Back\nA portion of the participation fee will be donated to the:\n\nTryton Foundation\nGNUHealth Foundation\nopenSUSE Project\n\nFurther details will be shared with the interested participants over registered email address. Register your interest at Expression of Interest â Code to Care: Tryton x GNUHealth Customisation Workshop\n1 post - 1 participant\nRead full topic\n\nAugust 04, 2025 06:00 AM UTC\n\nStÃ©phane Wirtel\nRelancer lâÃ©criture de mon livre *Introduction Ã  Python 3.13*\n\nâï¸ Ãcrire (et rÃ©Ã©crire) mon livre Introduction Ã  Python 3.13\nQuand jâai commencÃ© mon parcours en tant quâindÃ©pendant, mon objectif Ã©tait limpide : donner des formations sur Python. Expliquer comment lâutiliser, comment le comprendre, le structurer, le manipuler. Bref, transmettre avec passion tout ce que jâavais appris au fil des annÃ©es.\nÃ cette Ã©poque, Python 2 Ã©tait encore bien prÃ©sent, mais Python 3 commenÃ§ait Ã  faire doucement son chemin. En 2014, on utilisait la version 3.4, et dÃ¨s 2015, la 3.5 Ã©tait disponible.\n\nAugust 04, 2025 12:00 AM UTC\n\nArmin Ronacher\nIn Support Of Shitty Types\n\nYou probably know that I love Rust and TypeScript, and I’m a big proponent of\ngood typing systems.  One of the reasons I find them useful is that they enable\nautocomplete, which is generally a good feature.  Having a well-integrated type\nsystem that makes sense and gives you optimization potential for memory layouts\nis generally a good idea.\nFrom that, you’d naturally think this would also be great for agentic coding\ntools.  There’s clearly some benefit to it.  If you have an agent write\nTypeScript and the agent adds types, it performs well.  I don’t know if it\noutperforms raw JavaScript, but at the very least it doesn’t seem to do any\nharm.\nBut most agentic tools don’t have access to an LSP (language server protocol).\nMy experiments with agentic coding tools that do have LSP access (with type\ninformation available) haven’t meaningfully benefited from it.  The LSP\nprotocol slows things down and pollutes the context significantly.  Also, the\nmodels haven’t been trained sufficiently to understand how to work with this\ninformation.  Just getting a type check failure from the compiler in text\nform yields better results.\nWhat you end up with is an agent coding loop that, without type checks enabled,\nresults in the agent making forward progress by writing code and putting types\nsomewhere.  As long as this compiles to some version of JavaScript (if you use\nBun, much of it ends up type-erased), it creates working code.  And from there\nit continues.  But that’s bad progressâit’s the type of progress where it\nneeds to come back after and clean up the types.\nIt’s curious because types are obviously being written but they’re largely\nbeing ignored.  If you do put the type check into the loop, my tests actually\nshowed worse performance.  That’s because the agent manages to get the code\nrunning, and only after it’s done does it run the type check.  Only then, maybe\nat a much later point, does it realize it made type errors.  Then it starts\nfixing them, maybe goes in a loop, and wastes a ton of context.  If you make it\ndo the type checks after every single edit, you end up eating even more into the\ncontext.\nThis gets really bad when the types themselves are incredibly complicated and\nnon-obvious.  TypeScript has arcane expression functionality, and some\nlibraries go overboard with complex constructs (e.g., conditional\ntypes).\nLLMs have little clue how to read any of this.  For instance, if you give it\naccess to the .d.ts files from TanStack Router and the forward declaration\nstuff it uses for the router system to work properly, it doesn’t understand any\nof it.  It guesses, and sometimes guesses badly.  It’s utterly confused.  When\nit runs into type errors, it performs all kinds of manipulations, none of which\nare helpful.\nPython typing has an even worse problem, because there we have to work with a\nvery complicated ecosystem where different type checkers cannot even agree on\nhow type checking should work.  That means that the LLM, at least from my\ntesting, is not even fully capable of understanding how to resolve type check\nerrors from tools which are not from mypy.  It’s not universally bad, but if\nyou actually end up with a complex type checking error that you cannot resolve\nyourself, it is shocking how the LLM is also often not able to fully figure out\nwhat’s going on, or at least needs multiple attempts.\nAs a shining example of types adding a lot of value we have Go.  Go’s types are\nmuch less expressive and very structural.  Things conform to interfaces purely\nby having certain methods.  The LLM does not need to understand much to\ncomprehend that.  Also, the types that Go has are rather strictly enforced.  If\nthey are wrong, it won’t compile.  Because Go has a much simpler type system\nthat doesn’t support complicated constructs, it works much betterâboth for LLMs\nto understand the code they produce and for the LLM to understand real-world\nlibraries you might give to an LLM. \nI don’t really know what to do with this, but these behaviors suggest there’s\na lot more value in best-effort type systems or type hints like JSDoc.  Because\nat least as far as the LLM is concerned, it doesn’t need to fully understand\nthe types, it just needs to have a rough understanding of what type some object\nprobably is.  For the LLM it’s more important that the type name in the error\nmessage aligns with the type name in source.\nI think it’s an interesting question whether this behavior of LLMs today will\ninfluence future language design.  I don’t know if it will, but I think it\ngives a lot of credence to some of the decisions that led to languages like Go\nand Java.  As critical as I have been in the past about their rather simple\napproaches to problems and having a design that maybe doesn’t hold developers\nin a particularly high regard, I now think that they actually are measurably in\na very good spot.  There is more elegance to their design than I gave it\ncredit for.\n\nAugust 04, 2025 12:00 AM UTC\n\nAugust 03, 2025\nThe Python Coding Stack\nFlashy, Fancy Shortcuts Aren't Always Suitable [Python Shorts]\n\nIn the previous post, I talked about Python's or keyword and how it doesn't behave the way you may expect it to. Here's the link to that article if you missed it: Do You Really Know How `or` And `and` Work in Python?One place you may see the or expression used is when dealing with the infamous mutable default value problem in functions–see the second section in Python Quirks? Party Tricks? Peculiarities Revealed… if you're not familiar with this Python banana skin. It seems some LLM models are keen to suggest this option, too.To keep this post brief, I'll assume you're familiar with both how or works and the mutable default value issue.Let's see how the or keyword is used to solve the mutable default value problem:All code blocks are available in text format at the end of this article • #1 • The code images used in this article are created using Snappify. [Affiliate link]I'm using this function for demonstration purposes. Note that the function mutates the list and returns it.But let's look at the part that's more relevant for this post. This function uses an empty list if no value is passed to the shopping_list parameter. Since you can't use an empty list as the default value, you use None as the default value.The or expression then does all the hard work:If you don't pass a list to the shopping_list parameter when you call add_to_shopping_list(), the function uses the None default value for shopping_list. And since None is falsy, the or expression evaluates to its second operand, the empty list [].However, if you already have a list with items in it and you pass it to the shopping_list parameter when you call add_to_shopping_list(), then this list is the one used within the function.Let's try it out to confirm this is how the function works.First, try with an existing list:#2You create a food_groceries list containing a few items. You then pass it to add_to_shopping_list(). You display output and food_groceries—these are names referring to the same list. They're not different lists. You can refresh your memory about Python's pass-by-assignment in functions here: If You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read on…This is what you expect. Great.How about using the default value now:#3There's no second argument when you call add_to_shopping_list() this time. Therefore, the function creates an empty list and appends \"Washing Up Liquid\" to it.Again, this is the behaviour you expect.So, using the or expression to deal with the mutable default value in functions is cool, right?Subscribe nowNow Consider This…Have a look at this scenario:#4This scenario seems similar to the first one earlier, the one with the food_groceries. You create a list called clothing_items and you then pass it to the add_to_shopping_list() function.But now, although output shows the expected result, the list clothing_items is still empty.Here's what's happening:The list clothing_items is an empty list.You pass it to add_to_shopping_list(), so shopping_list now refers to the same list as clothing_items within the function.It's now the or expression's turn within the function, shopping_list = shopping_list or []. But the identifier (name) shopping_list now refers to the same list that clothing_items refers to. This is an empty list. Therefore, it's falsy……and since the first operand of the or expression is falsy, the expression evaluates to the second operand, which is also an empty list.But—and this is the key point—the or expression creates a new empty list rather than using the existing empty list (the one that clothing_items refers to).So, you still have an empty list within your function, but it's not the same one you're expecting.That's why output and clothing_items are different now. They're different lists. This didn't happen in your first example when you used food_groceries. In that example, output and food_groceries both referred to the same list.Support The Python Coding StackThe standard way of solving the mutable default value problem doesn't face this issue:#5This textbook approach to the mutable default value issue is less fancy, perhaps, but it works without surprises. It's also more readable, and that's important in Python!Do you want to try video courses designed and delivered in the same style as these posts? You can get a free trial at The Python Coding Place, and you also get access to a members-only forum.Try Out The Python Coding PlacePhoto by Dan Cristian Pădureț: https://www.pexels.com/photo/photo-of-multicolored-abstract-painting-1193743/Code in this article uses Python 3.13The code images used in this article are created using Snappify. [Affiliate link]You can also support this publication by making a one-off contribution of any amount you wish.Support The Python Coding StackFor more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules.And you can find out more about me at stephengruppetta.comFurther reading related to this article’s topic:Do You Really Know How `or` And `and` Work in Python?Python Quirks? Party Tricks? Peculiarities RevealedIf You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read onAppendix: Code BlocksCode Block #1def add_to_shopping_list(item, shopping_list=None):\n    shopping_list = shopping_list or []\n    shopping_list.append(item)\n    return shopping_list\nCode Block #2food_groceries = [\"Milk\", \"Eggs\", \"Bread\"]\noutput = add_to_shopping_list(\"Chocolate\", food_groceries)\n\noutput\n# ['Milk', 'Eggs', 'Bread', 'Chocolate']\nfood_groceries\n# ['Milk', 'Eggs', 'Bread', 'Chocolate']\nCode Block #3household_items = add_to_shopping_list(\"Washing Up Liquid\")\nhousehold_items\n# ['Washing Up Liquid']\nCode Block #4clothing_items = []\noutput = add_to_shopping_list(\"Shirt\", clothing_items)\n\noutput\n# ['Shirt']\nclothing_items\n# []\nCode Block #5def add_to_shopping_list(item, shopping_list=None):\n    if shopping_list is None:\n        shopping_list = []\n    shopping_list.append(item)\n    return shopping_list\n\nclothing_items = []\noutput = add_to_shopping_list(\"Shirt\", clothing_items)\n\noutput\n# ['Shirt']\nclothing_items\n# ['Shirt']\nFor more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules.And you can find out more about me at stephengruppetta.com\n\nAugust 03, 2025 07:54 PM UTC\n\nDjango Weblog\nDSF member of the month - Jake Howard\n\nFor July 2025, we welcome Jake Howard as our DSF member of the month! â­\nJake actively shares his knowledge through blog posts and community talks. He is part of the Security Team Working Group and he created the DEP 14. He has been a DSF member since June 2024. \nYou can learn more about Jake by visiting Jake's website and his GitHub Profile.\nLetâs spend some time getting to know Jake better!\nCan you tell us a little about yourself (hobbies, education, etc)\nIâm Jake. Iâm a Senior Systems Engineer at Torchbox, where Iâve been for a little over 4 years. âSystems Engineerâ is a fairly loaded title, and means different things to different people. I like to describe it as doing everything technical to do with Software Engineering which isnât Programming (Sysadmin, Devops, IT support, Security, Networking), but also doing a fair bit of Programming.\nMost of my hobbies revolve around technology. Iâm an avid self-hoster, running applications on servers both in âthe cloudâ and in my house. Thereâs been a server of some kind in my house for the last 10 years. Iâm generally quite a private person, so I like to know whatâs happening to my data. Since I started working remotely at the start of the 2020 pandemic, Iâve channeled some of this passion into posts on my website, with posts about all manner of things Iâve done from self-hosting to general software engineering.\nAway from my desk (sort of), Iâm a volunteer for Student Robotics, inspiring college students into STEM through competitive robotics (no, not quite like Robot Wars). In school, I was always the quiet one, but now I seem completely at home with public speaking, commentary and otherwise being in front of large crowds of people. I wish I knew the secret - Iâd make millions!\nMy GitHub is also pretty active, with contributions all over the place (OpenZFS, Nebula VPN, Gitea, Plausible Analytics, OpenCV, Ansibleâ¦).\nIâm curious, where your nickname âRealOrangeOneâ comes from?\nBecause a lot of life happens online (especially in the last 5 years), many people havenât even seen pictures of me, let alone met me in person. I am not in fact a talking piece of fruit. For a while, I tried to stay anonymous, avoiding photos or videos of me on the internet. But since I discovered I enjoy public speaking, Iâve sort of given up on that (for the most part).\nBy now, Iâm sure many people have speak. But, for those who donât know: I, like my father before me, am ginger ð¥ (the hair colour, not the plant).\nThe exact specifics of how being ginger lead to âTheOrangeOneâ are sadly lost to time. Iâve owned theorangeone.net for well over a decade at this point. Unfortunately, itâs not a particularly original nickname, and I have to be fast to claim it when signing up to new services. In some places (where I wasnât fast enough) Iâm forced to sub out âTheâ for âRealâ, which has lead to some confusions, but not too many. Canonically, I prefer âTheOrangeOneâ, but as we all know, naming things is hard.\nHow did you start using Django?\nIâve been using Django since around the 1.8 release. My job at the time was at a Django development agency, so it was the first real Python framework Iâd used. The first few weeks there was my first exposure to Django, pip, package management and collaborative software engineering - it was quite a lot to learn at once. I didnât realise it at the time, but I was working working as a junior alongside a couple fairly well-known names in the Django community like Tom Christie (DRF, Starlette, HTTPX) and Jamie Matthews (django-readers, django-zen-queries). We mostly built single-page apps with React, so I learned Django and Django Rest Framework at the same time, which means I now often have to look back at the docs to remember how forms and templates work.\nAs for contributing to Django, that came much later. My first commit to Django was in May 2024. Having used Django for a while, and written plenty of packages, Iâd never stopped to look at how upstream was developed. Around the time of DEP 14 kicking off, I needed to look a bit more at the inner workings of the Django project, to learn what was in store for me. When scrolling through Trac tickets, I found an interesting looking ticket, and got to work. At the time of writing, Iâve now closed 9 Trac tickets across 12 PRs, and some pretty cool features (simple block tags, better Accept header parsing, performance improvements to the URL router) now have my name on them (metaphorically speaking).\nI wouldnât call myself an âactiveâ contributor, but I try and keep an eye on the tickets and forum threads which interest me the most, and chime in when I can.\nWhat other framework do you know and if there is anything you would like to have in Django if you had magical powers?\nSince itâs the first framework I learned, and so far has done everything I need, Iâve mostly used Django. For a few smaller services, Iâve leaned more towards Starlette and AIOHTTP, but for anything even slightly large Iâve just used Django - since Iâd end up recreating much of Django using the smaller frameworks anyway. A better (likely official) path for single-file Django (ie without some of the magic module handling) might help draw a few more people in and fill a few more of these âmicro-serviceâ style use-cases.\nIâm a class-based views person - I like the encapsulation and easy extension of base views. As with any opinion on the internet, Iâm sure many people disagree with me, but to me itâs just personal preference. Iâm still surprised itâs a pattern not seen by many other Python frameworks.\nFollowing in the footsteps of Python, I often wonder if Django could also do with some dead battery removal (or at least extracting into separate packages). Django is a pretty big framework, and whilst the contrib apps are intended to be separate, they also require hooks and assumptions in other areas of the codebase. I might be wrong (it happens quite a lot), but I suspect some of those packages would be better suited externally, perhaps improving some developer momentum - and lightening the load for the Fellows. Djangoâs sitemap and syndication (RSS) frameworks are 2 places I wish would get some more love.\nOutside of Python, Iâm a big fan of Rust (as cliche as it may be). Whilst Rust is a popular language, there isnât really a âDjangoâ like (batteries included) framework - itâs all composing the pieces you need yourself. However, that doesnât stop people being very productive with it. As a result, most of the frameworks have very generic interfaces, letting developers pass state around as needed, rather than trying to do everything themselves. Outside of the obvious static typing debate (which Iâm in favour of), Iâd love to see Django embrace some dependencies, especially if they bring some performance improvements. It may end up being a bad idea, but it might also help those who want to use Djangoâs modules outside of Django.\nMany years ago, I tried to be a polyglot - switching between different programming languages (and frameworks) to find new ways of working and match the problem to the correct solution. Now, Iâve settled mostly on Python and Rust. They fit my needs well, Iâm very productive in them, and between the 2 thereâs not much they canât handle. Given my background, and the fact most sysadmin-y tools are written in it, Iâm really not a fan of Go.\nWhat projects are you working on now?\nOver time, Iâve slowly stepped back from having big side projects - being a new dad sure takes up time and energy. Large projects ended up feeling too much like work outside of work, and I end up either getting distracted or bored. After work, I want to do something fun, not that seems like yet another job. Iâm the kind of person who gets the sudden urge to research something interesting for an evening, dive in, then not think about it again for several weeks. Itâs not the most productive way of doing things, which is why my posts are all over the place, but it doesnât feel much like work for me - I lean heavily on what interests me at any time to drive what I want to do.\nWith that said, Iâm currently in the process of rebuilding my website. Of course, both the current and new versions are built on Django, but the new build should be easier to maintain, faster, and hopefully wonât need rewriting again in just a couple years. Most of my other projects have been small tools to make my home server that bit nicer.\nProfessionally, Iâm not really a developer anymore. As a sysadmin (ish), much of my day-to-day doesnât involve much programming. I spend much more of my time deploying, monitoring and administering Django applications than I do writing them. My main project at the moment is helping port a large Java / JS deployment over to Django and Wagtail, running on Kubernetes with some very high and interesting stability and scaling requirements. Since most of my professional live has been at software agencies, Iâve tended to bounce between different projects, rather than sitting on a single one. So Iâm also supporting on a few other smaller projects as and when Iâm needed.\nWhich Django libraries are your favorite (core or 3rd party)?\ndjango-tasks, of course!\nâ¦\nOh right, a serious answerâ¦\nI have to say, one of the most underrated modules in Django is django.utils. Itâs not as glamourous as the ORM, forms or cache, but itâs a treasure trove of useful methods. I personally always like looking at the internal helper functions large frameworks use - see the problems theyâve had to solve time and time again. Whilst thereâs not the same stability guarantees, Iâve definitely been helped out on a few occasions by some undocumented functions.\nIn that theme, Iâm a fan of libraries which do one thing and do it well. I quite like small libraries which aim to solve a problem. Thereâs definitely a line before that becomes a problem (anyone remember left-pad?), but libraries which scope creep are often harder to work with than the more narrow-scoped ones, whilst the smaller ones just keep on working and making my life easier. For example, django-environ makes reading and parsing environment variables into settings really easy and clean, and django-decorator-include helps including other urlpatterns whilst wrapping them in a decorator - particularly helpful for 3rd-party packageâs URLs.\nFinally, Iâve got a real soft-spot for whitenoise (and ServeStatic for ASGI users). Djangoâs documentation deters people pretty hard from serving media and static files using Django - and rightly so in performance-critical environments. However, for most people, having to additionally maintain (and secure) nginx is more maintenance than necessary. whitenoise serves static files using Django directly, without any extra configuration, whilst also pre-compressing files for a nice performance boost. To me, itâs such a universally-useful library, Iâd love to see it it included in Django itself someday.\nIâll throw a bonus shout out for granian, a new (ish) WSGI / ASGI server written in Rust. gunicorn has a near monopoly on running Python apps in production, especially in the WSGI space, so itâs nice to see a newcomer. granian isnât always faster, but doing the HTTP handling in Rust (and using popular libraries to do it) can improve stability and throughput, without holding the GIL. Iâve not run anything in production with it yet, but Iâve been using it on personal projects for almost a year without issue.\nWhat are the top three things in Django that you like?\nContrary to what Iâve already said, I actually like Djangoâs batteries. Sure, thereâs quite a few âdeadâ ones in need of some cleaning up and TLC, but having most of what I need already installed makes me far more productive. I donât need to think about how to render my form on the page, save the results as a model, or properly handle errors - everything âjust worksâ, and works together. Sure, batteries have their downsides - it makes swapping them out rather difficult, but Iâd rather ship my feature sooner than compare the trade-offs of different ORMs. The auto-reloading in django-tasks is only around 8 lines of code thanks to django.utils.autoreload being so easy to hook in to.\nSecondly: Forms, but not for the reasons you might think. Most forms are created to take submissions from the user, validate them, then probably save them to a model. However, theyâre great as general data validation. Iâve written plenty of views with complex querystring requirements, and leaning on forms to validate them saves a lot of boilerplate code. Sure, pydantic might be a bit faster and have more features, but given Iâm already productive with django.forms, and itâs already installed and well understood by other developers in my team, I donât feel the need to reach for something else.\nFinally, I wouldnât say itâs quite a âfavouriteâ, and itâs well-known as being far-from-perfect, but Iâve got a real soft-spot for the Django Admin. It lets me focus on building the core of an application, rather than the internal interface - particularly when there are no strong requirements for it, or itâs only going to be used by me and a few others. Since itâs a fair raw view of the database by default, Iâve definitely been bitten by some less-than-restrictive permissions, but thereâs generally all the hooks I need. I donât like building frontends, so only needing to build 1 rather than 2 makes me a lot happier, especially if it comes with authentication, permissions, read-only views and a dark mode ð!\nHow did you join the security team?\nIâd love to say itâs an interesting story, stroking my ego that I saved the day. But the reality is, as usual, far less glamorous.\nAs an engineer, Iâve tended towards 2 specialties: Security and Performance, which usually go hand-in-hand. In early 2023, I was invited to join the Wagtail CMS Security team after reporting and subsequently helping fix a memory exhaustion issue. I was already involved in all things security at Torchbox, especially our ISO-27001 certification, so I was already known when I submitted a vulnerability report.\nThibaud mentioned to me late last year that the project potentially looking for new members of the security team, to help with resourcing and some potential process improvements within the foundation. I naturally jumped at the opportunity - since the team is generally closed to new members and âfully-staffedâ. After a few gentle reminders (heâs a busy guy), I received a message from Sarah formally inviting me in March.\nSince then, Iâve tried to review every report which came through, and helped author a few patches. A few reports even had to be raised upstream with Pythonâs Security Response Team (PSRT). Itâs been an interesting experience, and Iâm looking forward to seeing how the team developers over the coming years.\nIâm aware that you have created DEP 14 on the Background Workers, how the work is going so far? Do you need support from the community on anything?\nDEP 14 (the proposal to add a native background workers API to Django) has been a really interesting journey. Iâm beyond humbled to see the community interest behind it. When I started down this road, Iâd only intended to start the conversations and help rally the community interest. Since then, and 6000 lines of code later, Iâm mostly single-handedly writing a database-backed production-grade task system.\nRight now, weâre at a bit of a cross-roads. Many of the foundational parts work, relatively well. The difficulty comes with the more complex features: Retries, dependencies, robust execution. Building a task system is easy - building a reliable one people want to actually use is incredibly difficult. If anyone out there is interested in getting involved, please do! Report issues, fix bugs, contribute to design discussions. Most of the APIs are based on what I think looks sensible. Software this large, pivotal and complex canât be built in isolation - so it needs a diverse audience to ensure we (I) make the right decisions, and design an API people actually want to use that will last and scale for years to come.\nThe next challenge on my list to tackle is timeouts - a highly requested feature. It sounds simple, but the reality is far from it. Many of those challenges sparked the topic of my upcoming PyCon UK talk later this year.\nDjango is celebrating its 20th anniversary this month. Any nice story to share?\nMy personal highlight was DjangoCon Europe 2024 - my first DjangoCon. I ended up bringing the stereotypically grey British weather with me, but I had a great week chatting Django with some interesting people, and putting faces to the names and handles Iâd seen online. After the talk announcing DEP 14 and background tasks, I was inundated with people voicing their support - many wondering how itâd taken this long.\nBut personally, Iâm more interested in whatâs to come. Of course, thereâs django-tasks, but the next sets of releases are shaping up to be pretty interesting. Over the last 3-4 years or so, Iâve personally noticed a bit of a resurgence in peopleâs appetites for change in Django. The 6.x Steering Council have a lot of interesting ideas, and clearly the community agree. People are happy with what Django can do now, but want to bring it a little more up-to-date - and are happy to put in the work to do it. Only a few weeks ago, django-csp was included in core, making it easier to make more secure applications. Iâm sure thatâs just the start. The fact people are still keen on working on a framework which just celebrated 20 years shows it must be doing something right!\nIs there anything else youâd like to say?\nIâd like to thank whoever nominated me to be a DSF member in the first place. To this date, I have no idea who you are.\nBeyond that, Iâm just looking forward to seeing what comes of Django, and Python in general over the next few years.\n\nThank you for doing the interview, Jake !\n\nAugust 03, 2025 01:20 PM UTC\n\nAugust 01, 2025\nReal Python\nThe Real Python Podcast â Episode #259: Design Patterns That Don't Translate to Python\n\nDo the design patterns learned in other programming languages translate to coding in Python? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ]\n\nAugust 01, 2025 12:00 PM UTC\n\nZero to Mastery\n[July 2025] Python Monthly Newsletter ð\n\n68th issue of Andrei Neagoie's must-read monthly Python Newsletter: Useless Design Patterns, Django turns 20, 330Ã faster Python, and much more. Read the full newsletter to get up-to-date with everything you need to know from last month.\n\nAugust 01, 2025 10:00 AM UTC\n\nWingware\nWing Python IDE Version 11.0.3 - August 1, 2025\n\nWing Python IDE version 11.0.3 has been released.  It improves Python code analysis,\nfixes problems debugging Django templates, fixes refactoring when the target file\nis in a hidden directory, and makes a number of other minor improvements.\nDownloads\nBe sure to Check for Updates in Wing's Help menu after downloading, to make\nsure that you have the latest hot fixes.\nWing Pro 11.0.3\nWing Personal 11.0.3\nWing 101 11.0.3\nWing 10 and earlier versions are not affected by installation of Wing 11 and\nmay be installed and used independently. However, project files for Wing 10\nand earlier are converted when opened by Wing 11 and should be saved under a\nnew name, since Wing 11 projects cannot be opened by older versions of Wing.\n New in Wing 11Improved AI Assisted DevelopmentWing 11 improves the user interface for AI assisted development by introducing two separate\ntools AI Coder and AI Chat. AI Coder can be used to write, redesign, or extend code\nin the current editor. AI Chat can be used to ask about code or iterate in creating a\ndesign or new code without directly modifying the code in an editor.\nWing 11's AI assisted development features now support not just OpenAI but also Claude, Grok,\nGemini, Perplexity, Mistral, Deepseek, and any other OpenAI completions API compatible AI provider.\nThis release also improves setting up AI request context, so that both automatically and\nmanually selected and described context items may be paired with an AI request. AI request\ncontexts can now be stored, optionally so they are shared by all projects, and may be used\nindependently with different AI features.\nAI requests can now also be stored in the current project or shared with all projects, and Wing\ncomes preconfigured with a set of commonly used requests. In addition to changing code in the\ncurrent editor, stored requests may create a new untitled file or run instead in AI Chat. Wing\n11 also introduces options for changing code within an editor, including replacing code,\ncommenting out code, or starting a diff/merge session to either accept or reject changes.\nWing 11 also supports using AI to generate commit messages based on the changes being committed\nto a revision control system.\nYou can now also configure multiple AI providers for easier access to different models.\nFor details see AI Assisted Development under Wing Manual in Wing 11's Help menu.\nPackage Management with uv Wing Pro 11 adds support for the uv package manager in the New Project dialog and the\nPackages tool.\nFor details see Project Manager > Creating Projects > Creating Python Environments and Package\nManager > Package Management with uv under Wing Manual in Wing 11's Help menu.\nImproved Python Code AnalysisWing 11 makes substantial improvements to Python code analysis, with better support for\nliterals such as dicts and sets, parametrized type aliases, typing.Self, type of\nvariables on the def or class line that declares them, generic classes with\n[...], __all__ in *.pyi files, subscripts in typing.Type and similar, type\naliases, type hints in strings, type[...] and tuple[...],\n@functools.cached_property, base classes found also in .pyi files, and\ntyping.Literal[...].\nUpdated LocalizationsWing 11 updates the German, French, and Russian localizations, and introduces a new experimental\nAI-generated Spanish localization. The Spanish localization and the new AI-generated strings in the\nFrench and Russian localizations may be accessed with the new User Interface > Include AI\nTranslated Strings preference.\nImproved diff/mergeWing Pro 11 adds floating buttons directly between the editors to make navigating differences\nand merging easier, allows undoing previously merged changes, and does a better job managing\nscratch buffers, scroll locking, and sizing of merged ranges.\nFor details see Difference and Merge under Wing Manual in Wing 11's Help menu.\nOther Minor Features and ImprovementsWing 11 also improves the custom key binding assignment user interface, adds a Files >\nAuto-Save Files When Wing Loses Focus preference, warns immediately when opening a project with\nan invalid Python Executable configuration, allows clearing recent menus, expands the set of\navailable special environment variables for project configuration, and makes a number of other\nbug fixes and usability improvements.\nChanges and IncompatibilitiesSince Wing 11 replaced the AI tool with AI Coder and AI Chat, and AI\nconfiguration is completely different than in Wing 10, you will need to reconfigure your\nAI integration manually in Wing 11. This is done with Manage AI Providers in the\nAI menu. After adding the first provider configuration, Wing will set that provider as\nthe default. You can switch between providers with Switch to Provider in the AI menu.\nIf you have questions, please don't hesitate to contact us at support@wingware.com.\n\nAugust 01, 2025 01:00 AM UTC\n\nMatt Layman\nPython and AI workflow with LangGraph\n\nIn this stream, I worked on a personal AI workflow that I’m building using LangGraph. I discussed human-in-the-loop and how to bring a person into the workflow process.\n\nAugust 01, 2025 12:00 AM UTC\n\nHoloViz\nPlotting made easy with hvPlot: 0.12 release\n\n\n\nAugust 01, 2025 12:00 AM UTC\n\nmeejah.ca\nShWiM: peer-to-peer terminal sharing\n\nSHell WIth Me combines magic-wormhole and tty-share for e2ee, p2p terminal sharing\n\nAugust 01, 2025 12:00 AM UTC\n\nJuly 31, 2025\nPython Morsels\nNested functions in Python\n\nFunctions in Python can be defined within another function.\n\n\n\nTable of contents\n\nA function defined within a function\nA function returned from another function\nThe enclosing scope\nClosures with nested functions\nWhy nest functions within functions?\nDecorators involve nested functions\nNested functions are possible in Python\n\n\n\n\nA function defined within a function\nPython's functions can be defined pretty much anywhere.\nYou can even define a function inside a function:\ndef greet_me(name=\"friend\"):\n    def greet():\n        print(\"Hello\", name)\n    greet()\n\nWhen we call this greet_me function, it defines a greet a function and then calls that function:\n>>> greet_me()\nHello friend\n\nNote that the inner function is allowed to use the name from the outer function.\nA function returned from another function\nInstead of calling our inner â¦\n\nRead the full article: https://www.pythonmorsels.com/nested-functions/\n\nJuly 31, 2025 03:30 PM UTC\n\nDjango Weblog\nDjangonaut Space is looking for contributors to be mentors\n\nHello Django ð Universe!\nð°ï¸â This is Djangonaut Space phoning home about Session 5! We're recruiting technical mentors (Navigators) to join our next ðstellarð mission.\nð©âð We are looking for people who regularly contribute to Django or a Django related package, that want to mentor others. Our next session will be Oct-Nov.\nð Come join us and be a cosmic contributor! Express your interest to be a mentor here.\nð Want to learn more about what it means to be a Navigator:\n\nHere's a high-level overview of the role\nHere's the workbook each Navigator is provided\n\nð¤ Interested people will have to complete a 30 minute meet & greet type interview with organizers.\nâ If you're interested in applying to be a Djangonaut, applications will open and close in September (dates to be determined). The latest information will be posted on our site, djangonaut.space. Please follow our social media accounts or subscribe to our newsletter for announcements.\nâï¸ We'll see you around the cosmos!\nDjangonaut Space session organizers\n\nJuly 31, 2025 02:34 PM UTC\n\nPyCharm\n\n\n\nThe Bazel Plugin for IntelliJ IDEA Is Now Generally Available!\nAfter much anticipation, we are finally ready to announce the general availability (GA) of the new Bazel plugin for IntelliJ IDEA, PyCharm, and GoLand â now developed by JetBrains! After months of focused development and valuable feedback from our EAP users, we’re officially launching our revamped Bazel experience.\nWhile we’ve been shipping updates regularly, the leap to our 2025.2 GA release marks a major milestone. Even though our primary focus for this release was on creating the best experience we can for Java, Kotlin, and Scala developers, we also brought support for the Python and Go ecosystems, and we will continue to maintain and improve it in the coming releases.\nIf you’re migrating from the previous plugin originally released by Google, you’ll notice a more straightforward workflow that aligns with the standard JetBrains IDE experience you expect from other build tool integrations such as Maven and Gradle. Now, let’s dive into what’s new!\nKey features in 2025.2\n\nBazel Query in Action\n\nGo is a go. We’re officially rolling out support for Go. You can now import your Go targets in Bazel projects into both IntelliJ IDEA (with the Go plugin) and GoLand. This brings the full IDE experience you rely on: code highlighting, completion, navigation, and the ability to run, debug, and get coverage for your tests.\nBuilt-in Bazel Query tool window: Go beyond sync and build with Bazel queries integrated directly into your IDE via their own dedicated tool window. Craft your queries with syntax completion and a helpful UI for flags to explore your project’s dependency graph without ever leaving the editor.\nDramatically faster indexing: We’ve optimized indexing to get you to your code faster. You can now use the import_depth and import_ijars settings in your .bazelproject file to prevent the indexing of deep transitive dependencies and index only header jars instead of full jars. Whatâs more, only the files directly referenced in your .bazelproject view are fully indexed for code intelligence, which can slash indexing times and memory usage in large projects with many auxiliary files.\n\nNew plugin, new user experience\nBack in December, we publicly announced the EAP (Early Access Program) version of our new plugin and defined what it would take to release it into GA, with an overview of the main differences between the original plugin and the new one.\nHereâs a quick recap for those moving from the older plugin: We’ve smoothed out the rough edges to make Bazel feel like a natural part of the IDE.\n\nSimplified project import: The old import wizard is a thing of the past. Now, simply open a directory containing your MODULE.bazel or WORKSPACE file. For more control, you can open a specific .bazelproject view file. If you manage a large monorepo, you can provide a default template for your team by checking in a template at tools/intellij/.managed.bazelproject.\nRedesigned UI elements: The Bazel tool window is now your central hub for actions like resyncing your project (with a Build and Resync option for generating sources) and keeping track of targets in your working set. We’ve also added a widget listing all targets the currently opened file belongs to. It allows you to run actions on these targets (build / test / jump to BUILD file / copy target label)\nReworked target mapping for JVM projects: A core improvement is the new internal representation for JVM targets, which mirrors the actual Bazel graph. This fundamental change enables more accurate highlighting, more accurate completions and more reliable refactoring.\n\nImprovements since 2025.1\nWindows compatibility\nWe understand that development doesn’t just happen on one OS. Thatâs why we worked on making our plugin compatible with Microsoft Windows, bringing most of the feature set to our Windows-based users.\nEnhanced Bazel configuration support\nWe believe editing your build files should be as easy as editing your source code, which is why we’ve improved the user experience for all Bazel-related configuration files.\nStarlark (.bzl, BUILD)\n\nStarlark Quick Documentation\n\nQuick documentation for Starlark rules: Hover over a Starlark rule or function to see its documentation directly in the editor. You’ll also get documentation as you type, guiding you through available parameters.\nAutomatic formatting: If you have buildifier on your PATH, the plugin will now automatically format your Starlark files on save\n\nBazel module configuration file (MODULE.bazel)\n\nIntelligent editing: The MODULE.bazel editor now offers smart completions for arguments and displays documentation as you edit.\n\nBazel project view file (.bazelproject) \n\n.bazelproject view highlighting and completions\n\nGuided editing: Get completions for section names and known values. The editor will now highlight completely unsupported sections as errors and sections that are supported in the old plugin originally by Google (but not in the new one) as warnings.\nManage directories from the Project view file tree: You can now right-click a directory in the project tree to add or remove it from your .bazelproject file, thus loading or unloading that directory in IntelliJ.\n\nBazelisk configuration file (.bazelversion):\n\nStay up to date: The editor will now highlight outdated Bazel versions specified in your .bazelversion file and offer a quick-fix to update to the latest release.\n\nLanguage ecosystem enhancements\n\nJVM:\n\nThe underlying project model mapping has been further improved, resulting in better performance during sync and more reliable refactorings for targets where glob patterns match the whole directory.\n\n\nScala:\n\nThe Bazel plugin now respects the scalacopts parameter in your scala_* targets, which unlocks Scala 3 highlighting features with the -Xsource:3 flag. At the same time, we’ve updated the Scala plugin to provide native integration with the Bazel plugin out of the box.\n\n\nPython:\n\nRun from the gutter: py_test and py_binary targets now get the familiar green Run arrow in the editor gutter.\nImproved dependency resolution: Python dependencies are now resolved correctly, enabling code navigation and eliminating false error highlighting.\nInterpreter from MODULE.bazel: The plugin now sets the Python interpreter based on what is defined in MODULE.bazel. This includes support for hermetic toolchains downloaded by rules_python â meaning you don’t need to have Python installed locally on your machine.\nDebugging: You can now attach the debugger to py_test targets.\n\n\n\nWhat happens to the Bazel plugin by Google?\nThe Bazel for IntelliJ plugin (also known as IJwB) by Google is being deprecated. Google has transferred the code ownership and maintenance to JetBrains. We will keep providing compatibility updates for new IntelliJ versions and critical fixes only throughout the year of 2025, but will be fully deprecating it in 2026. All our development effort for IntelliJ IDEA, GoLand, and PyCharm is now focused on the new plugin.\nThe Bazel for CLion plugin (CLwB) has also been transferred to JetBrains, and will continue to be actively developed. Learn more in the post Enhancing Bazel Support for CLion on the CLion Blog.\nGot feedback? Weâre listening!\nWe’re committed to making this the best Bazel experience possible. Please report any issues, ideas, or improvements straight to our issue tracker.\nFixed the problem yourself? We accept PRs on our hirschgarten repository.\nYou’ll also find us on the Bazel Community Slack, in the #intellij channel.\nHappy building!\n\nJuly 31, 2025 01:40 PM UTC\n\nBazel Plugin Release: General Availability\n\n\n\nJuly 31, 2025 10:20 AM UTC Pythonâs continue keyword functions as a statement that controls the flow of a loop. It allows you to skip code in a loop for the current iteration and jump immediately to the next one. Itâs used exclusively in for and while loops, letting you control the flow of execution, bypass specific conditions, and continue processing in a structured and predictable way.\nBy the end of this tutorial, youâll understand that:\n\nExecuting continue doesnât affect the else clause of a loop.\nUsing continue incorrectly may result in skipping necessary code.\nYou canât use continue in a function or class thatâs nested in a loop.\nOn a bytecode level, continue executes the same instructions as reaching the end of a loop.\n\nArmed with this knowledge, youâll be able to confidently write loops using continue and expand your skills as a Python programmer.\n\nGet Your Code: Click here to download the free sample code that shows you how to skip ahead in loops with Pythonâs continue keyword .\n\n\n Take the Quiz: Test your knowledge with our interactive âSkip Ahead in Loops With Python's Continue Keywordâ quiz. Youâll receive a score upon completion to help you track your learning progress:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Quiz\nSkip Ahead in Loops With Python's Continue Keyword\nTest your understanding of Python's continue keyword, which allows you to skip code in a loop for the current iteration and jump immediately to the next one.\n\n\n\nPythonâs continue Keyword\nLoops are control flow statements used to perform operations repeatedly a certain number of times. In a normal loop, the loop body runs from start to finish, with the number of iterations controlled by the type of loop:\n\nA for loop runs a specific number of times and is usually used to process a collection of data.\nA while loop runs as long as a specific condition evaluates to True. When the condition evaluates to False, the loop ends.\n\nIn both cases, you may find it useful to stop the execution of the loop body and move to the next iteration. The way to do that is with the continue keyword.\nIn any loop, continue stops the code currently executing, and jumps immediately back to the top of the loop, skipping to the next iteration.\nUnderstanding Its Behavior in for and while Loops\nIn a for loop, continue moves the iterator to the next item to be processed. If no other items are available, then the loop ends.\nAssume you have the following for loop that computes the sum of all numbers in a list:\n\n\nPython\n\n\n\n\ntotal = 0\n\nfor number in range(-10, 10):\n    total += number\n\nprint(total)\n\n\nCopied!\n\n\nThis works fine, but what if you want to add only the positive numbers, ignoring all the negative ones? You can modify this loop to add only positive numbers using continue:\n\n\nPython\n\n\n\n\ntotal = 0\n\nfor number in range(-10, 10):\n    if number < 0:\n        continue\n    total += number\n\nprint(total)\n\n\nCopied!\n\n\nIn this case, since Python executes continue only when the number is less than zero, it doesnât add those numbers to total.\nYouâve seen how the continue statement works in a for loopânow youâll see it working similarly in a while loop.\nIn a while loop, continue transfers control back to the condition at the top of the loop. If that condition is True, then the loop body will run again. If itâs False, then the loop ends.\nConsider the following while loop. It leverages Pythonâs walrus operator to get user input, casts it to an int, and adds the number to a running total. The loop stops when the user enters 0:\n\n\nPython\nsum_whole_numbers.py\n\n\n\n\nprint(\"Enter one whole number per input.\")\nprint(\"Type 0 to stop and display their sum:\")\n\ntotal = 0\n\nwhile (user_int := int(input(\"+ \"))) != 0:\n    total += user_int\n\nprint(f\"{total=}\")\n\n\nCopied!\n\n\nAgain, you only want to add the positive numbers that your users enter, so you modify the loop using continue:\n\n\nPython\nsum_whole_numbers.py\n\n\n\n\nprint(\"Enter one whole number per input.\")\nprint(\"Type 0 to stop and display their sum:\")\n\ntotal = 0\n\nwhile (user_int := int(input(\"+ \"))) != 0:\n    if user_int < 0:\n        continue\n    total += user_int\n\nprint(f\"{total=}\")\n\n\nCopied!\n\n\nYou can copy the code and try it out. When you run the script, Python keeps prompting you for input until you enter 0:\nRead the full article at https://realpython.com/python-continue/ Â»\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] Pythonâs continue keyword functions as a statement that controls the flow of a loop. It allows you to skip code in a loop for the current iteration and jump immediately to the next one. Itâs used exclusively in for and while loops, letting you control the flow of execution, bypass specific conditions, and continue processing in a structured and predictable way. By the end of this tutorial, youâll understand that: Armed with this knowledge, youâll be able to confidently write loops using continue and expand your skills as a Python programmer. Get Your Code: Click here to download the free sample code that shows you how to skip ahead in loops with Pythonâs continue keyword . Take the Quiz: Test your knowledge with our interactive âSkip Ahead in Loops With Python's Continue Keywordâ quiz. Youâll receive a score upon completion to help you track your learning progress: Interactive Quiz Test your understanding of Python's continue keyword, which allows you to skip code in a loop for the current iteration and jump immediately to the next one. Loops are control flow statements used to perform operations repeatedly a certain number of times. In a normal loop, the loop body runs from start to finish, with the number of iterations controlled by the type of loop: In both cases, you may find it useful to stop the execution of the loop body and move to the next iteration. The way to do that is with the continue keyword. In any loop, continue stops the code currently executing, and jumps immediately back to the top of the loop, skipping to the next iteration. In a for loop, continue moves the iterator to the next item to be processed. If no other items are available, then the loop ends. Assume you have the following for loop that computes the sum of all numbers in a list: This works fine, but what if you want to add only the positive numbers, ignoring all the negative ones? You can modify this loop to add only positive numbers using continue: In this case, since Python executes continue only when the number is less than zero, it doesnât add those numbers to total. Youâve seen how the continue statement works in a for loopânow youâll see it working similarly in a while loop. In a while loop, continue transfers control back to the condition at the top of the loop. If that condition is True, then the loop body will run again. If itâs False, then the loop ends. Consider the following while loop. It leverages Pythonâs walrus operator to get user input, casts it to an int, and adds the number to a running total. The loop stops when the user enters 0: Again, you only want to add the positive numbers that your users enter, so you modify the loop using continue: You can copy the code and try it out. When you run the script, Python keeps prompting you for input until you enter 0: [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 02:00 PM UTC In this quiz, you can test your knowledge on the fundamentals of LangChain, such as creating reusable instructions with prompt templates, LangChain chains, and LangChain’s debug mode.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you can test your knowledge on the fundamentals of LangChain, such as creating reusable instructions with prompt templates, LangChain chains, and LangChain’s debug mode. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC Are you ready to revisit the essentials of building Flask applications? In this quiz, you’ll practice Flask concepts such as views, blueprints, application factory patterns, and template usage.\nCheck your understanding of Flask fundamentals by exploring concepts covered in the tutorial Build a Scalable Flask Web Project From Scratch.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] Are you ready to revisit the essentials of building Flask applications? In this quiz, you’ll practice Flask concepts such as views, blueprints, application factory patterns, and template usage. Check your understanding of Flask fundamentals by exploring concepts covered in the tutorial Build a Scalable Flask Web Project From Scratch. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll revisit the fundamentals of object-oriented programming (OOP) in Python and how to work with classes, objects, and constructors.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll revisit the fundamentals of object-oriented programming (OOP) in Python and how to work with classes, objects, and constructors. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll practice core concepts from Introduction to Web Scraping With Python. You’ll revisit what web scraping is, when to use Beautiful Soup, and how to install and configure your environment.\nYou’ll also try out key syntax for creating BeautifulSoup objects, explore common HTML parsers, and check your understanding of tools like MechanicalSoup.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll practice core concepts from Introduction to Web Scraping With Python. You’ll revisit what web scraping is, when to use Beautiful Soup, and how to install and configure your environment. You’ll also try out key syntax for creating BeautifulSoup objects, explore common HTML parsers, and check your understanding of tools like MechanicalSoup. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll test your understanding of\nPython’s continue statement.\nThe continue statement allows you to skip code in a loop for the current iteration,\njumping immediately to the next iteration. It’s used exclusively in for and while loops,\nallowing you to control the flow of execution, bypass specific conditions,\nand continue processing in a structured and predictable manner.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll test your understanding of\nPython’s continue statement. The continue statement allows you to skip code in a loop for the current iteration,\njumping immediately to the next iteration. It’s used exclusively in for and while loops,\nallowing you to control the flow of execution, bypass specific conditions,\nand continue processing in a structured and predictable manner. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll revisit Python’s built-in exceptions, exploring their hierarchy and how to handle them gracefully. You’ll practice distinguishing between errors and exceptions, using try...except blocks, and identifying specific exceptions like IndexError and GeneratorExit.\nBrush up your skills by reviewing the Working With Python’s Built-in Exceptions course before you start!\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll revisit Python’s built-in exceptions, exploring their hierarchy and how to handle them gracefully. You’ll practice distinguishing between errors and exceptions, using try...except blocks, and identifying specific exceptions like IndexError and GeneratorExit. Brush up your skills by reviewing the Working With Python’s Built-in Exceptions course before you start! [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll test your understanding of the Python or Operator.\nYou’ll learn how to use it in both Boolean and non-Boolean contexts,\nand understand how it can be used to solve programming problems.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll test your understanding of the Python or Operator.\nYou’ll learn how to use it in both Boolean and non-Boolean contexts,\nand understand how it can be used to solve programming problems. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC In this quiz, you’ll practice your knowledge about Python’s namespace packages.\nWhat are they used for? How do you set up a namespace package? How could you create one before PEP 420? Complete this quick quiz to test your knowledge.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] In this quiz, you’ll practice your knowledge about Python’s namespace packages. What are they used for? How do you set up a namespace package? How could you create one before PEP 420? Complete this quick quiz to test your knowledge. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 04, 2025 12:00 PM UTC The 2025 version of the EuroPython conference took place both online and in person in July 2025. This was the third conference under our current Code of Conduct (CoC), and we had Code of Conduct working group members continuously available both online and in person.ReportsOver the course of the conference, the Code of Conduct team was made aware of the following issue:One person was uncomfortable with certain phrases being used in one of the poster sessions. The author was informed, and the phrases reported were removed by the author from their poster presentation promptly.Thank you the Code of Conduct team responded to the issue reported. The 2025 version of the EuroPython conference took place both online and in person in July 2025. This was the third conference under our current Code of Conduct (CoC), and we had Code of Conduct working group members continuously available both online and in person. Over the course of the conference, the Code of Conduct team was made aware of the following issue: Thank you the Code of Conduct team responded to the issue reported. August 04, 2025 07:00 AM UTC Code to Care â A Hands-On Tryton Technical Workshop\nWe are excited to announce âCode to Careâ, a 3-day live hands-on workshop on Tryton ERP customization, with a special focus on GNUHealth.\n\nDates: August 15â17, 2025\nMode: Online (Live Interactive Sessions)\nFee: â¹20,000 per participant\nLanguage: English\nRegister Interest: Expression of Interest â Code to Care: Tryton x GNUHealth Customisation Workshop\n\n About the Trainer\nWith over 10 years of hands-on experience with Tryton, the trainer has successfully implemented Tryton at Indiaâs largest medical institute, managing key domains such as HR, Payroll, Procurement, and clinical modules.\nWhat Youâll Explore\n\nHow to customise the existing modules of Tryton for your systemâs needs\nHow to build new modules to support or enhance your workflows and functionalities\nDeployment and maintenance best practices\nContributing to the Tryton and GNUHealth ecosystems\n\nGiving Back\nA portion of the participation fee will be donated to the:\n\nTryton Foundation\nGNUHealth Foundation\nopenSUSE Project\n\nFurther details will be shared with the interested participants over registered email address. Register your interest at Expression of Interest â Code to Care: Tryton x GNUHealth Customisation Workshop\n1 post - 1 participant\nRead full topic We are excited to announce âCode to Careâ, a 3-day live hands-on workshop on Tryton ERP customization, with a special focus on GNUHealth. With over 10 years of hands-on experience with Tryton, the trainer has successfully implemented Tryton at Indiaâs largest medical institute, managing key domains such as HR, Payroll, Procurement, and clinical modules. A portion of the participation fee will be donated to the: Further details will be shared with the interested participants over registered email address. Register your interest at Expression of Interest â Code to Care: Tryton x GNUHealth Customisation Workshop 1 post - 1 participant Read full topic August 04, 2025 06:00 AM UTC âï¸ Ãcrire (et rÃ©Ã©crire) mon livre Introduction Ã  Python 3.13\nQuand jâai commencÃ© mon parcours en tant quâindÃ©pendant, mon objectif Ã©tait limpide : donner des formations sur Python. Expliquer comment lâutiliser, comment le comprendre, le structurer, le manipuler. Bref, transmettre avec passion tout ce que jâavais appris au fil des annÃ©es.\nÃ cette Ã©poque, Python 2 Ã©tait encore bien prÃ©sent, mais Python 3 commenÃ§ait Ã  faire doucement son chemin. En 2014, on utilisait la version 3.4, et dÃ¨s 2015, la 3.5 Ã©tait disponible. Quand jâai commencÃ© mon parcours en tant quâindÃ©pendant, mon objectif Ã©tait limpide : donner des formations sur Python. Expliquer comment lâutiliser, comment le comprendre, le structurer, le manipuler. Bref, transmettre avec passion tout ce que jâavais appris au fil des annÃ©es. Ã cette Ã©poque, Python 2 Ã©tait encore bien prÃ©sent, mais Python 3 commenÃ§ait Ã  faire doucement son chemin. En 2014, on utilisait la version 3.4, et dÃ¨s 2015, la 3.5 Ã©tait disponible. August 04, 2025 12:00 AM UTC You probably know that I love Rust and TypeScript, and I’m a big proponent of\ngood typing systems.  One of the reasons I find them useful is that they enable\nautocomplete, which is generally a good feature.  Having a well-integrated type\nsystem that makes sense and gives you optimization potential for memory layouts\nis generally a good idea.\nFrom that, you’d naturally think this would also be great for agentic coding\ntools.  There’s clearly some benefit to it.  If you have an agent write\nTypeScript and the agent adds types, it performs well.  I don’t know if it\noutperforms raw JavaScript, but at the very least it doesn’t seem to do any\nharm.\nBut most agentic tools don’t have access to an LSP (language server protocol).\nMy experiments with agentic coding tools that do have LSP access (with type\ninformation available) haven’t meaningfully benefited from it.  The LSP\nprotocol slows things down and pollutes the context significantly.  Also, the\nmodels haven’t been trained sufficiently to understand how to work with this\ninformation.  Just getting a type check failure from the compiler in text\nform yields better results.\nWhat you end up with is an agent coding loop that, without type checks enabled,\nresults in the agent making forward progress by writing code and putting types\nsomewhere.  As long as this compiles to some version of JavaScript (if you use\nBun, much of it ends up type-erased), it creates working code.  And from there\nit continues.  But that’s bad progressâit’s the type of progress where it\nneeds to come back after and clean up the types.\nIt’s curious because types are obviously being written but they’re largely\nbeing ignored.  If you do put the type check into the loop, my tests actually\nshowed worse performance.  That’s because the agent manages to get the code\nrunning, and only after it’s done does it run the type check.  Only then, maybe\nat a much later point, does it realize it made type errors.  Then it starts\nfixing them, maybe goes in a loop, and wastes a ton of context.  If you make it\ndo the type checks after every single edit, you end up eating even more into the\ncontext.\nThis gets really bad when the types themselves are incredibly complicated and\nnon-obvious.  TypeScript has arcane expression functionality, and some\nlibraries go overboard with complex constructs (e.g., conditional\ntypes).\nLLMs have little clue how to read any of this.  For instance, if you give it\naccess to the .d.ts files from TanStack Router and the forward declaration\nstuff it uses for the router system to work properly, it doesn’t understand any\nof it.  It guesses, and sometimes guesses badly.  It’s utterly confused.  When\nit runs into type errors, it performs all kinds of manipulations, none of which\nare helpful.\nPython typing has an even worse problem, because there we have to work with a\nvery complicated ecosystem where different type checkers cannot even agree on\nhow type checking should work.  That means that the LLM, at least from my\ntesting, is not even fully capable of understanding how to resolve type check\nerrors from tools which are not from mypy.  It’s not universally bad, but if\nyou actually end up with a complex type checking error that you cannot resolve\nyourself, it is shocking how the LLM is also often not able to fully figure out\nwhat’s going on, or at least needs multiple attempts.\nAs a shining example of types adding a lot of value we have Go.  Go’s types are\nmuch less expressive and very structural.  Things conform to interfaces purely\nby having certain methods.  The LLM does not need to understand much to\ncomprehend that.  Also, the types that Go has are rather strictly enforced.  If\nthey are wrong, it won’t compile.  Because Go has a much simpler type system\nthat doesn’t support complicated constructs, it works much betterâboth for LLMs\nto understand the code they produce and for the LLM to understand real-world\nlibraries you might give to an LLM. \nI don’t really know what to do with this, but these behaviors suggest there’s\na lot more value in best-effort type systems or type hints like JSDoc.  Because\nat least as far as the LLM is concerned, it doesn’t need to fully understand\nthe types, it just needs to have a rough understanding of what type some object\nprobably is.  For the LLM it’s more important that the type name in the error\nmessage aligns with the type name in source.\nI think it’s an interesting question whether this behavior of LLMs today will\ninfluence future language design.  I don’t know if it will, but I think it\ngives a lot of credence to some of the decisions that led to languages like Go\nand Java.  As critical as I have been in the past about their rather simple\napproaches to problems and having a design that maybe doesn’t hold developers\nin a particularly high regard, I now think that they actually are measurably in\na very good spot.  There is more elegance to their design than I gave it\ncredit for. You probably know that I love Rust and TypeScript, and I’m a big proponent of\ngood typing systems.  One of the reasons I find them useful is that they enable\nautocomplete, which is generally a good feature.  Having a well-integrated type\nsystem that makes sense and gives you optimization potential for memory layouts\nis generally a good idea. From that, you’d naturally think this would also be great for agentic coding\ntools.  There’s clearly some benefit to it.  If you have an agent write\nTypeScript and the agent adds types, it performs well.  I don’t know if it\noutperforms raw JavaScript, but at the very least it doesn’t seem to do any\nharm. But most agentic tools don’t have access to an LSP (language server protocol).\nMy experiments with agentic coding tools that do have LSP access (with type\ninformation available) haven’t meaningfully benefited from it.  The LSP\nprotocol slows things down and pollutes the context significantly.  Also, the\nmodels haven’t been trained sufficiently to understand how to work with this\ninformation.  Just getting a type check failure from the compiler in text\nform yields better results. What you end up with is an agent coding loop that, without type checks enabled,\nresults in the agent making forward progress by writing code and putting types\nsomewhere.  As long as this compiles to some version of JavaScript (if you use\nBun, much of it ends up type-erased), it creates working code.  And from there\nit continues.  But that’s bad progressâit’s the type of progress where it\nneeds to come back after and clean up the types. It’s curious because types are obviously being written but they’re largely\nbeing ignored.  If you do put the type check into the loop, my tests actually\nshowed worse performance.  That’s because the agent manages to get the code\nrunning, and only after it’s done does it run the type check.  Only then, maybe\nat a much later point, does it realize it made type errors.  Then it starts\nfixing them, maybe goes in a loop, and wastes a ton of context.  If you make it\ndo the type checks after every single edit, you end up eating even more into the\ncontext. This gets really bad when the types themselves are incredibly complicated and\nnon-obvious.  TypeScript has arcane expression functionality, and some\nlibraries go overboard with complex constructs (e.g., conditional\ntypes).\nLLMs have little clue how to read any of this.  For instance, if you give it\naccess to the .d.ts files from TanStack Router and the forward declaration\nstuff it uses for the router system to work properly, it doesn’t understand any\nof it.  It guesses, and sometimes guesses badly.  It’s utterly confused.  When\nit runs into type errors, it performs all kinds of manipulations, none of which\nare helpful. Python typing has an even worse problem, because there we have to work with a\nvery complicated ecosystem where different type checkers cannot even agree on\nhow type checking should work.  That means that the LLM, at least from my\ntesting, is not even fully capable of understanding how to resolve type check\nerrors from tools which are not from mypy.  It’s not universally bad, but if\nyou actually end up with a complex type checking error that you cannot resolve\nyourself, it is shocking how the LLM is also often not able to fully figure out\nwhat’s going on, or at least needs multiple attempts. As a shining example of types adding a lot of value we have Go.  Go’s types are\nmuch less expressive and very structural.  Things conform to interfaces purely\nby having certain methods.  The LLM does not need to understand much to\ncomprehend that.  Also, the types that Go has are rather strictly enforced.  If\nthey are wrong, it won’t compile.  Because Go has a much simpler type system\nthat doesn’t support complicated constructs, it works much betterâboth for LLMs\nto understand the code they produce and for the LLM to understand real-world\nlibraries you might give to an LLM. I don’t really know what to do with this, but these behaviors suggest there’s\na lot more value in best-effort type systems or type hints like JSDoc.  Because\nat least as far as the LLM is concerned, it doesn’t need to fully understand\nthe types, it just needs to have a rough understanding of what type some object\nprobably is.  For the LLM it’s more important that the type name in the error\nmessage aligns with the type name in source. I think it’s an interesting question whether this behavior of LLMs today will\ninfluence future language design.  I don’t know if it will, but I think it\ngives a lot of credence to some of the decisions that led to languages like Go\nand Java.  As critical as I have been in the past about their rather simple\napproaches to problems and having a design that maybe doesn’t hold developers\nin a particularly high regard, I now think that they actually are measurably in\na very good spot.  There is more elegance to their design than I gave it\ncredit for. August 04, 2025 12:00 AM UTC In the previous post, I talked about Python's or keyword and how it doesn't behave the way you may expect it to. Here's the link to that article if you missed it: Do You Really Know How `or` And `and` Work in Python?One place you may see the or expression used is when dealing with the infamous mutable default value problem in functions–see the second section in Python Quirks? Party Tricks? Peculiarities Revealed… if you're not familiar with this Python banana skin. It seems some LLM models are keen to suggest this option, too.To keep this post brief, I'll assume you're familiar with both how or works and the mutable default value issue.Let's see how the or keyword is used to solve the mutable default value problem:All code blocks are available in text format at the end of this article • #1 • The code images used in this article are created using Snappify. [Affiliate link]I'm using this function for demonstration purposes. Note that the function mutates the list and returns it.But let's look at the part that's more relevant for this post. This function uses an empty list if no value is passed to the shopping_list parameter. Since you can't use an empty list as the default value, you use None as the default value.The or expression then does all the hard work:If you don't pass a list to the shopping_list parameter when you call add_to_shopping_list(), the function uses the None default value for shopping_list. And since None is falsy, the or expression evaluates to its second operand, the empty list [].However, if you already have a list with items in it and you pass it to the shopping_list parameter when you call add_to_shopping_list(), then this list is the one used within the function.Let's try it out to confirm this is how the function works.First, try with an existing list:#2You create a food_groceries list containing a few items. You then pass it to add_to_shopping_list(). You display output and food_groceries—these are names referring to the same list. They're not different lists. You can refresh your memory about Python's pass-by-assignment in functions here: If You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read on…This is what you expect. Great.How about using the default value now:#3There's no second argument when you call add_to_shopping_list() this time. Therefore, the function creates an empty list and appends \"Washing Up Liquid\" to it.Again, this is the behaviour you expect.So, using the or expression to deal with the mutable default value in functions is cool, right?Subscribe nowNow Consider This…Have a look at this scenario:#4This scenario seems similar to the first one earlier, the one with the food_groceries. You create a list called clothing_items and you then pass it to the add_to_shopping_list() function.But now, although output shows the expected result, the list clothing_items is still empty.Here's what's happening:The list clothing_items is an empty list.You pass it to add_to_shopping_list(), so shopping_list now refers to the same list as clothing_items within the function.It's now the or expression's turn within the function, shopping_list = shopping_list or []. But the identifier (name) shopping_list now refers to the same list that clothing_items refers to. This is an empty list. Therefore, it's falsy……and since the first operand of the or expression is falsy, the expression evaluates to the second operand, which is also an empty list.But—and this is the key point—the or expression creates a new empty list rather than using the existing empty list (the one that clothing_items refers to).So, you still have an empty list within your function, but it's not the same one you're expecting.That's why output and clothing_items are different now. They're different lists. This didn't happen in your first example when you used food_groceries. In that example, output and food_groceries both referred to the same list.Support The Python Coding StackThe standard way of solving the mutable default value problem doesn't face this issue:#5This textbook approach to the mutable default value issue is less fancy, perhaps, but it works without surprises. It's also more readable, and that's important in Python!Do you want to try video courses designed and delivered in the same style as these posts? You can get a free trial at The Python Coding Place, and you also get access to a members-only forum.Try Out The Python Coding PlacePhoto by Dan Cristian Pădureț: https://www.pexels.com/photo/photo-of-multicolored-abstract-painting-1193743/Code in this article uses Python 3.13The code images used in this article are created using Snappify. [Affiliate link]You can also support this publication by making a one-off contribution of any amount you wish.Support The Python Coding StackFor more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules.And you can find out more about me at stephengruppetta.comFurther reading related to this article’s topic:Do You Really Know How `or` And `and` Work in Python?Python Quirks? Party Tricks? Peculiarities RevealedIf You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read onAppendix: Code BlocksCode Block #1def add_to_shopping_list(item, shopping_list=None):\n    shopping_list = shopping_list or []\n    shopping_list.append(item)\n    return shopping_list\nCode Block #2food_groceries = [\"Milk\", \"Eggs\", \"Bread\"]\noutput = add_to_shopping_list(\"Chocolate\", food_groceries)\n\noutput\n# ['Milk', 'Eggs', 'Bread', 'Chocolate']\nfood_groceries\n# ['Milk', 'Eggs', 'Bread', 'Chocolate']\nCode Block #3household_items = add_to_shopping_list(\"Washing Up Liquid\")\nhousehold_items\n# ['Washing Up Liquid']\nCode Block #4clothing_items = []\noutput = add_to_shopping_list(\"Shirt\", clothing_items)\n\noutput\n# ['Shirt']\nclothing_items\n# []\nCode Block #5def add_to_shopping_list(item, shopping_list=None):\n    if shopping_list is None:\n        shopping_list = []\n    shopping_list.append(item)\n    return shopping_list\n\nclothing_items = []\noutput = add_to_shopping_list(\"Shirt\", clothing_items)\n\noutput\n# ['Shirt']\nclothing_items\n# ['Shirt']\nFor more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there!Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules.And you can find out more about me at stephengruppetta.com In the previous post, I talked about Python's or keyword and how it doesn't behave the way you may expect it to. Here's the link to that article if you missed it: Do You Really Know How `or` And `and` Work in Python? One place you may see the or expression used is when dealing with the infamous mutable default value problem in functions–see the second section in Python Quirks? Party Tricks? Peculiarities Revealed… if you're not familiar with this Python banana skin. It seems some LLM models are keen to suggest this option, too. To keep this post brief, I'll assume you're familiar with both how or works and the mutable default value issue. Let's see how the or keyword is used to solve the mutable default value problem: I'm using this function for demonstration purposes. Note that the function mutates the list and returns it. But let's look at the part that's more relevant for this post. This function uses an empty list if no value is passed to the shopping_list parameter. Since you can't use an empty list as the default value, you use None as the default value. The or expression then does all the hard work: If you don't pass a list to the shopping_list parameter when you call add_to_shopping_list(), the function uses the None default value for shopping_list. And since None is falsy, the or expression evaluates to its second operand, the empty list []. However, if you already have a list with items in it and you pass it to the shopping_list parameter when you call add_to_shopping_list(), then this list is the one used within the function. Let's try it out to confirm this is how the function works. First, try with an existing list: You create a food_groceries list containing a few items. You then pass it to add_to_shopping_list(). You display output and food_groceries—these are names referring to the same list. They're not different lists. You can refresh your memory about Python's pass-by-assignment in functions here: If You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read on… This is what you expect. Great. How about using the default value now: There's no second argument when you call add_to_shopping_list() this time. Therefore, the function creates an empty list and appends \"Washing Up Liquid\" to it. Again, this is the behaviour you expect. So, using the or expression to deal with the mutable default value in functions is cool, right? Subscribe now Have a look at this scenario: This scenario seems similar to the first one earlier, the one with the food_groceries. You create a list called clothing_items and you then pass it to the add_to_shopping_list() function. But now, although output shows the expected result, the list clothing_items is still empty. Here's what's happening: The list clothing_items is an empty list. You pass it to add_to_shopping_list(), so shopping_list now refers to the same list as clothing_items within the function. It's now the or expression's turn within the function, shopping_list = shopping_list or []. But the identifier (name) shopping_list now refers to the same list that clothing_items refers to. This is an empty list. Therefore, it's falsy… …and since the first operand of the or expression is falsy, the expression evaluates to the second operand, which is also an empty list. But—and this is the key point—the or expression creates a new empty list rather than using the existing empty list (the one that clothing_items refers to). So, you still have an empty list within your function, but it's not the same one you're expecting. That's why output and clothing_items are different now. They're different lists. This didn't happen in your first example when you used food_groceries. In that example, output and food_groceries both referred to the same list. Support The Python Coding Stack The standard way of solving the mutable default value problem doesn't face this issue: This textbook approach to the mutable default value issue is less fancy, perhaps, but it works without surprises. It's also more readable, and that's important in Python! Do you want to try video courses designed and delivered in the same style as these posts? You can get a free trial at The Python Coding Place, and you also get access to a members-only forum. Try Out The Python Coding Place Photo by Dan Cristian Pădureț: https://www.pexels.com/photo/photo-of-multicolored-abstract-painting-1193743/ Code in this article uses Python 3.13 The code images used in this article are created using Snappify. [Affiliate link] You can also support this publication by making a one-off contribution of any amount you wish. Support The Python Coding Stack For more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there! Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules. And you can find out more about me at stephengruppetta.com Further reading related to this article’s topic: Do You Really Know How `or` And `and` Work in Python? Python Quirks? Party Tricks? Peculiarities Revealed If You Haven't Got A Clue What \"Pass By Value\" or \"Pass By Reference\" mean, read on For more Python resources, you can also visit Real Python—you may even stumble on one of my own articles or courses there! Also, are you interested in technical writing? You’d like to make your own writing more narrative, more engaging, more memorable? Have a look at Breaking the Rules. And you can find out more about me at stephengruppetta.com August 03, 2025 07:54 PM UTC For July 2025, we welcome Jake Howard as our DSF member of the month! â­\nJake actively shares his knowledge through blog posts and community talks. He is part of the Security Team Working Group and he created the DEP 14. He has been a DSF member since June 2024. \nYou can learn more about Jake by visiting Jake's website and his GitHub Profile.\nLetâs spend some time getting to know Jake better!\nCan you tell us a little about yourself (hobbies, education, etc)\nIâm Jake. Iâm a Senior Systems Engineer at Torchbox, where Iâve been for a little over 4 years. âSystems Engineerâ is a fairly loaded title, and means different things to different people. I like to describe it as doing everything technical to do with Software Engineering which isnât Programming (Sysadmin, Devops, IT support, Security, Networking), but also doing a fair bit of Programming.\nMost of my hobbies revolve around technology. Iâm an avid self-hoster, running applications on servers both in âthe cloudâ and in my house. Thereâs been a server of some kind in my house for the last 10 years. Iâm generally quite a private person, so I like to know whatâs happening to my data. Since I started working remotely at the start of the 2020 pandemic, Iâve channeled some of this passion into posts on my website, with posts about all manner of things Iâve done from self-hosting to general software engineering.\nAway from my desk (sort of), Iâm a volunteer for Student Robotics, inspiring college students into STEM through competitive robotics (no, not quite like Robot Wars). In school, I was always the quiet one, but now I seem completely at home with public speaking, commentary and otherwise being in front of large crowds of people. I wish I knew the secret - Iâd make millions!\nMy GitHub is also pretty active, with contributions all over the place (OpenZFS, Nebula VPN, Gitea, Plausible Analytics, OpenCV, Ansibleâ¦).\nIâm curious, where your nickname âRealOrangeOneâ comes from?\nBecause a lot of life happens online (especially in the last 5 years), many people havenât even seen pictures of me, let alone met me in person. I am not in fact a talking piece of fruit. For a while, I tried to stay anonymous, avoiding photos or videos of me on the internet. But since I discovered I enjoy public speaking, Iâve sort of given up on that (for the most part).\nBy now, Iâm sure many people have speak. But, for those who donât know: I, like my father before me, am ginger ð¥ (the hair colour, not the plant).\nThe exact specifics of how being ginger lead to âTheOrangeOneâ are sadly lost to time. Iâve owned theorangeone.net for well over a decade at this point. Unfortunately, itâs not a particularly original nickname, and I have to be fast to claim it when signing up to new services. In some places (where I wasnât fast enough) Iâm forced to sub out âTheâ for âRealâ, which has lead to some confusions, but not too many. Canonically, I prefer âTheOrangeOneâ, but as we all know, naming things is hard.\nHow did you start using Django?\nIâve been using Django since around the 1.8 release. My job at the time was at a Django development agency, so it was the first real Python framework Iâd used. The first few weeks there was my first exposure to Django, pip, package management and collaborative software engineering - it was quite a lot to learn at once. I didnât realise it at the time, but I was working working as a junior alongside a couple fairly well-known names in the Django community like Tom Christie (DRF, Starlette, HTTPX) and Jamie Matthews (django-readers, django-zen-queries). We mostly built single-page apps with React, so I learned Django and Django Rest Framework at the same time, which means I now often have to look back at the docs to remember how forms and templates work.\nAs for contributing to Django, that came much later. My first commit to Django was in May 2024. Having used Django for a while, and written plenty of packages, Iâd never stopped to look at how upstream was developed. Around the time of DEP 14 kicking off, I needed to look a bit more at the inner workings of the Django project, to learn what was in store for me. When scrolling through Trac tickets, I found an interesting looking ticket, and got to work. At the time of writing, Iâve now closed 9 Trac tickets across 12 PRs, and some pretty cool features (simple block tags, better Accept header parsing, performance improvements to the URL router) now have my name on them (metaphorically speaking).\nI wouldnât call myself an âactiveâ contributor, but I try and keep an eye on the tickets and forum threads which interest me the most, and chime in when I can.\nWhat other framework do you know and if there is anything you would like to have in Django if you had magical powers?\nSince itâs the first framework I learned, and so far has done everything I need, Iâve mostly used Django. For a few smaller services, Iâve leaned more towards Starlette and AIOHTTP, but for anything even slightly large Iâve just used Django - since Iâd end up recreating much of Django using the smaller frameworks anyway. A better (likely official) path for single-file Django (ie without some of the magic module handling) might help draw a few more people in and fill a few more of these âmicro-serviceâ style use-cases.\nIâm a class-based views person - I like the encapsulation and easy extension of base views. As with any opinion on the internet, Iâm sure many people disagree with me, but to me itâs just personal preference. Iâm still surprised itâs a pattern not seen by many other Python frameworks.\nFollowing in the footsteps of Python, I often wonder if Django could also do with some dead battery removal (or at least extracting into separate packages). Django is a pretty big framework, and whilst the contrib apps are intended to be separate, they also require hooks and assumptions in other areas of the codebase. I might be wrong (it happens quite a lot), but I suspect some of those packages would be better suited externally, perhaps improving some developer momentum - and lightening the load for the Fellows. Djangoâs sitemap and syndication (RSS) frameworks are 2 places I wish would get some more love.\nOutside of Python, Iâm a big fan of Rust (as cliche as it may be). Whilst Rust is a popular language, there isnât really a âDjangoâ like (batteries included) framework - itâs all composing the pieces you need yourself. However, that doesnât stop people being very productive with it. As a result, most of the frameworks have very generic interfaces, letting developers pass state around as needed, rather than trying to do everything themselves. Outside of the obvious static typing debate (which Iâm in favour of), Iâd love to see Django embrace some dependencies, especially if they bring some performance improvements. It may end up being a bad idea, but it might also help those who want to use Djangoâs modules outside of Django.\nMany years ago, I tried to be a polyglot - switching between different programming languages (and frameworks) to find new ways of working and match the problem to the correct solution. Now, Iâve settled mostly on Python and Rust. They fit my needs well, Iâm very productive in them, and between the 2 thereâs not much they canât handle. Given my background, and the fact most sysadmin-y tools are written in it, Iâm really not a fan of Go.\nWhat projects are you working on now?\nOver time, Iâve slowly stepped back from having big side projects - being a new dad sure takes up time and energy. Large projects ended up feeling too much like work outside of work, and I end up either getting distracted or bored. After work, I want to do something fun, not that seems like yet another job. Iâm the kind of person who gets the sudden urge to research something interesting for an evening, dive in, then not think about it again for several weeks. Itâs not the most productive way of doing things, which is why my posts are all over the place, but it doesnât feel much like work for me - I lean heavily on what interests me at any time to drive what I want to do.\nWith that said, Iâm currently in the process of rebuilding my website. Of course, both the current and new versions are built on Django, but the new build should be easier to maintain, faster, and hopefully wonât need rewriting again in just a couple years. Most of my other projects have been small tools to make my home server that bit nicer.\nProfessionally, Iâm not really a developer anymore. As a sysadmin (ish), much of my day-to-day doesnât involve much programming. I spend much more of my time deploying, monitoring and administering Django applications than I do writing them. My main project at the moment is helping port a large Java / JS deployment over to Django and Wagtail, running on Kubernetes with some very high and interesting stability and scaling requirements. Since most of my professional live has been at software agencies, Iâve tended to bounce between different projects, rather than sitting on a single one. So Iâm also supporting on a few other smaller projects as and when Iâm needed.\nWhich Django libraries are your favorite (core or 3rd party)?\ndjango-tasks, of course!\nâ¦\nOh right, a serious answerâ¦\nI have to say, one of the most underrated modules in Django is django.utils. Itâs not as glamourous as the ORM, forms or cache, but itâs a treasure trove of useful methods. I personally always like looking at the internal helper functions large frameworks use - see the problems theyâve had to solve time and time again. Whilst thereâs not the same stability guarantees, Iâve definitely been helped out on a few occasions by some undocumented functions.\nIn that theme, Iâm a fan of libraries which do one thing and do it well. I quite like small libraries which aim to solve a problem. Thereâs definitely a line before that becomes a problem (anyone remember left-pad?), but libraries which scope creep are often harder to work with than the more narrow-scoped ones, whilst the smaller ones just keep on working and making my life easier. For example, django-environ makes reading and parsing environment variables into settings really easy and clean, and django-decorator-include helps including other urlpatterns whilst wrapping them in a decorator - particularly helpful for 3rd-party packageâs URLs.\nFinally, Iâve got a real soft-spot for whitenoise (and ServeStatic for ASGI users). Djangoâs documentation deters people pretty hard from serving media and static files using Django - and rightly so in performance-critical environments. However, for most people, having to additionally maintain (and secure) nginx is more maintenance than necessary. whitenoise serves static files using Django directly, without any extra configuration, whilst also pre-compressing files for a nice performance boost. To me, itâs such a universally-useful library, Iâd love to see it it included in Django itself someday.\nIâll throw a bonus shout out for granian, a new (ish) WSGI / ASGI server written in Rust. gunicorn has a near monopoly on running Python apps in production, especially in the WSGI space, so itâs nice to see a newcomer. granian isnât always faster, but doing the HTTP handling in Rust (and using popular libraries to do it) can improve stability and throughput, without holding the GIL. Iâve not run anything in production with it yet, but Iâve been using it on personal projects for almost a year without issue.\nWhat are the top three things in Django that you like?\nContrary to what Iâve already said, I actually like Djangoâs batteries. Sure, thereâs quite a few âdeadâ ones in need of some cleaning up and TLC, but having most of what I need already installed makes me far more productive. I donât need to think about how to render my form on the page, save the results as a model, or properly handle errors - everything âjust worksâ, and works together. Sure, batteries have their downsides - it makes swapping them out rather difficult, but Iâd rather ship my feature sooner than compare the trade-offs of different ORMs. The auto-reloading in django-tasks is only around 8 lines of code thanks to django.utils.autoreload being so easy to hook in to.\nSecondly: Forms, but not for the reasons you might think. Most forms are created to take submissions from the user, validate them, then probably save them to a model. However, theyâre great as general data validation. Iâve written plenty of views with complex querystring requirements, and leaning on forms to validate them saves a lot of boilerplate code. Sure, pydantic might be a bit faster and have more features, but given Iâm already productive with django.forms, and itâs already installed and well understood by other developers in my team, I donât feel the need to reach for something else.\nFinally, I wouldnât say itâs quite a âfavouriteâ, and itâs well-known as being far-from-perfect, but Iâve got a real soft-spot for the Django Admin. It lets me focus on building the core of an application, rather than the internal interface - particularly when there are no strong requirements for it, or itâs only going to be used by me and a few others. Since itâs a fair raw view of the database by default, Iâve definitely been bitten by some less-than-restrictive permissions, but thereâs generally all the hooks I need. I donât like building frontends, so only needing to build 1 rather than 2 makes me a lot happier, especially if it comes with authentication, permissions, read-only views and a dark mode ð!\nHow did you join the security team?\nIâd love to say itâs an interesting story, stroking my ego that I saved the day. But the reality is, as usual, far less glamorous.\nAs an engineer, Iâve tended towards 2 specialties: Security and Performance, which usually go hand-in-hand. In early 2023, I was invited to join the Wagtail CMS Security team after reporting and subsequently helping fix a memory exhaustion issue. I was already involved in all things security at Torchbox, especially our ISO-27001 certification, so I was already known when I submitted a vulnerability report.\nThibaud mentioned to me late last year that the project potentially looking for new members of the security team, to help with resourcing and some potential process improvements within the foundation. I naturally jumped at the opportunity - since the team is generally closed to new members and âfully-staffedâ. After a few gentle reminders (heâs a busy guy), I received a message from Sarah formally inviting me in March.\nSince then, Iâve tried to review every report which came through, and helped author a few patches. A few reports even had to be raised upstream with Pythonâs Security Response Team (PSRT). Itâs been an interesting experience, and Iâm looking forward to seeing how the team developers over the coming years.\nIâm aware that you have created DEP 14 on the Background Workers, how the work is going so far? Do you need support from the community on anything?\nDEP 14 (the proposal to add a native background workers API to Django) has been a really interesting journey. Iâm beyond humbled to see the community interest behind it. When I started down this road, Iâd only intended to start the conversations and help rally the community interest. Since then, and 6000 lines of code later, Iâm mostly single-handedly writing a database-backed production-grade task system.\nRight now, weâre at a bit of a cross-roads. Many of the foundational parts work, relatively well. The difficulty comes with the more complex features: Retries, dependencies, robust execution. Building a task system is easy - building a reliable one people want to actually use is incredibly difficult. If anyone out there is interested in getting involved, please do! Report issues, fix bugs, contribute to design discussions. Most of the APIs are based on what I think looks sensible. Software this large, pivotal and complex canât be built in isolation - so it needs a diverse audience to ensure we (I) make the right decisions, and design an API people actually want to use that will last and scale for years to come.\nThe next challenge on my list to tackle is timeouts - a highly requested feature. It sounds simple, but the reality is far from it. Many of those challenges sparked the topic of my upcoming PyCon UK talk later this year.\nDjango is celebrating its 20th anniversary this month. Any nice story to share?\nMy personal highlight was DjangoCon Europe 2024 - my first DjangoCon. I ended up bringing the stereotypically grey British weather with me, but I had a great week chatting Django with some interesting people, and putting faces to the names and handles Iâd seen online. After the talk announcing DEP 14 and background tasks, I was inundated with people voicing their support - many wondering how itâd taken this long.\nBut personally, Iâm more interested in whatâs to come. Of course, thereâs django-tasks, but the next sets of releases are shaping up to be pretty interesting. Over the last 3-4 years or so, Iâve personally noticed a bit of a resurgence in peopleâs appetites for change in Django. The 6.x Steering Council have a lot of interesting ideas, and clearly the community agree. People are happy with what Django can do now, but want to bring it a little more up-to-date - and are happy to put in the work to do it. Only a few weeks ago, django-csp was included in core, making it easier to make more secure applications. Iâm sure thatâs just the start. The fact people are still keen on working on a framework which just celebrated 20 years shows it must be doing something right!\nIs there anything else youâd like to say?\nIâd like to thank whoever nominated me to be a DSF member in the first place. To this date, I have no idea who you are.\nBeyond that, Iâm just looking forward to seeing what comes of Django, and Python in general over the next few years.\n\nThank you for doing the interview, Jake ! For July 2025, we welcome Jake Howard as our DSF member of the month! â­ Jake actively shares his knowledge through blog posts and community talks. He is part of the Security Team Working Group and he created the DEP 14. He has been a DSF member since June 2024. \nYou can learn more about Jake by visiting Jake's website and his GitHub Profile. Letâs spend some time getting to know Jake better! Iâm Jake. Iâm a Senior Systems Engineer at Torchbox, where Iâve been for a little over 4 years. âSystems Engineerâ is a fairly loaded title, and means different things to different people. I like to describe it as doing everything technical to do with Software Engineering which isnât Programming (Sysadmin, Devops, IT support, Security, Networking), but also doing a fair bit of Programming. Most of my hobbies revolve around technology. Iâm an avid self-hoster, running applications on servers both in âthe cloudâ and in my house. Thereâs been a server of some kind in my house for the last 10 years. Iâm generally quite a private person, so I like to know whatâs happening to my data. Since I started working remotely at the start of the 2020 pandemic, Iâve channeled some of this passion into posts on my website, with posts about all manner of things Iâve done from self-hosting to general software engineering. Away from my desk (sort of), Iâm a volunteer for Student Robotics, inspiring college students into STEM through competitive robotics (no, not quite like Robot Wars). In school, I was always the quiet one, but now I seem completely at home with public speaking, commentary and otherwise being in front of large crowds of people. I wish I knew the secret - Iâd make millions! My GitHub is also pretty active, with contributions all over the place (OpenZFS, Nebula VPN, Gitea, Plausible Analytics, OpenCV, Ansibleâ¦). Because a lot of life happens online (especially in the last 5 years), many people havenât even seen pictures of me, let alone met me in person. I am not in fact a talking piece of fruit. For a while, I tried to stay anonymous, avoiding photos or videos of me on the internet. But since I discovered I enjoy public speaking, Iâve sort of given up on that (for the most part). By now, Iâm sure many people have speak. But, for those who donât know: I, like my father before me, am ginger ð¥ (the hair colour, not the plant). The exact specifics of how being ginger lead to âTheOrangeOneâ are sadly lost to time. Iâve owned theorangeone.net for well over a decade at this point. Unfortunately, itâs not a particularly original nickname, and I have to be fast to claim it when signing up to new services. In some places (where I wasnât fast enough) Iâm forced to sub out âTheâ for âRealâ, which has lead to some confusions, but not too many. Canonically, I prefer âTheOrangeOneâ, but as we all know, naming things is hard. Iâve been using Django since around the 1.8 release. My job at the time was at a Django development agency, so it was the first real Python framework Iâd used. The first few weeks there was my first exposure to Django, pip, package management and collaborative software engineering - it was quite a lot to learn at once. I didnât realise it at the time, but I was working working as a junior alongside a couple fairly well-known names in the Django community like Tom Christie (DRF, Starlette, HTTPX) and Jamie Matthews (django-readers, django-zen-queries). We mostly built single-page apps with React, so I learned Django and Django Rest Framework at the same time, which means I now often have to look back at the docs to remember how forms and templates work. As for contributing to Django, that came much later. My first commit to Django was in May 2024. Having used Django for a while, and written plenty of packages, Iâd never stopped to look at how upstream was developed. Around the time of DEP 14 kicking off, I needed to look a bit more at the inner workings of the Django project, to learn what was in store for me. When scrolling through Trac tickets, I found an interesting looking ticket, and got to work. At the time of writing, Iâve now closed 9 Trac tickets across 12 PRs, and some pretty cool features (simple block tags, better Accept header parsing, performance improvements to the URL router) now have my name on them (metaphorically speaking). I wouldnât call myself an âactiveâ contributor, but I try and keep an eye on the tickets and forum threads which interest me the most, and chime in when I can. Since itâs the first framework I learned, and so far has done everything I need, Iâve mostly used Django. For a few smaller services, Iâve leaned more towards Starlette and AIOHTTP, but for anything even slightly large Iâve just used Django - since Iâd end up recreating much of Django using the smaller frameworks anyway. A better (likely official) path for single-file Django (ie without some of the magic module handling) might help draw a few more people in and fill a few more of these âmicro-serviceâ style use-cases. Iâm a class-based views person - I like the encapsulation and easy extension of base views. As with any opinion on the internet, Iâm sure many people disagree with me, but to me itâs just personal preference. Iâm still surprised itâs a pattern not seen by many other Python frameworks. Following in the footsteps of Python, I often wonder if Django could also do with some dead battery removal (or at least extracting into separate packages). Django is a pretty big framework, and whilst the contrib apps are intended to be separate, they also require hooks and assumptions in other areas of the codebase. I might be wrong (it happens quite a lot), but I suspect some of those packages would be better suited externally, perhaps improving some developer momentum - and lightening the load for the Fellows. Djangoâs sitemap and syndication (RSS) frameworks are 2 places I wish would get some more love. Outside of Python, Iâm a big fan of Rust (as cliche as it may be). Whilst Rust is a popular language, there isnât really a âDjangoâ like (batteries included) framework - itâs all composing the pieces you need yourself. However, that doesnât stop people being very productive with it. As a result, most of the frameworks have very generic interfaces, letting developers pass state around as needed, rather than trying to do everything themselves. Outside of the obvious static typing debate (which Iâm in favour of), Iâd love to see Django embrace some dependencies, especially if they bring some performance improvements. It may end up being a bad idea, but it might also help those who want to use Djangoâs modules outside of Django. Many years ago, I tried to be a polyglot - switching between different programming languages (and frameworks) to find new ways of working and match the problem to the correct solution. Now, Iâve settled mostly on Python and Rust. They fit my needs well, Iâm very productive in them, and between the 2 thereâs not much they canât handle. Given my background, and the fact most sysadmin-y tools are written in it, Iâm really not a fan of Go. Over time, Iâve slowly stepped back from having big side projects - being a new dad sure takes up time and energy. Large projects ended up feeling too much like work outside of work, and I end up either getting distracted or bored. After work, I want to do something fun, not that seems like yet another job. Iâm the kind of person who gets the sudden urge to research something interesting for an evening, dive in, then not think about it again for several weeks. Itâs not the most productive way of doing things, which is why my posts are all over the place, but it doesnât feel much like work for me - I lean heavily on what interests me at any time to drive what I want to do. With that said, Iâm currently in the process of rebuilding my website. Of course, both the current and new versions are built on Django, but the new build should be easier to maintain, faster, and hopefully wonât need rewriting again in just a couple years. Most of my other projects have been small tools to make my home server that bit nicer. Professionally, Iâm not really a developer anymore. As a sysadmin (ish), much of my day-to-day doesnât involve much programming. I spend much more of my time deploying, monitoring and administering Django applications than I do writing them. My main project at the moment is helping port a large Java / JS deployment over to Django and Wagtail, running on Kubernetes with some very high and interesting stability and scaling requirements. Since most of my professional live has been at software agencies, Iâve tended to bounce between different projects, rather than sitting on a single one. So Iâm also supporting on a few other smaller projects as and when Iâm needed. django-tasks, of course! â¦ Oh right, a serious answerâ¦ I have to say, one of the most underrated modules in Django is django.utils. Itâs not as glamourous as the ORM, forms or cache, but itâs a treasure trove of useful methods. I personally always like looking at the internal helper functions large frameworks use - see the problems theyâve had to solve time and time again. Whilst thereâs not the same stability guarantees, Iâve definitely been helped out on a few occasions by some undocumented functions. In that theme, Iâm a fan of libraries which do one thing and do it well. I quite like small libraries which aim to solve a problem. Thereâs definitely a line before that becomes a problem (anyone remember left-pad?), but libraries which scope creep are often harder to work with than the more narrow-scoped ones, whilst the smaller ones just keep on working and making my life easier. For example, django-environ makes reading and parsing environment variables into settings really easy and clean, and django-decorator-include helps including other urlpatterns whilst wrapping them in a decorator - particularly helpful for 3rd-party packageâs URLs. Finally, Iâve got a real soft-spot for whitenoise (and ServeStatic for ASGI users). Djangoâs documentation deters people pretty hard from serving media and static files using Django - and rightly so in performance-critical environments. However, for most people, having to additionally maintain (and secure) nginx is more maintenance than necessary. whitenoise serves static files using Django directly, without any extra configuration, whilst also pre-compressing files for a nice performance boost. To me, itâs such a universally-useful library, Iâd love to see it it included in Django itself someday. Iâll throw a bonus shout out for granian, a new (ish) WSGI / ASGI server written in Rust. gunicorn has a near monopoly on running Python apps in production, especially in the WSGI space, so itâs nice to see a newcomer. granian isnât always faster, but doing the HTTP handling in Rust (and using popular libraries to do it) can improve stability and throughput, without holding the GIL. Iâve not run anything in production with it yet, but Iâve been using it on personal projects for almost a year without issue. Contrary to what Iâve already said, I actually like Djangoâs batteries. Sure, thereâs quite a few âdeadâ ones in need of some cleaning up and TLC, but having most of what I need already installed makes me far more productive. I donât need to think about how to render my form on the page, save the results as a model, or properly handle errors - everything âjust worksâ, and works together. Sure, batteries have their downsides - it makes swapping them out rather difficult, but Iâd rather ship my feature sooner than compare the trade-offs of different ORMs. The auto-reloading in django-tasks is only around 8 lines of code thanks to django.utils.autoreload being so easy to hook in to. Secondly: Forms, but not for the reasons you might think. Most forms are created to take submissions from the user, validate them, then probably save them to a model. However, theyâre great as general data validation. Iâve written plenty of views with complex querystring requirements, and leaning on forms to validate them saves a lot of boilerplate code. Sure, pydantic might be a bit faster and have more features, but given Iâm already productive with django.forms, and itâs already installed and well understood by other developers in my team, I donât feel the need to reach for something else. Finally, I wouldnât say itâs quite a âfavouriteâ, and itâs well-known as being far-from-perfect, but Iâve got a real soft-spot for the Django Admin. It lets me focus on building the core of an application, rather than the internal interface - particularly when there are no strong requirements for it, or itâs only going to be used by me and a few others. Since itâs a fair raw view of the database by default, Iâve definitely been bitten by some less-than-restrictive permissions, but thereâs generally all the hooks I need. I donât like building frontends, so only needing to build 1 rather than 2 makes me a lot happier, especially if it comes with authentication, permissions, read-only views and a dark mode ð! Iâd love to say itâs an interesting story, stroking my ego that I saved the day. But the reality is, as usual, far less glamorous. As an engineer, Iâve tended towards 2 specialties: Security and Performance, which usually go hand-in-hand. In early 2023, I was invited to join the Wagtail CMS Security team after reporting and subsequently helping fix a memory exhaustion issue. I was already involved in all things security at Torchbox, especially our ISO-27001 certification, so I was already known when I submitted a vulnerability report. Thibaud mentioned to me late last year that the project potentially looking for new members of the security team, to help with resourcing and some potential process improvements within the foundation. I naturally jumped at the opportunity - since the team is generally closed to new members and âfully-staffedâ. After a few gentle reminders (heâs a busy guy), I received a message from Sarah formally inviting me in March. Since then, Iâve tried to review every report which came through, and helped author a few patches. A few reports even had to be raised upstream with Pythonâs Security Response Team (PSRT). Itâs been an interesting experience, and Iâm looking forward to seeing how the team developers over the coming years. DEP 14 (the proposal to add a native background workers API to Django) has been a really interesting journey. Iâm beyond humbled to see the community interest behind it. When I started down this road, Iâd only intended to start the conversations and help rally the community interest. Since then, and 6000 lines of code later, Iâm mostly single-handedly writing a database-backed production-grade task system. Right now, weâre at a bit of a cross-roads. Many of the foundational parts work, relatively well. The difficulty comes with the more complex features: Retries, dependencies, robust execution. Building a task system is easy - building a reliable one people want to actually use is incredibly difficult. If anyone out there is interested in getting involved, please do! Report issues, fix bugs, contribute to design discussions. Most of the APIs are based on what I think looks sensible. Software this large, pivotal and complex canât be built in isolation - so it needs a diverse audience to ensure we (I) make the right decisions, and design an API people actually want to use that will last and scale for years to come. The next challenge on my list to tackle is timeouts - a highly requested feature. It sounds simple, but the reality is far from it. Many of those challenges sparked the topic of my upcoming PyCon UK talk later this year. My personal highlight was DjangoCon Europe 2024 - my first DjangoCon. I ended up bringing the stereotypically grey British weather with me, but I had a great week chatting Django with some interesting people, and putting faces to the names and handles Iâd seen online. After the talk announcing DEP 14 and background tasks, I was inundated with people voicing their support - many wondering how itâd taken this long. But personally, Iâm more interested in whatâs to come. Of course, thereâs django-tasks, but the next sets of releases are shaping up to be pretty interesting. Over the last 3-4 years or so, Iâve personally noticed a bit of a resurgence in peopleâs appetites for change in Django. The 6.x Steering Council have a lot of interesting ideas, and clearly the community agree. People are happy with what Django can do now, but want to bring it a little more up-to-date - and are happy to put in the work to do it. Only a few weeks ago, django-csp was included in core, making it easier to make more secure applications. Iâm sure thatâs just the start. The fact people are still keen on working on a framework which just celebrated 20 years shows it must be doing something right! Iâd like to thank whoever nominated me to be a DSF member in the first place. To this date, I have no idea who you are. Beyond that, Iâm just looking forward to seeing what comes of Django, and Python in general over the next few years. Thank you for doing the interview, Jake ! August 03, 2025 01:20 PM UTC Do the design patterns learned in other programming languages translate to coding in Python? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.\n\n[ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] Do the design patterns learned in other programming languages translate to coding in Python? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects. [ Improve Your Python With ð Python Tricks ð â Get a short & sweet Python Trick delivered to your inbox every couple of days. >> Click here to learn more and see examples ] August 01, 2025 12:00 PM UTC 68th issue of Andrei Neagoie's must-read monthly Python Newsletter: Useless Design Patterns, Django turns 20, 330Ã faster Python, and much more. Read the full newsletter to get up-to-date with everything you need to know from last month. August 01, 2025 10:00 AM UTC Wing Python IDE version 11.0.3 has been released.  It improves Python code analysis,\nfixes problems debugging Django templates, fixes refactoring when the target file\nis in a hidden directory, and makes a number of other minor improvements.\nDownloads\nBe sure to Check for Updates in Wing's Help menu after downloading, to make\nsure that you have the latest hot fixes.\nWing Pro 11.0.3\nWing Personal 11.0.3\nWing 101 11.0.3\nWing 10 and earlier versions are not affected by installation of Wing 11 and\nmay be installed and used independently. However, project files for Wing 10\nand earlier are converted when opened by Wing 11 and should be saved under a\nnew name, since Wing 11 projects cannot be opened by older versions of Wing.\n New in Wing 11Improved AI Assisted DevelopmentWing 11 improves the user interface for AI assisted development by introducing two separate\ntools AI Coder and AI Chat. AI Coder can be used to write, redesign, or extend code\nin the current editor. AI Chat can be used to ask about code or iterate in creating a\ndesign or new code without directly modifying the code in an editor.\nWing 11's AI assisted development features now support not just OpenAI but also Claude, Grok,\nGemini, Perplexity, Mistral, Deepseek, and any other OpenAI completions API compatible AI provider.\nThis release also improves setting up AI request context, so that both automatically and\nmanually selected and described context items may be paired with an AI request. AI request\ncontexts can now be stored, optionally so they are shared by all projects, and may be used\nindependently with different AI features.\nAI requests can now also be stored in the current project or shared with all projects, and Wing\ncomes preconfigured with a set of commonly used requests. In addition to changing code in the\ncurrent editor, stored requests may create a new untitled file or run instead in AI Chat. Wing\n11 also introduces options for changing code within an editor, including replacing code,\ncommenting out code, or starting a diff/merge session to either accept or reject changes.\nWing 11 also supports using AI to generate commit messages based on the changes being committed\nto a revision control system.\nYou can now also configure multiple AI providers for easier access to different models.\nFor details see AI Assisted Development under Wing Manual in Wing 11's Help menu.\nPackage Management with uv Wing Pro 11 adds support for the uv package manager in the New Project dialog and the\nPackages tool.\nFor details see Project Manager > Creating Projects > Creating Python Environments and Package\nManager > Package Management with uv under Wing Manual in Wing 11's Help menu.\nImproved Python Code AnalysisWing 11 makes substantial improvements to Python code analysis, with better support for\nliterals such as dicts and sets, parametrized type aliases, typing.Self, type of\nvariables on the def or class line that declares them, generic classes with\n[...], __all__ in *.pyi files, subscripts in typing.Type and similar, type\naliases, type hints in strings, type[...] and tuple[...],\n@functools.cached_property, base classes found also in .pyi files, and\ntyping.Literal[...].\nUpdated LocalizationsWing 11 updates the German, French, and Russian localizations, and introduces a new experimental\nAI-generated Spanish localization. The Spanish localization and the new AI-generated strings in the\nFrench and Russian localizations may be accessed with the new User Interface > Include AI\nTranslated Strings preference.\nImproved diff/mergeWing Pro 11 adds floating buttons directly between the editors to make navigating differences\nand merging easier, allows undoing previously merged changes, and does a better job managing\nscratch buffers, scroll locking, and sizing of merged ranges.\nFor details see Difference and Merge under Wing Manual in Wing 11's Help menu.\nOther Minor Features and ImprovementsWing 11 also improves the custom key binding assignment user interface, adds a Files >\nAuto-Save Files When Wing Loses Focus preference, warns immediately when opening a project with\nan invalid Python Executable configuration, allows clearing recent menus, expands the set of\navailable special environment variables for project configuration, and makes a number of other\nbug fixes and usability improvements.\nChanges and IncompatibilitiesSince Wing 11 replaced the AI tool with AI Coder and AI Chat, and AI\nconfiguration is completely different than in Wing 10, you will need to reconfigure your\nAI integration manually in Wing 11. This is done with Manage AI Providers in the\nAI menu. After adding the first provider configuration, Wing will set that provider as\nthe default. You can switch between providers with Switch to Provider in the AI menu.\nIf you have questions, please don't hesitate to contact us at support@wingware.com. Wing Python IDE version 11.0.3 has been released.  It improves Python code analysis,\nfixes problems debugging Django templates, fixes refactoring when the target file\nis in a hidden directory, and makes a number of other minor improvements. Wing Pro 11.0.3 Wing Personal 11.0.3 Wing 101 11.0.3 Wing 10 and earlier versions are not affected by installation of Wing 11 and\nmay be installed and used independently. However, project files for Wing 10\nand earlier are converted when opened by Wing 11 and should be saved under a\nnew name, since Wing 11 projects cannot be opened by older versions of Wing. Wing 11 improves the user interface for AI assisted development by introducing two separate\ntools AI Coder and AI Chat. AI Coder can be used to write, redesign, or extend code\nin the current editor. AI Chat can be used to ask about code or iterate in creating a\ndesign or new code without directly modifying the code in an editor. Wing 11's AI assisted development features now support not just OpenAI but also Claude, Grok,\nGemini, Perplexity, Mistral, Deepseek, and any other OpenAI completions API compatible AI provider. This release also improves setting up AI request context, so that both automatically and\nmanually selected and described context items may be paired with an AI request. AI request\ncontexts can now be stored, optionally so they are shared by all projects, and may be used\nindependently with different AI features. AI requests can now also be stored in the current project or shared with all projects, and Wing\ncomes preconfigured with a set of commonly used requests. In addition to changing code in the\ncurrent editor, stored requests may create a new untitled file or run instead in AI Chat. Wing\n11 also introduces options for changing code within an editor, including replacing code,\ncommenting out code, or starting a diff/merge session to either accept or reject changes. Wing 11 also supports using AI to generate commit messages based on the changes being committed\nto a revision control system. You can now also configure multiple AI providers for easier access to different models. For details see AI Assisted Development under Wing Manual in Wing 11's Help menu. Wing Pro 11 adds support for the uv package manager in the New Project dialog and the\nPackages tool. For details see Project Manager > Creating Projects > Creating Python Environments and Package\nManager > Package Management with uv under Wing Manual in Wing 11's Help menu. Wing 11 makes substantial improvements to Python code analysis, with better support for\nliterals such as dicts and sets, parametrized type aliases, typing.Self, type of\nvariables on the def or class line that declares them, generic classes with\n[...], __all__ in *.pyi files, subscripts in typing.Type and similar, type\naliases, type hints in strings, type[...] and tuple[...],\n@functools.cached_property, base classes found also in .pyi files, and\ntyping.Literal[...]. Wing 11 updates the German, French, and Russian localizations, and introduces a new experimental\nAI-generated Spanish localization. The Spanish localization and the new AI-generated strings in the\nFrench and Russian localizations may be accessed with the new User Interface > Include AI\nTranslated Strings preference. Wing Pro 11 adds floating buttons directly between the editors to make navigating differences\nand merging easier, allows undoing previously merged changes, and does a better job managing\nscratch buffers, scroll locking, and sizing of merged ranges. For details see Difference and Merge under Wing Manual in Wing 11's Help menu. Wing 11 also improves the custom key binding assignment user interface, adds a Files >\nAuto-Save Files When Wing Loses Focus preference, warns immediately when opening a project with\nan invalid Python Executable configuration, allows clearing recent menus, expands the set of\navailable special environment variables for project configuration, and makes a number of other\nbug fixes and usability improvements. Since Wing 11 replaced the AI tool with AI Coder and AI Chat, and AI\nconfiguration is completely different than in Wing 10, you will need to reconfigure your\nAI integration manually in Wing 11. This is done with Manage AI Providers in the\nAI menu. After adding the first provider configuration, Wing will set that provider as\nthe default. You can switch between providers with Switch to Provider in the AI menu. If you have questions, please don't hesitate to contact us at support@wingware.com. August 01, 2025 01:00 AM UTC In this stream, I worked on a personal AI workflow that I’m building using LangGraph. I discussed human-in-the-loop and how to bring a person into the workflow process. August 01, 2025 12:00 AM UTC August 01, 2025 12:00 AM UTC SHell WIth Me combines magic-wormhole and tty-share for e2ee, p2p terminal sharing August 01, 2025 12:00 AM UTC Functions in Python can be defined within another function.\n\n\n\nTable of contents\n\nA function defined within a function\nA function returned from another function\nThe enclosing scope\nClosures with nested functions\nWhy nest functions within functions?\nDecorators involve nested functions\nNested functions are possible in Python\n\n\n\n\nA function defined within a function\nPython's functions can be defined pretty much anywhere.\nYou can even define a function inside a function:\ndef greet_me(name=\"friend\"):\n    def greet():\n        print(\"Hello\", name)\n    greet()\n\nWhen we call this greet_me function, it defines a greet a function and then calls that function:\n>>> greet_me()\nHello friend\n\nNote that the inner function is allowed to use the name from the outer function.\nA function returned from another function\nInstead of calling our inner â¦\n\nRead the full article: https://www.pythonmorsels.com/nested-functions/ Functions in Python can be defined within another function. Table of contents\n\nA function defined within a function\nA function returned from another function\nThe enclosing scope\nClosures with nested functions\nWhy nest functions within functions?\nDecorators involve nested functions\nNested functions are possible in Python Python's functions can be defined pretty much anywhere. You can even define a function inside a function: When we call this greet_me function, it defines a greet a function and then calls that function: Note that the inner function is allowed to use the name from the outer function. Instead of calling our inner â¦ July 31, 2025 03:30 PM UTC Hello Django ð Universe!\nð°ï¸â This is Djangonaut Space phoning home about Session 5! We're recruiting technical mentors (Navigators) to join our next ðstellarð mission.\nð©âð We are looking for people who regularly contribute to Django or a Django related package, that want to mentor others. Our next session will be Oct-Nov.\nð Come join us and be a cosmic contributor! Express your interest to be a mentor here.\nð Want to learn more about what it means to be a Navigator:\n\nHere's a high-level overview of the role\nHere's the workbook each Navigator is provided\n\nð¤ Interested people will have to complete a 30 minute meet & greet type interview with organizers.\nâ If you're interested in applying to be a Djangonaut, applications will open and close in September (dates to be determined). The latest information will be posted on our site, djangonaut.space. Please follow our social media accounts or subscribe to our newsletter for announcements.\nâï¸ We'll see you around the cosmos!\nDjangonaut Space session organizers Hello Django ð Universe! ð°ï¸â This is Djangonaut Space phoning home about Session 5! We're recruiting technical mentors (Navigators) to join our next ðstellarð mission. ð©âð We are looking for people who regularly contribute to Django or a Django related package, that want to mentor others. Our next session will be Oct-Nov. ð Come join us and be a cosmic contributor! Express your interest to be a mentor here. ð Want to learn more about what it means to be a Navigator: ð¤ Interested people will have to complete a 30 minute meet & greet type interview with organizers. â If you're interested in applying to be a Djangonaut, applications will open and close in September (dates to be determined). The latest information will be posted on our site, djangonaut.space. Please follow our social media accounts or subscribe to our newsletter for announcements. âï¸ We'll see you around the cosmos! Djangonaut Space session organizers July 31, 2025 02:34 PM UTC The Bazel Plugin for IntelliJ IDEA Is Now Generally Available!\nAfter much anticipation, we are finally ready to announce the general availability (GA) of the new Bazel plugin for IntelliJ IDEA, PyCharm, and GoLand â now developed by JetBrains! After months of focused development and valuable feedback from our EAP users, we’re officially launching our revamped Bazel experience.\nWhile we’ve been shipping updates regularly, the leap to our 2025.2 GA release marks a major milestone. Even though our primary focus for this release was on creating the best experience we can for Java, Kotlin, and Scala developers, we also brought support for the Python and Go ecosystems, and we will continue to maintain and improve it in the coming releases.\nIf you’re migrating from the previous plugin originally released by Google, you’ll notice a more straightforward workflow that aligns with the standard JetBrains IDE experience you expect from other build tool integrations such as Maven and Gradle. Now, let’s dive into what’s new!\nKey features in 2025.2\n\nBazel Query in Action\n\nGo is a go. We’re officially rolling out support for Go. You can now import your Go targets in Bazel projects into both IntelliJ IDEA (with the Go plugin) and GoLand. This brings the full IDE experience you rely on: code highlighting, completion, navigation, and the ability to run, debug, and get coverage for your tests.\nBuilt-in Bazel Query tool window: Go beyond sync and build with Bazel queries integrated directly into your IDE via their own dedicated tool window. Craft your queries with syntax completion and a helpful UI for flags to explore your project’s dependency graph without ever leaving the editor.\nDramatically faster indexing: We’ve optimized indexing to get you to your code faster. You can now use the import_depth and import_ijars settings in your .bazelproject file to prevent the indexing of deep transitive dependencies and index only header jars instead of full jars. Whatâs more, only the files directly referenced in your .bazelproject view are fully indexed for code intelligence, which can slash indexing times and memory usage in large projects with many auxiliary files.\n\nNew plugin, new user experience\nBack in December, we publicly announced the EAP (Early Access Program) version of our new plugin and defined what it would take to release it into GA, with an overview of the main differences between the original plugin and the new one.\nHereâs a quick recap for those moving from the older plugin: We’ve smoothed out the rough edges to make Bazel feel like a natural part of the IDE.\n\nSimplified project import: The old import wizard is a thing of the past. Now, simply open a directory containing your MODULE.bazel or WORKSPACE file. For more control, you can open a specific .bazelproject view file. If you manage a large monorepo, you can provide a default template for your team by checking in a template at tools/intellij/.managed.bazelproject.\nRedesigned UI elements: The Bazel tool window is now your central hub for actions like resyncing your project (with a Build and Resync option for generating sources) and keeping track of targets in your working set. We’ve also added a widget listing all targets the currently opened file belongs to. It allows you to run actions on these targets (build / test / jump to BUILD file / copy target label)\nReworked target mapping for JVM projects: A core improvement is the new internal representation for JVM targets, which mirrors the actual Bazel graph. This fundamental change enables more accurate highlighting, more accurate completions and more reliable refactoring.\n\nImprovements since 2025.1\nWindows compatibility\nWe understand that development doesn’t just happen on one OS. Thatâs why we worked on making our plugin compatible with Microsoft Windows, bringing most of the feature set to our Windows-based users.\nEnhanced Bazel configuration support\nWe believe editing your build files should be as easy as editing your source code, which is why we’ve improved the user experience for all Bazel-related configuration files.\nStarlark (.bzl, BUILD)\n\nStarlark Quick Documentation\n\nQuick documentation for Starlark rules: Hover over a Starlark rule or function to see its documentation directly in the editor. You’ll also get documentation as you type, guiding you through available parameters.\nAutomatic formatting: If you have buildifier on your PATH, the plugin will now automatically format your Starlark files on save\n\nBazel module configuration file (MODULE.bazel)\n\nIntelligent editing: The MODULE.bazel editor now offers smart completions for arguments and displays documentation as you edit.\n\nBazel project view file (.bazelproject) \n\n.bazelproject view highlighting and completions\n\nGuided editing: Get completions for section names and known values. The editor will now highlight completely unsupported sections as errors and sections that are supported in the old plugin originally by Google (but not in the new one) as warnings.\nManage directories from the Project view file tree: You can now right-click a directory in the project tree to add or remove it from your .bazelproject file, thus loading or unloading that directory in IntelliJ.\n\nBazelisk configuration file (.bazelversion):\n\nStay up to date: The editor will now highlight outdated Bazel versions specified in your .bazelversion file and offer a quick-fix to update to the latest release.\n\nLanguage ecosystem enhancements\n\nJVM:\n\nThe underlying project model mapping has been further improved, resulting in better performance during sync and more reliable refactorings for targets where glob patterns match the whole directory.\n\n\nScala:\n\nThe Bazel plugin now respects the scalacopts parameter in your scala_* targets, which unlocks Scala 3 highlighting features with the -Xsource:3 flag. At the same time, we’ve updated the Scala plugin to provide native integration with the Bazel plugin out of the box.\n\n\nPython:\n\nRun from the gutter: py_test and py_binary targets now get the familiar green Run arrow in the editor gutter.\nImproved dependency resolution: Python dependencies are now resolved correctly, enabling code navigation and eliminating false error highlighting.\nInterpreter from MODULE.bazel: The plugin now sets the Python interpreter based on what is defined in MODULE.bazel. This includes support for hermetic toolchains downloaded by rules_python â meaning you don’t need to have Python installed locally on your machine.\nDebugging: You can now attach the debugger to py_test targets.\n\n\n\nWhat happens to the Bazel plugin by Google?\nThe Bazel for IntelliJ plugin (also known as IJwB) by Google is being deprecated. Google has transferred the code ownership and maintenance to JetBrains. We will keep providing compatibility updates for new IntelliJ versions and critical fixes only throughout the year of 2025, but will be fully deprecating it in 2026. All our development effort for IntelliJ IDEA, GoLand, and PyCharm is now focused on the new plugin.\nThe Bazel for CLion plugin (CLwB) has also been transferred to JetBrains, and will continue to be actively developed. Learn more in the post Enhancing Bazel Support for CLion on the CLion Blog.\nGot feedback? Weâre listening!\nWe’re committed to making this the best Bazel experience possible. Please report any issues, ideas, or improvements straight to our issue tracker.\nFixed the problem yourself? We accept PRs on our hirschgarten repository.\nYou’ll also find us on the Bazel Community Slack, in the #intellij channel.\nHappy building! After much anticipation, we are finally ready to announce the general availability (GA) of the new Bazel plugin for IntelliJ IDEA, PyCharm, and GoLand â now developed by JetBrains! After months of focused development and valuable feedback from our EAP users, we’re officially launching our revamped Bazel experience. While we’ve been shipping updates regularly, the leap to our 2025.2 GA release marks a major milestone. Even though our primary focus for this release was on creating the best experience we can for Java, Kotlin, and Scala developers, we also brought support for the Python and Go ecosystems, and we will continue to maintain and improve it in the coming releases. If you’re migrating from the previous plugin originally released by Google, you’ll notice a more straightforward workflow that aligns with the standard JetBrains IDE experience you expect from other build tool integrations such as Maven and Gradle. Now, let’s dive into what’s new! Key features in 2025.2 Bazel Query in Action Back in December, we publicly announced the EAP (Early Access Program) version of our new plugin and defined what it would take to release it into GA, with an overview of the main differences between the original plugin and the new one. Hereâs a quick recap for those moving from the older plugin: We’ve smoothed out the rough edges to make Bazel feel like a natural part of the IDE. We understand that development doesn’t just happen on one OS. Thatâs why we worked on making our plugin compatible with Microsoft Windows, bringing most of the feature set to our Windows-based users. We believe editing your build files should be as easy as editing your source code, which is why we’ve improved the user experience for all Bazel-related configuration files. Starlark Quick Documentation .bazelproject view highlighting and completions The Bazel for IntelliJ plugin (also known as IJwB) by Google is being deprecated. Google has transferred the code ownership and maintenance to JetBrains. We will keep providing compatibility updates for new IntelliJ versions and critical fixes only throughout the year of 2025, but will be fully deprecating it in 2026. All our development effort for IntelliJ IDEA, GoLand, and PyCharm is now focused on the new plugin. The Bazel for CLion plugin (CLwB) has also been transferred to JetBrains, and will continue to be actively developed. Learn more in the post Enhancing Bazel Support for CLion on the CLion Blog. We’re committed to making this the best Bazel experience possible. Please report any issues, ideas, or improvements straight to our issue tracker. Fixed the problem yourself? We accept PRs on our hirschgarten repository. You’ll also find us on the Bazel Community Slack, in the #intellij channel. Happy building! July 31, 2025 01:40 PM UTC July 31, 2025 10:20 AM UTC"
  },
  {
    "url": "https://www.python.org/events/",
    "title": "Our Events | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. 08 Aug.\n        \n            2025\n        \n\n            2025\n        \n\nBuea, Cameroon 11 Aug.\n        \n            2025\n        \n\n         –\n            15 Aug.\n        \n\n        \n            2025\n        \n\nArusha, Tanzania 13 Aug.\n        \n            2025\n        \n\n         –\n            14 Aug.\n        \n\n        \n            2025\n        \n\nMogadishu, Somalia 15 Aug.\n        \n            2025\n        \n\n         –\n            17 Aug.\n        \n\n        \n            2025\n        \n\nSeoul, South Korea 18 Aug.\n        \n            2025\n        \n\n         –\n            22 Aug.\n        \n\n        \n            2025\n        \n\nKraków, Poland 23 Aug.\n        \n            2025\n        \n\n         –\n            24 Aug.\n        \n\n        \n            2025\n        \n\nHaringhata Farm, West Bengal, India 23 Aug.\n        \n            2025\n        \n\n            2025\n        \n\nLomé, Togo 26 Aug.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            4pm UTC\n                 – 7pm\n                    UTC\n                \n        \n    \nAmsterdam, The Netherlands and Online 28 Aug.\n        \n            2025\n        \n\n         –\n            30 Aug.\n        \n\n        \n            2025\n        \n\nNairobi, Kenya 28 Aug.\n        \n            2025\n        \n\n         –\n            31 Aug.\n        \n\n        \n            2025\n        \n\nGliwice, Poland 29 Aug.\n        \n            2025\n        \n\n         –\n            30 Aug.\n        \n\n        \n            2025\n        \n\nAthens, Greece 01 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nLimbe, Cameroon 01 Sept.\n        \n            2025\n        \n\n         –\n            03 Sept.\n        \n\n        \n            2025\n        \n\nBerlin, Germany 01 Sept.\n        \n            2025\n        \n\n         –\n            05 Sept.\n        \n\n        \n            2025\n        \n\nAbraka, Nigeria 04 Sept.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            6pm UTC\n                 – 8pm\n                    UTC\n                \n        \n    \nLeiden, The Netherlands 06 Sept.\n        \n            2025\n        \n\n         –\n            07 Sept.\n        \n\n        \n            2025\n        \n\nTaipei City, Taiwan 09 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nCinema City, Israel 10 Sept.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            4pm UTC\n                 – 7pm\n                    UTC\n                \n        \n    \nDüsseldorf, Germany 12 Sept.\n        \n            2025\n        \n\n         –\n            14 Sept.\n        \n\n        \n            2025\n        \n\nTřeštice, Czechia 12 Sept.\n        \n            2025\n        \n\n         –\n            16 Sept.\n        \n\n        \n            2025\n        \n\nMelbourne, Australia 12 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nAbraka, Nigeria 12 Sept.\n        \n            2025\n        \n\n         –\n            15 Sept.\n        \n\n        \n            2025\n        \n\nBengaluru, India 13 Sept.\n        \n            2025\n        \n\n         –\n            15 Sept.\n        \n\n        \n            2025\n        \n\nDosso, Niger 19 Sept.\n        \n            2025\n        \n\n         –\n            22 Sept.\n        \n\n        \n            2025\n        \n\nManchester, United Kingdom 26 Sept.\n        \n            2025\n        \n\n         –\n            27 Sept.\n        \n\n        \n            2025\n        \n\nHiroshima, Japan 27 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nKano, Nigeria 27 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nSanta Monica, CA, USA 30 Sept.\n        \n            2025\n        \n\n         –\n            01 Oct.\n        \n\n        \n            2025\n        \n\nParis, France 01 Oct.\n        \n            2025\n        \n\n            2025\n        \n\nYaounde, Cameroon 02 Oct.\n        \n            2025\n        \n\n         –\n            04 Oct.\n        \n\n        \n            2025\n        \n\nLagos, Nigeria 02 Oct.\n        \n            2025\n        \n\n         –\n            03 Oct.\n        \n\n        \n            2025\n        \n\nTallinn, Estonia 04 Oct.\n        \n            2025\n        \n\n            2025\n        \n\nAba, Abia State, Nigeria 06 Oct.\n        \n            2025\n        \n\n         –\n            08 Oct.\n        \n\n        \n            2025\n        \n\nOnline 08 Oct.\n        \n            2025\n        \n\n         –\n            10 Oct.\n        \n\n        \n            2025\n        \n\nOnline 08 Oct.\n        \n            2025\n        \n\n         –\n            12 Oct.\n        \n\n        \n            2025\n        \n\nJohannesburg, South Africa 11 Oct.\n        \n            2025\n        \n\n         –\n            12 Oct.\n        \n\n        \n            2025\n        \n\nKowloon, Hong Kong 13 Oct.\n        \n            2025\n        \n\n         –\n            19 Oct.\n        \n\n        \n            2025\n        \n\nJyväskylä, Finland 14 Oct.\n        \n            2025\n        \n\n         –\n            16 Oct.\n        \n\n        \n            2025\n        \n\nBrighton, UK 16 Oct.\n        \n            2025\n        \n\n            2025\n        \n\nUtrecht, The Netherlands 16 Oct.\n        \n            2025\n        \n\n         –\n            17 Oct.\n        \n\n        \n            2025\n        \n\nRapperswil, Switzerland 17 Oct.\n        \n            2025\n        \n\n         –\n            18 Oct.\n        \n\n        \n            2025\n        \n\nBangkok, Thailand 17 Oct.\n        \n            2025\n        \n\n         –\n            19 Oct.\n        \n\n        \n            2025\n        \n\nSevilla, Spain 17 Oct.\n        \n            2025\n        \n\n            2025\n        \n\nJyväskylä, Finland 18 Oct.\n        \n            2025\n        \n\n         –\n            19 Oct.\n        \n\n        \n            2025\n        \n\nSan Francisco, CA, USA 21 Oct.\n        \n            2025\n        \n\n         –\n            27 Oct.\n        \n\n        \n            2025\n        \n\nSao Paulo, Brazil 22 Oct.\n        \n            2025\n        \n\n         –\n            23 Oct.\n        \n\n        \n            2025\n        \n\nSan Francisco, USA 30 Oct.\n        \n            2025\n        \n\n         –\n            02 Nov.\n        \n\n        \n            2025\n        \n\nLyon, France 30 Oct.\n        \n            2025\n        \n\n         –\n            31 Oct.\n        \n\n        \n            2025\n        \n\nClarion Hotel Skanstull, Stockholm, Sweden 03 Nov.\n        \n            2025\n        \n\n         –\n            05 Nov.\n        \n\n        \n            2025\n        \n\nSan Diego, USA 15 Nov.\n        \n            2025\n        \n\n         –\n            16 Nov.\n        \n\n        \n            2025\n        \n\nDublin, Ireland 19 Nov.\n        \n            2025\n        \n\n         –\n            20 Nov.\n        \n\n        \n            2025\n        \n\nAbraka, Nigeria 20 March\n        \n            2026\n        \n\n         –\n            21 March\n        \n\n        \n            2026\n        \n\nVancouver, British Columbia, Canada 27 May\n        \n            2026\n        \n\n         –\n            30 May\n        \n\n        \n            2026\n        \n\nBologna, Italy 26 July\n        \n            2025\n        \n\n         –\n            27 July\n        \n\n        \n            2025\n        \n\nCleveland, USA 25 July\n        \n            2025\n        \n\n         –\n            26 July\n        \n\n        \n            2025\n        \n\nMoscow, Russia For Python events near you, please have a look at the Python events map. The Python events calendars are maintained by the events calendar team. Please see the events calendar project page for details on how to submit events, subscribe to the calendars, get Twitter feeds or embed them. Thank you. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/events/python-events/",
    "title": "Our Events | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. More 11 Aug.\n        \n            2025\n        \n\n         –\n            15 Aug.\n        \n\n        \n            2025\n        \n\nArusha, Tanzania 13 Aug.\n        \n            2025\n        \n\n         –\n            14 Aug.\n        \n\n        \n            2025\n        \n\nMogadishu, Somalia 15 Aug.\n        \n            2025\n        \n\n         –\n            17 Aug.\n        \n\n        \n            2025\n        \n\nSeoul, South Korea 18 Aug.\n        \n            2025\n        \n\n         –\n            22 Aug.\n        \n\n        \n            2025\n        \n\nKraków, Poland 23 Aug.\n        \n            2025\n        \n\n            2025\n        \n\nLomé, Togo 23 Aug.\n        \n            2025\n        \n\n         –\n            24 Aug.\n        \n\n        \n            2025\n        \n\nHaringhata Farm, West Bengal, India Subscribe to Python Event Calendars: For Python events near you, please have a look at the Python events map. The Python events calendars are maintained by the events calendar team. Please see the events calendar project page for details on how to submit events, subscribe to the calendars, get Twitter feeds or embed them. Thank you. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/events/python-user-group/",
    "title": "Our Events | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. More 08 Aug.\n        \n            2025\n        \n\n            2025\n        \n\nBuea, Cameroon 26 Aug.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            4pm UTC\n                 – 7pm\n                    UTC\n                \n        \n    \nAmsterdam, The Netherlands and Online 01 Sept.\n        \n            2025\n        \n\n            2025\n        \n\nLimbe, Cameroon 01 Sept.\n        \n            2025\n        \n\n         –\n            05 Sept.\n        \n\n        \n            2025\n        \n\nAbraka, Nigeria 04 Sept.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            6pm UTC\n                 – 8pm\n                    UTC\n                \n        \n    \nLeiden, The Netherlands 10 Sept.\n        \n            2025\n        \n\n            2025\n        \n\n        \n            4pm UTC\n                 – 7pm\n                    UTC\n                \n        \n    \nDüsseldorf, Germany Subscribe to Python Event Calendars: For Python events near you, please have a look at the Python events map. The Python events calendars are maintained by the events calendar team. Please see the events calendar project page for details on how to submit events, subscribe to the calendars, get Twitter feeds or embed them. Thank you. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/events/python-events/past/",
    "title": "Our Events | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. More 26 July\n        \n            2025\n        \n\n         –\n            27 July\n        \n\n        \n            2025\n        \n\nCleveland, USA 25 July\n        \n            2025\n        \n\n         –\n            26 July\n        \n\n        \n            2025\n        \n\nMoscow, Russia 14 July\n        \n            2025\n        \n\n         –\n            17 July\n        \n\n        \n            2025\n        \n\nSeattle, USA 14 July\n        \n            2025\n        \n\n         –\n            20 July\n        \n\n        \n            2025\n        \n\nPrague, Czech Republic 07 July\n        \n            2025\n        \n\n         –\n            13 July\n        \n\n        \n            2025\n        \n\nTacoma, WA, USA 04 July\n        \n            2025\n        \n\n         –\n            06 July\n        \n\n        \n            2025\n        \n\nBelém, Pará, Brazil Subscribe to Python Event Calendars: For Python events near you, please have a look at the Python events map. The Python events calendars are maintained by the events calendar team. Please see the events calendar project page for details on how to submit events, subscribe to the calendars, get Twitter feeds or embed them. Thank you. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/events/python-user-group/past/",
    "title": "Our Events | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. More 22 July\n        \n            2025\n        \n\n         –\n            23 July\n        \n\n        \n            2025\n        \n\nIndianapolis, IN, USA and Online 15 July\n        \n            2025\n        \n\n            2025\n        \n\n        \n            4pm UTC\n                 – 7pm\n                    UTC\n                \n        \n    \nAmsterdam, The Netherlands, and Online 10 July\n        \n            2025\n        \n\n            2025\n        \n\n        \n            6pm UTC\n                 – 8pm\n                    UTC\n                \n        \n    \nLeiden, The Netherlands 03 July\n        \n            2025\n        \n\n            2025\n        \n\n        \n            2am UTC\n                 – 4am\n                    UTC\n                \n        \n    \nOnline 29 June\n        \n            2025\n        \n\n            2025\n        \n\nSerekunda, Gambia 28 June\n        \n            2025\n        \n\n         –\n            29 June\n        \n\n        \n            2025\n        \n\nLeipzig, Germany Subscribe to Python Event Calendars: For Python events near you, please have a look at the Python events map. The Python events calendars are maintained by the events calendar team. Please see the events calendar project page for details on how to submit events, subscribe to the calendars, get Twitter feeds or embed them. Thank you. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event",
    "title": "PythonEventsCalendar - Python Wiki",
    "content": "Python Events Calendar\n\nIntroduction\nThe Python events calendar is a combination of Google calendars maintained by the Python Calendar Team and published on the following sites: http://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) https://twitter.com/PythonEvents (@PythonEvents) http://legacy.python.org/ (in the side bar on the left of all pages) As of January 2020, we have more then 520 events listed and the calendars are proving to be a really useful resource for the Python community. The events are also listed on a map mashup created by Luis Miguel Morillas: http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Introduction\nThe Python events calendar is a combination of Google calendars maintained by the Python Calendar Team and published on the following sites: http://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) https://twitter.com/PythonEvents (@PythonEvents) http://legacy.python.org/ (in the side bar on the left of all pages) As of January 2020, we have more then 520 events listed and the calendars are proving to be a really useful resource for the Python community. The events are also listed on a map mashup created by Luis Miguel Morillas: http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. The Python events calendar is a combination of Google calendars maintained by the Python Calendar Team and published on the following sites: http://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) https://twitter.com/PythonEvents (@PythonEvents) http://legacy.python.org/ (in the side bar on the left of all pages) As of January 2020, we have more then 520 events listed and the calendars are proving to be a really useful resource for the Python community. The events are also listed on a map mashup created by Luis Miguel Morillas: http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. http://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) https://twitter.com/PythonEvents (@PythonEvents) http://legacy.python.org/ (in the side bar on the left of all pages) As of January 2020, we have more then 520 events listed and the calendars are proving to be a really useful resource for the Python community. The events are also listed on a map mashup created by Luis Miguel Morillas: http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. The events are also listed on a map mashup created by Luis Miguel Morillas: http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. http://lmorillas.github.io/python_events/ Here's a snapshot showing all events from Jan 2012 - March 2016:   Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources  \nSubmitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Contents\nPython Events Calendar\nIntroduction\nSubmitting an Event\nBulk Submission of Events\nAvailable Calendars\niCal Downloads\nRSS Feeds\nTwitter Feed\nEmbedding Calendars\nKnown sites embedding the calendars\nPython Calendar Team\nMailing List\nGuidelines for entries\nGeneral Requirements\nCalendar Choice\nNotes regarding calendar entries\nCalendar embedding code\nResources Submitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Submitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Submitting an Event\nIf you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. If you would like to get new events listed on these calendars, please write to events@python.org using the following email template: Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Please add the following event:\n\n * name of the event: \n * type of event: \n * focus on Python: \n * approximate number of attendees: \n * location (incl. country): \n * dates/times/recurrence (incl. time zone): \n * HTML link using the format <a href=\"http://url/\">name of the event</a>: Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Please mention: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) the location (venue address, including city and country) the dates/times (including the time zone) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. the location (venue address, including city and country) a link with more details for the event (using HTML format \"<a href=\"http://url/\">name of the event</a>\"); URL shorteners are not permitted. For recurring events, please also include the recurrence information, e.g. \"monthly, every second Thursday\". Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Notes: For online events, please use \"Online Event\" as location. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these.  Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. \nBulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. For training sessions, courses, webinars and similar type of events, please post your event on the PythonTraining page. We currently do not have a calendar for these. Please leave at least 4 weeks notice when submitting events. The calendar team is volunteer driven. While we try to add new events swiftly, we cannot guarantee response times. Bulk Submission of Events\nIn some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. In some cases, the email format may not be suitable, e.g. if you want to submit several larger events in one go. For those cases, please list the events in the email sent to events@python.org, including the following details: the name of the event (including the user group name for user group events) type of event (conference, bar camp, sprint, user group meeting, etc.) focus on Python and approximate size (number of attendees) optional: a link with more details for the event for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. for each event, or as summary, so that we can tell which type of event you want entered and whether they are suitable for our calendars. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Please attach the event data as iCal file events.ics using the following entry format: title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. title: name of the event start and end dates: start day and end day of the event, entered as \"all day events\" for larger events, or as start and end date and time in the local timezone for smaller single day events location: city, [state,] country description: <a href=\"http://url/\">name of the event</a> The same notes as above apply to bulk submissions. \nAvailable Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Available Calendars\nPython Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget \niCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Python Events Calendar - mostly meant for conferences and larger local events Calendar ID: j7gov1cmnqr9tvg14k621j7t5c@group.calendar.google.com Python User Group Calendar - meant for user group events and other smaller local events Calendar ID: 3haig2m9msslkpf2tn1h56nn9g@group.calendar.google.com Both calendars combined - in a single calendar widget iCal Downloads\nPython Events Calendar Python User Group Calendar \nRSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Python Events Calendar Python User Group Calendar RSS Feeds\nNote: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Note: Google has switched off calendar RSS feeds on Nov 18 2015, so these links no longer work. Python Events Calendar Python User Group Calendar \nTwitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Python Events Calendar Python User Group Calendar Twitter Feed\n@PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. \nEmbedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. @PythonEvents This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. This Twitter account automatically gets all new entries from both calendars as tweets. The triggering is setup using IFTTT. Note that changes to the calendar entries are not reported on the Twitter feed. This appears to be a limitation of the IFTTT trigger. Embedding Calendars\nIf you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. If you'd like to embed the calendar into your site, please ping us at events@python.org before doing so. We'd like to keep a list of sites where the calendars are displayed. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. You can find the embedding code further below on this page. \nKnown sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Known sites embedding the calendars\nhttps://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) \nPython Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. https://www.python.org/ (in the events box on the front page and on the events calendar pages) http://www.pycon.org/ (calendar widget under the conference listing) http://legacy.python.org/ (in the side bar on the left of all pages) https://pythonz.net/events (a Russian site) Python Calendar Team\nThe following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. The following people are team members with admin rights to the calendars: Marc-Andre Lemburg These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. These are our team members with editing rights: Helio Loureiro Abdur-Rahmaan Janhangeer Olivia Sauls If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. If you want to join the team, please sign up to the python-events ML and send a short intro about yourself together with a Google compatible email address to the list. Thanks. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. The following people are inactive team members: Mayank Pathak Giri Prasath Dinakaran Sergey Sokolov Anton Caceres Marcelo Elizeche Landó Richard Jones Mike Müller Tarek Ziade Tetsuya Morimoto Skip Montanaro Tim Golden Mats Wichmann Laura Creighton Carol Willing Oier Etxaniz \nMailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Mailing List\nThe team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. The team uses the python-events mailing list for discussion and to process events. The events@python.org address is an alias for the python-events mailing list. https://mail.python.org/mailman/listinfo/python-events  As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. https://mail.python.org/mailman/listinfo/python-events As team member, it is vital for you to subscribe to this list, otherwise you won't get the incoming event notifications. \nGuidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Guidelines for entries\nIn order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. In order to make the decision of whether to add an event and where to add it easier, we've setup a few guidelines: \nGeneral Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. General Requirements\nEvents have to have some focus on Python: The event should either target Python, a project written in Python, or showcase Python in some form. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. The calendars should only list the events themselves. CFP deadlines, registration openings or similar announcements should not be listed in the calendars. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Training events, conferences which don't have a strong Python focus or offer a decent sized Python track, should not be listed on the calendars. \nCalendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Examples are Python conferences, conferences which have a Python track or tutorial, conferences at which Python is presented in some form. The same goes for conferences which target a project written mostly in Python (e.g. Plone, Django, OpenStack, etc.) and for other events such as sprints. For online events, such as PyWeek game jams, only a reasonably short events (no longer than two weeks) should be added as single event. For longer events, it's better to just add the start and end date as separate events to the calendar, or even just the start date and then mention the end in the description. The same criteria for calendars apply to online events as well. Please add these using Online or Online Event as location. For hybrid events (both in-person and virtual), mentioning and Online after the in-person location is possible as well. Calendar Choice\nPython Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Python User Group Calendar: This calendar is meant for user group activities which are open to Python people from outside the group (new prospective members and visitors from out of town). It is not meant for user group internal events. Examples: monthly local user group meetings, project sprints that are open for non-group members, workshops organized by user groups, Django Girls events. \nNotes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Python Events Calendar: This calendar should only receive entries for conferences of at least around 100 attendees, which at least around a 3rd non-local attendees. Smaller events with at least around 30 attendees such as sprints, barcamps, PyDays or smaller unconferences may also be added to this calendar if they are scheduled as separate event, will have a significant impact and reach out to larger regions. Examples: PyCon conferences, OSCON, Django conferences, Plone conference, Need-for-speed sprints, Python BarCamps/Unconferences/etc. Notes regarding calendar entries\nSome notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Some notes regarding entries for the admins: Please add the conference/event year to the title, if available. For user group meetings or other events that occur more often that's probably not necessary. For conference events please use \"All day\" entries (even for one day events), for user group events please add the start time and end time, if known. Full day user group events can also be added as \"All day\" event to give them a little more attention in the calendar. We use this for e.g. Django Girls workshops or similar full day events. Please include the location as \"City, Country\", not with the full address, since this can often be inaccurate and confused the Google maps mashup. For online events, please use \"Online\" or \"Online from City, Country\" (not sure whether this will work with mapping tool, but worth a try). Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. Please test the provided link and make sure that it's not a URL shortener URL or one which redirects to an unrelated website. We have had situations where the added events later linked to unrelated or spam pages. For familiar sounding event names, please check whether the requests are genuine. In case of naming collisions, try to get the organizers to talk to each other and sort out the issue. Please don't add more than the link to the description. If an event does not have a website, it's fine to add a one line text description. Background: The entries should not be used as advertisement text for events, only as reference. Please don't select an event color. People who subscribe to the calendars will have their own color preferences, so it's better to leave the default color set for all events. Reminders should not be setup for the events. They can be annoying for users subscribing to the calendars. We've always used \"Show me as: Available\" for new entries. No idea whether that matters or not. Please select \"Privacy: Default\" for new entries. This makes the entries public, since that's the default setting. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Please add a link to the event as description. We have standardized on using HTML links for this, e.g. <a href=\"http://www.pycon.fr/2012/>PyCon FR 2012</a>. Note that the Google calendar UI was changed some time ago to no longer accept HTML as input. Instead, you have to paste the HTML link directly into the description box. Calendar interfaces using iCal typically require adding the full HTML. All calendar admins can invite more people as calendar admins. When doing so, please add the new admins to the above list and ping the team by writing to events@python.org and also to get their email address added to the python-events mailing list. \nCalendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Calendar embedding code\nThis iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. This iframe code is used on pycon.org to display the calendar: <iframe src=\"https://www.google.com/calendar/embed?showTitle=0&amp;showCalendars=0&amp;height=400&amp;wkst=2&amp;bgcolor=%23FFFFFF&amp;src=j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com&amp;color=%23125A12&amp;src=3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com&amp;color=%232F6309&amp;ctz=Europe%2FLondon\" style=\" border-width:0 \" width=\"800\" height=\"450\" frameborder=\"0\" scrolling=\"no\">\n  <a href=\"https://www.google.com/calendar/ical/j7gov1cmnqr9tvg14k621j7t5c%40group.calendar.google.com/public/basic.ics\">Python Events iCal Calendar</a><br/>\n  <a href=\"https://www.google.com/calendar/ical/3haig2m9msslkpf2tn1h56nn9g%40group.calendar.google.com/public/basic.ics\">Python User Group iCal Calendar</a>\n</iframe>\nResources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Resources\nGoogle Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. Google Calendar API v2 Atom Reference. This describes the available RSS feed options available on the Google calendar feed links. PythonEventsCalendar  (last edited 2025-07-26 09:09:05 by MarcAndreLemburg) Unable to edit the page? See the FrontPage for instructions."
  },
  {
    "url": "http://legacy.python.org/Algorithms",
    "title": "Page Not Found",
    "content": "Add an event to this calendar. Times are shown in UTC/GMT. Add an event to this calendar. The URL you requested was not found on this server. Try our home page, or our search engines - or use one of\nthe other links on the left hand navigation."
  },
  {
    "url": "https://www.python.org/dev/",
    "title": "Python Developer's Guide | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Python's Developer Guide can be found at https://devguide.python.org/. The pieces of documentation hosted here are: The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://github.com/python/cpython/issues",
    "title": "GitHub · Where software is built",
    "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page."
  },
  {
    "url": "https://mail.python.org/mailman/listinfo/python-dev",
    "title": "Mailman 3 \nInfo | python-dev@python.org - python.org",
    "content": "Python core developers The discussions around the core Python development have moved on to our Discourse forum server: Core Development category Do not post general Python questions to this list.  For help with Python please see the Python help page. On this list the key Python developers discuss the future of the language and its implementation.  Topics include Python design issues, release mechanics, and maintenance of existing releases. More information on Python's development process can be found in the Python Developer's Guide. This is a fairly high volume mailing list so even the digests can result in substantial amounts of email occasionally.  Consider using\nGmane. To contact the list owners, use the following email address: python-dev-owner@python.org Archives To subscribe or unsubscribe from this list, please sign in first.\n        If you have not previously signed in, you may need to set up an account\n        with the appropriate email address. Sign In Postorius Documentation\n                    •\n                    GNU Mailman\n                    •\n                    Postorius Version 1.3.13"
  },
  {
    "url": "https://www.python.org/dev/core-mentorship/",
    "title": "Python Core Mentorship | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. The Python Core Mentorship Program is predicated on\nthe idea that Python core, and Python as a whole would be served by\nfurther lowering the barrier to entry of contribution to Python core\n(original proposal) The mission of the Python Core Mentor Program is to provide an open\nand welcoming place to connect students, programmers â and\nanyone interested in contributing to the Python core\ndevelopment. This project is based on the idea that the best way to\nwelcome new people into any project is a venue which connects them to\na variety of mentors who can assist in guiding them through the\ncontribution process, including discussions on lists such as python-dev,\nand python-ideas, the bug tracker, Git questions, code reviews, etc. Additionally, mentors assist in something incredibly critical to\nmaintain contributor interest: getting patches through the process and actually\ncommitted. We all know â not everyone who is mentor will have all the\nanswers, so mentors also act as conduits to others who will have the\nanswer. The most important point to make is that everyone is welcome and no\none, no matter who they are, is turned away. Keep to the code of conduct, and\nhelp those around you. The following code of conduct is not meant as a means for punishment, action\nor censorship for the mailing list or project. Instead, it is meant to set the\ntone and expectations and comfort level for mentors and those wishing to be\nmentored on the list. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/dev/security/",
    "title": "Python Security | Python.org",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. See the security issue information for pypi.org\nhere. The Python Software Foundation and the Python developer community take\nsecurity vulnerabilities very seriously.  A Python Security Response Team (PSRT) has\nbeen formed that does triage on all reported vulnerabilities and works to resolve them.  To reach the response team, send email to\nsecurity at python dot org. Only the response team members will see your\nemail, and it will be treated confidentially. The PSRT mailing list is tightly controlled, so you can have confidence that\nyour security issue will only be read by a highly trusted cabal of Python\ndevelopers.  If for some reason you wish to further encrypt your message to this mailing list\n(for example, if your mail system does not use TLS),\nyou can use our shared OpenPGP key which is also available on the public\nkeyservers. The PSRT accepts security reports for the following projects: The PSRT does not accept reports for third-party redistributions of Python or pip.\nThose reports should be directed towards their corresponding distribution security contact. The following is an overview of the vulnerability handling process from reporting to disclosure: While we sincerely appreciate and encourage reports of suspected security problems in supported Python releases and the PSF web infrastructure, please note that the Python Software Foundation does not run any bug bounty programs. We are a nonprofit organization, depending on donation and support from the community. Security advisories are published to multiple public locations. Advisories are sent via email to the\nsecurity-announce@python.org mailing list. Subscribe to the mailing list if you'd like to be updated\non newly published security advisories. The mailing list has a public archive including all historical advisories sent to the list. There is also an advisory database published to GitHub using the Open Source Vulnerability (OSV) format which can be consumed\nusing automated tooling. If you need to contact the Python Software Foundation CNA directly, such as for updating or disputing a CVE record,\nyou can send an email to cna at python dot org. Be sure that the CVE record in question was\nissued by the PSF CNA and not a different CNA. Key fingerprint: Key data: The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://github.com/python/pythondotorg/issues",
    "title": "GitHub · Where software is built",
    "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation. There was an error while loading. Please reload this page. There was an error while loading. Please reload this page."
  },
  {
    "url": "https://status.python.org/",
    "title": "Python Infrastructure Status",
    "content": "Resend OTP in:  seconds Didn't receive the OTP?\n                    Resend OTP Resend OTP in: 30 seconds Didn't receive the OTP?\n                      Resend OTP The URL we should send the webhooks to We'll send you email if your endpoint fails No incidents or maintenance related to this downtime. No incidents reported today. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported. No incidents reported."
  },
  {
    "url": "https://www.python.org/psf-landing/",
    "title": "Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. In 2024, the PSF awarded $655,000 USD to 257 groups or individuals in 61 countries around the world. We support and maintain python.org,\r\n          The Python Package Index,\r\n          Python Documentation,\r\n          and many other services the Python Community relies on. We produce and underwrite the\r\n          PyCon US Conference,\r\n          the largest annual gathering for the Python community.\r\n         Support from sponsors, attendees, PyLadies, and CPython enabled us to award more than $384,000 USD in travel grants to 254 attendees for PyCon US 2025. Help the PSF promote, protect, and advance the Python programming language and community! Membership FAQ Assist the foundation's goals with a donation. The PSF is a recognized 501(c)(3) non-profit organization. How to Contribute Learn how you can help the PSF and the greater Python community! How to Volunteer Without our sponsors we wouldn't be able to help the Python community grow and prosper. Sponsorship Possibilities The Python Software Foundation welcomes grant proposals for projects related to the development of Python, Python-related technology, and educational resources. Proposal Guidelines, FAQ and Examples Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://www.python.org/about/legal/",
    "title": "Legal Statements | Python Software Foundation",
    "content": "Notice: While JavaScript is not essential for this website, your interaction with the content will be limited. Please turn JavaScript on for the full experience. Trademarks Python and PyCon are trademarks or registered trademarks of the\nPython Software Foundation. All other trademarks mentioned on this website are the property of their\nrespective owners. Copyright Except where otherwise specified, the contents of this website are\ncopyright Â© 1990-2014, Python Software Foundation,\n9450 SW Gemini Dr., ECM# 90772, Beaverton, OR 97008, USA.  All rights reserved. Licenses Python, its standard libraries, and Jython, are distributed under the\nPython License. The intellectual property rights behind\nPython and Jython are held and managed by the Python Software Foundation. The licenses, trademarks, and copyrights for other implementations of Python\n(such as IronPython, Stackless Python, and PyPy) may vary and are managed by\ntheir respective owners. Third-Party Content The Python Software Foundation (âPSFâ) does not claim ownership of any\nthird-party code or content (âthird party contentâ) placed on the web\nsite and has no obligation of any kind with respect to such third\nparty content. Any third party content provided in connection with\nthis web site is provided on a non-confidential basis. The PSF is free\nto use or disseminate such content on an unrestricted basis for any\npurpose, and third party content providers grant the PSF and all other\nusers of the web site an irrevocable, worldwide, royalty-free,\nnonexclusive license to reproduce, distribute, transmit, display,\nperform, and publish such content, including in digital form. Third party content providers represent and warrant that they have\nobtained the proper governmental authorizations for the export and\nreexport of any software or other content contributed to this web site\nby the third-party content provider, and further affirm that any\nUnited States-sourced cryptographic software is not intended for use\nby a foreign government end-user. Individuals and organizations are advised that the PyPI website is hosted in\nthe US, with mirrors in several countries outside the US (see\nhttp://www.pypi-mirrors.org/).  Any uploads of packages must comply\nwith United States export controls under the Export Administration\nRegulations. Legal Mailing List If you have any questions, please send them to the legal mailing list at:  legal@python.org. The Python Software Foundation is the organization behind Python. Become a member of the PSF and help advance the software and our mission. Copyright ©2001-2025.\n                             Python Software Foundation\n                             Legal Statements\n                             Privacy Notice"
  },
  {
    "url": "https://policies.python.org/python.org/Privacy-Notice/",
    "title": "Privacy Notice - Python Software Foundation Policies",
    "content": "Last updated October 4, 2024 This privacy notice describes what personal information the Python Software Foundation (“PSF”)\ncollects from  users,\nwhen and how we share that information,\nand why.\nThis notice is an addendum to the PSF Privacy Notice,\nwhich also applies to  users. PSF may collect the following information from  users: The PSF uses some third party services to assist with operating online services supporting . Specifically, we use: Amazon Web Services (AWS) to host the PyCon US website. We store your registration information on AWS servers. Their use of this information is subject to the Data Processing Addendum between AWS and PSF and the AWS Privacy Notice (https://aws.amazon.com/privacy/).] Fastly to host the  website and services. Your information transits their servers. Their use of this information is subject to the Data Processing Terms between Fastly and PSF and the Fastly Privacy Policy. Sentry to aggregate error information from the  website and services. Some personal information may be stored on their servers. Their use of this information is subject to the Data Processing Addendum between Sentry and PSF and the Sentry Privacy Policy. Google Analytics to aggregate traffic analytics. Some personal information may be transited to their servers. Their use of this information is subject to the Google Ads Data Processing Terms between Google and PSF and the Google Privacy Policy. Plausible to aggregate traffic analytics. Some personal information may be stored on ther servers. Their use of this information is subject to the Plausible Analytics Data Processing Agreement between Plausible and PSF asnd the Plausible Privacy Policy. We use the information you provide in the following ways, in addition to the uses described in the PSF Privacy Notice:"
  },
  {
    "url": "http://legacy.python.org/Natural_language_processing",
    "title": "Page Not Found",
    "content": "Add an event to this calendar. Times are shown in UTC/GMT. Add an event to this calendar. The URL you requested was not found on this server. Try our home page, or our search engines - or use one of\nthe other links on the left hand navigation."
  },
  {
    "url": "https://developer.mozilla.org/en-US/",
    "title": "MDN Web Docs",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Documenting web technologies, including CSS, HTML, and JavaScript, since 2005. A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN. The CSS anchor positioning module defines features that allow you to tether elements together. Certain elements are defined as anchor elements; anchor-positioned elements can then have their size and position set based on the size and location of the anchor elements to which they are bound. This article explains the theory behind how the View Transition API works, how to create view transitions and customize the transition animations, and how to manipulate active view transitions. This covers view transitions for both DOM state updates in a single-page app (SPA), and navigating between documents in a multi-page app (MPA). The Temporal object enables date and time management in various scenarios, including built-in time zone and calendar representation, wall-clock time conversions, arithmetics, formatting, and more. It is designed as a full replacement for the Date object. MDN 2024 content projectsdeveloper.mozilla.org A new learning experience on MDNdeveloper.mozilla.org Introducing the new MDN Community pagedeveloper.mozilla.org [es] auto-fix content issuesmdn/translated-content [ru] sync translated contentmdn/translated-content 2025/07/10 æç¹ã®è±èªçã«åæmdn/translated-content Fix typo `Scx`->`scx` for short name of `Script_Extensions` propmdn/content 2025/07/10 æç¹ã®è±èªçã«åæmdn/translated-content 2025/07/29 æç¹ã®è±èªçã«åæmdn/translated-content the example comment is out of date, now Chrome give the same message with Firefox/Safarimdn/content Fix hyperlinkmdn/content Feat/property examplemdn/content <selectedcontent>: update to description and specificationmdn/content Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web",
    "title": "Web technology for developers | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support The open Web presents incredible opportunities for developers. To take full advantage of these technologies, you need to know how to use them. Below you'll find links to our Web technology documentation. The Web Developer Guides provide practical, how-to content to help you use Web technologies for your goals or needs. Tutorials to take you step-by-step through learning HTML, CSS, JavaScript, and Web APIs. Enabling as many people as possible to use websites, even when those people's abilities are limited in some way. Making content as available and interactive as possible, as soon as possible. Protecting users' personal data. Protecting users from data leaks and data theft, side-channel attacks, and attacks such as cross-site scripting, content injection, and click-jacking. Definitions of Web-related terms. JavaScript programming APIs you can use to build apps on the Web. HTML provides the fundamental building blocks for structuring Web documents and apps. Cascading Style Sheets are used to describe the appearance of Web documents and apps. JavaScript is the Web's native programming language. WebAssembly allows programs written in C, C++, Rust, Swift, C#, Go, and more to run on the Web. HTTP is the fundamental Internet protocol for fetching documents, stylesheets, scripts, images, videos, fonts, and other resources over the Web â and for sending data back to Web servers. Formats, codecs, protocols, APIs, and techniques for embedding and streaming video, audio, and image content in Web documents and apps. Scalable Vector Graphics lets you create images that scale smoothly to any size. MathML lets you display complex mathematical notation on the Web. Uniform Resource Identifiers are used by various technologies, including the browser itself via the address bar, to identify resources in various ways. WebDriver is a browser-automation mechanism for remotely controlling a browser by emulating the actions of a real person using the browser. It's widely used for cross-browser testing of Web apps. Web Extensions are a way for you to give users enhanced capabilities in their browsers â for doing things such as blocking ads and other content, customizing the appearance of pages, and more. Web App Manifests let you enable users to install Web apps to their device home screens, with aspects such as portrait/landscape screen orientation and display mode (e.g., full screen) pre-set. Progressive Web Apps provide a user experience similar to native mobile apps. OpenSearch allows a website to describe a search engine for itself, so that a browser or other client application can use that search engine. The Extensible Markup Language is a strict serialization of the Document Object Model. Extensible Stylesheet Language Transformations is an XML-based language used, in conjunction with specialized processing software, for the transformation of XML documents. XPath uses a non-XML syntax to provide a flexible way of addressing (pointing to) different parts of an XML document. It can also be used to test addressed nodes within a document to determine whether they match a pattern or not. EXSLT a set of extensions to XSLT. Documentation for the set of web-developer tools built into Firefox. Documentation for the set of web-developer tools built into Chrome. Documentation for the set of web-developer tools built into Safari. Documentation for the set of web-developer tools built into Edge. This page was last modified on Jul 29, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/HTML",
    "title": "HTML: HyperText Markup Language | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support HTML (HyperText Markup Language) is the most basic building block of the Web. It defines the meaning and structure of web content. Other technologies besides HTML are generally used to describe a web page's appearance/presentation (CSS) or functionality/behavior (JavaScript). \"Hypertext\" refers to links that connect web pages to one another, either within a single website or between websites. Links are a fundamental aspect of the Web. By uploading content to the Internet and linking it to pages created by other people, you become an active participant in the World Wide Web. HTML uses \"markup\" to annotate text, images, and other content for display in a Web browser. HTML markup includes special \"elements\" such as <head>, <title>, <body>, <header>, <footer>, <article>, <section>, <p>, <div>, <span>, <img>, <aside>, <audio>, <canvas>, <datalist>, <details>, <embed>, <nav>, <search>, <output>, <progress>, <video>, <ul>, <ol>, <li> and many others. An HTML element is set off from other text in a document by \"tags\", which consist of the element name surrounded by < and >. The name of an element inside a tag is case-insensitive. That is, it can be written in uppercase, lowercase, or a mixture. For example, the <title> tag can be written as <Title>, <TITLE>, or in any other way. However, the convention and recommended practice is to write tags in lowercase. The articles below can help you learn more about HTML. This article provides a brief tour of what HTML is and how to use it, aimed at people who are completely new to web development. Our Learn web development section's HTML module teaches all the HTML fundamentals from the ground up. The HTML guides help you build with HTML on the web, covering topics such as forms, CORS, content preloading, and responsive images. Forms are a very important part of the Web â these provide much of the functionality you need for interacting with websites, e.g., registering and logging in, sending feedback, buying products, and more. This module gets you started with creating the client-side/front-end parts of forms. The crossorigin attribute, in combination with an appropriate CORS header, allows images defined by the <img> element to be loaded from foreign origins and used in a <canvas> element as if they were being loaded from the current origin. Some HTML elements that provide support for CORS, such as <img> or <video>, have a crossorigin attribute (crossOrigin property), which lets you configure the CORS requests for the element's fetched data. The preload value of the <link> element's rel attribute allows you to write declarative fetch requests in your HTML <head>, specifying resources that your pages will need very soon after loading, which you therefore want to start preloading early in the lifecycle of a page load, before the browser's main rendering machinery kicks in. This ensures that they are made available earlier and are less likely to block the page's first render, leading to performance improvements. This article provides a basic guide to how preload works. In this article, we'll learn about the concept of responsive images â images that work well on devices with widely differing screen sizes, resolutions, and other such features â and look at what tools HTML provides to help implement them. This helps to improve performance across different devices. HTML consists of elements, each of which may be modified by some number of attributes. HTML documents are connected to each other with links. Browse a list of all HTML elements. Elements in HTML have attributes. These are additional values that configure the elements or adjust their behavior in various ways. Global attributes may be specified on all HTML elements, even those not specified in the standard. This means that any non-standard elements must still permit these attributes, even though those elements make the document HTML5-noncompliant. HTML elements are usually \"inline-level\" or \"block-level\" elements. An inline-level element occupies only the space bounded by the tags that define it. A block-level element occupies the entire space of its parent element (container), thereby creating a \"block box\". HTML comments are used to add explanatory notes to the markup or to prevent the browser from interpreting specific parts of the document. The <audio> and <video> elements allow you to play audio and video media natively within your content without the need for external software support. HTML is comprised of several kinds of content, each of which is allowed to be used in certain contexts and is disallowed in others. Similarly, each context has a set of other content categories it can contain and elements that can or can't be used in them. This is a guide to these categories. Historical information on quirks mode and standards mode. This article covers most of the ways you use CSS to add color to HTML content, listing what parts of HTML documents can be colored and what CSS properties to use when doing so. This page was last modified on Jul 9, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/CSS",
    "title": "CSS: Cascading Style Sheets | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Cascading Style Sheets (CSS) is a stylesheet language used to describe the presentation of a document written in HTML or XML (including XML dialects such as SVG, MathML or XHTML). CSS describes how elements should be rendered on screen, on paper, in speech, or on other media. CSS is among the core languages of the open web and is standardized across Web browsers according to W3C specifications. Previously, the development of various parts of CSS specification was done synchronously, which allowed the versioning of the latest recommendations. You might have heard about CSS1, CSS2.1, or even CSS3. There will never be a CSS3 or a CSS4; rather, everything is now just \"CSS\" with individual CSS modules having version numbers. After CSS 2.1, the scope of the specification increased significantly and the progress on different CSS modules started to differ so much, that it became more effective to develop and release recommendations separately per module. Instead of versioning the CSS specification, W3C now periodically takes a snapshot of the latest stable state of the CSS specification and individual modules progress. CSS modules now have version numbers, or levels, such as CSS Color Module Level 5. This article provides a brief tour of what CSS is and how to use it, aimed at people who are completely new to web development. Our Learn web development section's CSS basics module teaches CSS fundamentals from the ground up. Here we look at fundamentals including setting font, boldness, italics, line and letter spacing, drop shadows, and other text features. We round off the module by looking at applying custom fonts to your page, and styling lists and links. Now it's time to look at how to correctly lay out your boxes in relation to one another, and the browser viewport. This module looks at floats, positioning, other modern layout tools, and building responsive designs that will adapt to different devices, screen sizes, and resolutions. The CSS reference is an exhaustive reference for seasoned Web developers, describing every property and concept of CSS, including: The CSS layout cookbook aims to bring together recipes for common layout patterns, things you might need to implement in your sites. In addition to providing code you can use as a starting point in your projects, these recipes highlight the different ways layout specifications can be used and the choices you can make as a developer. This page was last modified on Jul 14, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/JavaScript",
    "title": "JavaScript | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support JavaScript (JS) is a lightweight interpreted (or just-in-time compiled) programming language with first-class functions. While it is most well-known as the scripting language for Web pages, many non-browser environments also use it, such as Node.js, Apache CouchDB and Adobe Acrobat. JavaScript is a prototype-based, garbage-collected, dynamic language, supporting multiple paradigms such as imperative, functional, and object-oriented. JavaScript's dynamic capabilities include runtime object construction, variable parameter lists, function variables, dynamic script creation (via eval), object introspection (via for...in and Object utilities), and source-code recovery (JavaScript functions store their source text and can be retrieved through toString()). This section is dedicated to the JavaScript language itself, and not the parts that are specific to Web pages or other host environments. For information about APIs that are specific to Web pages, please see Web APIs and DOM. The standards for JavaScript are the ECMAScript Language Specification (ECMA-262) and the ECMAScript Internationalization API specification (ECMA-402). As soon as one browser implements a feature, we try to document it. This means that cases where some proposals for new ECMAScript features have already been implemented in browsers, documentation and examples in MDN articles may use some of those new features. Most of the time, this happens between the stages 3 and 4, and is usually before the spec is officially published. Do not confuse JavaScript with the Java programming language â JavaScript is not \"Interpreted Java\". Both \"Java\" and \"JavaScript\" are trademarks or registered trademarks of Oracle in the U.S. and other countries. However, the two programming languages have very different syntax, semantics, and use. JavaScript documentation of core language features (pure ECMAScript, for the most part) includes the following: For more information about JavaScript specifications and related technologies, see JavaScript technologies overview. Learn how to program in JavaScript from the ground up with our beginner's tutorials. This article provides a brief tour of what JavaScript is and how to use it, aimed at people who are completely new to web development. Our Learn web development section's JavaScript module teaches all the JavaScript fundamentals from the ground up. JavaScript frameworks are an essential part of modern front-end web development, providing developers with tried and tested tools for building scalable, interactive web applications. Many modern companies use frameworks as a standard part of their tooling, so many front-end development jobs now require framework experience. In this set of articles, we aim to give you a comfortable starting point to help you begin learning frameworks. A much more detailed guide to the JavaScript language, aimed at those with previous programming experience either in JavaScript or another language. The object-oriented nature of JavaScript is important to understand if you want to go further with your knowledge of the language and write more efficient code, therefore we've provided this module to help you. In this module, we take a look at asynchronous JavaScript, why it is important, and how it can be used to effectively handle potential blocking operations, such as fetching resources from a server. Explores what APIs are, and how to use some of the most common APIs you'll come across often in your development work. An overview of the basic syntax and semantics of JavaScript for those coming from other programming languages to get up to speed. Overview of available data structures in JavaScript. JavaScript provides three different value comparison operations: strict equality using ===, loose equality using ==, and the Object.is() method. How different methods that visit a group of object properties one-by-one handle the enumerability and ownership of properties. A closure is the combination of a function and the lexical environment within which that function was declared. Explanation of the widely misunderstood and underestimated prototype-based inheritance. Memory life cycle and garbage collection in JavaScript. Browse the complete JavaScript reference documentation. Get to know standard built-in objects: Array, Boolean, Error, Function, JSON, Math, Number, Object, RegExp, String, Map, Set, WeakMap, WeakSet, and others. Learn more about the behavior of JavaScript's operators instanceof, typeof, new, this, the operator precedence, and more. Learn how do-while, for-in, for-of, try-catch, let, var, const, if-else, switch, and more JavaScript statements and keywords work. Learn how to work with JavaScript's functions to develop your applications. JavaScript classes are the most appropriate way to do object-oriented programming. This page was last modified on Jul 8, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/HTTP",
    "title": "HTTP: Hypertext Transfer Protocol | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support HTTP is an application-layer protocol for transmitting hypermedia documents, such as HTML.\nIt was designed for communication between web browsers and web servers, but it can also be used for other purposes, such as machine-to-machine communication, programmatic access to APIs, and more. HTTP follows a classical client-server model, with a client opening a connection to make a request, then waiting until it receives a response from the server.\nHTTP is a stateless protocol, meaning that the server does not keep any session data between two requests, although the later addition of cookies adds state to some client-server interactions. The HTTP reference documentation contains detailed information about headers, request methods, status responses, and lists relevant specifications and standards documents. Message headers are used to send metadata about a resource or a HTTP message, and to describe the behavior of the client or the server. Request methods indicate the purpose of the request and what is expected if the request is successful.\nThe most common methods are GET and POST for retrieving and sending data to servers, respectively, but there are other methods which serve different purposes. Response status codes indicate the outcome of a specific HTTP request.\nResponses are grouped in five classes: informational, successful, redirections, client errors, and server errors. This page lists relevant resources about HTTP since it was first specified in the early 1990s. The following subsections are also notable: The Content-Security-Policy (CSP) response header allows website administrators to specify which resources the user agent is allowed to load for a given page.\nThis section lists directives that can be used in a CSP header, with individual documentation pages that describe how the directives work and how to use them. The Permissions-Policy response header provides a mechanism to allow or deny the use of browser features in a document or within any <iframe> element in the document.\nThis section lists directives that can be used in a Permissions-Policy header, with individual documentation pages that describe how the directives work and how to use them. HTTP is an extensible protocol that relies on concepts like resources and Uniform Resource Identifiers (URIs), a basic message structure, and client-server communication model.\nOn top of these concepts, numerous extensions have been developed over the years that add functionality and updated semantics, including additional HTTP methods and headers. The HTTP guides are listed in order from general overviews to specialized, use-case-driven topics.\nBeginners are encouraged to start with the foundational guides before exploring more focused articles. The basic features of HTTP, what it can do, its intended use in web architecture, and its position in the protocol stack. HTTP was created in the early 1990s and has been extended several times.\nThis article goes through its history and describes HTTP/0.9, HTTP/1.0, HTTP/1.1, through HTTP/2 and HTTP/3, as well as novelties introduced over the years. Describes the flow of an HTTP session, from establishing a connection, sending a request, to receiving a response. HTTP messages transmitted as requests and responses have a defined structure.\nThis article describes this general structure, its purpose, and the different types of messages. Since HTTP/1.0, different types of content can be transmitted.\nThis article explains how this is accomplished using the Content-Type header and the MIME standard.\nA shortlist of common types used by web developers can be found in Common MIME types. Browsers and servers compress their messages before sending them over the network to reduce the amount of data that needs to be transmitted, improving transfer speed and bandwidth utilization. Caching is a highly important mechanism for delivering fast experiences on the Web and for efficient use of resources.\nThis article describes different methods of caching and how to use HTTP headers to control them. Authentication is a way to verify the identity of a client when making requests to a server.\nIt ensures that only authorized users or systems can access certain resources. Although HTTP is a stateless protocol, a server can send a Set-Cookie header with the response.\nThe client then returns the cookie's value with every subsequent request to the server in the form of a Cookie request header.\nThis adds the ability to store and exchange a small amount of data which effectively adds state to some client-server interactions. URL redirection, also known as URL forwarding, is a technique to give more than one URL address to a page, a form, a whole website, or a web application.\nHTTP has a special kind of response, called a HTTP redirect, for this operation. In conditional requests, the outcome of a request depends on the value of a validator in the request.\nThis method is used heavily in caching and use cases such as resuming a download, preventing lost updates when modifying a document on the server, and more. A range request asks the server to send a specific part (or parts) of a resource back to a client instead of the full resource.\nRange requests are useful for cases when a client knows they need only part of a large file, or for cases where an application allows the user to pause and resume a download. HTTP defines a set of message headers, starting with Accept as a way for a browser to announce the format, language, or encoding it prefers.\nThis article explains how this advertisement happens, how the server is expected to react, and how it chooses the most adequate response to a request. HTTP/1.1 was the first version of HTTP to support persistent connections and pipelining.\nThis article explains both concepts, including the pros and cons of each. HTTP/1.1 provides a mechanism to upgrade an already-established connection to a different protocol using the Upgrade header.\nA client can upgrade a connection from HTTP/1.1 to HTTP/2, or an HTTP(S) connection to a WebSocket (ws / wss). A proxy can be on the user's local computer, or anywhere between the user's computer and a destination server on the Internet.\nThis page outlines some basics about proxies and introduces a few configuration options. Client Hints are a set of response headers that a server can use to proactively request information from a client about the device, network, user, and user-agent-specific preferences.\nThe server can then determine which resources to send, based on the information that the client chooses to provide. Network Error Logging is a mechanism that can be configured via the NEL HTTP response header.\nThis experimental header allows websites and applications to opt-in to receive reports about failed (or even successful) network fetches from supporting browsers. It's very rarely a good idea to use user agent sniffing to detect a browser, but there are edge cases that require it.\nThis document will guide you in doing this as correctly as possible when this is necessary, with an emphasis on considerations to make before embarking on this route. Permissions Policy provides mechanisms for web developers to explicitly declare what functionality can and cannot be used on a website.\nYou define a set of \"policies\" that restrict what APIs the site's code can access or modify the browser's default behavior for certain features. Cross-site HTTP requests are requests for resources from a different domain than that of the resource making the request.\nWeb pages today very commonly load cross-site resources, for example, a page 'Domain A' (http://domaina.example/) requests an image on 'Domain B' (http://domainb.foo/image.jpg) via the img element.\nCORS allows web developers to control how their site reacts to cross-site requests. CSP allows website administrators to use the Content-Security-Policy response header to control which resources the client is allowed to load for a given page.\nThe CSP guide describes the overall Content Security Policy mechanism which helps detect and mitigate certain types of attacks, including Cross-Site Scripting (XSS) and data injection attacks. CORP lets websites and applications opt in to protection against specific requests from other origins (such as those issued with elements like <script> and <img>), to mitigate speculative side-channel attacks. A collection of tips to help operational teams with creating secure web applications. Uniform Resource Identifiers (URIs) are used to describe and locate resources on the web and are an essential component in HTTP requests. This guide covers a few server configuration changes that may be necessary for your web server to correctly serve Ogg media files.\nThis information may also be useful if you encounter other media types your server isn't already configured to recognize. Helpful tools and resources for understanding and debugging HTTP. Network monitor A project designed to help developers, system administrators, and security professionals configure their sites safely and securely. Tools to check your cache-related headers. An HTTP/2 client, server and proxy implementation written in C with load test and benchmarking tools and an HPACK encoder and decoder. A command-line tool for transferring data specified with URL syntax.\nSupports HTTP, HTTPS, WS, WSS, among many other protocols. A very comprehensive article on browser internals and request flow through HTTP protocol. This page was last modified on Jul 4, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/API",
    "title": "Web APIs | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support When writing code for the Web, there are a large number of Web APIs available. Below is a list of all the APIs and interfaces (object types) that you may be able to use while developing your Web app or site. Web APIs are typically used with JavaScript, although this doesn't always have to be the case. This is a list of all the APIs that are available. This is a list of all the interfaces (that is, types of objects) that are available. This page was last modified on Jul 29, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions",
    "title": "Browser extensions - Mozilla | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Extensions, or add-ons, can modify and enhance the capability of a browser. Extensions for Firefox are built using the WebExtensions API cross-browser technology. The technology for extensions in Firefox is, to a large extent, compatible with the extension API supported by Chromium-based browsers (such as Google Chrome, Microsoft Edge, Opera, Vivaldi). In most cases, extensions written for Chromium-based browsers run in Firefox with just a few changes. Whether you're just beginning or looking for more advanced advice, learn about how extensions work and use the WebExtensions API from our extensive range of tutorials and guides. Get comprehensive details about the methods, properties, types, and events of the WebExtensions APIs and full details about the manifest keys. Discover how to build and publish extensions for Firefox: get the lowdown on developer tools, publication and distribution, and porting on Extension Workshop. Note:\nIf you have ideas or questions or need help, you can reach us on the community forum or in the Add-ons Room on Matrix. Discover what extensions can do before building your first extension and your second extension. Learn about the anatomy of an extension and get an overview of the extension development and publication workflow, Firefox style. Explore a little deeper with a comprehensive selection of example extensions that you can run right in Firefox. Continue your learning by discovering a list of resources to follow. Get detailed information on the concepts that underpin extensions. Discover all the user interface components you can use in your extensions, with coding examples and tips. A range of tutorials to get you started on specific aspects of extension development. When you are ready to create your extension for Firefox or port your Chrome extension, head over to Extension Workshop. It has details on: Get comprehensive details about the methods, properties, types, and events for all the JavaScript APIs. There is also detailed information about the compatibility of each API with the major browsers. Most reference pages also include coding examples and links to the extension examples that use the API. Get full details about the manifest keys, including all their properties and settings. This page was last modified on Jul 17, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web/Accessibility",
    "title": "Accessibility | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Accessibility (often abbreviated to A11y â as in, \"a\", then 11 characters, and then \"y\") in web development means enabling as many people as possible to use websites, even when those people's abilities are limited in some way. For many people, technology makes things easier. For people with disabilities, technology makes things possible. Accessibility means developing content to be as accessible as possible, no matter an individual's physical and cognitive abilities and how they access the web. The Web is fundamentally designed to work for all people, whatever their hardware, software, language, location, or ability.\nWhen the Web meets this goal, it is accessible to people with a diverse range of hearing, movement, sight, and cognitive ability. \nâ (W3C - Accessibility) The Accessibility guides cover authoring principles, WCAG compliance, accessible widgets and navigation, mobile accessibility, and other key topics that will help you understand why accessibility is crucial for the web and how to improve it in your projects. This document lists guidelines and regulations, how-to's, and tools for checking and repairing accessibility problems with websites. This article discusses making web content accessible for those with vestibular disorders, and those who support them, by taking advantage of personalization and accessibility settings built into the operating systems. Most JavaScript libraries offer a library of client-side widgets that mimic the behavior of familiar desktop interfaces.\nWhile this results in a widget that looks like its desktop counterpart, there usually isn't enough semantic information in the markup to be usable by an assistive technology.\nThis document describes techniques to improve accessibility of such widgets. Until now, web developers who wanted to make their styled <div> and <span> based widgets accessible have lacked proper techniques.\nKeyboard accessibility is part of the minimum accessibility requirements, which a developer should be aware of.\nThis document describes techniques to make JavaScript widgets accessible with the keyboard. This document provides a concise checklist of accessibility requirements for mobile app developers. A set of articles that provide quick explanations to help you understand the steps that need to be taken to conform to the recommendations outlined in the Web Content Accessibility Guidelines (WCAG). Cognitive accessibility covers accessibility considerations for people with cognition and learning disabilities.\nThis document introduces cognitive accessibility and improving accessibility of the web for people with cognitive and learning differences. This document describes visual patterns that can induce physical symptoms in people who have photosensitive epilepsy, vestibular disorders, or other perceptual issues. While understanding color, luminance, and saturation is important for design and readability for all sighted users, they are essential for those with reduced vision and color-deficient vision and those with specific neurological, cognitive, and other impairments. Some types of visual web content can induce seizures in people with certain brain disorders.\nThis article helps you understand the types of content that can be problematic and find tools and strategies to help you avoid them. This is a collection of articles to learn how to use Accessible Rich Internet Applications (ARIA) to make your HTML documents more accessible. The MDN Accessibility Learning Area contains modern, up-to-date tutorials covering the following accessibility essentials: This article starts off the module with a good look at what accessibility actually is â this includes what groups of people we need to consider and why, what tools different people use to interact with the Web, and how we can make accessibility part of our web development workflow. A great deal of web content can be made accessible just by making sure that the correct HTML elements are used for the correct purpose at all times. This article looks in detail at how HTML can be used to ensure maximum accessibility. CSS and JavaScript, when used properly, also have the potential to allow for accessible web experiences. They can significantly harm accessibility if misused. This article outlines some CSS and JavaScript best practices that should be considered to ensure that even complex content is as accessible as possible. Following on from the previous article, sometimes making complex UI controls that involve unsemantic HTML and dynamic JavaScript-updated content can be difficult. WAI-ARIA is a technology that can help with such problems by adding in further semantics that browsers and assistive technologies can recognize and let users know what is going on. Here we'll show how to use it at a basic level to improve accessibility. Another category of content that can create accessibility problems is multimedia â video, audio, and image content need to be given proper textual alternatives so that they can be understood by assistive technologies and their users. This article shows how. With web access on mobile devices being so popular and popular platforms such as iOS and Android having fully-fledged accessibility tools, it is important to consider the accessibility of your web content on these platforms. This article looks at mobile-specific accessibility considerations. Reference documentation for Accessible Rich Internet Applications (ARIA) attributes and roles. This page was last modified on May 7, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn_web_development",
    "title": "Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Welcome to MDN Learning Web Development (also known as Learn). This resource provides a structured set of tutorials teaching the essential skills and practices for being a successful front-end developer, along with challenges and further recommended resources. Teaches the essential skills and knowledge every front-end developer needs for career success and industry relevance, as defined in the MDN Curriculum. Created by the MDN community and refined with insights from students, educators, and developers from the broader web community. Designed to take you from \"beginner\" to \"comfortable\" (not \"beginner\" to \"expert\"), giving you enough knowledge to use more advanced resources (such as the rest of MDN). Note:\nLast updated: December 2024 (see changelog). Our Getting started modules provide setup tutorials and essential concepts and background information for complete beginners. You should start here if you are a complete beginner (i.e., you've not installed a code editor or written any code yet). Our Core modules provide a structured set of tutorials teaching the essential skills and practices for being a successful front-end developer. Our Extension modules cover useful additional skills to learn as you start to expand your knowledge and develop specialisms. Go onto these after you finish our Core. Use our modules to guide your teaching, check out our Educators page for more ideas, or enroll your students in Scrimba's Frontend Developer Career PathMDN learning partner. Throughout the course, you'll find several articles designed to help you assess whether you have understood what we are teaching you in the course. There are two types: Most of the questions feature HTML/CSS/JavaScript code blocks that show the starting code for each task. The recommended way to complete each one is to press the \"Play\" button in one of the code blocks to open the example in the MDN Playground and then edit the code according to the question instructions. If you make a mistake, you can clear your work using the Reset button in the MDN Playground. If you get really stuck, you can (usually) view the solution at the bottom of each question section, or reach out for help. Note:\nIf you'd prefer to work in your own editor or in an online editor (such as CodePen or JSFiddle), you can copy the code from the MDN Playground into your chosen environment. Some questions don't include code blocks to start from, and instead ask you to download starter files to work on your local machine with. Sometimes this is due to the complex nature of the question, and sometimes we just wanted to change things up a bit. The code examples you'll encounter in the Learning Area are all available on GitHub: If you want to get in touch with us about anything, use the communication channels. We'd love to hear from you about anything you think is wrong or missing on the site, requests for new learning topics, requests for help with items you don't understand, or any other questions or concerns. If you're interested in helping develop/improve the content, take a look at how you can help and get in touch! We are more than happy to talk to you, whether you are a learner, teacher, experienced web developer, or someone else interested in helping to improve the learning experience. Scrimba's Frontend Developer Career Path teaches all you need to know to be a competent front-end web developer, with fun interactive lessons and challenges, knowledgeable teachers, and a supportive community. Go from zero to landing your first front-end job! Many of the course components are available as standalone free versions. A great interactive site for learning programming languages from scratch. Interactive site with tutorials and projects to learn web development. An excellent resource for aspiring web developers â Learn JavaScript in an interactive environment, with short lessons and interactive tests, guided by automated assessment. The first 40 lessons are free, and the complete course is available for a small one-time payment. This page was last modified on Jul 24, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Structuring_content",
    "title": "Structuring content with HTML - Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support HTML is the technology that defines the content and structure of any website. Written properly, it should also define the semantics (meaning) of the content in a machine-readable way, which is vital for accessibility, search engine optimization, and making use of the built-in features browsers provide for content to work optimally. This module covers the basics of the language, before looking at key areas such as document structure, links, lists, images, forms, and more. Before starting this module, you don't need any previous HTML knowledge, but you should have at least basic familiarity with using computers and using the web passively (i.e., just looking at it and consuming content). You should have a basic work environment set up (as detailed in Installing basic software), and understand how to create and manage files (as detailed in Dealing with files). Both are parts of our Getting started with the web complete beginner's module. Note:\nIf you are working on a computer, tablet, or another device where you can't create files, you can try running the code in an online editor such as CodePen or JSFiddle. Covers the absolute basics of HTML, to get you started â we define elements, attributes, and other important terms, and show where they fit in the language. We also show how a typical HTML page is structured and how an HTML element is structured, and explain other important basic language features. Along the way, we'll play with some HTML to get you interested! The head of an HTML document is the part that is not displayed in the web browser when the page is loaded. It contains metadata information such as the page <title>, links to CSS (if you want to style your HTML content with CSS), links to custom favicons, and metadata (data about the HTML, such as who wrote it, and important keywords that describe the document). One of HTML's main jobs is to give text structure so that a browser can display an HTML document the way its developer intends. This article explains how HTML can be used to provide fundamental page structure by defining headings and paragraphs. The previous article looked at why semantics are important in HTML, and focused on headings and paragraphs. This article continues on the theme of semantics, looking at HTML elements that apply emphasis and importance to text (parallel to italics and bold text in print media). Lists are everywhere in lifeâfrom your shopping list to the list of directions you subconsciously follow to get to your house every day, to the lists of instructions you are following in these tutorials! It may not surprise you that HTML has a convenient set of elements that allows us to define different types of list. On the web, we have three types of lists: unordered, ordered, and description lists. This lesson shows you how to use the different types. In addition to defining individual parts of your page (such as \"a paragraph\" or \"an image\"), HTML also boasts a number of block level elements used to define areas of your website (such as \"the header\", \"the navigation menu\", \"the main content column\"). This article looks into how to plan a basic website structure, and write the HTML to represent this structure. There are many other elements in HTML for defining text semantics, which we didn't get to in the Emphasis and importance article. The elements described in this article are less known, but still useful to know about (and this is still not a complete list by any means). Here you'll learn about marking up quotations, computer code and other related text, subscript and superscript, contact information, and more. Links (also known as hyperlinks) are really important â they are what makes the Web a web. This article shows the syntax required to make a link, and discusses link best practices. We all learn to write a letter sooner or later; it is also a useful example to test our text formatting skills. In this challenge, you'll have a letter to mark up as a test for your HTML text formatting skills, as well as hyperlinks and proper use of the HTML <head> element. Structuring a page of content ready for laying it out using CSS is a very important skill to master, so in this challenge you'll be tested on your ability to think about how a page might end up looking, and choose appropriate structural semantics to build a layout on top of. In the beginning, the web was just text, and it was really quite boring. Fortunately, it wasn't too long before the ability to embed images (and other more interesting types of content) inside web pages was added. In this article we'll look at how to use the <img> element in depth, including the basics, annotating it with captions using <figure>, and detailing how it relates to CSS background images. Now that we are comfortable with adding simple images to a webpage, the next step is to start adding video and audio players to your HTML documents! In this article we'll look at doing just that with the <video> and <audio> elements; we'll then finish off by looking at how to add captions/subtitles to your videos. In this challenge, we'll test your knowledge of some of the techniques discussed in the last couple of lessons, getting you to add some images and video to a funky splash page all about Mozilla! This article gets you started with HTML tables, covering the very basics such as rows, cells, headings, making cells span multiple columns and rows, and how to group together all the cells in a column for styling purposes. In this article we look at more HTML table accessibility features such as captions/summaries, grouping your rows into table head, body and footer sections, and scoping columns and rows. In this challenge, we provide you with some data on the planets in our solar system. Your job is to structure it into an accessible HTML table. HTML forms and buttons are powerful tools for interacting with users â most commonly they are used for collecting data from users or allowing them to control a user interface. In this article we provide an introduction to the basics of forms and buttons. Writing HTML is fine, but what if something goes wrong, and you can't work out where the error in the code is? This article will introduce you to some tools that can help you find and fix errors in HTML. This page lists HTML tests you can try so you can verify if you've understood the content in this module. These tutorials are not part of the learning pathway, but they are interesting nonetheless â you should consider these as stretch goals, to optionally study when you are done with the main Core articles. Vector graphics are very useful in many circumstances â they have small file sizes and are highly scalable, so they don't pixelate when zoomed in or blown up to a large size. In this article we'll show you how to include one in your webpage. Developers commonly think of embedding media such as images, video and audio into web pages. In this article we take somewhat of a sideways step, looking at some elements that allow you to embed a wide variety of content types into your webpages: the <iframe>, <embed> and <object> elements. <iframe>s are for embedding other web pages, and the other two allow you to embed external resources such as PDF files. Scrimba's Learn HTML and CSS course teaches you HTML and CSS through building and deploying five awesome projects, with fun interactive lessons and challenges taught by knowledgeable teachers. Another useful resource for learning HTML basics. This interactive lesson provides a useful description of HTML, with particular emphasis on why the semantic aspect of it is important. This page was last modified on Jul 3, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics",
    "title": "CSS styling basics - Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support CSS (Cascading Style Sheets) is used to style and layout web pages â for example, to alter the font, color, size, and spacing of your content, split it into multiple columns, or add animations and other decorative features. This module provides all the CSS fundamentals you'll need for now, including syntax, features, and techniques. Before starting this module, you should have a basic work environment set up (as detailed in Installing basic software), and understand how to create and manage files (as detailed in Dealing with files). You should also be familiar with HTML (work through our Structuring content with HTML module if not). Note:\nIf you are working on a computer, tablet, or another device where you can't create files, you can try running the code in an online editor such as CodePen or JSFiddle. CSS allows you to create great-looking web pages, but how does it work under the hood? This article explains what CSS, what the basic syntax looks like, and how your browser applies CSS to HTML to style it. In this article, we will take a simple HTML document and apply CSS to it, learning some practical details of the language along the way. We will also review the CSS syntax features you've not looked at yet. In this challenge you will style a simple bio page, testing you on some of the skills you learned in the last couple of lessons including writing selectors and text styling. In this article we'll recap some selector fundamentals, including the basic type, class, and ID selectors. As you know from your study of HTML, elements can have attributes that give further detail about the element being marked up. In CSS you can use attribute selectors to target elements with certain attributes. This lesson will show you how to use these very useful selectors. The next set of selectors we will look at are referred to as pseudo-classes and pseudo-elements. There are a large number of these, and they often serve quite specific purposes. Once you know how to use them, you can look through the different types to see if there is something which works for the task you are trying to achieve. The final selectors we will look at are called combinators. Combinators are used to combine other selectors in a way that allows us to select elements based on their location in the DOM relative to other elements (for example, child or sibling). Everything in CSS has a box around it, and understanding these boxes is key to being able to create more complex layouts with CSS, or to align items with other items. In this lesson, we will take a look at the CSS Box model. You'll get an understanding of how it works and the terminology that relates to it. The aim of this lesson is to develop your understanding of some of the most fundamental concepts of CSS â the cascade, specificity, and inheritance â which control how CSS is applied to HTML and how conflicts between style declarations are resolved. CSS rules contain declarations, which in turn are composed of properties and values. Each property used in CSS has a value type that describes what kind of values it is allowed to have. In this lesson, we will take a look at some of the most frequently used value types, what they are, and how they work. Understanding how big the different features in your design will be is important. In this lesson we will summarize the various ways elements get a size via CSS and define a few terms about sizing that will help you in the future. In this lesson, we will take a look at some of the creative things you can do with CSS backgrounds and borders. From adding gradients, background images, and rounded corners, backgrounds and borders are the answer to a lot of styling questions in CSS. Overflow is what happens when there is too much content to fit inside an element box. In this lesson, you will learn how to manage overflow using CSS. In this lesson we will take a look at how certain special elements are treated in CSS. Images, other media, and form elements behave a little differently from regular boxes in terms of your ability to style them with CSS. Understanding what is and isn't possible can save some frustration, and this lesson will highlight some of the main things that you need to know. Styling an HTML table isn't the most glamorous job in the world, but sometimes we all have to do it. This article explains how to make HTML tables look good, with some specific table styling techniques highlighted. This article will give you guidance on how to go about debugging a CSS problem, and show you how the DevTools included in all modern browsers can help you to find out what is going on. This challenge provides a number of related exercises that must be completed in order to create the final design â a business card/gamer card/social media profile. If you want to make the right impression, writing a letter on nice letterheaded paper can be a really good start. In this challenge you will create an online template to achieve such a look. In this challenge, you'll get some more practice in creating cool-looking boxes by trying to create an eye-catching box. These tutorials are not part of the learning pathway, but they are interesting nonetheless â you should consider these as stretch goals, to optionally study when you are done with the main Core articles. This article acts as a box of tricks, providing an introduction to some interesting advanced styling features such as box shadows, blend modes, and filters. This lesson aims to introduce you to cascade layers, a more advanced feature that builds on the fundamental concepts of the CSS cascade and CSS specificity. In recent years, CSS has evolved in order to better support different directionality of content, including right-to-left but also top-to-bottom content (such as Japanese) â these different directionalities are called writing modes. As you progress in your study and begin to work with layout, an understanding of writing modes will be very helpful to you, therefore we will introduce them in this article. As you start to work on larger stylesheets and big projects you will discover that maintaining a huge CSS file can be challenging. In this article we will take a brief look at some best practices for writing your CSS to make it easily maintainable, and some of the solutions you will find in use by others to help improve maintainability. Scrimba's Learn HTML and CSS course teaches you HTML and CSS through building and deploying five awesome projects, with fun interactive lessons and challenges taught by knowledgeable teachers. This interactive lesson provides a useful introduction to CSS syntax. This page was last modified on Jul 3, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Scripting",
    "title": "Dynamic scripting with JavaScript - Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support JavaScript is a huge topic, with so many different features, styles, and techniques to learn, and so many APIs and tools built on top of it. This module focuses mostly on the essentials of the core language, plus some key surrounding topics â learning these topics will give you a solid basis to work from. Before starting this module, you don't need any previous JavaScript knowledge, but you should have worked through the previous modules in the course. You should at least know HTML and the basic fundamentals of CSS. Note:\nIf you are working on a computer, tablet, or another device where you can't create files, you can try running the code in an online editor such as CodePen or JSFiddle. Welcome to the MDN beginner's JavaScript course! In this first article we will look at JavaScript from a high level, answering questions such as \"what is it?\", and \"what is it doing?\", and making sure you are comfortable with JavaScript's purpose. Now you've learned something about the theory of JavaScript, and what you can do with it, we are going to give you a crash course on the basic features of JavaScript via a completely practical tutorial. Here you'll build up a simple \"Guess the number\" game, step by step. When you built up the \"Guess the number\" game in the previous article, you may have found that it didn't work. Never fear â this article aims to save you from tearing your hair out over such problems by providing you with some simple tips on how to find and fix errors in JavaScript programs. After reading the last couple of articles you should now know what JavaScript is, what it can do for you, how you use it alongside other web technologies, and what its main features look like from a high level. In this article, we will get down to the real basics, looking at how to work with the most basic building blocks of JavaScript â Variables. At this point in the course, we discuss maths in JavaScript â how we can combine operators and other features to successfully manipulate numbers to do our bidding. Next, we'll turn our attention to strings â this is what pieces of text are called in programming. In this article, we'll look at all the common things that you really ought to know about strings when learning JavaScript, such as creating strings, escaping quotes in strings, and joining them together. Now we've looked at the very basics of strings, let's move up a gear and start thinking about what useful operations we can do on strings with built-in methods, such as finding the length of a text string, joining and splitting strings, substituting one character in a string for another, and more. In this lesson we'll look at arrays â a neat way of storing a list of data items under a single variable name. Here we look at why this is useful, then explore how to create an array, retrieve, add, and remove items stored in an array, and more besides. In this challenge, you'll be tasked with taking some of the knowledge you've picked up in this module's articles and applying it to creating a fun app that generates random silly stories. Have fun! In any programming language, the code needs to make decisions and carry out actions accordingly depending on different inputs. For example, in a game, if the player's number of lives is 0, then it's game over. In a weather app, if it is being looked at in the morning, show a sunrise graphic; show stars and a moon if it is nighttime. In this article, we'll explore how so-called conditional statements work in JavaScript. Programming languages are very useful for rapidly completing repetitive tasks, from multiple basic calculations to just about any other situation where you've got a lot of similar items of work to complete. Here we'll look at the loop structures available in JavaScript that handle such needs. Another essential concept in coding is functions, which allow you to store a piece of code that does a single task inside a defined block, and then call that code whenever you need it using a single short command â rather than having to type out the same code multiple times. In this article we'll explore fundamental concepts behind functions such as basic syntax, how to invoke and define them, scope, and parameters. With most of the essential theory dealt with in the previous article, this article provides practical experience. Here you will get some practice building your own, custom function. Along the way, we'll also explain some useful details of dealing with functions. There's one last essential concept about functions for us to discuss â return values. Some functions don't return a significant value, but others do. It's important to understand what their values are, how to use them in your code, and how to make functions return useful values. We'll cover all of these below. In this article, we discuss some important concepts surrounding events, and look at the fundamentals of how they work in browsers. This article introduces the concepts of event bubbling, event capture, and event delegation, which are all about what happens when you add a listener to an element that contains another element, and an event then happens to the contained element. Now that we've looked at the fundamental building blocks of JavaScript, we'll test your knowledge of loops, functions, conditionals and events by getting you to build a fairly common item you'll see on a lot of websites â a JavaScript-powered image gallery. In this article, we'll look at fundamental JavaScript object syntax, and revisit some JavaScript features that we've already seen earlier in the course, reiterating the fact that many of the features you've already dealt with are objects. When writing web pages and apps, one of the most common things you'll want to do is change the document structure in some way. This is usually done by manipulating the Document Object Model (DOM) via a set of built-in browser APIs for controlling HTML and styling information. In this article we'll introduce you to DOM scripting. Another very common task in modern websites and applications is making network requests to retrieve individual data items from the server to update sections of a webpage without having to load an entire new page. This seemingly small detail has had a huge impact on the performance and behavior of sites, so in this article, we'll explain the concept and look at technologies that make it possible. JavaScript Object Notation (JSON) is a standard text-based format for representing structured data based on JavaScript object syntax. It is commonly used for transmitting data in web applications (e.g., sending some data from the server to the client, so it can be displayed on a web page, or vice versa). You'll come across it quite often, so in this article, we give you all you need to work with JSON using JavaScript, including parsing JSON so you can access data within it, and creating JSON. In this lesson, we will return to the subject of debugging JavaScript (which we first looked at in What went wrong?). Here we will delve deeper into techniques for tracking down errors, but also look at how to code defensively and handle errors in your code, avoiding problems in the first place. This page lists JavaScript tests you can try so you can verify if you've understood the content in this module. Scrimba's Learn JavaScript course teaches you JavaScript through solving 140+ interactive coding challenges, building projects including a game, a browser extension, and even a mobile app. Scrimba features fun interactive lessons taught by knowledgeable teachers. An excellent resource for aspiring web developers â Learn JavaScript in an interactive environment, with short lessons and interactive tests, guided by automated assessment. The first 40 lessons are free, and the complete course is available for a small one-time payment. This page was last modified on Jul 3, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Accessibility",
    "title": "Accessibility on the web - Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Access to web content such as public services, education, e-commerce sites, and entertainment is a human right. No one should be excluded based on disability, race, geography, or other human characteristics. This module discusses the best practices and techniques you should learn to make your websites as accessible as possible. You should be familiar with HTML, CSS, and JavaScript before starting this module. Note:\nIf you are working on a computer, tablet, or another device where you can't create files, you can try running the code in an online editor such as CodePen or JSFiddle. This article starts off the module with a good look at what accessibility is â this includes what groups of people we need to consider and why, what tools different people use to interact with the web, and how we can make accessibility part of our web development workflow. Next we turn our attention to accessibility tooling, providing information on the kinds of tools you can use to help solve accessibility issues, and the assistive technologies used by people with disabilities as they browse the web. You'll be using these tools throughout subsequent articles. A great deal of web content can be made accessible just by making sure the correct HTML elements are always used for the correct purpose. This article looks in detail at how HTML can be used to ensure maximum accessibility. CSS and JavaScript, when used properly, also have the potential to allow for accessible web experiences, but if misused they can significantly harm accessibility. This article outlines some CSS and JavaScript best practices that should be considered to ensure that even complex content is as accessible as possible. Following on from the previous article, sometimes making complex UI controls that involve unsemantic HTML and dynamic JavaScript-updated content can be difficult. WAI-ARIA is a technology that can help with such problems by adding in further semantics that browsers and assistive technologies can recognize and use to let users know what is going on. Here we'll show how to use it at a basic level to improve accessibility. Another category of content that can create accessibility problems is multimedia â video, audio, and image content need to be given proper textual alternatives, so they can be understood by assistive technologies and their users. This article shows how. With web access on mobile devices being so popular, and popular platforms such as iOS and Android having fully-fledged accessibility tools, it is important to consider the accessibility of your web content on these platforms. This article looks at mobile-specific accessibility considerations. In this challenge, we present to you a simple site with several accessibility issues that you need to diagnose and fix. An excellent series of video tutorials by Marcy Sutton. Includes code examples, screen reader references, and other useful resources. Includes guides, checklists, tools, and more. Includes a list of web accessibility evaluation tools. Scrimba's Learn Accessible Web Design course teaches you how to write accessible HTML by solving interactive coding challenges and fixing a real-world website. This page was last modified on Jul 3, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/plus",
    "title": "MDN Plus",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Support MDN and enjoy a focused, ad-free experience alongside other features such as curated collections, custom web platform updates, and more. Subscribers to paid tiers of MDN Plus have the option to browse MDN without ads. No need to scroll through page after page to find your answers. Introducing an AI assistant that can answer all your web development questions in real time. Powered by OpenAI GPT-4o and GPT-4o mini. Your playground to learn and share your amazing work with the world. By simply logging in, you can now spread your creativity far and wide. The Web doesn't have a changelog, but MDN can help. You can personalize and filter compatibility changes based on browsers or the tech category you are interested in whether that is JavaScript, CSS, etc. No more haphazard hunting through the vast virtual library: unleash your inner curator and collect your favorite articles in one place for convenient consultation. * Do you need MDN Plus for your company? Let us know and weâll get back to you when it becomes available. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/plus/ai-help",
    "title": "AI Help | MDN Plus",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/plus/updates",
    "title": "Updates | MDN Plus",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/plus/docs/features/overview",
    "title": "Feature Overview | MDN Plus",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support More MDN. Your MDN. Get started. This page is an overview of the MDN Plus documentation and related resources. MDN Plus is a premium subscription service launched by Mozilla. The service\nallows users to customize their MDN Web Docs experience through premium features\nsuch as Collections, filtering Updates and MDN Offline. Learn more about MDN Plus on our website or in this\nblogpost. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/plus/docs/faq",
    "title": "FAQ | MDN Plus",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support MDN Plus is a premium subscription service launched in March 2022 by Mozilla.\nThe service allows users to customize their MDN Web Docs experience through\npremium features such as Updates,\nCollections and\nMDN Offline. The extensive research we have done in 2020 and 2021 showed us that MDN users\nwould appreciate a customized experience on MDN. Weâve got information on what\nyou would find useful and what you would be interested in. All the premium\nfeatures we propose today reflect that feedback. MDN Plus is an initial step\ntowards making the experience on the site more interactive and helpful for our\nusers. A three-tiered pricing model has been put in place in order to try and\naccommodate our usersâ needs as much as possible: Subscribing for a yearly plan activates a 20% discount for all the paid options. Currently, you can only upgrade your plan. For getting a downgrade, please\ncancel your current subscription first and then activate the new one. Nothing. We will continue to develop & maintain our web documentation that will\nremain free and accessible for everyone. There will be no change there. Even\nmore, we believe that MDN Web Docs will benefit from MDN Plus, as we plan to\nreinvest part of the gains from MDN Plus and improve our documentation as well\nas the overall user experience on the website. MDN content is made available under a CC BY-SA 2.5 license. That license doesn't\npreclude Mozilla (or other users of MDN content) from having a paid product. MDN\nPlus adds premium features like updates and collections on top of the free\ncontent. Regular users can still access and reuse MDN content under the Creative\nCommons license. Since its beginning in 2005, MDN Web Docs has been a project hosted and provided\nby Mozilla. Mozilla covers the cost of infrastructure, development and\nmaintenance of the MDN platform, including a team of engineers and its own team\nof dedicated writers. Mozilla wants MDN Plus to help ensure that MDN's open source content continues\nto be supported into the future. MDN Plus has been built only with Mozilla\nresources, and any revenue generated by MDN Plus will stay within Mozilla. We\nare looking into ways to reinvest some of these additional funds into open\nsource projects contributing to MDN but it is still in the early stages. The existence of a new subscription model will not detract from MDN's current\nfree Web Docs offering in any way. The current experience of accessing web\ndocumentation will not change for users who do not wish to sign up for a premium\nsubscription. Open Web Docs (OWD) and Mozilla will continue to work closely\ntogether on MDN for the best possible web platform documentation for everyone.\nFor more information about how our organizations work together, please check\nthis article. The free version of MDN Plus is available worldwide. Anyone can create an MDN\nPlus account and try out a limited version of the premium features. As for the\npaid plans, they are currently available as follows: in the United States,\nCanada (since March 24th, 2022), Austria, Belgium, Finland, France, the United\nKingdom, Germany, Ireland, Italy, Malaysia, the Netherlands, New Zealand, Puerto\nRico, Sweden, Singapore, Switzerland, Spain (since April 28th, 2022), Estonia,\nGreece, Latvia, Lithuania, Portugal, Slovakia and Slovenia (since June 15th,\n2022). We are working on expanding this list even further. In case you have an idea you would like to share about MDN Plus, you can add\nyour suggestions to our mdn-community\nrepo. If a subscriber, you can also leave us feedback by accessing the âFeedbackâ\noption in the user menu. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/curriculum/",
    "title": "MDN Curriculum | MDN Curriculum",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support The MDN Curriculum provides a structured guide to the essential skills and practices for being a successful front-end developer, along with recommended learning resources. Last updated: February 2024 Defines the essential skills and knowledge every front-end developer needs for career success and industry relevance. Created by Mozilla and refined with insights from students, educators, and developers from the broader web community. Includes learning resource recommendations covering every curriculum topic, helping you become job-ready. Learn more Learn our curriculum with Scrimba's interactive Frontend Developer Career Path. Develop a great attitude towards learning, researching, and collaborating to enhance your chances of success. Best Practices Familiarize yourself with your development environment and the tools you'll use to build websites. Tooling Understand how the web works at a high level, and the process for creating web technologies. Web Standards & Semantics Learn the fundamentals of HTML, the language used to define and structure web content. Web Standards & Semantics Dive into the fundamentals of CSS, the language you'll use to style and layout websites. Styling Focus on using CSS to style text and apply custom web fonts. Styling Learn modern techniques for creating flexible layouts that work on a wide variety of devices. Styling Focus on the core JavaScript language and fundamental surrounding topics. Scripting Understand the need for universal access to web content and how to write accessible code. Best Practices Appreciate basic design theory, how to speak design language, and what makes websites look good. Best Practices Understand why version control is necessary, and use GitHub to store code and collaborate with others. Tooling Add animations to your toolbox to enhance user experience and perceived performance. Web Standards & Semantics Create custom JavaScript objects to gain a deeper understanding of object-oriented programming. Scripting Study common WebAPIs in depth to appreciate how WebAPIs work in general. Scripting Explore how to create performant, fast-loading websites and enhance perceived performance. Best Practices Learn how to protect data from unauthorized access and how to treat user data responsibly. Best Practices Explore the need for testing, and learn how to implement common test types. Best Practices Study the features of popular JavaScript frameworks, and use them to implement common use cases. Tooling Look at popular CSS tooling and understand what code problems they can solve. Tooling Understand the purpose and usage of other tooling types commonly found in a web project. Tooling Scrimba's Frontend Developer Career Path teaches the MDN Curriculum Core with fun interactive lessons and challenges, knowledgeable teachers, and a supportive community. Go from zero to landing your first front-end job! How can youboost your employability with the MDNCurriculum? Learn about research collaboration and other essential soft skills.Balance between modern tooling and long-term best practices.Get access to high-quality recommended resources.Get guidance from trusted voices. Begin with our \"Getting started\" and \"Core\" modules to grasp the essential skills for web development. Core modules Dive deeper with our \"Extensions\" modules to develop specialized skills. Extensions modules Our \"Soft skills\" module, part of \"Getting started\", offers crucial insights to help you land your job. Getting started modules Use our modules to guide your teaching, or enroll your students in Scrimba's Frontend Path. Frontend Path Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/blog/",
    "title": "MDN Blog",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support From individual pixels to fully decoded images on your screen, raw pixel data gets transformed, compressed, and efficiently delivered. Learn about the techniques and optimizations that shrink image information without any perceivable loss in quality. MDN turns 20! Let's look at how we started, how MDN became the most trusted resource for web developers, the impact it's had on the open web, and yes, there's cake, too. Images help bring more color and life to the web. This post describes how images are represented by humans and on different devices, with details about color spaces and vision theory. Browsers are starting to roll out changes in default UA styles for nested section headings. This post describes the incoming changes, how to identify if it's an issue on your websites, and hints for conformant and better-structured pages. Global Privacy Control (GPC) is on the way to becoming a formal privacy standard with the recent publication of its first working draft. Let's take a look at what the implications are for developers and users. A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN. Learn techniques to improve the Largest Contentful Paint metric, a part of Core Web Vitals, for your website. Let's have a look at MDN Web Docs content projects in 2024, with highlights of our top picks and recommended reading, and at what's next on MDN for 2025. There's a new Learn Web Development section on MDN that merges the MDN Curriculum with the Learn section. Here's the background to these changes, what's new, and what you can expect to see in the future. Join JavaScriptmas this December for daily coding challenges designed to boost your skills and bring festive fun. Solve challenges on Scrimba, learn something new, and take part for a chance to win exciting prizes! Learn about reading network request waterfalls, identifying common network performance issues, and optimizing page rendering. Here are six effective strategies for landing your first developer job. These are especially relevant if you're self-taught or breaking into the tech industry without a traditional CS degree. We are thrilled to announce the new MDN community page that will be a hub to recognize our contributors and a place for contributors to get involved. Learn about lesser-known web performance bottlenecks connected to excessive JavaScript usage, like long tasks, large bundle sizes, and hydration issues. For many of us, the holidays are over, and the time has come to focus.  Now is an ideal time to dive into learning web development, and you're in luck â MDN and Scrimba are offering a 30% discount on select courses for the next month! This post demonstrates how to use the Streams API in a web application to fetch and transform text on the fly.  By processing the data as it arrives, this approach enhances performance, responsiveness, and resource efficiency. Learn how to use Intl.Segmenter for locale-sensitive text segmentation in JavaScript to simplify localization, count words or sentences in different languages, and more. Learn how to use Git stash to break down large commits. Discover a better approach for saving work when switching branches. This post explores what mobile app debugging is, commonly used techniques, and how you can debug mobile apps on multiple devices. The 'name' attribute of the HTML details element is gaining more support across browsers. Learn how this feature allows creating exclusive accordions without scripting widgets from scratch. This article explains how to use the Broadcast Channel API to build synchronized and interconnected web applications. We have chosen Scrimba as a course partner for the MDN Curriculum. This blog post explores what the partnership means practically, and how we will provide an even better web education experience together. First released in 2016, the HTTP Observatory became popular in the web community with a combination of helpful security audits and educational material. Fast forward to 2024, and we are delighted to announce that Observatory's new home is MDN. Read on to find out more about what this entails, and give the HTTP Observatory a warm welcome! This guide explains how to use Static Site Generation in Next.js to build scalable and secure web applications with fast loading times and a focus on performance. New JavaScript Set methods are landing across browsers. Learn about sets, how you can use these methods to compare different sets, create new sets with specific properties, and more. This guide introduces you to rate limits and slow down mechanisms. Learn how to apply slow down and rate limit mechanisms in Express applications. This post takes a look at what page visibility is, how you can use the Page Visibility API in your applications, and describes pitfalls to avoid if you build features around this functionality. We've been writing about web development and the web platform on the MDN Blog since May 2023. Here's our highlights and top posts along with our favorites. This guide introduces you to service workers and their lifecycle. Learn how to deploy a project using service workers with HTTPS on Vultr. Interop 2023 has successfully concluded, and the Interop 2024 project is now officially underway. Learn what Interop is, discover the updates from Interop 2023 now on MDN, and find out what's coming to the web next. This guide introduces you to the common types of tests and the testing conventions. Learn how to test JavaScript with Jest on Vultr. Working with colors on the web just got more interesting! In this article, weâll explore how to use the CSS color-mix() function to create variations in color palettes. This post reflects on the conventional test automation methods using Selenium and Appium. Learn how you can use TestGrid's unified testing platform to enhance the conventional methods and also leverage the modern codeless testing techniques. The long-awaited MDN Curriculum is now live on MDN, providing a structured guide to the essential front-end development skills and best practices for industry newcomers. Learn all the key details in this article. This article provides an overview of the core components required for creating effective technical documentation. Learn the best practices to make your documentation clear, consistent, and well-structured. This guide explains Bun functionalities as a runtime package manager and a bundler. It also explains the benefits of built-in Bun APIs and how to use Bun's Vultr marketplace application. Aligning with Interop 2023's emphasis on cross-browser consistency, this post walks you through various `border-image` properties that you can control to create captivating web designs. Learn how to use custom graphics for enhancing the look of your websites that appear consistent across different browsers. Learn how to build AI-powered apps using OpenLLM and Vultr Cloud GPU. This guide shows how to generate API responses using a Large Language Model. It also covers instructions for setting up an Nginx server and implementing SSL security. The tail end of 2023 welcomes positive news for web privacy, as Chrome announces it is to join Firefox and Safari in deprecating third-party cookies in 2024. Find out more details about these changes, and what they mean for web developers. Today we're updating the Baseline widgets and introducing a new one, along with the updated definition of Baseline. The JavaScript console is an essential tool for web development. Learn new and fun ways to use the console to display data and debug your code. CSS container queries are a powerful new tool for our CSS layout toolbox. In this post we'll dive into the practicalities of building a layout with container queries. Learn how to deploy a Node.js application on Vultr using PM2 to create persistent services. This guide shows how to efficiently use resources via PM2 cluster mode. It also covers Nginx server setup and SSL security. Discover essential tips and tricks for using Visual Studio Code (VS Code), a powerful IDE. Learn how to leverage its integrated editing features and Git support, and explore a few extensions. Observatory 2.0 is launching soon as part of the Mozilla Developer Network as the MDN Observatory with new security scoring standards and other exciting updates. This guide explores the various types of CI/CD pipelines and helps you understand their specific use cases. Learn how to leverage rules to create highly efficient DevSecOps workflows. What can web designers and developers do to build a more sustainable web? This post explores the environmental impacts of web technologies and looks at some of the ways we can build greener websites. Thinking about making the move from GitHub to GitLab? This guide demystifies the migration process, addressing common concerns for DevSecOps teams that are looking to seamlessly transition between the two platforms. This post provides a step-by-step guided tutorial on how to migrate your data from GitHub into GitLab. MDN has created a curriculum for aspiring front-end developers to build a rewarding and successful career. Take a look at the curriculum, who it's for, and the research it's based on. The new CSS linear() timing function enables custom easing in animations. Explore how linear() works compared with other timing functions used for easing, with practical examples. Relying on external resources for your website is always fraught with risks. Learn how to protect your website and its visitors by using SRI to secure third-party content. Scroll-driven animations are coming to CSS! In this post, we'll look at a few types of animations and learn how to link them to the scroll progress of a container. We recently launched a feature called AI Explain, but we have rolled this back for now. In this post, we look into the story behind AI Explain: its development, launch, and the reasons that led us to press the pause button. grep is a powerful tool for searching code from the terminal. This post will show you how to use grep and why it's an essential developer tool. We're introducing an AI assistant powered by MDN and OpenAI GPT 3.5 to answer all your web development questions in real time. Hues are a bright way to define colors in CSS. Learn about hues, color wheels, how to use color functions, and how you can create vibrant color palettes for your website using hue. MDN is launching a code Playground. Users can prototype ideas and expand all live samples into an interactive experience. Discover CSS :lang(), experimental media queries, manipulating graphics with WebGPU, client-server communication with WebTransport, ECMAScript module support, and more. Learn how to use JavaScript to draw any regular shape to a HTML canvas with a single function, and how to modify it to draw multiple shapes. See the latest updates to the MDN reference pages about JavaScript regular expressions, including new sections on sub-features and browser compatibility information. In celebration of Global Accessibility Awareness Day in 2023, we share some tools and guidelines to help you make the web more accessible. Learn what HTML landmark roles are, how they improve accessibility, and how you can include them on your website effectively. MDN leads the way in implementing WebDX community group's efforts, delivering a clear and simple baseline for the web platform to developers. Learn how the CSS `:not()` pseudo-class behaves when multiple selectors are passed as argument. Learn what's new in CSS Colors Module Level 4, including color spaces, color functions, fancy gradients, and support for wide-gamut displays. The MDN blog publishes web development news, tutorials, and insights as an extension of MDN Web Docs, helping you discover, learn, and create for the web. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/play",
    "title": "Playground | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/observatory",
    "title": "HTTP Header Security Test - HTTP Observatory | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/",
    "title": "MDN Web Docs",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Documenting web technologies, including CSS, HTML, and JavaScript, since 2005. A new way to handle dates and times is being added to JavaScript. Let's take a look at Temporal, what problems it solves, the current state, and what you'll find in the new documentation about it on MDN. The CSS anchor positioning module defines features that allow you to tether elements together. Certain elements are defined as anchor elements; anchor-positioned elements can then have their size and position set based on the size and location of the anchor elements to which they are bound. This article explains the theory behind how the View Transition API works, how to create view transitions and customize the transition animations, and how to manipulate active view transitions. This covers view transitions for both DOM state updates in a single-page app (SPA), and navigating between documents in a multi-page app (MPA). The Temporal object enables date and time management in various scenarios, including built-in time zone and calendar representation, wall-clock time conversions, arithmetics, formatting, and more. It is designed as a full replacement for the Date object. MDN 2024 content projectsdeveloper.mozilla.org A new learning experience on MDNdeveloper.mozilla.org Introducing the new MDN Community pagedeveloper.mozilla.org [es] auto-fix content issuesmdn/translated-content [ru] sync translated contentmdn/translated-content 2025/07/10 æç¹ã®è±èªçã«åæmdn/translated-content Fix typo `Scx`->`scx` for short name of `Script_Extensions` propmdn/content 2025/07/10 æç¹ã®è±èªçã«åæmdn/translated-content 2025/07/29 æç¹ã®è±èªçã«åæmdn/translated-content the example comment is out of date, now Chrome give the same message with Firefox/Safarimdn/content Fix hyperlinkmdn/content Feat/property examplemdn/content <selectedcontent>: update to description and specificationmdn/content Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://bsky.app/profile/developer.mozilla.org",
    "title": "@developer.mozilla.org on Bluesky",
    "content": "This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is.\n    Learn more about Bluesky at bsky.social and atproto.com.\n    \nProfile\nMDN Web Docs\ndeveloper.mozilla.org\ndid:plc:a4klb3lge3phlc4az4uspfpo\nThe official MDN Web Docs account, now on Bluesky. We deliver the best web docs around.\n\nVisit → https://mdn.dev\nContribute → https://mdn.dev/community Learn more about Bluesky at bsky.social and atproto.com.\n    \nProfile\nMDN Web Docs\ndeveloper.mozilla.org\ndid:plc:a4klb3lge3phlc4az4uspfpo\nThe official MDN Web Docs account, now on Bluesky. We deliver the best web docs around.\n\nVisit → https://mdn.dev\nContribute → https://mdn.dev/community MDN Web Docs developer.mozilla.org did:plc:a4klb3lge3phlc4az4uspfpo The official MDN Web Docs account, now on Bluesky. We deliver the best web docs around.\n\nVisit → https://mdn.dev\nContribute → https://mdn.dev/community"
  },
  {
    "url": "https://github.com/mdn/",
    "title": "MDN Web Docs · GitHub",
    "content": "We read every piece of feedback, and take your input very seriously. To see all available qualifiers, see our documentation. MDN Web Docs is an open-source, collaborative project that documents web platform technologies, including CSS, HTML, JavaScript, and Web APIs. We also provide extensive 🧑‍🎓 learning resources for beginning developers and students. Note: By participating in and contributing to our projects and discussions, you acknowledge that you have read and agree to the Mozilla community participation guidelines. MDN's mission is to provide a blueprint for a better internet and empower a new generation of developers and content creators to build it. The power of MDN Web Docs lies in its vast community of active readers and contributors. Since 2005, approximately 45,000 contributors have created the documentation we know and love. Together, contributors have created over 45,000 documents that make up an up-to-date, comprehensive, and free resource for web developers worldwide. In addition to English-language articles, over 35 volunteers lead translation and localization efforts for Chinese, French, Japanese, Korean, Portuguese, Russian, and Spanish. You can be part of MDN Web Docs, whether it be through ✍️ content contributions, ⚙️ engineering, or ↔️ translation work. The MDN Web Docs project welcomes contributions from everyone who shares our goals and wants to contribute constructively and respectfully within our community. 🧘‍♂️ We're proud to have a global community of contributors and developers, who also want to say 👋 You can ask questions or get in touch with the MDN Web Docs team and community through any of our communication channels. Browser compatibility data for Web technologies as displayed on MDN JSON\n\n\n\n\n\n            5.3k\n          \n\n\n\n\n            2.3k The content behind MDN Web Docs Markdown\n\n\n\n\n\n            9.7k\n          \n\n\n\n\n            22.8k The source repository of all translated content for MDN Web Docs Markdown\n\n\n\n\n\n            1.9k\n          \n\n\n\n\n            8.3k The platform code behind MDN Web Docs TypeScript\n\n\n\n\n\n            1.2k\n          \n\n\n\n\n            537 A place to provide feedback and suggestions for MDN Web Docs 140\n          \n\n\n\n\n            45 Planning and proposals for MDN Web Docs content 1.3k\n          \n\n\n\n\n            182 The source repository of all translated content for MDN Web Docs The content behind MDN Web Docs The platform code behind MDN Web Docs Browser compatibility data for Web technologies as displayed on MDN MDN's next fr(ont)e(n)d Experimental German locale for MDN, aimed at German-speaking users that prefer to read MDN Web Docs in their native language Sample todo app built with the Vue framework Small app to display bugzilla bugs which need dev docs"
  },
  {
    "url": "https://developer.mozilla.org/en-US/about",
    "title": "About MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Empowering developers worldwide to build a better, open web MDN is an open-source, collaborative project owned by Mozilla Corporation and\ndeveloped by Mozilla, in partnership with a global community of volunteers and\npartners. Mozillaâs MDN team leads the platformâs development, content strategy,\nand overall direction, while the community actively\ncontributes to content creation, translations, and browser compatibility\nimprovements, ensuring MDN remains a vital and evolving resource for all. Your comprehensive resource for web development documentation, covering\neverything from CSS, HTML,\nJavaScript, Web APIs, and\nother web technologies. Ideal for beginners, MDN Learn offers guides and a\nstructured curriculum to kickstart your web development\njourney. Enhance your learning with interactive courses from our partner,\nScrimba. Stay updated with the latest in web development. Our blog\nfeatures updates, tips, tutorials from web experts, MDN announcements, and\ncurated sponsored content. Experience a personalized MDN with our premium subscription service. Enjoy\nfeatures like AI-powered assistance and\nCollections to streamline your workflow. Experiment and learn with tools like Playground for live coding\nand HTTP Observatory for analyzing website security,\ndesigned to enhance your development experience. From our beginnings to becoming the go-to resource for web developers worldwide,\nhere's how we've evolved: Launched as\nDevMo by Mozilla,\na community-driven wiki dedicated to documenting open web standards. Rebranded as\nMozilla Developer Network (MDN),\nreinforcing our commitment to the developer community. Achieved a milestone when major tech companies began contributing and\nsupporting web documentation on our platform, solidifying MDN as the central\nhub for web development knowledge. Transitioned from its wiki-based origins to a modern,\nGitHub-hosted project with the launch of Yari,\nmodernizing our infrastructure and workflows. Unveiled a\nmajor redesign and\nintroduced\nMDN Plus,\noffering personalized features for an enhanced user experience. Launched new tools, including Playground for coding experiments, AI Help for\neasier content discovery, and HTTP Observatory for website security scans. We\nalso introduced a Blog to cover interesting topics beyond MDNâs core focus,\nand a Curriculum to provide a structured approach to learning web development. We had fun exploring the\ninternet archives\nand putting together a fast-forward trip through our transformation! MDN is built on the fundamental principle of accuracy, which has established\nit as the trusted source of web documentation globally. We are committed to\nproviding expertly curated and rigorously reviewed content to maintain the\nhighest standards of quality and accuracy. Our dedication is backed by the\nbest developer community in the worldâa passionate group of contributors who\nhelp us enhance and refine our documentation. We welcome the knowledge and\nexperience of our diverse developer community to ensure that millions of users\ncan rely on MDN daily for up-to-date and accurate information. MDN is a Mozilla project that is powered by its global community. We are a\ndiverse group of developers, writers, and technologists working together to\nbuild resources for a better web. Our open-source approach welcomes\ncontributions from anyone. Each of the individuals who have\ncontributed over the past decades has strengthened MDN. Through our GitHub\nrepository, contributors can make changes and get their work reviewed and\nintegrated into MDNâs content. MDN is committed to providing documentation for a wide range of web\ntechnologies while continuously adapting to the changing landscape of web\ndevelopment. We strive to create content thatâs relevant and valuable to\ndevelopers working across different browsers and platforms. We are dedicated\nto regularly updating our resources to reflect the latest standards and best\npractices, helping developers stay current in a rapidly changing field. At MDN, our mission is to provide developers with the resources they need to\ncreate innovative and accessible web experiences. We offer a free,\nhigh-quality, and comprehensive platform that includes documentation on\nessential web technologies like CSS,\nHTML, JavaScript, and\nWeb APIs. Our learning materials\nare tailored for a diverse audienceâfrom beginners and students to professors\nteaching web development. From technical writers and engineers to product and community managers, we are a\ndiverse group dedicated to driving MDNâs mission, growth, and success. Get to\nknow the people behind MDN and discover what motivates us to keep you at the\nforefront of web development! Technical Writer @dletorey Based in London, UK, Dave has been with MDN since 2022 where he's been focused\non Firefox release documentation, particularly HTML, CSS and SVG updates. He's\npassionate about web standards, accessibility, performance and digital\ninclusion. He is constantly learning about new web technologies and standards. Outside of work he loves live music and tries to attend as many gigs and music\nfestivals as he can. He particularly loves post-punk, indie, house and techno.\nHe also attends many conferences and meetups and is an organizer of\nLondon Web Standards &\nState of the Browser. He also has a large\ncollection of red hats, so is easy to spot in the wild. He is never happier\nthan when in a field with his friends. Open Web Docs (OWD), an independent open source organization, is one of the most productive contributors to MDN Web Docs. OWD contributes as part of their mission to support âweb platform documentation for the benefit of web developers & designers worldwide.â The team at OWD has led or contributed to many projects to improve documentation on MDN. They're an invaluable partner in the day-to-day work of making MDN. Read more about OWDâs activities in their 2024 Impact and Transparency Report and get continuous updates on their Mastodon account. Serving over 15 million users monthly from around the globe, MDN connects\ndevelopers with the tools and information they need to easily build projects on\nthe open web. Educational influence: Our resources are integral to many coding bootcamps and\nuniversity courses worldwide. Trusted reference: Platforms like freeCodeCamp and Codecademy frequently link\nto MDN articles as authoritative references. Industry standard: Tools like Can I use leverage\nMDNâs browser compatibility data\nfor up-to-date information on web feature support across browsers. Collaborative partnerships: We work closely with partners such as\nOpen Web Docs,\nMicrosoft,\nGoogle, Igalia,\nW3C, and others to drive web innovation and serve the\ncommon good. Be a part of our mission to foster innovation and inclusivity on the web.\nStart contributing today and make a lasting impact\non the global developer community. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.mozilla.org/en-US/careers/listings/?team=ProdOps",
    "title": "Mozilla Careers — All open positions at Mozilla",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work Bring your drive, your creativity, your big ideas and your new perspectives to make the difference we’re aiming for. No jobs found that match the selected filters. Subscribe to our open positions RSS feed Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/advertising",
    "title": "Advertise with us",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://support.mozilla.org/products/mdn-plus",
    "title": "MDN Plus Help",
    "content": "Popular Searches:\nUpdate Firefox\nProfiles\nFirefox Sync Explore the knowledge base. We’re here for you! If you haven’t found a solution after exploring our help articles, you can get in touch with our support team. MDN offline allows you to browse MDN without an internet connection. It can also enable you to have a faster experience while saving data. Learn how to help submit and resolve bugs in the MDN content repo. Learn how to contact support if you lost access to Mozilla account and MDN Plus. This article shows MDN Plus subscribers the steps to enjoy an ad-free experience. Grow and share your expertise with others. Answer questions and improve our knowledge base. Learn More Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation. Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/MDN/Community/Issues",
    "title": "Creating and working on issues - MDN Web Docs | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support As a contributor, you can report and work on issues.\nAfter you report an issue, the issue gets triaged. Issue triaging is typically done by people assigned the role of a maintainer or an owner. While reporting an issue or participating in a conversation in an issue, always ensure that your inputs are contributing to the overall progress of the project. Consider whether the issues you open and your comments in an issue are constructive and on topic and are not just adding noise. Do the following: Avoid doing the following: If you want to suggest new documentation or ways to improve the website, see Proposing new content or features. Issues are used to track bugs. An issue must be a single actionable task or a collection of related actionable tasks and must have a clear outcome. If you think you've found a bug with the content on MDN Web Docs or with the look and feel of the website, search the current open issues in the relevant repository and make sure nobody else has reported the issue. Depending on the type of problem you've discovered, you can report it by filing an issue on one of the main MDN GitHub repositories.\nIf the information you provide in the issue is incomplete, you might be asked to provide more details during the issue triaging process. Here are some hints for opening issues: If the issue you're opening is not to report a bug but to perform a series of tasks, you can create the issue as a task list.\nExplain the context or reason for performing the tasks in the description.\nEnsure that you list all the actionable tasks as a checklist. For example: Remember that if you take on an issue, the expectation is for the work to be completed in a timely manner. If you're not able to make any progress for a week after being assigned or can no longer complete the required task, leave a comment and unassign yourself from the issue. These are the general steps for working on an issue: Find an issue: If you're looking to contribute, search for issues with good first issue, help wanted or p3 label. Most repositories have issues with these labels. You are welcome to browse and pick an issue that is suitable for your skill set. Another useful place to look for issues to work on is the MDN Contributors Task Board. This project view lists open issues from multiple repositories. You can filter the list based on the topics (Labels column) you're interested in. See the description of some of the labels that get applied during the issue triage process. Note:\nAn issue with the needs triage label indicates that the MDN Web Docs core team has not reviewed the issue yet, and you shouldn't begin work on it. Assign the issue to yourself: After finding an issue you'd like to work on, make sure that the issue is not assigned to anybody else. Add a comment saying you would like to work on the issue, and if you are able to, assign the issue to yourself. Do the research: Most issues need some investigation before work can start. Make the changes: Fork and branch the repository. Do your work and open a pull request in the repository. Reference the issue in the pull request description. Depending on the files you've updated in the pull request, a reviewer will be assigned to your pull request automatically. (Teams per topic area are defined in the CODEOWNERS file). After opening the pull request, if you find you no longer have the time to make changes or incorporate review feedback, let the team know as soon as possible in a comment in the pull request. This will help the team assign another interested contributor to complete the work on the pull request and close the linked issue. After your pull request has been reviewed and merged, you can mark the linked issue as closed. If you opened the pull request with Fixes #<issue> verbiage, the issue will be closed automatically when the pull request is merged. If you spot a bug â whether it's a problem with the website's look and feel or an error in documentation â you can try to fix it yourself in a pull request.\nIf the bug is small (such as a typo or a minor sentence improvement) or involves a quick fix, you can submit a pull request with the appropriate changes. For any other type of bug, begin by opening an issue.\nAdd a comment about your intent to work on the issue and, if possible, describe your proposed solution or steps to fix it. Note:\nYour time and effort might go waste if you open a pull request without opening an issue first.\nWait for the issue to be triaged, so that the MDN Web Docs team can verify that the issue is legit and approve your proposed solution. Using the guidelines on working on an issue, try to fix the problem by updating the appropriate source, such as: Each repository includes useful information to guide you on how to contribute.\nFor more information, see our main GitHub repositories. If you are a maintainer or an owner in the MDN Web Docs GitHub organization, you are responsible for triaging issues in one or more MDN Web Docs repository. The overall process for triaging includes some general and some issue-specific tasks. When an issue is opened, the needs triage label is set on the issue automatically. You can search for this label to look for issues that need to be triaged. Contributors or anybody else should not work on the issue until the issue has been triaged. (Triagers should remember to remove the needs triage label after triaging the issue.) In the mdn/content repository, an additional Content: label, such as Content:CSS or Content:WebAPI, is set on the issue automatically. This gets set based on the MDN URL mentioned in the issue. You can use the content-specific label to look for issues to be triaged in your specific topic area. If an issue concerns an active, non-en-US locale, set the appropriate label, such as l10n-fr, l10n-zh, or l10n-ja. The teams for those locales will pick these issues up and triage them. You don't need to actively triage issues all the time. Set aside time, say 30 minutes every week, to triage issues on a regular basis in your area of responsibility. Triaging doesn't have to be done as part of a synchronous meeting or even at the same time as everyone else, but it should be done regularly to make sure that the backlog of untriaged bugs doesn't get too high. Apart from triaging incoming issues every week, review the list of old bugs to see if there are any that are stalled, need closing, or are no longer relevant. The idle label is automatically set on issues that have had no activity for 30 days. These are the guidelines to follow while triaging each issue. These are some of the things to keep in mind while reviewing the validity of an issue: Review each issue against the following checklist to ensure that the issue contains the described information for someone to start working on the bug: If any of the above information is not present, then you should ask the author of the issue to provide these details, and add the needs info label to the issue. Resume triaging the issue only after those details have been provided (after which, you can remove the needs info label). It is okay to wait for up to a week to get a response from the author. For each bug, set a priority label based on the severity of the issue to help people who want to work on the most important issues or areas. Critical issue: This type of issue needs to be fixed as soon as possible, regardless of where it appears on the site. This type of issue could damage MDN's reputation severely and/or harm users. Examples of this issue include an incorrect code snippet, which if used in production, could create a severe security problem and undesirable content such as malware, profanity, pornography, hate speech, or links to such content. Major issue: This type of issue could severely affect a page's usefulness. For example, a significant amount of out-of-date information, a complex and important code example that doesn't work, a significant amount of prose that is badly written and hard to understand, or a large number of broken links. Minor issue: This is a type of improvement issue that can make the existing content better but does not affect learning or only has a minor effect on learning. Since these types of issues are not actively planned for, help from contributors to fix these issues is welcome and much appreciated. Fixing some of these issues can also provide the necessary practice to beginner contributors who are starting to get familiar with the contribution process. Examples include typos, bad grammar, a broken link, a small amount of out-of-date information or badly-written prose, or a code snippet that doesn't work. In general, critical issues should be fixed immediately and are most likely handled by MDN Web Docs staff and peers. If possible, add information that can help contributors to fix the issue. The information can be in the form of steps, general approach, links to other similar fixed issues, or reading resources. A well-laid out plan or steps is especially required in issues that are labeled good first issue and can help ramp up new contributors quickly. You can time-box this task to 5-10 minutes. For example, as a triager, you can add the following information to the issue you are triaging: Next, set the following labels as appropriate: effort: small, effort: medium, effort: large: Some contributors like to search for bugs based on the time and effort that will be needed to fix the bug. So where possible, you should try to provide an estimate of the required effort. good first issue: Set this label on the issue if the fix for the issue is really simple and if fixing the issue would provide good practice for a newcomer who is getting used to the process. help wanted: Set this label if the issue requires help from someone who knows about or is familiar with the topic. This is a popular label and some contributors use it to search for issues to work on in open source projects in their areas of familiarity or expertise. broken link external: Set this label if the issue involves a broken link to an external page. document not written: Set this label if the issue involves a necessary document that has not been written yet, usually because a link points to it. needs content update: Set this label if the issue fix in another repository will need an equivalent fix in the mdn/content repository. Note:\nAfter the triage process is complete, remove the needs triage label. This page was last modified on Jun 23, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/community",
    "title": "Contribute to MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Where web enthusiasts learn, collaborate, and create MDNâs strength comes from the passion and dedication of our global community.\nSince our founding in 2005, weâve grown into a thriving network. Together, weâve\ncreated a comprehensive, open, and free resource that serves web developers\nacross the globe. With volunteers leading translation efforts in\n7 languages, weâre\ntruly international. We are an open-source community dedicated to building resources for a better\nweb. Our diverse contributors, including developers, technical writers,\nstudents, educators, designers, and more, come from various backgrounds and\nplatforms. Anyone can contribute, and each contribution strengthens our\ncommunity, driving innovation and improving this vital resource for developers\nworldwide. There are many other things I like about MDN: the openness of its\ngovernance, the respect for contributors' work, the professional\nconversations, and the always timely reviews. MDN has consistently\ndemonstrated the ideal form of an open-source project. Joshua Chen (MDN contributor) MDN Web Docs has the most up-to-date and accurate information and the\ncontent is presented in an easy-to-understand manner. I also like that it's\navailable in many languages (very important!). Yuji (MDN contributor) There are millions of web developers in China, and many of them begin their\ndeveloper journey at MDN Web Docs. Contributing to MDN Web Docs is an\nexcellent way to help people who are starting out. YiTao Yin (MDN contributor) If you wish to be a part of the featured contributors here,\nlet us know. If youâre featured here\nand would like to opt-out,\nplease file an issue on GitHub. We collaborate on GitHub, our project's home, on\nvarious tasks such as writing and improving documentation, fixing bugs, and\nproviding review feedback. It starts here, with you. Want to start right away,\nbut not sure how? Follow\nour guide\nto make your first contribution. Watch this video on\nhow to get started with contributing to MDN. Video from the community team on contributing to MDN Become part of this globally cherished group thatâs dedicated to documenting web\ntechnologies. Whether youâre an expert or a beginner, thereâs a place for you in\nour inclusive community. Check out some of the ways you can contribute and\nengage. Submit pull requests to fix reported issues. Squash bugs Fix inaccuracies and fill in missing information. Start writing Participate in translating content into one of our supported languages. Find your locale Share your knowledge and expertise and guide fellow learners. Help on Discord New to MDN or open-source projects? Tackle our beginner-friendly issues to help\nimprove MDN. Connect with the community. Engage with domain experts. Help others learn. Join MDN Discord Every month, get exclusive updates from the MDN team. Share your ideas and\ncontributions. RSVP to the next community call While working in Mozilla spaces and communities, please adhere to the\nMozilla Community Participation Guidelines,\nwhich promote respect, inclusion, and a harassment-free environment for all\ncommunity members. MDN's resources are freely available under various open-source licenses. For\ndetailed information on reusing MDN content, check out our\nAttribution and Copyright Licensing\npage. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://discourse.mozilla.org/c/mdn/236",
    "title": "Latest MDN topics - Mozilla Discourse",
    "content": "Several MDN readers have a similar problem: They want to read MDN content in English, but instead are getting pages in a different language. This is a hard problem to solve, but there are some things you can do to see mo… Powered by Discourse, best viewed with JavaScript enabled"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Learn",
    "title": "Learn web development | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support Welcome to MDN Learning Web Development (also known as Learn). This resource provides a structured set of tutorials teaching the essential skills and practices for being a successful front-end developer, along with challenges and further recommended resources. Teaches the essential skills and knowledge every front-end developer needs for career success and industry relevance, as defined in the MDN Curriculum. Created by the MDN community and refined with insights from students, educators, and developers from the broader web community. Designed to take you from \"beginner\" to \"comfortable\" (not \"beginner\" to \"expert\"), giving you enough knowledge to use more advanced resources (such as the rest of MDN). Note:\nLast updated: December 2024 (see changelog). Our Getting started modules provide setup tutorials and essential concepts and background information for complete beginners. You should start here if you are a complete beginner (i.e., you've not installed a code editor or written any code yet). Our Core modules provide a structured set of tutorials teaching the essential skills and practices for being a successful front-end developer. Our Extension modules cover useful additional skills to learn as you start to expand your knowledge and develop specialisms. Go onto these after you finish our Core. Use our modules to guide your teaching, check out our Educators page for more ideas, or enroll your students in Scrimba's Frontend Developer Career PathMDN learning partner. Throughout the course, you'll find several articles designed to help you assess whether you have understood what we are teaching you in the course. There are two types: Most of the questions feature HTML/CSS/JavaScript code blocks that show the starting code for each task. The recommended way to complete each one is to press the \"Play\" button in one of the code blocks to open the example in the MDN Playground and then edit the code according to the question instructions. If you make a mistake, you can clear your work using the Reset button in the MDN Playground. If you get really stuck, you can (usually) view the solution at the bottom of each question section, or reach out for help. Note:\nIf you'd prefer to work in your own editor or in an online editor (such as CodePen or JSFiddle), you can copy the code from the MDN Playground into your chosen environment. Some questions don't include code blocks to start from, and instead ask you to download starter files to work on your local machine with. Sometimes this is due to the complex nature of the question, and sometimes we just wanted to change things up a bit. The code examples you'll encounter in the Learning Area are all available on GitHub: If you want to get in touch with us about anything, use the communication channels. We'd love to hear from you about anything you think is wrong or missing on the site, requests for new learning topics, requests for help with items you don't understand, or any other questions or concerns. If you're interested in helping develop/improve the content, take a look at how you can help and get in touch! We are more than happy to talk to you, whether you are a learner, teacher, experienced web developer, or someone else interested in helping to improve the learning experience. Scrimba's Frontend Developer Career Path teaches all you need to know to be a competent front-end web developer, with fun interactive lessons and challenges, knowledgeable teachers, and a supportive community. Go from zero to landing your first front-end job! Many of the course components are available as standalone free versions. A great interactive site for learning programming languages from scratch. Interactive site with tutorials and projects to learn web development. An excellent resource for aspiring web developers â Learn JavaScript in an interactive environment, with short lessons and interactive tests, guided by automated assessment. The first 40 lessons are free, and the complete course is available for a small one-time payment. This page was last modified on Jul 24, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://hacks.mozilla.org/",
    "title": "Home - Mozilla Hacks - the Web developer blog",
    "content": "Beginning in version 138, Firefox will offer an alternative to DLL injection for Data Loss Prevention (DLP) deployments in enterprise environments. DLL Injection DLL injection into Firefox is a topic we’ve covered on the Hacks blog before. In 2023, we blogged about the Firefox capability to let users block third-party DLLs from being loaded. We […] Sign up for the Mozilla Developer Newsletter: If you haven’t previously confirmed a subscription to a Mozilla-related newsletter you may have to do so. Please check your inbox or your spam filter for an email from us. Interop 2025 continues the mission to make the web more consistent across browsers, building on 2024’s 95% interoperability score. This year, 19 focus areas target key developer needs and long-standing issues, including WebRTC improvements, Storage Access API, and CSS Zoom. Mozilla and Filament have introduced Uniffi for React Native, a tool that allows developers to leverage the safety and performance benefits of Rust in cross-platform React Native apps. Discover the latest release of Llamafile 0.8.14, an open-source AI tool by Mozilla Builders. With a new command-line chat interface, enhanced performance, and support for powerful models, Llamafile makes it easy to run large language models (LLMs) on your own hardware. Learn more about the updates and how to get involved with this cutting-edge project. As AI continues to evolve, so do the threats against it. As these GenAI systems become more sophisticated and widely adopted, ensuring their security and ethical use becomes paramount. 0Din is a groundbreaking GenAI bug bounty program dedicated specifically to help secure GenAI systems and beyond. In this blog, you'll learn about 0Din, how it works, and how you can participate and make a difference in securing our AI future. We’re pleased to announce that, as of version 23, the Puppeteer browser automation library now has first-class support for Firefox. This means that it’s now easy to write automation and perform end-to-end testing using Puppeteer, and run against both Chrome and Firefox. Process separation remains one of the most important parts of the Firefox security model and securing our IPC (Inter-Process Communication) interfaces is crucial to keep privileges in the different processes separated. We take a more detailed look at our newest tool for finding vulnerabilities in these interfaces – snapshot fuzzing. Today we’re proud to announce the next Mozilla Builders project: sqlite-vec. Led by independent developer Alex Garcia, this project brings vector search functionality to the beloved SQLite embedded database. Alex has been working on this problem for a while, and we think his latest approach will have a great impact by providing application developers with a powerful new tool for building Local AI applications. Firefox 130 will introduce an experimental new capability to automatically generate alt-text for images using a fully private on-device AI model. The feature will be available as part of Firefox’s built-in PDF editor, and our end goal is to make it available in general browsing for users with screen readers. When Mozilla’s Innovation group first launched the llamafile project late last year, we were thrilled by the immediate positive response from open source AI developers. It’s become one of Mozilla’s top three most-favorited repositories on GitHub, attracting a number of contributors, some excellent PRs, and a growing community on our Discord server. In this blog post, we delve into the motivations for choosing Rust for our crash reporter, outline the unique challenges of designing an application that operates when the main browser has failed, and discuss the new architecture we've implemented. We also share insights into the technical nuances of the implementation, demonstrating how Rust's features are leveraged to handle crashes more effectively and securely. Except where otherwise noted, content on this site is licensed\n          under the\n          Creative Commons Attribution Share-Alike License v3.0\n          or any later version."
  },
  {
    "url": "https://www.mozilla.org/",
    "title": "Internet for people, not profit — Mozilla Global",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work It is available in the following languages: Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.mozilla.org/privacy/websites/",
    "title": "Websites, Communications & Cookies Privacy Notice — Mozilla",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work We care about your privacy. When Mozilla (that’s us) collects information about you, our Mozilla Privacy Policy describes how we handle that information. This privacy notice applies to Mozilla operated websites and mobile apps, which include the domains mozilla.org, and firefox.com, among others. This includes, for example, addons.mozilla.org, bugzilla.mozilla.org, careers.mozilla.org, community.mozilla.org, developer.mozilla.org, mozillafoundation.org, people.mozilla.org, support.mozilla.org, and wiki.mozilla.org. We may receive personal information from you based on your interaction with us on social media platforms, if you submit a job, intern, grant or fellow application, if you volunteer as a Mozilla community member, if you submit user feedback or a request to us, if you sign up for an account or for a subscription, or if you engage with a product or policy campaign. Social Media: If you engage with our accounts on external social media platforms, such as X (formerly Twitter) and Facebook, we may receive personal information about you. If you use these networks, their privacy policies apply, and you are encouraged to read them. Job, Intern, Grant & Fellow Applicants: Applicants for employment, internship, grant, or fellowship opportunities with Mozilla are required to give us a name, street address, telephone number, email address, and resume, and sometimes additional information as well. We use this information to process and evaluate applications and to communicate with applicants about opportunities. We use Greenhouse to handle employment and intern applications, and Fluxx.io to handle fellowship and MOSS grant applications. Contributors: Volunteering for Mozilla as a community contributor may require Mozilla and others to communicate with you at the email address that you provide in connection to your contribution and to recognize your efforts. If you contribute to Bugzilla, or our code bases, then your email address and possibly your name will be publicly available to all internet users. If you create an account on Mozilla Connect (platform powered by Khoros, which has its own privacy notice), your name, your avatar, your posts and replies and other information you share will be accessible to Mozilla and other community members. If you create a profile at people.mozilla.org, it will be accessible to Mozilla employees and Mozilla contributors; you can edit your profile data at Profile Settings. We sometimes use contributor information from sources (such as Bugzilla) in dashboards to visually share aggregated data on the Mozilla community. An example is https://wiki.mozilla.org/Contribute/Dashboards. Where possible, we try to minimize contact information that is publicly displayed. User Feedback: You can provide feedback to us on our products and services on webpages like connect.mozilla.org, through an in-product experience, or through channels such as email, Bugzilla, Matrix, a social media account, our Get Involved page, or through a group like Student Ambassadors. Please minimize the personal information you choose to share on these forums because your comments may be accessible to the public. Accounts & Subscriptions: Some websites, for example Add-Ons for Firefox, Relay, Monitor, and MDN, require account creation. For account management we use Mozilla accounts, GitHub, or custom systems; learn more about how to manage your Mozilla account data. You may periodically receive emails in connection with your account or through subscriptions. Our email management vendors are SalesForce Marketing Cloud, Amazon Simple Email Service, Mailchimp, SocketLabs, Campaign Monitor, or Braze, and you can unsubscribe using the link at the bottom of the relevant email. Product & Policy Campaigns: Some of our webpages host product or policy campaigns. For example, you can request a link by email or SMS to install Firefox on your mobile device or petition your legislators on internet issues. We may use third parties to manage these campaigns and handle any data that you choose to submit. We may use cookies, clear GIFs, third-party web analytics, device information, and IP addresses for functionality and to better understand user interaction with our products, services, and communications. Functionality: We may use information such as cookies, device information, and IP addresses to enhance functionality of certain products, services, and communications. For example: Metrics: We may also use cookies, device information, and IP addresses, along with clear GIFs, cookies and third-party services to help us understand in the aggregate how users engage with our products, services, communications, websites, online campaigns, snippets, devices, and other platforms. We use: Fraud Prevention: Mozilla has implemented third-party technology, Google’s Invisible reCAPTCHA, that operates in the background on some of our websites in order to identify fraudulent activity. Use of the Invisible reCAPTCHA is governed by the Google Privacy Policy and Terms of Use. You can control individual cookie preferences, indicate your cookie preferences to others, and opt out of web analytics and optimization tools. Cookie History: You can accept or decline individual cookies in your preferences in the Tools/Options/Privacy history section. Note that certain features of our products and services may not function properly without the aid of cookies. Do Not Track: Mozilla does not track users across third-party websites to provide targeted advertising. In addition, if you have configured your browser to send a “Do Not Track” signal when accessing our websites, Mozilla will not utilize any of the tools described in the Metrics section. Email: Our marketing communications are optional to receive and you can unsubscribe from the footer of the email or by updating your Mozilla email preferences, or for Thunderbird’s newsletter, on the Thunderbird website. Analytics & Optimization: Follow the instructions below to prevent data collection about your visits to Mozilla websites: Social Media: The social sharing buttons on Mozilla websites are designed not to share data with the applicable social media provider until you specifically click the relevant social media icon. Some Mozilla websites allow you to make purchases (such as apps or gear), contribute funds to specific Mozilla projects, or make donations in support of Mozilla public and charitable programs. These transactions are often handled by Mozilla’s third-party vendors. Payment Processing: When you purchase something via a Mozilla website, contribute funds or make donations, you will send payment through one of our third-party payment providers: Stripe, Apple Pay, PayPal, Venmo or Google Pay. Mozilla receives a record of your account (including your billing address and the last four digits of your payment method) and (where relevant) the status of your account’s subscription; we may also receive your name, mailing address, and/or email address. This data is used for payment processing, fraud detection and record-keeping purposes. Contact and Donation Information: We use Acoustic, Salesforce, Fundraise Up and Campaign Monitor to email receipts and store records, which are retained for 10 years from the date of last payment. If you make a donation to the Mozilla Foundation or Thunderbird, we use Fundraise Up to manage our donations and provide transactional receipts to donors. To make requests regarding your personal data, please contact us through our Data Subject Access Request Portal. If you have any other questions regarding personal data or our privacy practices, please contact us at compliance@mozilla.com. We respond to all requests we receive from individuals wishing to exercise their data protection rights in accordance with applicable data protection laws. Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.mozilla.org/about/legal/terms/mozilla",
    "title": "Websites & Communications Terms of Use — Mozilla",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work June 23, 2016 Please read the terms of this entire document  (“Terms”) carefully because it explains your rights and responsibilities when you visit any of Mozilla’s websites (“Websites”), or related feeds, social media, newsletters, source code repositories, and emails (together with Websites, these are collectively referred to as “Communications”). By accessing or signing up to receive Communications, you agree to be bound by these Terms. Our Websites include multiple domains such as mozilla.org, mozillians.org, firefox.com, mozillafestival.org, openstandard.com, openbadges.org and webmaker.org. You may also recognize our Websites by nicknames such as Bugzilla@Mozilla, BMO, MozWiki, MoPad, MozReps, MDN, Marketplace, One and Done, SUMO, and AMO.\nSome of our Websites connect you with links, apps or add-ons that are provided by other parties and are subject to separate Terms. Some Websites require you to register for an account in order to access additional features of a Website or another Mozilla service. If applicable, additional terms will be presented to you. You are responsible for all activities under your account. Some Websites allow you to create a username during registration. Your use of a username must comply with our Acceptable Use Policy. Our Communications include content such as articles, images, photographs, comments, software code, audio and video clips, and other materials (collectively, “Content”).  Content is authored by Mozilla, contributors to Mozilla projects, and other sources. Content authored by Mozilla is generally made available for public sharing and reuse through open licenses such as Creative Commons (for expressive material) or the Mozilla Public License (for software code).  In most cases we ask Mozilla contributors to release Content under open licenses. Some Content in our Communications is acquired from sources that prohibit further use of their Content without advance permission.  Where possible, the Content or Website footer will display a notice with the applicable license. You agree to abide by such notices.  Note the following specifics: You may contribute Content when interacting with our Communications, including but not limited to commenting on an article, blogging, contributing code, or contributing graphics or written material (each a “Submission”). Unless your Submission is made under a separate agreement with Mozilla, in which case that agreement will govern, then For Submissions to Mozilla's open source projects: For all other Submissions, you agree to the following in connection with each: The Mozilla Websites, Communications & Cookies Privacy Notice describes how we handle information that we receive from you in connection with our Communications. The Privacy Notice explains, for example, that we place certain cookies on our Websites and how you can opt-out. If you subscribe to receive our newsletters or register for an account in connection with any of our Websites, you may receive transactional emails from us in connection with your account (for example, legal, privacy, and security updates). Some of our Websites have online tools that allow you to send emails to others. For example, you can invite your contacts to events on Mozillians.  You agree not to misuse others’ email addresses (for example, by spamming them). Other Websites, like MozReps, provide tools that enable users to arrange physical events for anyone to attend. Please exercise caution and good judgment when attending events. For more information on how to report claims of copyright or trademark infringement, please see: https://www.mozilla.org/about/legal/report-infringement/. These Terms will continue to apply until ended by either you or Mozilla. You can choose to end them at any time for any reason by discontinuing your use of our Communications and, if applicable, deleting your account. We may suspend or terminate your access to our Communications at any time for any reason, including, but not limited to, if we reasonably believe: (i) you have violated these Terms, our Acceptable Use Policy, or other relevant policy; (ii) you create risk or possible legal exposure for us; or (iii) our provision of the Communications to you is no longer commercially viable. In all such cases, these Terms shall terminate, except that the following sections shall continue to apply: Indemnification, Disclaimer; Limitation of Liability, Miscellaneous. You agree to defend, indemnify and hold harmless Mozilla, its contractors, contributors, licensors, and partners; and the respective directors, officers, employees and agents of the foregoing (\"Indemnified Parties\") from and against any and all third party claims and expenses, including attorneys' fees, arising out of or related to your use of our Communications (including, but not limited to, from your Submissions or from your violation of any these Terms). THE COMMUNICATIONS ARE PROVIDED \"AS IS\" WITH ALL FAULTS. TO THE EXTENT PERMITTED BY LAW, MOZILLA AND THE INDEMNIFIED PARTIES HEREBY DISCLAIM ALL WARRANTIES, WHETHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION WARRANTIES THAT THE COMMUNICATIONS ARE FREE OF DEFECTS, MERCHANTABLE, FIT FOR A PARTICULAR PURPOSE, AND NON-INFRINGING. YOU BEAR THE ENTIRE RISK AS TO USING THE COMMUNICATIONS FOR YOUR PURPOSES AND AS TO THE QUALITY AND PERFORMANCE OF THE COMMUNICATIONS, INCLUDING WITHOUT LIMITATION THE RISK THAT YOUR HARDWARE, SOFTWARE, OR CONTENT IS DELETED OR CORRUPTED, THAT SOMEONE ELSE GAINS UNAUTHORIZED ACCESS TO YOUR INFORMATION, OR THAT ANOTHER USER MISUSES OR MISAPPROPRIATES YOUR SUBMISSION. THIS LIMITATION WILL APPLY NOTWITHSTANDING THE FAILURE OF ESSENTIAL PURPOSE OF ANY REMEDY. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF IMPLIED WARRANTIES, SO THIS DISCLAIMER MAY NOT APPLY TO YOU. EXCEPT AS REQUIRED BY LAW, MOZILLA AND THE INDEMNIFIED PARTIES WILL NOT BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, OR EXEMPLARY DAMAGES ARISING OUT OF OR IN ANY WAY RELATING TO THESE TERMS OR THE USE OF OR INABILITY TO USE THE COMMUNICATIONS, INCLUDING WITHOUT LIMITATION DIRECT AND INDIRECT DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, LOST PROFITS, LOSS OF DATA, AND COMPUTER FAILURE OR MALFUNCTION, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES AND REGARDLESS OF THE THEORY (CONTRACT, TORT, OR OTHERWISE) UPON WHICH SUCH CLAIM IS BASED. THE COLLECTIVE LIABILITY OF MOZILLA AND THE INDEMNIFIED PARTIES UNDER THIS AGREEMENT WILL NOT EXCEED $500 (FIVE HUNDRED DOLLARS). SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL, CONSEQUENTIAL, OR SPECIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU. We may update these Terms from time to time to address a new feature of the Communications or to clarify a provision. The updated Terms will be posted online. If the changes are substantive, we will announce the update through our usual channels for such announcements such as blog posts, banners, emails, or forums. Your continued use of our Communications after the effective date of such changes constitutes your acceptance of such changes. To make your review more convenient, we will post an effective date at the top of this page. These Terms constitute the entire agreement between you and Mozilla concerning our Communications and supersede any prior versions of these Terms. The Communications and these Terms are governed by the laws of the state of California, U.S.A., excluding its conflict of law provisions. All claims and disputes arising out of the Communications or these Terms shall be brought exclusively in the courts of Santa Clara County, California, and you consent to personal jurisdiction in those courts. If any portion of these Terms is held to be invalid or unenforceable, the remaining portions will remain in full force and effect. In the event of a conflict between a translated version of these Terms and the English language version, the English language version shall control. In the event of a conflict between these Terms and relevant additional terms, the additional terms shall control. Mozilla \nAttn: Mozilla – Legal Notices\n149 New Montgomery St.\n4th Floor\nSan Francisco, CA 94105\nUSA\nTelephone: 650-903-0800\nFax: 650-903-0875\nLegal-notices at mozilla.com Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.mozilla.org/about/governance/policies/participation/",
    "title": "Community Participation Guidelines — Mozilla",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work Software and other innovations designed to advance our mission. Become a volunteer contributor in a number of different areas. Read about our vision for the Web and how we intend to pursue that vision. The heart of Mozilla is people. We put people first and do our best to recognize, appreciate and respect the diversity of our global contributors. The Mozilla Project welcomes contributions from everyone who shares our goals and wants to contribute in a healthy and constructive manner within our community. As such, we have adopted this code of conduct and require all those who participate to agree and adhere to these Community Participation Guidelines in order to help us create a safe and positive community experience for all. These guidelines aim to support a community where all people should feel safe to participate, introduce new ideas and inspire others, regardless of: Openness, collaboration and participation are core aspects of our work — from development on Firefox to collaboratively designing curriculum. We gain strength from diversity and actively seek participation from those who enhance it. These guidelines exist to enable diverse individuals and groups to interact and collaborate to mutual advantage. This document outlines both expected and prohibited behavior. These guidelines outline our behavior expectations as members of the Mozilla community in all Mozilla activities, both offline and online. Your participation is contingent upon following these guidelines in all Mozilla activities, including but not limited to: These guidelines work in conjunction with our Anti-Harassment/Discrimination Policies[1], which sets out protections for, and obligations of, Mozilla employees. The Anti-Harassment/Discrimination Policy is crafted with specific legal definitions and requirements in mind. While these guidelines / code of conduct are specifically aimed at Mozilla’s work and community, we recognize that it is possible for actions taken outside of Mozilla’s online or in person spaces to have a deep impact on community health. (For example, in the past, we publicly identified an anonymous posting aimed at a Mozilla employee in a non-Mozilla forum as clear grounds for removal from the Mozilla community.) This is an active topic in the diversity and inclusion realm. We anticipate wide-ranging discussions among our communities about appropriate boundaries. The following behaviors are expected of all Mozillians: Value each other’s ideas, styles and viewpoints. We may not always agree, but disagreement is no excuse for poor manners. Be open to different possibilities and to being wrong. Be respectful in all interactions and communications, especially when debating the merits of different options. Be aware of your impact and how intense interactions may be affecting people. Be direct, constructive and positive. Take responsibility for your impact and your mistakes – if someone says they have been harmed through your words or actions, listen carefully, apologize sincerely, and correct the behavior going forward. We are likely to have some discussions about if and when criticism is respectful and when it’s not. We must be able to speak directly when we disagree and when we think we need to improve. We cannot withhold hard truths. Doing so respectfully is hard, doing so when others don’t seem to be listening is harder, and hearing such comments when one is the recipient can be even harder still. We need to be honest and direct, as well as respectful. Seek diverse perspectives. Diversity of views and of people on teams powers innovation, even if it is not always comfortable. Encourage all voices. Help new perspectives be heard and listen actively. If you find yourself dominating a discussion, it is especially important to step back and encourage other voices to join in. Be aware of how much time is taken up by dominant members of the group. Provide alternative ways to contribute or participate when possible.\n\n  \n  Be inclusive of everyone in an interaction, respecting and facilitating people’s participation whether they are:\n  \n\nRemote (on video or phone)\nNot native language speakers\nComing from a different culture\nUsing pronouns other than “he” or “she”\nLiving in a different time zone\nFacing other challenges to participate\n\n\n  Think about how you might facilitate alternative ways to contribute or participate. If you find yourself dominating a discussion, step back. Make way for other voices and listen actively to them.\n  \nUnderstand Different Perspectives\n\n  Our goal should not be to “win” every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. “Winning” is when different perspectives make our work richer and stronger.\n  \nAppreciate and Accommodate Our Similarities and Differences\n\n  Mozillians come from many cultures and backgrounds. Cultural differences can encompass everything from official religious observances to personal habits to clothing. Be respectful of people with different cultural practices, attitudes and beliefs. Work to eliminate your own biases, prejudices and discriminatory practices. Think of others’ needs from their point of view. Use preferred titles (including pronouns) and the appropriate tone of voice. Respect people’s right to privacy and confidentiality. Be open to learning from and educating others as well as educating yourself; it is unrealistic to expect Mozillians to know the cultural practices of every ethnic and cultural group, but everyone needs to recognize one’s native culture is only part of positive interactions.\n  \nLead by Example\n\n  By matching your actions with your words, you become a person others want to follow. Your actions influence others to behave and respond in ways that are valuable and appropriate for our organizational outcomes. Design your community and your work for inclusion. Hold yourself and others accountable for inclusive behaviors. Make decisions based on the highest good for Mozilla’s mission. Be inclusive of everyone in an interaction, respecting and facilitating people’s participation whether they are: Think about how you might facilitate alternative ways to contribute or participate. If you find yourself dominating a discussion, step back. Make way for other voices and listen actively to them. Our goal should not be to “win” every disagreement or argument. A more productive goal is to be open to ideas that make our own ideas better. Strive to be an example for inclusive thinking. “Winning” is when different perspectives make our work richer and stronger. Mozillians come from many cultures and backgrounds. Cultural differences can encompass everything from official religious observances to personal habits to clothing. Be respectful of people with different cultural practices, attitudes and beliefs. Work to eliminate your own biases, prejudices and discriminatory practices. Think of others’ needs from their point of view. Use preferred titles (including pronouns) and the appropriate tone of voice. Respect people’s right to privacy and confidentiality. Be open to learning from and educating others as well as educating yourself; it is unrealistic to expect Mozillians to know the cultural practices of every ethnic and cultural group, but everyone needs to recognize one’s native culture is only part of positive interactions. By matching your actions with your words, you become a person others want to follow. Your actions influence others to behave and respond in ways that are valuable and appropriate for our organizational outcomes. Design your community and your work for inclusion. Hold yourself and others accountable for inclusive behaviors. Make decisions based on the highest good for Mozilla’s mission. The following behaviors are considered to be unacceptable under these guidelines. Violence and threats of violence are not acceptable - online or offline. This includes incitement of violence toward any individual, including encouraging a person to commit self-harm. This also includes posting or threatening to post other people’s personally identifying information (“doxxing”) online. Conflicts will inevitably arise, but frustration should never turn into a personal attack. It is not okay to insult, demean or belittle others. Attacking someone for their opinions, beliefs and ideas is not acceptable. It is important to speak directly when we disagree and when we think we need to improve, but such discussions must be conducted respectfully and professionally, remaining focused on the issue at hand. Hurtful or harmful language related to: is not acceptable. This includes deliberately referring to someone by a gender that they do not identify with, and/or questioning the legitimacy of an individual’s gender identity. If you’re unsure if a word is derogatory, don’t use it. This also includes repeated subtle and/or indirect discrimination; when asked to stop, stop the behavior in question. Unwelcome sexual attention or unwelcome physical contact is not acceptable. This includes sexualized comments, jokes or imagery in interactions, communications or presentation materials, as well as inappropriate touching, groping, or sexual advances. Additionally, touching a person without permission, including sensitive areas such as their hair, pregnant stomach, mobility device (wheelchair, scooter, etc) or tattoos is unacceptable. This includes physically blocking or intimidating another person. Physical contact or simulated physical contact (such as emojis like “kiss”) without affirmative consent is not acceptable. The sharing or distribution of sexualized images or text is unacceptable. Sustained disruption of events, forums, or meetings, including talks and presentations, will not be tolerated. This includes: We will treat influencing or leading such activities the same way we treat the activities themselves, and thus the same consequences apply. Bad behavior from any Mozillian, including those with decision-making authority, will not be tolerated. Intentional efforts to exclude people (except as part of a consequence of the guidelines or other official action) from Mozilla activities are not acceptable and will be dealt with appropriately. Reports of harassment/discrimination will be promptly and thoroughly investigated by the people responsible for the safety of the space, event or activity. Appropriate measures will be taken to address the situation. Anyone being asked to stop unacceptable behavior is expected to comply immediately. Violation of these guidelines can result in anyone being asked to leave an event or online space, either temporarily or for the duration of the event, or being banned from participation in spaces, or future events and activities in perpetuity. Mozilla Staff are held accountable, in addition to these guidelines, to Mozilla’s staff Anti-Harassment/Discrimination Policies [1]. Mozilla staff in violation of these guidelines may be subject to further consequences, such as disciplinary action, up to and including termination of employment. For contractors or vendors, violation of these guidelines may affect continuation or renewal of contract. In addition, any participants who abuse the reporting process will be considered to be in violation of these guidelines and subject to the same consequences. False reporting, especially to retaliate or exclude, will not be accepted or tolerated. If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above, please use cpg-report@mozilla.com to report. Reports are triaged by the Community Participation Guidelines Response Lead. After receiving a concise description of your situation, they will review and determine the next steps. In addition to conducting any investigation, they can provide a range of resources, from a private consultation to other community resources. They will involve other colleagues or outside specialists (such as legal counsel), as needed to appropriately address each situation. Please also report to us if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines, even if the situation is not happening to you. If you feel you have been unfairly accused of violating these guidelines, please follow the same reporting process. Each physical or virtual Mozilla space shall have a designated contact. All Mozilla events will have designated a specific safety guideline with emergency and anti-abuse contacts at the event as well as online. These contacts will be posted prominently throughout the event, and in print and online materials. Event leaders are requested to speak at the event about the guidelines and to ask participants to review and agree to them when they sign up for the event. Reports will receive an email notice of receipt. Once an incident has been investigated and a decision has been communicated to the relevant parties, all have the opportunity to appeal this decision by sending an email to cpg-questions@mozilla.com. Everyone is encouraged to ask questions about these guidelines. If you are organizing an event or activity, reach out for tips for building inclusion for your event, activity or space. Your input is welcome and you will always get a response within 24 hours (or on the next weekday, if it is the weekend) if you reach out to cpg-questions@mozilla.com. Please review this change log for updates to this document. This set of guidelines is distributed under a Creative Commons Attribution-ShareAlike license. These guidelines have been adapted with modifications from Mozilla’s original Community Participation Guidelines, the Ubuntu Code of Conduct, Mozilla’s View Source Conference Code of Conduct, and the Rust Language Code of Conduct, which are based on Stumptown Syndicate’s Citizen Code of Conduct. Additional text from the LGBTQ in Technology Code of Conduct and the WisCon code of conduct. This document and all associated processes are only possible with the hard work of many, many Mozillians. Mozilla may amend the guidelines from time to time and may also vary the procedures it sets out where appropriate in a particular case. Your agreement to comply with the guidelines will be deemed agreement to any changes to it. This policy does not form part of any Mozilla employee’s contract of employment or otherwise have contractual effect. [1] The anti-harassment policy is accessible to paid staff here. Get the Mozilla newsletter and help us keep it open and free. Afghanistan\nAkrotiri\nAlbania\nAlgeria\nAmerican Samoa\nAndorra\nAngola\nAnguilla\nAntarctica\nAntigua and Barbuda\nArgentina\nArmenia\nAruba\nAshmore and Cartier Islands\nAustralia\nAustria\nAzerbaijan\nBahamas, The\nBahrain\nBaker Island\nBangladesh\nBarbados\nBassas da India\nBelarus\nBelgium\nBelize\nBenin\nBermuda\nBhutan\nBolivia\nBosnia and Herzegovina\nBotswana\nBouvet Island\nBrazil\nBritish Indian Ocean Territory\nBrunei\nBulgaria\nBurkina Faso\nBurma\nBurundi\nCambodia\nCameroon\nCanada\nCape Verde\nCaribbean Netherlands\nCayman Islands\nCentral African Republic\nChad\nChile\nChina\nChristmas Island\nClipperton Island\nCocos (Keeling) Islands\nColombia\nComoros\nCongo (Brazzaville)\nCongo (Kinshasa)\nCook Islands\nCoral Sea Islands\nCosta Rica\nCroatia\nCuba\nCuraçao\nCyprus\nCzechia\nCôte d’Ivoire\nDenmark\nDhekelia\nDiego Garcia\nDjibouti\nDominica\nDominican Republic\nEcuador\nEgypt\nEl Salvador\nEquatorial Guinea\nEritrea\nEstonia\nEswatini\nEthiopia\nEuropa Island\nFalkland Islands (Islas Malvinas)\nFaroe Islands\nFiji\nFinland\nFrance\nFrench Guiana\nFrench Polynesia\nFrench Southern and Antarctic Lands\nGabon\nGambia, The\nGaza Strip\nGeorgia\nGermany\nGhana\nGibraltar\nGlorioso Islands\nGreece\nGreenland\nGrenada\nGuadeloupe\nGuam\nGuatemala\nGuernsey\nGuinea\nGuinea-Bissau\nGuyana\nHaiti\nHeard Island and McDonald Islands\nHonduras\nHong Kong\nHowland Island\nHungary\nIceland\nIndia\nIndonesia\nIran\nIraq\nIreland\nIsle of Man\nIsrael\nItaly\nJamaica\nJan Mayen\nJapan\nJarvis Island\nJersey\nJohnston Atoll\nJordan\nJuan de Nova Island\nKazakhstan\nKenya\nKingman Reef\nKiribati\nKorea, North\nKorea, South\nKosovo\nKuwait\nKyrgyzstan\nLaos\nLatvia\nLebanon\nLesotho\nLiberia\nLibya\nLiechtenstein\nLithuania\nLuxembourg\nMacau\nMadagascar\nMalawi\nMalaysia\nMaldives\nMali\nMalta\nMarshall Islands\nMartinique\nMauritania\nMauritius\nMayotte\nMexico\nMicronesia, Federated States of\nMidway Islands\nMoldova\nMonaco\nMongolia\nMontenegro\nMontserrat\nMorocco\nMozambique\nNamibia\nNauru\nNavassa Island\nNepal\nNetherlands\nNew Caledonia\nNew Zealand\nNicaragua\nNiger\nNigeria\nNiue\nNorfolk Island\nNorth Macedonia\nNorthern Mariana Islands\nNorway\nOman\nPakistan\nPalau\nPalmyra Atoll\nPanama\nPapua New Guinea\nParacel Islands\nParaguay\nPeru\nPhilippines\nPitcairn Islands\nPoland\nPortugal\nPuerto Rico\nQatar\nRomania\nRussia\nRwanda\nRéunion\nSaint Barthélemy\nSaint Helena, Ascension, and Tristan da Cunha\nSaint Kitts and Nevis\nSaint Lucia\nSaint Martin\nSaint Pierre and Miquelon\nSaint Vincent and the Grenadines\nSamoa\nSan Marino\nSaudi Arabia\nSenegal\nSerbia\nSeychelles\nSierra Leone\nSingapore\nSint Maarten\nSlovakia\nSlovenia\nSolomon Islands\nSomalia\nSouth Africa\nSouth Georgia and South Sandwich Islands\nSouth Sudan\nSpain\nSpratly Islands\nSri Lanka\nSudan\nSuriname\nSvalbard\nSweden\nSwitzerland\nSyria\nSão Tomé and Príncipe\nTaiwan\nTajikistan\nTanzania\nThailand\nTimor-Leste\nTogo\nTokelau\nTonga\nTrinidad and Tobago\nTromelin Island\nTunisia\nTurkey\nTurkmenistan\nTurks and Caicos Islands\nTuvalu\nUganda\nUkraine\nUnited Arab Emirates\nUnited Kingdom\nUnited States\nUruguay\nUzbekistan\nVanuatu\nVatican City\nVenezuela\nVietnam\nVirgin Islands, British\nVirgin Islands, U.S.\nWake Island\nWallis and Futuna\nWest Bank\nWestern Sahara\nYemen\nZambia\nZimbabwe Bahasa Indonesia\nDeutsch\nEnglish\nEspañol\nFrançais\nPolski\nPortuguês\nРусский\n正體中文 I’m okay with Mozilla handling my info as explained in this Privacy Notice Sign Up Now\n          \n        \n\n              \n              \n                We will only send you Mozilla-related information. You can unsubscribe at any time. If you haven’t previously confirmed a subscription to a Mozilla-related newsletter, you may have to do so. Please check your inbox or your spam filter for an email from us. Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.mozilla.org",
    "title": "Internet for people, not profit — Mozilla Global",
    "content": "In addition to Cookies necessary for this site to function, we’d like your permission to set some additional Cookies to better understand your browsing needs and improve your experience. Rest assured — we value your privacy. All products Our Mission Our Work It is available in the following languages: Privacy-first advertising solutions for brands, publishers, and platforms. Visit Mozilla Corporation’s not-for-profit parent, the Mozilla Foundation.\n          Portions of this content are ©1998–2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://foundation.mozilla.org/",
    "title": "Welcome to Mozilla Foundation\n                \n            \n            \n                \n                - Mozilla Foundation",
    "content": "Stand up for a better internet Stand up for a better internet Hear from Mozilla Foundation’s Executive Director Nabiha Syed on why she is defiantly optimistic that the future of technology can be good— and we all need to roll up our sleeves to make it happen together. Mozilla Festival 2025 Bold ideas shaping the future of technology Forget what you’ve been told. The future of tech isn’t set—it’s ours to shape. Join us in Barcelona, November 7–9, to unlearn defaults, imagine boldly and build what’s next. Mozilla Festival 2025 Community Let’s roll up our sleeves Our tech future is not inevitable—it’s a design choice. And we can choose differently together. Join us to build a tech future––Powered by people. Open by design. Community The internet we deserve starts with you. Sign up for our newsletter. If you haven’t previously confirmed your opt-in to a Mozilla-related email subscription you may have to do so now.\n          Please check your inbox or spam filter for an email from us to click and confirm your subscription. If you have already confirmed your opt-in to receive Mozilla-related emails, you can now\n          \n            manage your subscriptions\n          \n          and update your email preferences. We're building alternative digital futures Tech education that empowers, for everyone Powered by our people and purpose Dr. Mary is reimagining tech education in Kenya. As the Dean of Computing and Informatics at Meru University, she's bridging the gap between what her students are learning and the real challenges in their communities. For her, building better technology starts with listening, \"it's not just about coding, it's about understanding people's needs.\" Irvin is inspiring more people to build a better digital future. As a longtime contributor and volunteer community lead for the Mozilla Taiwan Community, Irvin is passionate about turning open-source technology into local impact. He has dedicated years to projects like Firefox, Support Mozilla (SUMO), MDN, and Common Voice. Surabhi explores how media can be a lever to drive social impact at RNW Media in the Netherlands. She is focused on the intersection of media and AI ethics. She wants real impact for her community, where \"diverse groups of people have a voice, and the decision-making power to co-design and co-create tech that serves public interest.\" Together we have the power to make good on the promise of the Internet. Join our global movement and make a contribution today to shape the future of technology. Stay updated with our newsletter If you haven’t previously confirmed your opt-in to a Mozilla-related email subscription you may have to do so now.\n          Please check your inbox or spam filter for an email from us to click and confirm your subscription. If you have already confirmed your opt-in to receive Mozilla-related emails, you can now\n          \n            manage your subscriptions\n          \n          and update your email preferences. Mozilla Foundation is a global non-profit and parent of the Mozilla Corporation. Most content available under a Creative Commons license."
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/MDN/Writing_guidelines/Attrib_copyright_license",
    "title": "Attribution and copyright licensing - MDN Web Docs | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support MDN Web Docs content is available free of charge and is available under various open-source licenses. This section covers the types of content we provide and the copyrights and licenses that are in effect for each type if you choose to reuse any of it. Note:\nThe content on MDN Web Docs has been prepared with the contributions of authors from both inside and outside Mozilla. Unless otherwise indicated, the content is available under the terms of the Creative Commons Attribution-ShareAlike license (CC-BY-SA), v2.5 or any later version. Your reuse of the content here is published under the same license as the original content â CC-BY-SA v2.5 or any later version.\nWhen reusing the content on MDN Web Docs, you need to ensure that attribution is given to the material as well as to \"Mozilla Contributors\".\nGood attribution is the title of the document, with a hyperlink (online) or URL (in print) to the specific page of the content being sourced, and any modifications you've made briefly described.\nFor example, to provide attribution for this page, you can write: \"Attributions and copyright licensing\" by Mozilla Contributors, licensed under CC-BY-SA 2.5. You may also want to link \"Mozilla Contributors\" to a contributors.txt file linked in the page footer you're referencing for a list of authors, if reasonable.\nSee Recommended practices for attribution for more details. Code samples added on or after August 20, 2010 are in the public domain CC0. No licensing notice is necessary but if you need one, you can use: Any copyright is dedicated to the Public Domain: https://creativecommons.org/publicdomain/zero/1.0/ Code samples added before August 20, 2010 are available under the MIT license; you should insert the following attribution information into the MIT template: \"Â© <date of last wiki page revision> <name of person who put it in the wiki>\". Since the launch of the new Yari MDN platform on December 14 2020, there is currently no way to determine which one you need. We are working on this and will update this content soon. If you wish to contribute to MDN Web Docs, you agree that your documentation is available under the Attribution-ShareAlike license (or occasionally an alternative license already specified by the page you are editing) and that your code samples are available under Creative Commons CC-0 (a Public Domain dedication). Warning:\nNo new pages may be created using alternate licenses. Copyright for contributed materials remains with the author unless the author assigns it to someone else. If you have any questions or concerns about anything discussed here, please contact the MDN Web Docs team. The rights in the logos, trademarks, and service marks of the Mozilla Foundation, as well as the look and feel of this website, are not licensed under the Creative Commons license, and to the extent they are works of authorship (like logos and graphic design), they are not included in the work that is licensed under those terms. If you use the text of documents and wish to also use any of these rights, or if you have any other questions about complying with our licensing terms for this collection, you should contact the Mozilla Foundation here: licensing@mozilla.org. In general, we do not approve of copying content from other sources and putting it on MDN.\nMDN should be made up of original content wherever possible.\nIf we receive a pull request and discover that it contains plagiarized content, we will close it and request that the submitter resubmit the change with the content rewritten into their own words. Note:\nUnless there is a good reason to republish the content, we will probably say \"no\".\nThe MDN writing team's decision is final. If someone wants to donate an article to MDN that they previously published on their blog or it makes sense to copy a complex reference sheet to MDN, there may be justification for republishing it. For these cases, discuss your plan with the MDN team beforehand: If the content is published under a closed license: If the content is published under an open license: We regularly get users asking us questions about how to link to MDN Web Docs and whether or not it is even allowed. The short answer is: yes, you can link to MDN Web Docs! Not only is the hypertext link the essence of the web, it is both a way to point your users to valuable resources as well as a show of trust toward the work our community does. This page was last modified on Feb 19, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://www.bbc.co.uk/",
    "title": "BBC - Home",
    "content": "He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Keep up with the latest from BBC Sport Scottish Premiership: Yengi blows huge Aberdeen chance with Hearts leading Salah opens scoring but Athletic Bilbao level against new-look Liverpool Might Newcastle owner's cash help Liverpool buy Isak? Who pressed their Ashes case? Who disappointed? England's player ratings 'Totally torn apart' - how Morecambe decline threatens a whole community The top stories from England, Scotland, Wales and Northern Ireland Priest denies controlling and abusive behaviour Deputy First Minister Kate Forbes to stand down as MSP Six men deny city centre disorder after 'group fight' A5 decision to be appealed, says minister Find your regional news Latest news and must-see moments Radio personality James Whale dies aged 74 Known for his late-night TV shows and radio phone-ins, he entertained and offended in equal measure. Chloe Ayling: What is it about me and this story that makes it so hard to believe? Video, 00:01:22Chloe Ayling: What is it about me and this story that makes it so hard to believe? Man dies after fall at Oasis Wembley concert Spider-Man Tom Holland filming on streets of Glasgow 'I don't look back at those TV Burp years particularly fondly' Leslie Nielsen on the original The Naked Gun's beginnings. Video, 00:02:00Leslie Nielsen on the original The Naked Gun's beginnings Five summer-filled films for all the family Meal ideas, cooking tips and more, updated daily to keep you inspired Nourishing leafy salads that won't leave you hungry Try these filling recipes that are packed with good-for-you ingredients and perfect for a satisfying lunch or dinner. The colourful Mary Berry lunch you can have on the table in minutes What is iodine and why do I need it? Food content creators break Guinness World Record for largest Scotch egg Is it weird to cut your toast if you're an adult? Audio, 00:04:45Is it weird to cut your toast if you're an adult? Easy ways to boost your health during your working day Man who had Â£5k-a-year takeaway habit drops 10st Tips on navigating results day as a parent 'I was a shopping addict - it needs to be taken more seriously' 'Mealtimes can become a battleground for people with dementia' Audio, 26 minutes'Mealtimes can become a battleground for people with dementia' Could you be owed car finance compensation? Here's how to check A court ruling could lead to millions claiming compensation over being mis-sold car finance deals. 'Tuition fees aren't the problem':  How much to save for your kids to go to university. Video, 00:01:34'Tuition fees aren't the problem':  How much to save for your kids to go to university 'My energy bills are so high I asked the council to buy back my flat' Fake or Fortune: Is a Â£35 painting worth Â£50,000? 'Extreme overseas day-tripping is quite addictive' Video, 00:01:13'Extreme overseas day-tripping is quite addictive' Are these the most mesmerising gardens in the world? Ed Sheeran fan flies 3,000 miles to sing at castle 'I'd be lost without football': The ex-Premier League striker playing at 49 The remarkable story of the rescue penguin that followed a teacher home. Audio, 40 minutesThe remarkable story of the rescue penguin that followed a teacher home A closer look at the week's stories Will there be another heatwave this summer or will August bring more rain? The big fat 'fake' Indian wedding - Gen Z's latest party trend When Scotland was the world's UFO hot spot Results Day 2025: The facts, the myths, and the freebies Panorama delves into the Southport riots and asks why so many took part. Video, 28 minutesPanorama delves into the Southport riots and asks why so many took part Highlights from this year's season Beverley Knight delivers infectious groove with an Aretha Franklin classic. Audio, 3 minutesBeverley Knight delivers infectious groove with an Aretha Franklin classic Kahchun Wong conducts the HallÃ© in Mahler's visionary symphony. Audio, 104 minutesKahchun Wong conducts the HallÃ© in Mahler's visionary symphony The spine-tingling Traitors theme song reimagined with a full orchestra. Audio, 3 minutesThe spine-tingling Traitors theme song reimagined with a full orchestra A stunning moment of tranquility with Bach's Air on a G String. Video, 10 minutesA stunning moment of tranquility with Bach's Air on a G String JADE performs her breakthrough track accompanied by a full orchestra. Video, 5 minutesJADE performs her breakthrough track accompanied by a full orchestra Find the latest news, features, audio and TV coverage. Watch a selection of standout clips from across the BBC Add to your watchlist on iPlayer and subscribe on Sounds Jeremy Clarkson recounts a disastrous World War Two mission What do your dreams say about you? The rise and fall of the 90s lads mag phenomenon Hugh Bonneville narrates Sherlock Holmesâ greatest cases A brand new missing person case in thrilling New Zealand detective drama. Video, 52 minutesA brand new missing person case in thrilling New Zealand detective drama In series two of The Gone, the hunt for an Irish journalist leads detective duo Richter and Huia to a decade-old unsolved murder. The incredible rescue teams saving lives on the wild landscape of Eryri. Video, 28 minutesThe incredible rescue teams saving lives on the wild landscape of Eryri Are standing desks a good way to improve posture - or just a fad? Audio, 25 minutesAre standing desks a good way to improve posture - or just a fad? The story of a cricketing icon, which ended in scandal and tragedy. Audio, 27 minutesThe story of a cricketing icon, which ended in scandal and tragedy Loose Women stars attempt to name that tune first in The Hit List. Video, 44 minutesLoose Women stars attempt to name that tune first in The Hit List Scotland ALBA Wales Cymru NI Copyright Â© 2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
  },
  {
    "url": "https://www.bbc.co.uk",
    "title": "BBC - Home",
    "content": "He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Keep up with the latest from BBC Sport Scottish Premiership: Yengi blows huge Aberdeen chance with Hearts leading Salah opens scoring but Athletic Bilbao level against new-look Liverpool Might Newcastle owner's cash help Liverpool buy Isak? Who pressed their Ashes case? Who disappointed? England's player ratings 'Totally torn apart' - how Morecambe decline threatens a whole community The top stories from England, Scotland, Wales and Northern Ireland Priest denies controlling and abusive behaviour Deputy First Minister Kate Forbes to stand down as MSP Six men deny city centre disorder after 'group fight' A5 decision to be appealed, says minister Find your regional news Latest news and must-see moments Radio personality James Whale dies aged 74 Known for his late-night TV shows and radio phone-ins, he entertained and offended in equal measure. Chloe Ayling: What is it about me and this story that makes it so hard to believe? Video, 00:01:22Chloe Ayling: What is it about me and this story that makes it so hard to believe? Man dies after fall at Oasis Wembley concert Spider-Man Tom Holland filming on streets of Glasgow 'I don't look back at those TV Burp years particularly fondly' Leslie Nielsen on the original The Naked Gun's beginnings. Video, 00:02:00Leslie Nielsen on the original The Naked Gun's beginnings Five summer-filled films for all the family Meal ideas, cooking tips and more, updated daily to keep you inspired Nourishing leafy salads that won't leave you hungry Try these filling recipes that are packed with good-for-you ingredients and perfect for a satisfying lunch or dinner. The colourful Mary Berry lunch you can have on the table in minutes What is iodine and why do I need it? Food content creators break Guinness World Record for largest Scotch egg Is it weird to cut your toast if you're an adult? Audio, 00:04:45Is it weird to cut your toast if you're an adult? Easy ways to boost your health during your working day Man who had Â£5k-a-year takeaway habit drops 10st Tips on navigating results day as a parent 'I was a shopping addict - it needs to be taken more seriously' 'Mealtimes can become a battleground for people with dementia' Audio, 26 minutes'Mealtimes can become a battleground for people with dementia' Could you be owed car finance compensation? Here's how to check A court ruling could lead to millions claiming compensation over being mis-sold car finance deals. 'Tuition fees aren't the problem':  How much to save for your kids to go to university. Video, 00:01:34'Tuition fees aren't the problem':  How much to save for your kids to go to university 'My energy bills are so high I asked the council to buy back my flat' Fake or Fortune: Is a Â£35 painting worth Â£50,000? 'Extreme overseas day-tripping is quite addictive' Video, 00:01:13'Extreme overseas day-tripping is quite addictive' Are these the most mesmerising gardens in the world? Ed Sheeran fan flies 3,000 miles to sing at castle 'I'd be lost without football': The ex-Premier League striker playing at 49 The remarkable story of the rescue penguin that followed a teacher home. Audio, 40 minutesThe remarkable story of the rescue penguin that followed a teacher home A closer look at the week's stories Will there be another heatwave this summer or will August bring more rain? The big fat 'fake' Indian wedding - Gen Z's latest party trend When Scotland was the world's UFO hot spot Results Day 2025: The facts, the myths, and the freebies Panorama delves into the Southport riots and asks why so many took part. Video, 28 minutesPanorama delves into the Southport riots and asks why so many took part Highlights from this year's season Beverley Knight delivers infectious groove with an Aretha Franklin classic. Audio, 3 minutesBeverley Knight delivers infectious groove with an Aretha Franklin classic Kahchun Wong conducts the HallÃ© in Mahler's visionary symphony. Audio, 104 minutesKahchun Wong conducts the HallÃ© in Mahler's visionary symphony The spine-tingling Traitors theme song reimagined with a full orchestra. Audio, 3 minutesThe spine-tingling Traitors theme song reimagined with a full orchestra A stunning moment of tranquility with Bach's Air on a G String. Video, 10 minutesA stunning moment of tranquility with Bach's Air on a G String JADE performs her breakthrough track accompanied by a full orchestra. Video, 5 minutesJADE performs her breakthrough track accompanied by a full orchestra Find the latest news, features, audio and TV coverage. Watch a selection of standout clips from across the BBC Add to your watchlist on iPlayer and subscribe on Sounds Jeremy Clarkson recounts a disastrous World War Two mission What do your dreams say about you? The rise and fall of the 90s lads mag phenomenon Hugh Bonneville narrates Sherlock Holmesâ greatest cases A brand new missing person case in thrilling New Zealand detective drama. Video, 52 minutesA brand new missing person case in thrilling New Zealand detective drama In series two of The Gone, the hunt for an Irish journalist leads detective duo Richter and Huia to a decade-old unsolved murder. The incredible rescue teams saving lives on the wild landscape of Eryri. Video, 28 minutesThe incredible rescue teams saving lives on the wild landscape of Eryri Are standing desks a good way to improve posture - or just a fad? Audio, 25 minutesAre standing desks a good way to improve posture - or just a fad? The story of a cricketing icon, which ended in scandal and tragedy. Audio, 27 minutesThe story of a cricketing icon, which ended in scandal and tragedy Loose Women stars attempt to name that tune first in The Hit List. Video, 44 minutesLoose Women stars attempt to name that tune first in The Hit List Scotland ALBA Wales Cymru NI Copyright Â© 2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
  },
  {
    "url": "https://www.bbc.co.uk/news",
    "title": "Home - BBC News",
    "content": "Sign in or create an account to watch, listen and join in He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Dame Stella Rimington, former MI5 director general, dies at 90 Two teenagers detained for torture and killing of cats Calvin Harris shares placenta photos after birth of son He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Dame Stella Rimington, former MI5 director general, dies at 90 Two teenagers detained for torture and killing of cats Calvin Harris shares placenta photos after birth of son He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Dame Stella Rimington, former MI5 director general, dies at 90 Two teenagers detained for torture and killing of cats Calvin Harris shares placenta photos after birth of son He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Dame Stella Rimington, former MI5 director general, dies at 90 Two teenagers detained for torture and killing of cats Calvin Harris shares placenta photos after birth of son He is a human skeleton, Gaza hostage's brother tells BBC Ilay David says the Hamas video showing his brother Evyatar emaciated and weak is a \"new form of cruelty\" that has left his parents shattered. More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Tommy Robinson arrested in connection with assault at St Pancras station Eurostar passengers face delays and cancellations due to French rail issue British couple held in Iran are in country's worst prisons, says son Four-year-old girl dies following incident at water park Danish zoo asks for unwanted pets to feed its predators How an hour at The Oval provided the most dramatic Test finale you could wish for Farage calls on police to share immigration status of charged suspects Dame Stella Rimington, former MI5 director general, dies at 90 Two teenagers detained for torture and killing of cats Calvin Harris shares placenta photos after birth of son Watch our pick of standout clips from across the BBC I regret taking my 11-year-old to a riot following Southport attack, says stepmum A year after anti-migrant protests, two rioters tell BBC Panorama about their motives and regrets. Could you be owed car finance compensation? Here's how to check BBC finds electrocuted, drowned and starved cats in online torture groups How to stay safe during a storm and what to do in a power cut 'There is nothing left' - British expat family forced to flee wildfire in Cyprus Start your week on a high with uplifting stories delivered to your inbox every Monday Watch: Florida police officer removes alligator from family's pool with bare hands. Video, 00:00:56Watch: Florida police officer removes alligator from family's pool with bare hands 'Extreme overseas day-tripping is quite addictive' Video, 00:01:13'Extreme overseas day-tripping is quite addictive' Watch: Twin waterspouts merge near Italian coastline. Video, 00:00:32Watch: Twin waterspouts merge near Italian coastline Damage and disruption as Storm Floris crashes into UK. Video, 00:00:44Damage and disruption as Storm Floris crashes into UK Watch: Australians play in snowy winter wonderland. Video, 00:00:39Watch: Australians play in snowy winter wonderland Radio personality James Whale dies aged 74 Known for his late-night TV shows and radio phone-ins, he entertained and offended in equal measure. US governor threatens Democrats with bribery charges for stalling Texas vote Hundreds of Israeli ex-officials appeal to Trump to help end Gaza war Honours system gets new role to make awards more inclusive Tesla awards boss Elon Musk $29bn in shares Policing boss defects from Tories to Reform Watch: Florida police officer removes alligator from family's pool with bare hands. Video, 00:00:56Watch: Florida police officer removes alligator from family's pool with bare hands Tommy Robinson arrested in connection with assault Girl, 4, dies at Waterworld park Calvin Harris shares placenta photos after birth of son Parents held in Iran's 'worst prisons', says son Farage calls on police to share immigration status of charged suspects He is a human skeleton, Gaza hostage's brother tells BBC Dame Stella Rimington, former MI5 director general, dies at 90 Teenagers detained for cats' torture and killing Danish zoo asks for unwanted pets to feed its predators Eurostar passengers face delays and cancellations due to French rail issue Top stories, breaking news, live reporting, and follow news topics that match your interests Watch live on iPlayer Listen to Live News on Sounds Why has Sydney Sweeney's jeans ad sparked a race debate? Audio, 35 minutesWhy has Sydney Sweeney's jeans ad sparked a race debate? The Battle For Car Finance Compensation. Audio, 33 minutesThe Battle For Car Finance Compensation The rise and fall of the scandalous Maxwell family A staggering tale of money, sex, privacy and power Comedian Bob Mortimer chooses his desert island tracks It's make-or-break time for more aspiring entrepreneurs Exposing the techniques criminals use to steal your money Documents are discovered from the Moors Murders. Video, 57 minutesDocuments are discovered from the Moors Murders Set sail on an unexpected voyage on a cargo ship Based on Sally Rooney's best selling novel 'England and India provide most intense, dramatic and emotional finale' The fifth Test finale between England and India at The Oval was the most intense, dramatic and emotional sport you could see, says chief cricket reporter Stephan Shemilt. Scottish Premiership: Yengi blows huge Aberdeen chance with Hearts leading Salah opens scoring but Athletic Bilbao level against new-look Liverpool Might Newcastle owner's cash help Liverpool buy Isak? Who pressed their Ashes case? Who disappointed? England's player ratings 'Totally torn apart' - how Morecambe decline threatens a whole community 'Here he comes!' - Watch as Woakes walks out to bat one-handed. Video, 00:00:20'Here he comes!' - Watch as Woakes walks out to bat one-handed Instagram TikTok Facebook X Copyright Â© 2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
  },
  {
    "url": "https://www.bbc.co.uk/sport",
    "title": "BBC Sport - Scores, Fixtures, News - Live Sport",
    "content": "Sign in or create an account to watch, listen and join in 'England and India provide most intense, dramatic and emotional finale' The fifth Test finale between England and India at The Oval was the most intense, dramatic and emotional sport you could see, says chief cricket reporter Stephan Shemilt. Might Newcastle owner's cash help Liverpool buy Isak? Scottish Premiership: Can Hearts add to lead against Aberdeen in second half? Salah opens scoring but Athletic Bilbao level against new-look Liverpool Green shoots for Man Utd or another false dawn? Who pressed their Ashes case? Who disappointed? England's player ratings Everton's Â£28m move for Dewsbury-Hall set to end bit-part Chelsea career 'Excellent prospect' Ngumoha stars for Liverpool - but who is he? Son poised to join LAFC for potential record fee Lions tour awards - best player, moment and try, and a word on the Welshman... Swiatek becomes latest seed to fall at Canadian Open I knew we'd struggle in some games last season - Amorim. Video, 00:03:15I knew we'd struggle in some games last season - Amorim Most dropped catches? Why don't stadiums have roofs? Your questions answered. Video, 00:05:39Most dropped catches? Why don't stadiums have roofs? Your questions answered 'He's a legend of English cricket' - Root speaks fondly of Thorpe after tribute. Video, 00:01:19'He's a legend of English cricket' - Root speaks fondly of Thorpe after tribute Rangers 'didn't run enough, didn't fight enough, weren't brave enough' Video, 00:03:14Rangers 'didn't run enough, didn't fight enough, weren't brave enough' It's been an incredible journey - Beirne. Video, 00:00:33It's been an incredible journey - Beirne Why Guardiola's new number two could be Man City's biggest summer signing Manchester City have spent more than Â£150m on players this summer but Pep Guardiola's biggest signing could be Jurgen Klopp's old assistant. Cazorla ready for fairytale finish after son's words of inspiration Liverpool remember Jota at first Anfield game since his death India win all-time classic despite Woakes heroics 'Torn apart' - how Morecambe decline threatens a whole community England-India Test series best since 2005 Ashes - Agnew The final year of The Hundred as we know it? Lions' Sheehan gets four-match ban for 'reckless' play Hooker Dan Sheehan has been banned for four matches, which will be reduced to three if he completes World Rugby's coaching intervention, for his clearout in the British and Irish Lions' final Test defeat by Australia. How to join BBC Sport's FPL league 'He definitely has it', so why did Hamilton say he was useless? Richardson misses Worlds 200m spot a week after arrest Celtic and Rangers learn Champions League opponents Cardile working for Aston Martin after dispute resolved Jota and Silva's boyhood club honours brothers in new kit Brady's Premier League aim and 'group chat' with Reynolds and McElhenney. Video, 00:03:42Brady's Premier League aim and 'group chat' with Reynolds and McElhenney Sarr scores incredible long-range goal as Spurs beat Arsenal. Video, 00:00:27Sarr scores incredible long-range goal as Spurs beat Arsenal 'No way!' - Mullins prevents home run with incredible catch. Video, 00:00:23'No way!' - Mullins prevents home run with incredible catch 'No-one's been better for the game of golf than my father' Video, 00:05:36'No-one's been better for the game of golf than my father' We want to be part of something very special - Itoje. Video, 00:00:41We want to be part of something very special - Itoje Spurs eye Rodrygo to replace Son - Monday's gossip Tottenham are keen on signing Real Madrid and Brazil winger Rodrygo to replace Son Heung-min, while Manchester United are ready to match Newcastle United's bid for Slovenia and RB Leipzig striker Benjamin Sesko. Sheff Wed players want answers 'to avoid strike action' Yamashita holds off Hull to win Women's Open Son makes emotional Spurs farewell - how much will they miss him? Lions centre Aki's baby born in car before first Test 'Total privilege' - Crichton confirmed as Rangers head coach Bayern midfielder Palhinha joins Spurs on loan The stunning football pitches hiding in the Arctic Circle. Video, 00:04:56The stunning football pitches hiding in the Arctic Circle How does British football food rate with European fans? Video, 00:01:39How does British football food rate with European fans? Exploring Britain's most extraordinary cricket grounds. Video, 00:06:49Exploring Britain's most extraordinary cricket grounds Inside the $5bn World Cup stadium in California. Video, 00:02:49Inside the $5bn World Cup stadium in California Hunting for New York City's lost cricket ground. Video, 00:03:15Hunting for New York City's lost cricket ground Becky Zerlentes - the first female boxer to die in fight in US The story of Becky Zerlentes - the first female boxer to die in a fight in the United States. The Welsh Way - Inside the 'Harvard of coaching' How Hampton overcame odds to become England's number one Why 'best-dressed officials' are missing from Wimbledon How Earps went from Â£25 a game to 'Mary, Queen of Stops' The runner who went viral and sparked a campaign for change for women Blood brothers - bonds and betrayal on a rugby pitch Lions player ratings - the incredible, the unlucky & the poor The mountain retreat - upholding an Italian tradition 'A strange kind of glory' as legacy-defining win eludes Lions Cronje, match-fixing and plane crash that left a complex legacy In Pictures: Sporting photos of the week Test Match Special. England v India: India triumph in classic at The Oval. Audio, 42 minutesTest Match Special England v India: India triumph in classic at The Oval Jonathan Agnew presents reaction from Indiaâs thrilling final day victory over England. F1: Chequered Flag. Hungarian Grand Prix Review. Audio, 25 minutesF1: Chequered Flag Hungarian Grand Prix Review Rugby Union Weekly. Wallabies deny Lions series whitewash. Audio, 44 minutesRugby Union Weekly Wallabies deny Lions series whitewash Football Daily. Neil Warnock on football, Yorkshire puddings and chiropody. Audio, 44 minutesFootball Daily Neil Warnock on football, Yorkshire puddings and chiropody Football Daily. Will Alexander Isak leave Newcastle for Liverpool? AudioFootball Daily Will Alexander Isak leave Newcastle for Liverpool? Sport's Strangest Crimes. Hansie Cronje: Fall From Grace, Introducing... Hansie Cronje: Fall From Grace. Audio, 3 minutesSport's Strangest Crimes Hansie Cronje: Fall From Grace Introducing... Hansie Cronje: Fall From Grace Discover the BBC's best sports podcasts More from BBC News Where and how to watch BBC News Premier League European Football Find out more about our BBC Sport app Instagram TikTok Facebook X YouTube Copyright Â© 2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
  },
  {
    "url": "https://www.bbc.co.uk/weather",
    "title": "BBC Weather - Home",
    "content": "Weather forecasts for thousands of locations around the world This video can not be played More than 43,000 homes lose power as Storm Floris brings gusts of up to 82 mph Video shows a plane struggling to land at Leeds airport, while travel disruption is expected to continue on Tuesday. Weather for the week ahead Storm Floris brought strong winds and disruption across parts of the UK on Monday, but how's the weather looking for the rest of the week? Watch: Twin waterspouts merge near Italian coastline. Video, 00:00:32Watch: Twin waterspouts merge near Italian coastline A local tour boat operator paused an excursion so he could film the waterspouts off Italy's east coast. Watch: Australians play in snowy winter wonderland. Video, 00:00:39Watch: Australians play in snowy winter wonderland Parts of New South Wales were blanketed with the heaviest snow in 20 years. Why does it always rain in the school summer holidays? Video, 00:00:27Why does it always rain in the school summer holidays? BBC Weather's Ben Rich explains why it always seem to rain during the summer holidays. BBC Weather in association with MeteoGroup, external All times are British Summer Time (Europe/London, GMT+1) unless otherwise stated."
  },
  {
    "url": "https://www.bbc.co.uk/iplayer",
    "title": "BBC iPlayer - Home",
    "content": "She escaped captivity, but not scrutiny. The truth, trauma and woman behind the headlines Sir David Attenborough narrates the joy and drama of animal parenting in a changing world The wry and witty ‘tec and her team solve more murders washed up in Scotland’s waters It’s the adventure of a lifetime, with one simple question… where the X are they? Make today film day! Enjoy modern hits and must-see classics - a film feast for everyone. Surrender yourself to the dark side. Arresting drama and captivating true crime"
  },
  {
    "url": "https://www.bbc.co.uk/sounds",
    "title": "BBC Sounds - Music. Radio. Podcasts",
    "content": "Use BBC.com or the new BBC App to listen to BBC podcasts, Radio 4 and the World Service outside the UK. Find out how to listen to other BBC stations Save shows to listen to later, subscribe to your favourites and get fresh recommendations everyday. Sign in or Register LIVE,·20:00 - 22:00 LIVE,·20:00 - 22:00 LIVE,·21:00 - 23:00 LIVE,·21:00 - 23:00 LIVE,·21:00 - 22:00 LIVE,·19:30 - 22:00 LIVE,·21:00 - 22:00 LIVE,·21:00 - 21:45 LIVE,·21:00 - 22:00 LIVE,·20:00 - 22:00 LIVE,·12:37 - 00:00 LIVE,·18:00 - 00:00 LIVE,·18:00 - 00:00 LIVE,·21:00 - 23:00 LIVE,·21:00 - 22:00 LIVE,·21:00 - 21:06 LIVE,·21:00 - 21:06 LIVE,·20:00 - 22:00 LIVE,·21:00 - 22:25 LIVE,·20:04 - 22:00 LIVE,·20:04 - 22:00 LIVE,·19:00 - 22:00 LIVE,·19:00 - 23:00 LIVE,·19:00 - 23:00 LIVE,·21:00 - 06:00 A cricket scandal no one saw coming Lifelong friends Lily Allen & Miquita Oliver discuss the world around them twice a week. Hansie Cronje was banned from cricket for life due to his role in a match-fixing scandal. Lionesses Ella Toone and Alessia Russo invite Vick Hope into their iconic friendship. Fights, drugs, scandals and skinny jeans — Kate Nash spills the secrets of 00s indie. Join Annie Macmanus and Nick Grimshaw as they explore the week in music. The home of Marianna Spring's investigations for BBC Radio 4. The world's greatest classical music festival - stunning concerts and collaborations. Huge sets and more from Ibiza. Radio 1's biggest anthems... A unique journey to dreamland, mixing instrumental music and Radio 4's Shipping Forecast. Highlights of the unmissable music and performances at the BBC Proms 2025. Back to back 90s classics from every genre; a non-stop celebration of the very best music 3 mins 55 mins 1 min 13 mins 2 mins 4 mins Mark Wood I Lie To You? 63 mins Hansie Cronje: Fall From Grace 1. Bigger than Becks 26 mins 2025 Soul Revolution 149 mins Harry Hill, comedian 58 mins The Greatest Thing You’ll Ever Learn… with Zawe Ashton 39 mins Heroism 1. The Classical Hero 28 mins"
  },
  {
    "url": "https://www.bbc.co.uk/usingthebbc/terms",
    "title": "A few rules for us and you",
    "content": "We want everyone to enjoy the BBC. But there are a few rules to stick to. Take a look through our explainers to see what you (and we) can and can't do. Our explainers cover questions we often get asked. Can't find what you're after? Take a look at the full version of the Terms of Use. The page will automatically reload. You may need to reload again if the build takes longer than expected. Hides preview environment warning banner on preview pages. Select a theme and theme mode and click \"Load theme\" to load in your theme combination."
  },
  {
    "url": "https://www.bbc.co.uk/aboutthebbc",
    "title": "Learn more about what we do",
    "content": "Everything you need to know about the workings of the BBC We’re impartial and independent, and every day we create distinctive, world-class programmes and content which inform, educate and entertain millions of people in the UK and around the world. We do this across: Established by a Royal Charter, the BBC is principally funded through the licence fee paid by UK households. Our role is to fulfil our mission and promote our Public Purposes. Our commercial operations including BBC Studios, the BBC’s award-winning production company and world-class distributor, provide additional revenue for investment in new programming and services for UK audiences. The BBC’s Board ensures that we deliver our mission and public purposes which are set out in the Charter. The Executive Committee is responsible for day-to-day management. We are regulated by Ofcom. The page will automatically reload. You may need to reload again if the build takes longer than expected. Hides preview environment warning banner on preview pages. Select a theme and theme mode and click \"Load theme\" to load in your theme combination."
  },
  {
    "url": "https://www.bbc.co.uk/usingthebbc/privacy",
    "title": "Keeping your info safe and sound",
    "content": "We take your privacy very seriously. And so should you. So have a look around to see what we’re doing with your personal information and how we’re keeping it secure. Read our privacy promise. The page will automatically reload. You may need to reload again if the build takes longer than expected. Hides preview environment warning banner on preview pages. Select a theme and theme mode and click \"Load theme\" to load in your theme combination."
  },
  {
    "url": "https://www.bbc.co.uk/usingthebbc/cookies",
    "title": "Take control of your cookies",
    "content": "Cookies help us remember you and show you more things we think youâll like. Have a browse to see what we use them for and how you can change your settings to suit you. The page will automatically reload. You may need to reload again if the build takes longer than expected. Hides preview environment warning banner on preview pages. Select a theme and theme mode and click \"Load theme\" to load in your theme combination."
  },
  {
    "url": "https://www.bbc.co.uk/accessibility",
    "title": "Accessibility - BBC",
    "content": "All audiences are important to the BBC. This page provides links to resources and documents to help users access the BBC's content and to find out about the BBC approach to the development of rich accessible products and services."
  },
  {
    "url": "https://www.bbc.co.uk/guidance",
    "title": "Parental Controls - BBC iPlayer",
    "content": "The Parental Guidance Lock helps you control what people in your household can and can't watch on iPlayer or listen to in BBC Sounds. Turn it on and we'll ask you to create a PIN. Then, whenever anyone tries to watch or listen to anything with a BBC Guidance label, they'll have to enter the PIN. What are Guidance labels? Bear in mind, this will only turn it on in the internet browser on this device. So you may want to also turn on the Parental Guidance Lock separately on your other devices."
  },
  {
    "url": "https://www.bbc.co.uk/contact",
    "title": "Contact Home | Contact the BBC",
    "content": "Menu Choose from these areas for information or help about our programmes and services. Information about issues, programmes or services which we're often asked about. Get support to resolve problems with BBC iPlayer, along with how to guides, FAQs and updates on known issues. Get support to resolve problems with BBC Sounds, along with how to guides, FAQs and updates on known issues. Details of organisations offering support if you're affected by issues in our coverage The BBC helped raise over £105 million for charities in 2020/21.  Find out more about how you can help. Latest transmitter status and support for problems or interference receiving BBC TV and Radio"
  },
  {
    "url": "https://www.bbc.co.uk/bbcnewsletter",
    "title": "BBC emails for you",
    "content": "Every week we send a round-up of the latest shows, services and experiences from across the BBC in our email updates. Plus, if we spot anything extra we think you'll love, we'll let you know about that too. Simply sign up today and don't worry â you can unsubscribe from these emails at any time. Copyright Â© 2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking."
  },
  {
    "url": "http://legacy.python.org/Flask",
    "title": "Page Not Found",
    "content": "Add an event to this calendar. Times are shown in UTC/GMT. Add an event to this calendar. The URL you requested was not found on this server. Try our home page, or our search engines - or use one of\nthe other links on the left hand navigation."
  },
  {
    "url": "https://towardsdatascience.com/",
    "title": "Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Three pillars that shaped my first year in data science management - prioritization, empowerment, and recognition A simpler path to coding real-time web applications. A waterfall chart can be a powerful tool for conveying information. But it has some… Explore the handoff and agents-as-tools patterns, their use cases, and how to customize them using… A common misconception about the working state of code in data, AI or software engineering… POS tagging, dependency parser and named entity recognition. Generative Molecular Design (Part 1): common molecular representations in data science. Mariya Mansurova explains how hands-on learning, agentic AI, and engineering habits shape her writing and… Models don’t just fail with noise; they fail in silence, by narrowing their attention to… Image segmentation is a popular task in computer vision, with the goal of partitioning an… Learn how to LLMs are benchmarked, and try out the newly released ARC AGI 3 Solving the Heat Equation using DeepXDE. A hands-on journey exploring fine-tuning techniques that unlock the power of small vision models. A practical exploration focusing on performance and speed Data storytelling can enlighten—but it can also deceive. When persuasive narratives meet biased framing, cherry-picked… Part 1: Data, Sales Pitches, Bugs, and Breakthroughs How experimentation is more powerful than knowing counterfactuals Why are there so many hotels named after cities they are not in? Follow along… A hands-on guide to building and validating LLM evaluators Let’s observe the matter on the atomic level This week, we highlight three articles that focus on emerging topics and techniques around large… One of our guiding principles as a publication is that authors’ work remains theirs. This… Are LLMs good or bad for our mental health? It’s more complicated than that. In today’s fast-paced, distraction-heavy world, data literacy isn’t just about understanding charts or analyzing numbers—it’s… Why you should read this article Most data scientists whip up a Jupyter Notebook, play… “Will that break a query folding?” “Does your query fold?”… Maybe someone asked you those… A guide to ideating, validating, and prioritizing your AI use cases Learn how to create AI Agents using the OpenAI Agents SDK to automate Jira ticket… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://contributor.insightmediagroup.io/",
    "title": "Contribute to Towards Data Science! | TDS Contributor Portal",
    "content": "Thank you for your interest in Towards Data Science! If you never published with us before, please send your article via the form below. If you published with us previously but don’t yet have your contributor account on our new platform, please fill out the form below, too. You can leave the “Submit your article for review” section blank if you don’t have any articles to share. Please take your time to review both our privacy policy and our author terms and conditions of use. Make sure you read all points well and that you understand them. By clicking the checkbox below or by submitting content to us, you agree to be bound by the TDS Author Terms and Conditions of Use. Once you have completed our registration form and are approved as a TDS contributor, you will be able to log in here. You can then submit your article directly from your account, and won’t need to fill out this form again. TDS Contributor Portal"
  },
  {
    "url": "https://towardsdatascience.com/latest/",
    "title": "Latest stories published on Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Three pillars that shaped my first year in data science management - prioritization, empowerment, and recognition A simpler path to coding real-time web applications. A waterfall chart can be a powerful tool for conveying information. But it has some… Explore the handoff and agents-as-tools patterns, their use cases, and how to customize them using… A common misconception about the working state of code in data, AI or software engineering… POS tagging, dependency parser and named entity recognition. Generative Molecular Design (Part 1): common molecular representations in data science. Mariya Mansurova explains how hands-on learning, agentic AI, and engineering habits shape her writing and… Models don’t just fail with noise; they fail in silence, by narrowing their attention to… Image segmentation is a popular task in computer vision, with the goal of partitioning an… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/tag/editors-pick/",
    "title": "Editors Pick | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Explore the handoff and agents-as-tools patterns, their use cases, and how to customize them using… A common misconception about the working state of code in data, AI or software engineering… Solving the Heat Equation using DeepXDE. A hands-on journey exploring fine-tuning techniques that unlock the power of small vision models. A practical exploration focusing on performance and speed Data storytelling can enlighten—but it can also deceive. When persuasive narratives meet biased framing, cherry-picked… Part 1: Data, Sales Pitches, Bugs, and Breakthroughs How experimentation is more powerful than knowing counterfactuals Why are there so many hotels named after cities they are not in? Follow along… A hands-on guide to building and validating LLM evaluators Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/tag/deep-dives/",
    "title": "Deep Dives | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Explore the handoff and agents-as-tools patterns, their use cases, and how to customize them using… Are LLMs good or bad for our mental health? It’s more complicated than that. In today’s fast-paced, distraction-heavy world, data literacy isn’t just about understanding charts or analyzing numbers—it’s… Why you should read this article Most data scientists whip up a Jupyter Notebook, play… “Will that break a query folding?” “Does your query fold?”… Maybe someone asked you those… A guide to ideating, validating, and prioritizing your AI use cases Learn how to create AI Agents using the OpenAI Agents SDK to automate Jira ticket… Building a tool to interactively visualize the forward pass of any Pytorch model from within… It’s here already from Nvidia and it’s called cuNumeric. Can large language models learn to reason abstractly from just a few examples? In this… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://newsletter.towardsdatascience.com/subscription-to-the-newsletter",
    "title": "Subscribe to The Variable Newsletter",
    "content": "The flagship newsletter from Towards Data Science The Variable is our weekly newsletter. Along with exclusive content, you can expect to receive the best of Towards Data Science: from hands-on tutorials and cutting-edge research to the latest on data science and machine learning tools. Our commitment is to provide actionable, high-quality insights to keep subscribers updated on the latest trends and breakthroughs in the world of data science, data analytics, data engineering, machine learning, and artificial intelligence. We look forward to seeing you in your inbox each Thursday! By providing your email address and submitting this form, you consent to receive newsletters and other communication from Towards Data Science. You can unsubscribe at any time by clicking the \"unsubscribe\" link in the footer of our emails or by contacting us at privacy@insightmediagroup.io. For more information about how we handle your data, please refer to our Privacy Policy. Copyright © 2025 Insight Media Group, LLC. All Rights Reserved."
  },
  {
    "url": "https://towardsdatascience.com/questions-96667b06af5/",
    "title": "Write for Towards Data Science | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Share your learnings, projects, and explorations with curious minds. Quick Links: We are looking for writers to propose up-to-date content focused on data science, machine learning, artificial intelligence and programming. If you love to write about these topics, read on! Reach a broader audience with your articles. We are one of the most popular data science sites in the world. TDS started as a publication on Medium, amassing more than 700k followers and becoming the most-read publication on the site. Now on a self-hosted platform, TDS is the leading destination in the data science community. Here are a few things we do to ensure your articles reach the largest audience possible: Earn money with the Towards Data Science Author Payment Program. When publishing in TDS, our authors can decide to apply to our Payment Program, which enables them to earn from their work. You can read more about our Author Payment Program here. Before submitting your article, there are a few essential things you need to know. Make sure you read each point well, and that you understand them, as by submitting an article to TDS, you are agreeing to comply with all of them. Please take a few minutes to familiarize yourself with our Author Terms and Conditions of Use — they govern the relationship between contributors and TDS. Any article you share with us must be entirely your own original work; you can’t take other writers’ words and present them as your own, and we also don’t allow AI-generated text, even when you’re the one who prompted its creation. How to get your article ready for publication! We aim to strike a balance between innovating, informing and philosophizing. We want to hear from you! If you are not a professional writer, consider the following points when preparing your article. We want to publish high quality, professional articles that people want to read. 1. Is your story a story that needs to be told? Before you start writing, ask yourself: is this story a story that needs to be told? If you have read many articles addressing the same issue or explaining the same concept, think twice before writing another one. If you have a radical, new take on an old chestnut, we want to hear from you… but, we need you to persuade us that your article is something special that distinguishes itself from the pack and speaks to our audience. Conversely, if your article addresses an underserved area or presents a new idea or method, that’s just what we are after! 2. What is your message? Let us know what your main message is, right from the start. Give your piece a snappy introduction that tells us: Once you’ve got that out of the way, you can be as conversational as you like, but keep calling back to the central message and give us a solid conclusion. Remember though, Towards Data Science is not your personal blog, keep it sharp and on-topic! 3. On the internet, nobody knows you are a dog You’ve got a new idea or a new way of doing things, you want to tell the community and start a discussion. Fantastic, that’s what we want too, but we’re not going to take for granted that you know what you are talking about or that we should uncritically believe what you say… you’ve got to persuade us (your audience) that: You can do this by explaining the background, showing examples, providing an experiment or just laying out how data you have extracted from various sources allowed you to synthesise this new idea. Are there arguments that counter your opinion or your findings? Explain why that interpretation conflicts with your idea and why your idea comes out on top. 4. Do you have a short title with an insightful subtitle? If you scroll up to the top of this page, you will see an example of a title and subtitle. Your post needs to have a short title and a longer subtitle that tell readers what your article is about or why they should read it. Your header is useful for attracting potential readers and making your intentions clear. To remain consistent and give readers the best experience possible, we do not allow titles or subtitles written in all-caps. We also ask that you avoid profanity in both your title and subtitle. When your subtitle is directly under the title and formatted correctly, it will show up in some post previews, which helps with your click-through rate. 5. What makes your post valuable to readers? A successful post has a clearly defined and well-scoped goal, and follows through on its promise. If your title tells us you’re going to unpack a complex algorithm, show the benefits of a new library, or walk us through your own data pipeline, make sure the rest of the post delivers. Here are a few pointers to help you plan and execute a well-crafted post: If your article is full of neutral, generic verbs (like to be, have, go, become, make, etc.), try to mix in more precise action verbs. When it makes sense, use specific, lively descriptors instead of dull ones (for example, you could replace “easy” with “frictionless,” “accessible,” or “straightforward,” depending on the context). There are few things editors appreciate more than a clean first draft, so don’t forget to proofread your post a couple of times before sharing it with TDS: look for spelling, punctuation, and grammar issues, and do your best to fix them. What we hope to offer to our readers are clear explanations, a smooth overall flow — pay attention to those transitions! — and a strong sense of what you’re aiming to achieve with your post. If you’d like to expand your toolkit beyond the basics, the Internet is full of great writing resources. Here are a few ideas to help you get started: For example, if you’re talking about a data pipeline you built, text can only take you so far; adding a diagram or flowchart could make things even clearer. If you’re covering an algorithm or another abstract concept, make it more concrete with graphs, drawings, or gifs to complement your verbal descriptions. (If you’re using images someone else created, you’ll need to source and cite them carefully — read our image guidelines below for more details.) A strong visual component will hook your readers’ attention and guide them along as they read your post. It will also help you develop a personal style as an author, grow your following, and draw more attention on social media. 6. Are your code and equations well displayed? TDS readers love to tinker with the ideas and workflows you share with them, which means that including a code implementation and relevant equation(s) in your post is often a great idea. To make code snippets more accessible and usable, avoid screenshots. Use WordPress’s code blocks & inline code To share math equations with your readers, Embed.fun is a great option. Alternatively, you can use Unicode characters and upload an image of the resulting equation. When you include code or an equation within your article, be sure to explain it and add some context around it so readers of all levels can follow along. To learn more about using these embeds and others in your post, check out this resource. 7. Check your facts Whenever you provide a fact, if it’s not self-evident, let us know where you learned it. Tell us who your sources are and where your data originated. If we want to have a conversation we all need to be on the same page. Maybe something you say will spark a discussion, but if we want to be sure we are not at cross purposes, we need to go back to the original and read for ourselves in case we are missing a vital piece of the puzzle that makes everything you say make sense. 8. Is your conclusion to the point and not promotional? Please make sure that you include a conclusion at the end of your article. It’s a great way to help your readers review and remember the essential points or ideas you’ve covered. You can also use your conclusion to link an original post or a few relevant articles. Adding an extra link to your author profile or to a social media account is fine, but please avoid call-to-action (CTA) buttons. For your references, please respect this format: [X] N. Name, Title (Year), Source For example, your first reference should look like this: [1] A. Pesah, A. Wehenkel and G. Louppe, Recurrent Machines for Likelihood-Free Inference (2018), NeurIPS 2018 Workshop on Meta-Learning 9. Are your tags precise enough? The more specific your tags, the easier it is for readers to find your article and for us to classify and recommend your post to the relevant audience. We may change one or two tags before publication. We would do this only to keep our different sections relevant to our readers. For instance, we would want to avoid tagging a post on linear regression as “Artificial Intelligence”. 10. Do you have an amazing image? A great image attracts and excites readers. That’s why all the best newspapers always display incredible pictures. This is what you can do to add a fantastic featured image to your post: If you decide to purchase a license for an image to be used in your article, please note that we only allow the use of images under a license that: (i) does not expire; and (ii) that can be used for commercial purposes on the TDS Publication. You are responsible for ensuring you comply with the license terms of use. You must also include a caption below the image, as follows, or as otherwise required by the license provider: “Image via [license provider’s name] under license to [your name].” Finally, please email us a copy of a receipt or other evidence of the purchased license, along with the corresponding license terms of use. If you’ve chosen to create images for your article using an AI tool (like DALL·E 2, DALL·E, Midjourney, or Stable Diffusion, among others), it’s your responsibility to ensure that you’ve read, understood, and followed the tool’s terms. Any image you use on TDS must be licensed for commercial use, including AI-generated images. Not all AI tools permit images to be used for commercial purposes and some require payment to permit you to use the image. The images you generate with AI tools cannot violate the copyright of other creators. If the AI generated image resembles or is identical to an existing copyrighted image or fictional character (like Harry Potter, Fred Flinstone etc.), you are not permitted to use it on TDS. Use your best judgment and avoid AI-generated images that copy or closely emulate another work. If in doubt, use an image search tool — like Google Lens, TinEye, or others — to check whether your images are too similar to an existing work. We may also ask that you provide details of the text prompts you used in the AI tool to confirm you did not use the names of copyrighted works. Your text prompts cannot use the names of real people, nor can your images be used if they feature a real person (whether a celebrity, politician, or anyone else). Please remember to cite the source of your images even if you aren’t legally obligated to do so. If you created an image yourself, you can add (Image by author) in the caption. Whichever way you decide to go, your image source should look like this: Your image should both have the source and the link to that source. If you created an image yourself, you can add “Image by author”. If you’ve created an image that was lightly inspired by an existing image, please add the caption “Image by Author, inspired by source[include the link].” If you’ve edited an existing image, please make sure you have the right to use and edit that image and include the caption “Image by source[include the link], edited with permission by the author.” Danger zone: Do not use images (including logos and gifs) you found online without explicit permission from the owner. Adding the source to an image doesn’t grant you the right to use it. 11. Where did you get your data? The Towards Data Science team is committed to the creation of a respectful community of data science authors, researchers, and readers. For our authors, this means respecting the work of others, taking care to honor copyrights associated with images, published material, and data. Please always ensure that you have the right to collect, analyze, and present the data you’re using in your article. There are plenty of great sources of data that are freely available. Try searching university databases, government open data sites, and international institutions, such as the UCI Irvine Machine Learning Repository, U.S. Government, and World Bank Open Data. And don’t forget about sites that hold specific data relating to fields like physics, astrophysics, earth science, sports, and politics like CERN, NASA, and FiveThirtyEight. TDS is a commercial publication. Before submitting your article to us, please verify your dataset is licensed for commercial use, or obtain written permission to use it. Please note that not all the datasets on the websites we’ve listed are fine to use. No matter where you obtain your data, we advise you to double-check that the dataset permits commercial use. If you aren’t confident you have the right to use it for commercial purposes, consider contacting the owner. Many authors receive a quick, positive response to a well-constructed email. Explain how you intend to use the data, share your article or idea, and provide a link to TDS. When you receive permission, please forward a copy to us at [email protected]. This is especially important if you plan to use web scraping to create your own dataset. If the website does not explicitly allow data scraping for commercial purposes, we strongly recommend that you contact the website owner for permission. Without explicit permission, we won’t be able to publish your work, so please forward us a copy via email. And sometimes, simple works best! If you just want a dataset to explain how an algorithm works, you can always create an artificial or simulated dataset. Here’s a quick tutorial, and an article that uses a simulated dataset you might find helpful. Please remember to add a link to the site where the dataset is stored, and credit the owner/creator in your article. Ideally, this is done on first mention of the dataset, or in a resource list at the end of the article. Please carefully follow any instructions relating to attribution that you find on the site. If you have created your own artificial or simulated dataset, it is important to mention that too. We know interpreting a license can be challenging. It is your responsibility to be certain that you can present your data and findings in an article published with TDS, but if you’re stuck, please reach out to our editorial team for assistance. We would rather work with you in the early stages of your project than to have to decline your completed article due to a dataset license issue. 12. Is your content original? While we do accept content that has already been published (for example, on your personal blog or website), our focus is on promoting and sharing new and original content with our readers. That means that by publishing your article in TDS first (or exclusively), you have a greater chance to be featured on our publication, our social media channels, and in our newsletter. We love original content because it’s something that our audience hasn’t seen before. We want to give as much exposure to new material as possible and keep TDS fresh and up-to-date. Originality also means that you (and your coauthors, if any) are the sole creator of each and every element in your post. Any time you rely on someone else’s words, you have to cite and quote them properly, otherwise we consider it an instance of plagiarism. This applies to human authors, of course, but also to AI-generated text. We generally don’t allow any language created by tools like ChatGPT on TDS; if your article discusses these tools and you wish to include examples of text you generated, please keep them to a minimum, cite their source and the prompt you used, and make it very clear (for example, by using block quotes) where the AI-generated portions begin and end. 13. Did you get any feedback before submitting your post? Get into the habit of always asking a friend for feedback before publishing your article. Having worked so hard on that article, you wouldn’t want to let a silly mistake push readers away. 14. Has your Author profile been completed correctly? Please include your real name, a photo, and a bio. We don’t publish posts from anonymous writers — it’s easier to build trust with readers when they associate your words with an actual person. Use your profile to introduce yourself, your expertise, your and achievements — optimizing it will help you develop a meaningful relationship with your audience beyond a single post. If you are a company and would like to publish with us, please note that we almost exclusively publish articles submitted directly from the author. 15. Are you getting better? Take a minute to reflect on the work you have been doing so far, and the current article you wish to publish. What value are you bringing, and to whom? In which ways are this article better or worse than the ones you previously published? Have a lot to say? Good. We love to dive deep into complex topics, and so do our readers. Here’s how you can publish longform posts, columns, and online books on TDS. Longform posts We love long reads! If your article’s reading time is shorter than 25 minutes, we recommend that you don’t break it into multiple pieces — keep it as-is. A single post makes it easier for readers to search and find all the information they need, and less likely that they’ll miss an important part of your argument. To create a smoother reading experience, you can add a table of contents to orient your audience around your post. Adding high-quality images and lots of white space is always a good idea, too — a long text doesn’t have to be a wall of text. We regularly add the most engaging and thoughtful longform posts to our Deep Dives page. Columns If your post’s reading time exceeds 25 minutes, or if you plan to focus on the same topic over multiple articles and a longer stretch of time, you can create your own TDS column. All it takes are three steps: You can create a TDS column and invite multiple authors to contribute. Just let your colleague(s) know which tag you decided to use so that they can add the same one to their articles. Here are some examples from our team. Online Books A column is a great format to use if you have an open-ended topic that you plan to write about for a while. If, on the other hand, your idea has a finite, defined scope and a clear sense of progression from one post to the next, you may want to create a series of articles that feels more like an online book. Here is the format we recommend using. Keep the reading time of each article — or “chapter” — between 12 to 25 minutes, and aim for a series that has at least 5 articles (but probably not more than, say, 16). You can add links to previous or subsequent items from within each article — for example, in the introduction and/or conclusion. To publish your online book, you can submit all your articles to our editorial team in one go, or one by one as you finish working on each. We’ll review them and publish them as they come along. Let us know your post is part of a planned online book project. Please ensure that each article or online book chapter follows the same guidelines and rules as any other post that TDS publishes. If you ever decide to sell or exclusively license your book to a third party publisher, you will have to make sure you have their consent to continue to publish the book with TDS. If you do not have such consent, it is your responsibility to remove your content from the TDS publication. To become a writer, please send your article using our form. We aim to respond to authors as quickly as possible and to let them know whether or not we’ve accepted their articles. On rare occasions, the volume of submissions we receive makes it difficult to respond to everyone; as a general rule, if you haven’t heard from us within a week of submitting your post, it’s safe to assume we won’t move forward with publishing it. Contribute to Towards Data Science ✨ If you’re having an issue with our online form, please let us know via email ([email protected]) so we can help you complete the process. Please do not email us an article that you have already sent via our form. Our FAQ can be found here. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Three pillars that shaped my first year in data science management - prioritization, empowerment, and recognition A simpler path to coding real-time web applications. A waterfall chart can be a powerful tool for conveying information. But it has some… Explore the handoff and agents-as-tools patterns, their use cases, and how to customize them using… A common misconception about the working state of code in data, AI or software engineering… POS tagging, dependency parser and named entity recognition. Generative Molecular Design (Part 1): common molecular representations in data science. Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca",
    "title": "Towards Data Science | LinkedIn",
    "content": "View all 311 employees Towards Data Science is a community-powered publication that showcases work in data science, machine learning and artificial intelligence. Every day newcomers, seasoned researchers and industry practitioners publish tutorials, research notes and real-world case studies that help the field move forward.\n\nContributors receive editorial guidance, best-in-class publishing tools and prominent placement on our site, newsletter and social feeds. Accepted articles are eligible for the TDS Author Payment Program, which compensates writers based on reader engagement. If you have an idea worth sharing, submit your draft, join the conversation and connect with a global audience of data professionals.\n\nInsight Partners is an investor in Towards Data Science. External link for Towards Data Science 548 Market St San Francisco, California 94104, US 642,747 followers Build your own AI assistant! Iqbal Rahmadhan walks you through creating a weather agent using the OpenAI Agents SDK, Streamlit, and Python. 642,747 followers Before you dive into machine learning, read this. In this candid article, Pascal Janetzky reflects on the messy truths of ML. From buggy pipelines to overhyped pitches and unexpected breakthroughs. 642,747 followers Beyond just generating a chart: the hidden pitfalls of AI in BI. This article, thanks to DataCamp, exposes why relying solely on LLMs can lead to misinterpretations from missing metadata, uncaptured external data, and arbitrary compression. Learn how to prevent narrative bias. 642,747 followers Master tool-use agents! Iqbal Rahmadhan explains tool_use_behavior in OpenAI Agents SDK (run_llm_again vs stop_on_first_tool) and how to leverage the Trace dashboard for effective debugging. 642,747 followers Connect your Streamlit app to remote AI tools. Destin Gong's guide details how to build an MCP client that accesses diverse functionalities from external servers (e.g., codebase summarization, open-source model recommendations). 642,747 followers Gen Z, your biggest career advantage is your youth! Marina Tosic highlights how more time to fail, agility, and adaptability are your superpowers in the fast-evolving tech landscape. Unsolicited advice for thriving in Data Science. 642,747 followers Visualize your topics like never before. 📊 Alex Davis's article explores BERTopic's powerful visualizations (2D maps, term importance, heatmaps) and how to leverage hierarchical topic modeling to zoom in/out on themes. Customize your insights with ease. 642,747 followers Mariya Mansurova shares a comprehensive guide to MCP, the protocol standardizing AI agent-tool communication. Learn how to build your own server, access external capabilities, and truly live by the \"Don't Repeat Yourself\" principle. 642,747 followers How do you truly compare complex entities like wines, cities, or businesses?? Andrea D’Agostino's article, with a hands-on Python example, demonstrates how POSETs build dominance matrices and Hasse diagrams to bring coherence to scoring systems. 642,747 followers Migrating between ML frameworks can be a hassle, wasting valuable time in MLOps. MLArena provides a hassle-free, algorithm-agnostic solution for streamlined model experimentation and deployment. Say goodbye to boilerplate code, enjoy smart automation while retaining ample customization tools for expert tweaks.  \n\nBy Mena Ning Wang, PhD IT Services and IT Consulting San Francisco, California Software Development New delhi, Haryana Software Development Palo Alto, California E-Learning Providers Gurgaon, Haryana Software Development Vermont, Victoria Software Development Business Consulting and Services Milwaukee, WI Technology, Information and Internet Software Development Professional Training and Coaching Washington, District of Columbia Agree & Join LinkedIn By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. or By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now"
  },
  {
    "url": "https://towardsdatascience.com/category/data-science/data-visualization/",
    "title": "Data Visualization | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. In today’s fast-paced, distraction-heavy world, data literacy isn’t just about understanding charts or analyzing numbers—it’s… Visualizing historical tornado trends Using the Elastic Beanstalk service Writing, testing and using them. Is there a way to use the out-of-the-box features of Power BI to be IBCS… Explore the shift from static reports to intelligent apps with our first ebook. How I built an AI-powered prototype to turn images into insights A guide to building a front-end data application. A useful tool in your quiver Use LazyPredict and PyCaret to skip the grunt work and jump straight to performance. Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/mbostock/",
    "title": "Mike Bostock, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. With the web opening new frontiers in collaboration, the web’s native language of JavaScript is… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://pudding.cool/2019/02/gyllenhaal/",
    "title": "The Gyllenhaal Experiment",
    "content": "The Pudding explains ideas debated in\n\t\t\t\tculture with visual essays. We pour our â¤ï¸ into these stories, but they take time and money. For just $1/month, you\n\t\t\t\tcan help support us. Join our\n\t\t\t\tgrowing community of data-driven enthusiasts. Help\n\t\t\t\t\tfund us About By\n\t\t\t\t\n\t\t\t\t\tRussell Samora\n\t\t\t\t & \n\t\t\t\t\tMatt Daniels Colin Morris recently explored the\n\t\t\t\t\tdifficulties of spelling by identifying Reddit comments with (sp?) next to words. Itâs no\n\t\t\t\t\tsurprise that Jake Gyllenhaal, along with other celebrities, were atop the list. Letâs take this idea one step\n\t\t\t\t\tfurther. This chart shows over 600k attempts of people trying to search for Britney Spears. Here are\n\t\t\t\t\tthe 8 most common permutations of her first name, Britney. People nail the\n\t\t\t\t\tfirst three letters—then things get interesting. Below weâll put you through a series of tests. You can skip ahead if you just\n\t\t\t\t\twant to see the results, but be warned, once you do there is no turning back! The charts below show the top 8 most common permutations for each celebrity in our test from all\n\t\t\t\t\t\tthe reader submissions. Want more data-driven experiments? Check out or Birthday Paradox explainer, that uses you as part of the data. Britney Spears search data is from Google\n\t\t\t\t\t\tsearches circa 2011. A slightly modified scale was used to represent the flow diagrams to make all\n\t\t\t\t\tbranchesâ text legible, so the sizes donât perfectly represent the proportion of responses. We chose to only\n\t\t\t\t\tshow the top 8 permutations of each name. Phonetic data is based on the International Phonetic Alphabet via\n\t\t\t\t\tWikipedia. You can download the data yourself here. Get in touch with Russell Samora or Matt Daniels, or email us at sup@pudding.cool."
  },
  {
    "url": "https://ciechanow.ski/internal-combustion-engine/",
    "title": "Internal Combustion Engine â Bartosz Ciechanowski",
    "content": "The invention of the internal combustion engine in the 19th century has revolutionized transportation over land, water, and air. Despite their omnipresence in modern day, the operation of an engine may be cryptic. Over the course of this article I’d like to explain the functionality of all the basic engine parts shown in the demonstration below. You can drag it around to see it from other angles: It’s hard to talk about a mechanical device without visualizing its motion, so many demonstrations in this blog post are animated. By default all animations are enabled, but if you find them distracting, or if you want to save power, you can globally pause them.disabled, but if you’d prefer to have things moving as you read you can globally unpause them. An engine like this may seem complicated, but we will build it up from first principles. In fact, we’ll start with a significantly simpler way of generating a rotational motion. Let’s look at a simple crank. It consists of a handle, a crank arm, and a shaft. When a force is applied to the handle the shaft rotates which we can observe by looking at the attached disk: The force applied at a distance from the shaft generates torque. The harder we push on the handle, the bigger the torque on the shaft. This cranking mechanism is precisely what converts linear force into torque in a manual coffee grinder or a bicycle. It’s one thing to power something using our own muscles, but the entire point of building an engine is to avoid manual labor and have the device exert the effort instead. To do that we need to find a reliable source of a strong force that is easy to direct. Thankfully, such device was invented hundreds of years ago â a cannon does exactly what we need. In the demonstration below you can observe how a cannon ball is fired from a cannon. The diagonal lines indicate a cross section view â it lets us see what’s going on inside an otherwise obscured region: As the gun powder is set on fire it quickly produces a huge amount of gases, which push the cannon ball down the barrel. Since the ball snugly fits inside it can only go in one direction. While reliable and easy to direct, a cannon ball won’t be very effective at pushing the crank: We’ve only been able to do a partial turn of the shaft and the cannon ball is long gone. However, with a few modifications we can harness the pushing power of the explosion in a significantly better way. Firstly, we’ll replace a cannonball with a piston that has a cylindrical shape and a hole drilled in it. We’ll then use a pin to attach to it a rod that can swing freely on a crankshaft: As the name implies, the crankshaft consists of both the rotating shaft and the crank on which a force is applied. By putting this assembly inside a simplified cannon shell, a cylinder, we’ve managed to solve the problem of the escaping cannon ball, as the piston is limited in its downward movement and will return up as the crankshaft keeps turning: Notice that the piston has now a minimum and a maximum position it can reach within the cylinder. A single movement over that length in either up or down direction is called a stroke. If we now trigger the explosion, the combustion gases will push the piston down, which turns the crankshaft: It’s still not a very exciting machine as it only does useful turning work once. To make it more practical we need to keep repeating the cycle of explosions â we have to add in new fuel, trigger a combustion process, and remove the exhaust gases, over and over again. Solid fuels like black powder are not very practical for an automated machine. It’s much easier to deal with fuels in fluid forms â their intake can be controlled by various valves. We’ll modify the cylinder we’ve built so far by adding new openings at the top of the combustion chamber: It may be hard to see how the various openings are laid out, so let’s take a look at the cross section view: Through the first large curved opening we’ll provide a mixture of gasoline and air and through the second one we’ll remove the exhaust gases. Those two openings will be guarded by the intake valve and the exhaust valve. Finally, to light the mixture, we’ll use an electric spark generated by an exposed ends of a wire. Let’s see how all the pieces fit together: We’re now ready to use this machine to do useful work. At first we’ll open the intake valve while the piston is moving down letting the air with fuel come in which I’ve symbolized using the yellow color. This is the intake stroke: Once the piston reaches its lowest position the intake valve closes, and the piston starts to move back which compresses the mixture of air and fuel which increases the thermal efficiency of the combustion. This is the compression stroke: Voltage runs through the open ends of the wire, generating a spark which ignites the air-fuel mixture. The expanding gases created by the combustion push the piston down, creating torque on the crankshaft. This is the power stroke: Note that the flame propagation inside a cylinder is quite complex, and what you see here is a simplified visualization. The cylinder is now filled with the exhaust gases which we can vent out through another hole by opening the exhaust valve. This is the exhaust stroke: We’re now back to where we started and the cycle is complete. Let’s look at those four steps together: Since the piston moves down twice and up twice, it does a total of four strokes and the engine we’ve built is known as a four-stroke engine. Notice that it takes two revolutions of the crankshaft for the piston to do one full cycle of the work as it goes through the four phases: intake, compression, power, and exhaust. While functional, the engine weâve built is more of a toy example that doesn’t show a lot of the engineering ingenuity behind many components of real internal combustion engines. Let’s build on the principles we’ve devised so far by constructing a more realistic machine â an engine that one could find in a car. Let’s start with the biggest and heaviest part of an engine â the engine block. It forms the main body and mounting structure for other parts: Notice that this block contains four large cylindrical openings that define the four cylinders. Recall that a piston exerts a pushing force on the crankshaft only during the power stroke, so only for about a quarter of time. This uneven action creates a lot of vibration. While it’s often acceptable for smaller engines e.g. in a lawn mower, a typical car engine has more than one cylinder to ensure a more even delivery of power. I’ll discuss these concepts in more depth near the end of the article. Since the four cylinders are inline, the engine we’ll build is known as an inline four cylinder engine. Other engines may use different arrangements of cylinders, usually in a flat or V-shape configuration. The sides of the block are reinforced by various ribs to improve the rigidity of the structure â the body has to withhold the power of the explosions inside the cylinders. You may also have realized that the top part of the block is perfectly flat â we’ll soon attach another component there. If you look at the cross section of the block you’ll notice that the areas around cylinders are empty: Those passages are there for the coolant to flow around the cylinders and take the heat of the combustion away. While I’m not going to dive into details of engine cooling, it’s worth noting that engines should run at a specific operating temperature and the coolant pump, thermostats, and radiators make sure that the engine isn’t running too cold or too hot. Let’s look at the first big part we’ll mount onto the engine â the crankshaft: Notice that the crankshaft has five main cylindrical parts that define its axis of rotation, they’re called the main journals. There are also four rod journals that are positioned off-axis. All the journals are connected via  webs. Note that while the sections have different colors here, the entire crankshaft is made from a single piece of metal. You may wonder why the two inner rod journals are offset differently than the two outer ones, so let’s pop the piston assemblies on and see how they’ll move on the crankshaft: Since the rod journals are at different locations each of the four pistons can run at a different phase of the four stroke cycle. Notice that the distance between the center of the main journals and the rod journals  defines how far up and down the piston goes in the cylinder. A real piston and its connecting rod have some mass so they end up creating a weight imbalance on a rotating crankshaft. To counteract that mass, the webs have elongated shape to form a counterweight that helps to even out the inertial forces on the shaft. One could assume the installation of the crankshaft in the engine block is as simple as putting it directly in a designated spot at the bottom: Unfortunately, that wouldn’t really work. During engine operation the pistons exert a lot of force on the crankshaft and the main journals would just rub against the housing creating a lot of friction that would wear the parts down. To fix that we need to firstly put in some bearings that will help to make the rotation of the crankshaft smooth: These strips of metal don’t look like much, but bearings are usually made from a softer material which causes them to wear first which prevents degradation of the crankshaft itself in case any contact occurs. Most of the time, however, the crankshaft doesn’t actually touch the bearings at all. Notice the small hole in the bearing that matches the corresponding hole inside the engine block: Through that hole the engine pumps oil under pressure. The crankshaft’s diameter is slightly smaller than the bearings' inner diameter so oil fills the tiny gap between the two surfaces. Presence of oil is critical here as it creates conditions for hydrodynamic lubrication. Oil sticks to the bearings and the crankshaft, but since the crankshaft rotates it creates a variation in velocity of oil between the two surfaces. In the demonstration below the small arrows symbolize the local velocity of the liquid: The difference of diameters causes a wedge-like shape to develop which then creates an area of increased pressure that lifts the crankshaft journal away from the bearings. Note that the size of the gap in the demonstration is not to scale, but in real running engines the rotating crankshaft should float completely on a very thin surface of oil. You may have noticed that one half of the bearings also contains a small gutter which creates a small pool of oil under pressure. Moreover, the crankshaft has small holes in it: Those passages are actually connected inside and the oil from the pool in the bearing travels through the little passages in the crankshaft itself. This brilliant solution distributes the oil from the main journals to the rod journals, which are constantly changing their position inside the engine. The demonstration below shows one of the many typical arrangements of these passages and the presence of oil in and on the crankshaft: Let’s finally put the crankshaft in. We’ll clamp it down using five end caps that have their corresponding bearings put in and we’ll screw everything together: Those screws have to be tightened to a precise torque â it has to be high enough so that end caps are able to keep the crankshaft in place despite the force of explosions pushing down on it through the piston rods, but the torque on the screws can’t be too high to avoid any deformation of the circular shape of the final opening in which the crankshaft lies. The crankshaft itself is there to receive the force from the pistons, so let’s look at at one up close: Firstly, notice all the empty spaces inside the piston. They’re there to reduce the weight â a piston should be as light as possible to minimize the inertial forces created by its reciprocating motion. In this piston the top part known as the piston crown has a dish-like cavity in it. Other pistons may be flat or have more complicated shapes. Pistons have actually slightly smaller diameter than the cylinders, otherwise they could seize during movement resulting in a catastrophic engine failure. However, the piston still needs to seal the combustion chamber and prevent gases from leaking around the piston. This problem is solved by piston rings that are placed in the grooves on top of the piston: On their own piston rings have a fairly big gap, but when placed in the cylinder they’re squeezed into a fitting shape. Note that the fitted ring still has a tiny gap: While the gap shrinks when a ring gets warmer and expands, the gap should never completely close as the ring may break under pressure. The clearances in the sizing of the rings are very precise. Since the ring is squeezed into a smaller shape, it wants to expand and that tension helps it form a tight seal with the walls of the cylinder. That tension is also reinforced by the pressurized gases getting into the piston grooves and pushing the rings further against the cylinder walls. The pistons in our engine have three rings with the top two primarily helping to keep the pressure inside the combustion chamber. The third one serves a different role â it’s an oil control ring. The walls of a cylinder under a piston are constantly sprayed with a supply of oil to ensure a smooth movement during strokes. On a downstroke the oil ring scrapes the excessive amount of oil which escapes through the openings in the ring and the groove of the piston: Top of a piston faces the enormous heat of combustion. The rings are in contact with the piston and the cylinder walls so they heavily participate in heat dissipation. Moreover, since the top part of the piston is in closer contact with the hot gases, it reaches a higher temperature than the lower parts and therefore it expands more. To account for that the pistons are tapered on top so that when the different areas of a piston reach their operating temperatures the shape is more even. The area of the piston below the rings is called a skirt. Parts of the skirt are, through a thin layer of oil, in contact with cylinder walls during stroke which stabilizes the piston. While they look perfectly round, piston skirts are actually slightly oval. A piston is attached to its rod  via a gudgeon pin. Perhaps you’ve noticed that a piston has tiny grooves near the end of the pin opening. We’ll put snap rings  inside them â they’ll prevent the pin from leaving the hole: Piston rods themselves are very strong as they have to withstand the force of the explosions pressing on the piston during combustion, while also resisting a stretching and pushing forces due to the inertia of the piston changing its direction of movement. The pistons with the connecting rods and bearings can be slid down the cylinders and attached to the crankshaft: Recall that the rod bearings are lubricated by the oil coming in through crankshaft passages. Let’s see how the entire thing rotates: The movement of a piston as the function of the crankshaft angle deserves a closer look. In the demonstration below I marked the maximum, minimum, and a midpoint traveled distance of a piston during its stroke. You can control the rotation of the crankshaft with a slider: Notice that when the crank arm has done a 90Â° turn, i.e. halfway through between top to bottom angle, the piston  has moved up by less than half of its total stroke distance. It’s a simple geometrical consequence of the length of arms of the triangle formed by the crank arm, the connecting rod, and the vertical baseline. The maximum piston’s position is known as its top dead center, and its lowest position is known as bottom dead center. As the cylinders move up and down between those two extremes they sweep cylindrical volumes: The area of a cylinder’s circular cross section A times the stroke length of a piston S define the displacement volume of that cylinder, and the displacement V of the entire engine is the sum of displacements of all of its n cylinders: If a single cylinder’s displacement is half a liter, then a four cylinder engine would be known as a 2.0-L engine. In the most basic setup, the bigger the engine displacement, the more air the engine can suck in, and the higher its peak power. With the parts we’ve assembled so far we’re getting close to completing the combustion chamber. We’ve created the walls with the engine block and the movable bottom with the pistons. We just need to seal it from the top â this is where the cylinder head comes in: There are a lot of openings there. Firstly, notice four large sections at the bottom â these dome-like cavities will form the top of the combustion chamber. Each of those four sections is the same, so let’s look at the individual segment up close: In this engine each cylinder has four major openings â through two of them the intake air is sucked in and through the other two the exhaust gases are let out. If you look at the head from a side you’ll notice that each pair of the openings is joined into one elliptical hole that exits on the side of the head. Those passages are known as intake and exhaust ports. Additionally, there is a smaller hole drilled centrally through the axis of each opening â they’re there for the intake and exhaust valves: Modern engines typically use more than one intake and exhaust valves per cylinder as it increases the flow of gases in and out of the combustion chamber. Moreover, the intake valves are usually a little bigger: When the piston moves down on an intake stroke, the pressure difference it creates is no larger than that of the incoming air, which, for traditional engines, is roughly equal to the atmospheric pressure. However, at the end of the combustion stroke the pressure inside the cylinder is many times higher than the atmospheric pressure, so it’s significantly easier to expel the combustion gases than to suck the intake air in. For this reason the intake openings and valves are larger. Let’s look at the operation of those valves up close. Firstly, the seal they form has to be very tight so that the only pathway for the expanding gases created in the combustion is to push the piston down. The edges of a valve and its seat have a conical section so that the seal becomes tighter as the valve is pressed up: In our toy engine the valves magically opened and closed on their own, so let’s see how it can be actually done in practice. To keep a valve shut weâre going to use a spring to keep tension on a closed valve. We can then lock the spring against the valve using simple locking mechanism that consists of two valve keepers, a spring retainer, and an inverted bucket: Notice that the top part of a valve has a little groove in it so that the keepers can lock in place. The keepers themselves form a section of a cone that the retainer wedges against: Since the keepers are locked in the grooves, the retainer can’t move which holds the spring under tension. The bucket provides a big and smooth surface for a force to transfer onto a valve â now whenever we push on the bucket the spring  will push it back in place: We’ve got the return mechanism all figured out, but this still leaves the problem of actually pressing the valves â they need to be opened at a certain cadence which depends on the movement of the piston inside the cylinder. That periodic nature of the operation implies that we should use some sort of rotary motion to push the valves. We can shape a piece of metal so that it pushes on the valve at different offsets as it rotates on a shaft â it’s known as a cam. The spring ensures that the bucket is tightly pressed against the cam and follows its shape: The shape of the cam defines when, for how long, and how much the valve is opened. In the demonstration below you can control the height and angular span of the section of the cam profile that deviates from a circle and see how it affects the position of the bucket and thus valve lift at different angles. The plot in the upper part shows the offset of the bucket relative to its normal position as the function of cam rotation angle: The shape of the cam is critical for defining the way the engine operates. All cams for a set of intake or exhaust valves are usually placed on a single camshaft: Most modern engines use two camshafts, one for intake valves and another for exhaust valves. Let’s see how the camshafts open and close valves during a typical engine operation: All cylinders go through four stages in a predefined order. The timing of the valves is actually not as straightforward as it was in our toy engine so let’s look at it up close: Firstly, notice that the intake valves close after the piston reaches the bottom end of the intake stroke â the air coming through the valve has some inertia, which, especially at high engine speeds, makes it pile up in the cylinder despite the opposite movement of the piston. Similarly, the exhaust valves open before the piston reaches the bottom of the power stroke, as the majority of the useful work has already been done by the gases and the pressure surplus in the cylinder should be minimized so that the exhaust stroke doesn’t have to actively compress the exhaust gases. The intake valves open slightly before the piston reaches the top end of the exhaust stroke. When combustion gases escape through the exhaust valves they help to create a “scavenging” effect that helps to pull the intake air in. For that reason the exhaust valves close after the piston reaches the top of the exhaust stroke. An engine running at low speed may have a different ideal parameters than an engine running at full speed, so many modern engines use a few methods to vary both the timing and lift of the valves during operation. We can now assemble the pieces together. While the top surface of the engine block and the bottom surface of the cylinder head are very flat and smooth, they need to form a perfect seal as they have to prevent the combustion gases from leaking through. A gasket, often made from a softer, compressible metal, is placed in-between the two and the head is bolted to the block: The bolts are tightened to the predetermined torque in multiple steps and in a specific order to ensure that the head doesn’t deform during assembly. The sturdiness of the head bolts is critical as they literally have to contain the force of the explosions inside the cylinder. The camshafts are then installed â they’re held in place by their caps and bolts: The only remaining piece of the puzzle is to how to turn the camshafts while ensuring they’re synchronized to the movement of the pistons. To achieve this, most engines use a rubber timing belt that is driven by a gear mounted to the crankshaft itself. The timing belt is teethed so that it locks in the notches on the timing gears, the rollers keep the belt in tension: Recall that in a four stroke engine a single cycle of operation requires two full revolutions of the crankshaft as the piston goes through intake, compression, power, and exhaust phases. However, the intake and exhaust valves open just once during that cycle so the camshafts should do just a single revolution during that time. To fulfill this requirement the crankshaft gear is twice as small as the camshaft gears â this ensures that the camshafts rotate just once when the crankshaft rotates twice which you can verify by observing a small black dots at the perimeter of the gears: In the examples so far weâve assumed that the intake valve let in a mixture of air and fuel, and that indeed was the case in older engines that used a carburetor to create the mix. Modern engines, however, use a fuel injection system where the amount and timing of fuel injection is controlled by an electronic Engine Control Unit, often abbreviated as ECU. At appropriate time the solenoid inside the injector is energized which electromagnetically pulls the needle up, which in turns lets the pressurized fuel escape through the tiny outlet holes. When the power to the solenoid is cut, the spring pushes the needle back to seal the nozzle: In some engines the injection happens in the intake port, very close to the intake valve itself, but in our engine we’ll use a direct injection system in which the fuel is put directly into the cylinder itself. The fuel and air mixture in the combustion chamber is lit by a spark plug. In a simplified form a spark plug consists of two pieces of metal separated by a  ceramic insulator. The outer shell is connected to the engine body which acts as a ground, and the central electrode is connected to a source of voltage  high enough to bridge the gap to the tip of the outer shell which creates a spark: That high voltage is generated by an ignition coil. In older engines there was a single coil that sequentially provided high voltage to individual spark plugs, but in modern engines each spark plug will usually have its own coil with the discharge controlled by the ECU. With a set of injectors and spark plugs in hand we can finally fill the remaining holes in the engine head. I’ll hide the rest of the engine so that we get a better view as to where they fit: Let’s see how an injector and a spark plug work together during the strokes. Note that normally the injector is attached to a fuel rail feeding it highly pressurized gasoline and a spark plug is connected to its coil, but for the sake of clarity they’re not being shown here. The air is depicted as a blue gas that turns yellow when mixed with fuel: You may notice that the spark plug fires before the piston reaches the end of the compression phase â it’s done on purpose since it takes a moment for the burning of the air-fuel mixture to begin. Let’s take a look at all four cylinders firing in sequence: All the demonstrations in this article run at very slow speeds, but it’s worth mentioning how fast things happen in an actual car. For an engine running at a modest speed of 1500 revolutions per minute there are 25 revolutions of the crankshaft every second. Each cylinder fires once per two revolutions of the crankshaft, but since we have four cylinders, there are actually around 50 explosions per second happening in an engine running at that speed. The ratio of air to fuel in the combustion chamber is important as simply adding more fuel doesn’t necessarily make the cylinder pressure bigger as we’ll just end up with incompletely burnt gasoline. To actually increase the speed of the engine we need to increase the supply of fuel and air. Let’s look at the pressure inside the cylinder a bit closer. During the intake stroke the piston creates a negative pressure difference which sucks the air in. During the compression stroke the pressure increases due to shrinking volume, only to increase even more due to combustion. It’s finally reduced by the expanding volume of the chamber as the piston goes down during the power stroke: The pushing force generated on the piston is proportional to the pressure in the cylinder, however, the torque generated by that force is also affected by other factors. Firstly, observe that during the compression stroke, the pressure inside the chamber pushes on the piston and through the rod against the rotation of the crankshaft: Secondly, the magnitude of torque depends on the effective length of the arm of force, but that length changes during crankshaft rotation. For instance, when the piston is at the top dead center the gases merely push the crankshaft down, but they don’t turn it because the force arm has the length of zero. In the demonstration below the red dashed line shows the direction of the rod force and the black line show the arm of force on the crank: If we account for these effects we can calculate that the torque generated by the pressure inside one cylinder is roughly as follows: However, these are not all the forces that affect the crankshaft. Both piston and its connecting rod have some mass and during crankshaft’s rotation they keep changing their direction of motion. Let’s look at the plot of the velocity of a piston as it moves up and down the cylinder when the crankshaft rotates with a constant angular velocity: You may be surprised that the plot isn’t symmetrical, but it’s just a consequence of the already discussed behavior of the crank mechanism taking more time to move through the lower half of the stroke compared to the upper half of the stroke. Let’s consider a piston in a top dead center position â as the piston reaches that point its velocity is 0, so the crankshaft has to drag the piston down. Roughly halfway through its stroke the piston reaches its maximum velocity, and now the crankshaft has to actually slow the piston down so that it stops moving by the time it reaches the bottom dead center position. The piston, however, wants to keep going, so it exerts a pushing force. These inertial forces keep oscillating back and forth creating an inertial torque on the crankshaft: The magnitude of the inertial forces depends on the speed of the piston and thus on the crankshaft’s rotational velocity. That system is very dynamic since crankshaft’s rotational velocity in turn is affected by the inertial forces acting on it. The resulting output crankshaft’s torque is a sum of these pressure-based and inertia-based torques. The cumulative diagram of torque on the crankshaft from a single piston looks roughly like this: As you can see, the torque created by a single piston is very varied. Even when we overlap the resulting torque from all four pistons the total torque is still fairly uneven: A torque T acting on a shaft creates an angular acceleration Î± that is proportional to that torque: This is a rotational equivalent of traditional linear equation that ties force F to mass m and linear acceleration a: In rotational motion the equivalent of mass m is moment of inertia I. Similarly to how velocity is affected by acceleration, angular velocity is affected by angular acceleration. Let’s see how angular velocity of the crankshaft varies over time under a constant load: As you can see the value fluctuates a lot. To reduce the angular acceleration and thus variation in angular velocity we have to increase the rotational inertia I of the system. For that purpose a heavy flywheel is attached to the crankshaft with a bunch of  bolts: Because the flywheel  is heavy and has a large moment of inertia the variation in angular velocity of the crankshaft is reduced which makes the engine work more evenly: Notice that the flywheel has gear teeth cut on its perimeter. Those teeth mesh with a pinion gear that is powered by an electric starter motor when the engine is being turned on. Once the flywheel starts moving its inertia helps to keep the crankshaft going which in turn lets the engine continue to operate on its own. Note that while the high inertia of the flywheel helps to smooth out the variation in angular velocity, it also comes at a cost â a very heavy flywheel is difficult to spin up so the engine becomes less responsive to throttle input. In cars with manual transmission an engaged clutch presses against the flywheel to transfer the rotational motion to the transmission and further down to the wheels. Cars with automatic transmission don’t have a flywheel but instead they use a flexplate which is connected to a torque converter which serves as a source of large inertia to smooth out the engine’s efforts. The journey through our engine ends here, but there are still many components needed to fully embed an engine in a car. The previously mentioned cooling system ensures the engine is kept at an appropriate operating temperature. The intake and exhaust manifolds direct the flow of gases into and out of the cylinders. An oil pump, oil filter, and oil passages in the engine block and cylinder head ensure all components are properly lubricated. Many modern engines also employ a turbocharger that uses the exhaust gases to increase the amount of air that gets pushed into the cylinders. All those components make sure that the engine can operate at expected level of power and efficiency. Engineering Explained is one of the most popular channels dedicated to car technologies enthusiasts. Over the years Jason Fenske has covered a breadth of topics including a comparisons of injection systems, intake manifold design, and even rotary engines. PapadakisRacing has some great videos on engine assembly and teardowns. If you think you’d enjoy more videos from the latter category I Do Cars does in-depth deconstruction videos of used engines. How a Car Works is a fantastic video course on the operation of a car. In high quality episodes the presenter goes into a lot more details than what I’ve described here. While the course is paid, there are some free preview videos available on YouTube. Unchallenged for decades, the internal combustion engines in cars are slowly being supplanted by their electric equivalents, which are simpler, quieter, and more environmentally friendly. Despite their drawbacks, classic engines still have something mythical about them â their intricate mechanisms are synchronized together to create carefully controlled conditions for harnessing fire in a truly Promethean way."
  },
  {
    "url": "https://www.nytimes.com/interactive/2020/11/03/us/elections/forecast-president.html",
    "title": "Battleground States: Election Forecast - The New York Times",
    "content": "Advertisement Updated June 1, 2021, 2:51 PM ET Disabling auto-updates may improve reliability when using a screen reader or keyboard to navigate. If Joe Biden wins one of these three states, he is likely to win the presidency. If President Trump wins all three, it could be days or more before a winner is declared. Florida Florida — Georgia Georgia — North Carolina N.C. — Maggie Astor  Jan. 7, 2021 Vice President Mike Pence affirms Joseph R. Biden Jr. and Kamala Harris as the next president and vice president. Here are shifts in precincts where we believe nearly all votes have been counted. High-income neighborhoods are precincts where the median household income is $100,000 or more; white areas with fewer college graduates are precincts where more than 80 percent of adults are white and fewer than 25 percent have a college degree; urban and suburban areas are limited to major metropolitan areas; Cuban areas in Miami represent precincts in Miami-Dade County where more than 25 percent of registered voters are Cuban-American, according to L2. These estimates do not include parts of Florida that do not report data by precinct; parts of North Carolina that report early or absentee votes by county; some late-arriving mail ballots; or most provisional ballots. For precincts that have not yet reported all their votes, we make an estimate based on the results so far, what voting methods are left, and what we know about the people who live in those places. Once a state has counted all its votes, our estimated margin and the reported margin will match. As a rule, when our estimated margin is steady in the presence of new data, our forecast is more trustworthy. Once a state has counted all its votes, our estimated margin and the reported margin will match. As a rule, when our estimated margin is steady in the presence of new data, our forecast is more trustworthy. Once a state has counted all its votes, our estimated margin and the reported margin will match. As a rule, when our estimated margin is steady in the presence of new data, our forecast is more trustworthy. The interactive diagram below illustrates both candidates’ paths to the presidency. Here, we let you control the outcomes of the most competitive contests, assuming Mr. Trump and Mr. Biden will win states where they are heavily favored. Select a winner in the states below to see either candidate's paths to victory. Paths will narrow as races are called. *In Maine and Nebraska, two electoral votes are apportioned to the winner of the state popular vote, and the rest of the votes are given to the winner of the popular vote in each congressional district. Maine’s Second Congressional District and Nebraska’s Second Congressional District are worth one electoral vote each. Nicholas Fandos, in Washington Congress confirmed Joe Biden’s victory, defying a mob that stormed the Capitol after being egged on by President Trump.  Read more › Maggie Astor  Jan. 7, 2021 Vice President Mike Pence affirms Joseph R. Biden Jr. and Kamala Harris as the next president and vice president. Astead Herndon, in Atlanta  Jan. 6, 2021 Today encapsulated the politics of progress and grievance that have defined the Trump years: Senate wins for Warnock and Ossoff, and a mob at the Capitol.  Read more › Jonathan Martin, in Atlanta  Jan. 6, 2021 Democrats have now captured control of the Senate as Jon Ossoff has defeated David Perdue, following the Rev. Raphael Warnock’s victory over Senator Kelly Loeffler. See live results › The New York Times  Jan. 6, 2021 A mob of people loyal to President Trump stormed the Capitol, halting Congress’s counting of the electoral votes to confirm President-elect Joseph R. Biden Jr.’s victory.  Read more › Trip Gabriel  Dec. 14, 2020 Joseph R. Biden Jr. has received a majority of votes from the Electoral College, formally securing the presidency in the manner set out in the Constitution.  Read more › Isabella Grullón Paz  Dec. 14, 2020 The 538 members of the Electoral College are meeting to cast ballots for president based on the election results in their states, formalizing Joseph R. Biden Jr.’s victory. Track the Electoral College results › The New York Times  Dec. 5, 2020 California has certified its electors for the 2020 election, officially giving Joseph R. Biden Jr. more than the 270 pledged electors needed to become president.  Read more › Reid Epstein, in Washington  Nov. 30, 2020 The chairwoman of the Wisconsin Elections Commission has certified Biden as the winner in Wisconsin, formalizing his narrow victory in a state Trump carried four years ago.  Read more › Glenn Thrush, in Washington  Nov. 30, 2020 Arizona has officially certified Biden’s narrow victory in the state, further undermining Trump’s efforts to portray his decisive national loss as a matter still under dispute.  Read more › Michael D. Shear, in Washington  Nov. 23, 2020 President Trump authorized his government to begin the transition to President-elect Joseph R. Biden Jr.’s administration.  Read more › Source: Election results from National Election Pool/Edison Research By Michael Andre, Aliza Aufrichtig, Gray Beltran, Matthew Bloch, Larry Buchanan, Andrew Chavez, Nate Cohn, Matthew Conlen, Annie Daniel, Asmaa Elkeurti, Andrew Fischer, Josh Holder, Will Houp, Jonathan Huang, Josh Katz, Aaron Krolik, Jasmine C. Lee, Rebecca Lieberman, Ilana Marcus, Jaymin Patel, Charlie Smart, Ben Smithgall, Umi Syam, Rumsey Taylor, Miles Watkins and Isaac WhiteAdditional data collection by Alice Park, Rachel Shorey, Thu Trinh and Quoctrung BuiCandidate photo research and production by Earl Wilson, Alana Celii, Lalena Fisher, Yuriria Avila, Amanda Cordero, Laura Kaltman, Andrew Rodriguez, Alex Garces, Chris Kahley, Andy Chen, Chris O'Brien, Jim DeMaria, Dave Braun and Jessica WhiteReporting contributed by Alicia Parlapiano Advertisement"
  },
  {
    "url": "https://www.nytimes.com/interactive/2014/upshot/dialect-quiz-map.html",
    "title": "The U.S. Dialect Quiz: How Y’all, Youse and You Guys Talk - The New York Times",
    "content": "Advertisement By Josh Katz and Wilson AndrewsDec. 21, 2013 What does the way you speak say about where you’re from? Answer all the questions below to see your personal dialect map. Loading... Your last answer Least similar Most similar You’re viewing another reader’s map. Click here to take the quiz and see your own. Your Map See the pattern of your dialect in the map below. Three of the most similar cities are shown. Least similar Most similar Share Your Map: Loading... Share Your Map: These maps show your most distinctive answer for each of these cities. About This Quiz The data for the quiz and maps shown here come from over 350,000 survey responses collected from August to October 2013 by Josh Katz, a graphics editor for the New York Times who developed this quiz and has since written “Speaking American,” a visual exploration of American regional dialects. Most of the questions used in this quiz are based on those in the Harvard Dialect Survey, a linguistics project begun in 2002 by Bert Vaux and Scott Golder. The original questions and results for that survey can be found on Dr. Vaux’s current website. The colors on the large heat map correspond to the probability that a randomly selected person in that location would respond to a randomly selected survey question the same way that you did. The three smaller maps show which answer most contributed to those cities being named the most (or least) similar to you. Additional work by Eric Buth. Advertisement"
  },
  {
    "url": "https://medium.com/@anildash/the-missing-building-blocks-of-the-web-3fa490ae5cbc",
    "title": "The Missing Building Blocks of the Web | by Anil Dash | Medium",
    "content": "Sign up Sign in Sign up Sign in -- 55 Listen Share Though the world wide web has been around for more than a quarter century, people have been theorizing about hypertext and linked documents and a global network of apps for at least 75 years, and perhaps longer. And while some of those ideas are now obsolete, or were hopelessly academic as concepts, or seem incredibly obvious in a world where we’re all on the web every day, the time is perfect to revisit a few of the overlooked gems from past eras. Perhaps modern versions of these concepts could be what helps us rebuild the web into something that has the potential, excitement, and openness that got so many of us excited about it in the first place. [An aside: Our team at Glitch has been hard at work on delivering many of the core ideas discussed in this piece, including new approaches to View Source, Authoring, Embedding, and more. If these ideas resonate with you, we hope you’ll check out Glitch and see how we can bring these abilities back to the web.] For the first few years of the web, the fundamental way that people learned to build web pages was by using the “View Source” feature in their web browser. You would point your mouse at a menu that said something like “View Source” (nobody was browsing the web on a touchscreen back then) and suddenly you’d see the HTML code that made up the page you were looking at. If you squinted, you could see the text you’d been reading, and wrapped around it was a fairly comprehensible set of tags — you know, that <p>paragraph</p> kind of stuff. It was one of the most effective technology teaching tools ever created. And no surprise, since the web was invented for the purpose of sharing knowledge. These days, View Source is in bad shape. Most mobile devices don’t support the feature at all. And even on the desktop, the feature gets buried away, or hidden unless you enable special developer settings. It’s especially egregious because the tools for working with HTML in a browser are better than ever. Developers have basically given ordinary desktop web browsers the potential to be smart, powerful tools for creating web pages. But that leads to the other problem. Most complicated web pages these days aren’t actually written by anyone. They’re assembled, by little programs that take the instructions made by a coder, and then translate those instructions into the actual HTML (and CSS, and JavaScript, and images, and everything else) that goes to your browser. If you’re an expert, maybe you can figure out what tools were being used to assemble the page, and go to GitHub and find some version of those tools to try out. But it’s the difference between learning to cook by looking over someone’s shoulder or being told where a restaurant bought its ingredients. Bringing View Source back could empower a new generation of creators to see the web as something they make, not just a place where big companies put up sites that we all dump our personal data into. When Tim Berners-Lee invented the world wide web, he assumed that, just like in earlier hypertext systems, every web browser would be able to write web pages just as easily as it read them. In fact, that early belief led many who pioneered the web to assume that the format of HTML itself didn’t matter that much, as many different browsing tools would be able to create it. In some ways, that’s true — billions of people make things on the web all the time. Only they don’t know they’re making HTML, because Facebook (or Instagram, or whatever other app they’re using) generates it for them. Interestingly, it’s one of Facebook’s board members that helped cause this schism between reading and writing on the web. Marc Andreessen pioneered the early Mosaic web browser, and then famously went on to spearhead Netscape, the first broadly-available commercial web browser. But Netscape wasn’t made as a publicly-funded research project at a state university — it was a hot startup company backed by a lot of venture capital investment. It’s no surprise, then, that the ability to create web pages was reserved for Netscape Gold, the paid version of that first broadly consumer-oriented web browser. Reading things on the web would be free, sure. But creating things on the web? We’d pay venture-backed startup tech companies for the ability to do that, and they’d mediate it for us. Notwithstanding Facebook’s current dominance, there are still a lot of ways to publish actual websites instead of just dumping little bits of content into the giant social network. There are all kinds of “site building” tools that let you pick a template and publish. Professionals have authoring tools or content management systems for maintaining big, serious websites. But these days, there are very few tools you could just use on your computer (or your tablet, or your phone) to create a web page or web site from scratch. All that could change quickly, though— the barriers are lower than ever to reclaiming the creative capability that the web was supposed to have right from its birth. Okay, this one’s nerdy. But I’m just gonna put it out there: You’re supposed to be able to include other websites (or parts of other websites) in your web pages. Sure, we can do some of that — you’ve seen plenty of YouTube videos embedded inside articles that you’ve read, and as media sites pivot to video, that’s only gotten more commonplace. But you almost never see a little functional part of one website embedded in another. Old-timers might remember when Flash ruled the web, and people made simple games or interactive art pieces that would then get shared on blogs or other media sites. Except for the occasional SoundCloud song on someone’s Tumblr, it’s a grim landscape for anyone that can imagine a web where bits and pieces of different sites are combined together like Legos. Most of the time, we talk about this functionality as “embedding” a widget from one site into another. There was even a brief fad during the heyday of blogs more than a decade ago where people started entire companies around the idea of making “widgets” that would get shared on blogs or even on company websites. These days that capability is mostly used to put a Google Map onto a company’s site so you can find their nearest location. Those old hypertext theory people had broader ambitions, though. They thought we might someday be able to pull live, updated pieces of other sites into our own websites, mixing and matching data or even whole apps as needed. This ability to include part of one web page into another was called “transclusion”, and it’s remained a bit of a holy grail for decades. There’s no reason that this can’t be done today, especially since the way we build web pages in the modern era often involves generating just partial pages or only sending along the data that’s updated on a particular site. If we can address the security and performance concerns of sharing data this way, we could address one of the biggest unfulfilled promises of the web. This one is so obvious, but we seem to have forgotten all about it: The web was designed so that everybody was supposed to have their own website, at its own address. Of course, things got complicated early on — it was too hard to run your own website (let alone your own web server!) and the relative scarcity of domain names made them expensive and a pain for everybody to buy. If you just wanted to share some ideas, or talk to your friends, or do your work, managing all that hassle became too much trouble, and pretty soon a big, expensive industry of web consultants sprung up to handle the needs of anybody who still actually wanted their own website—and had the money to pay for it. But things have gotten much easier. There are plenty of tools for easily building a website now, and many of them are free. And while companies still usually have a website of their own, an individual having a substantial website (not just a one-page placeholder) is pretty unusual these days unless they’re a Social Media Expert or somebody with a book to sell. There’s no reason it has to be that way, though. There are no technical barriers for why we couldn’t share our photos to our own sites instead of to Instagram, or why we couldn’t post stupid memes to our own web address instead of on Facebook or Reddit. There are social barriers, of course — if we stubbornly used our own websites right now, none of our family or friends would see our stuff. Yet there’s been a dogged community of web nerds working on that problem for a decade or two, trying to see if they can get the ease or convenience of sharing on Facebook or Twitter or Instagram to work across a distributed network where everyone has their own websites. Now, none of that stuff is simple enough yet. It’s for nerds, or sometimes, it’s for nobody at all. But the same was true of the web itself, for years, when it was young. This time, we know the stakes, and we can imagine the value of having a little piece of the internet that we own ourselves, and have some control over. It’s not impossible that we could still complete the unfinished business that’s left over from the web’s earliest days. And I have to imagine it’ll be kind of fun and well worth the effort to at least give it a try. In a similar vein, you may also enjoy this look at the lost infrastructure of the early era of social media. medium.com And if these ideas matter to you, join us on Glitch, the friendly and open community where people who love the web are creating the most innovative new apps and sites on the web. Some rights reserved -- -- 55 I help make @Glitch so you can make the internet. Trying to make tech more ethical & humane. (Also an advisor to Medium.) More: http://anildash.com/ Help Status About Careers Press Blog Privacy Rules Terms Text to speech"
  },
  {
    "url": "https://www.skypack.dev",
    "title": "Skypack: search millions of open source JavaScript packages",
    "content": "Menu Additional Links Ever tried to load JavaScript from a CDN and realized that it doesn’t work in a browser\n\t\t\t\twithout a bundler? Skypack operates like your favorite CDN but with an important difference:\n\t\t\t\tpackages are preoptimized for browser use. But Skypack doesn’t stop there: it handles minification, browser polyfilling, gzip/brotli,\n\t\t\t\tHTTP/3, caching, and more! Skypack is\n\t\t\t\tfree to use\n\t\t\t\tfor personal and commercial purposes, forever. The basic CDN is production-ready and is\n\t\t\t\tbacked by Cloudflare, Google Cloud, and AWS. We’re fully committed to building a core piece\n\t\t\t\tof infrastructure you can rely on. To give us feedback on Skypack, please\n\t\t\t\tvisit our issue tracker\n\t\t\t\tor\n\t\t\t\tjoin our Discord. If you’re\n\t\t\t\tinterested in advanced features such as a custom domain, or you have specific needs, please\n\t\t\t\tcontact us at\n\t\t\t\tinfo@pika.dev."
  },
  {
    "url": "https://benschmidt.org/post/2020-01-15/2020-01-15-webgpu/",
    "title": "Javascript and the next decade of data programming | Ben Schmidt",
    "content": "I’ve recently been getting pretty far into the weeds about what the future of\ndata programming is going to look like. I use pandas and dplyr in python and R\nrespectively. But I’m starting to see the shape of\nsomething that’s interesting coming down the pike.\nI’ve been working on a project that involves scatterplot visualizations\nat a massive scale–up to 1 billion points sent to the browser. In doing this,\ntwo things have become clear: I tweeted about\nit once, after I had experimented with binary, serialized alternatives\nto JSON. As webgpu and new binary serialization formats--like Arrow--come of age, it's going to be harder and harder to stomach geojson's slowness. More and more of R and python will become js or wasm wrappers. Just like in the 2000s they were wrappers around Java. It'll be very weird. I’m writing about Python and R because they’re completely dominant in the\nspace of data programming. (By data programming, I mean basically ‘data science’;\nnot being a scientist, I have trouble using it to describe what I do.)\nSome dinosaurs in economists still use Stata, and some wizards use Julia, but\nif you want to work with data that’s basically it.\nThe big problem with the programming lessons we use to work\nwith data they run largely on CPUs, and often predominantly on a single core.\nThis has always been an issue in terms of speed; when I first switched to\nPython around 2011, I furiously searched ways around the GIL (global\ninterpreter lock) that keeps the language from using multiple cores even on\nthreads. Things have gotten a little better on some fronts–in general, it\nseems like at least linear algebra routines can make use of a computer’s\nfull resources. Separately, the graphical and interface primitives of all programs have started\nto move to the web. If I had started doing this kind of work seriously even a\ncouple years later, I would never even have noticed there used to be another\nway. I never really used tcl/tk interfaces\nin R, but I was always aware that they existed; the very first version, private\nversion  of the Google Ngrams browser that JB Michel wrote in like 2008 or\nsomething was built around some Python library.\nThis was normal. But in the last decade, it’s become\nobvious that if you\nwant to build user-facing elements to describe something like “a button”\nor “a mouseover”, the path of least resistance is to use the HTML conception,\nnot the operating system conception of them. The\nfifteen-year-old freshman\nwho built the first Bookworm UI quickly saw it needed a javascript plotting library.\nThis integration is becoming tighter and tighter\nin data programming land. I have collaborators and grad students who transition\nseamlessly into bundling their R packages into Shiny apps, into decorating their\nGoogle colab notebooks with all sorts of sliders and text entry fields, into\npublishing R and Python code as online books with HTML/JS navigation. Jupyter notebooks and the RStudio IDE themselves are part of this transformation; what appears to be\nPython code held together by an invisible skein of Javascript. Again, these\nare platforms that have more or less displaced earlier models. When I first learned\nR, I pasted from textedit into the core R GUI; I went a little down the road\ninto ESS-mode in emacs as well. But if you need to continually be checking\nrandom samples of a dataframe, re-running modules, and seeing if your\nregular expressions correctly clean a dataset, you are using a notebook\ninterface today, even if you bundle your code into a module at some point. And for visualization, Javascript is creeping into this space. Like many people,\nI’ve been relieved to be able to use Altair instead of matplotlib for\nvisualizing pandas dataframes; and I don’t think twice about dropping\nggplotly into lessons about ggplot for students who start wondering about\ntooltips on mouseover. ggplot and matplotlib are still king of the roost\nfor publication-ready plots, but after becoming accustomed to interactive,\nresponsive charts on the web, we are coming to expect exploratory charts to\ndo the same thing; just as select menus and buttons from HTML fill this role\nin notebook interface, JS charting libraries do the same for chart interface. Let me be clear–something I’ll say in this following section is certainly\nwrong. I’m not fully expert in what I’m about to say. I don’t know\nwho is! There are some analogies to web cartography, where I’ve learned a lot\nfrom Vladimir Agafonkin. Many of the tools I’m\nthinking about I learned about in a set of communications with\nDoug Duhaime and David McClure.\nBut the field is unstable enough that I think others may stumble in the same\ndirection I have. This whole period, GPUs have also been displacing CPUs for\ncomputation. The R/Python interfaces to these are tricky. Numba kind of\nworks; I’ve fiddled with gnumpy from time to time; and I’ve never intentionally\nused a GPU in R, although it’s possible I did without knowing it. The path of\nleast resistance to GPU computation in Python and R is often to use Tensorflow\nor Torch even for purposes that don’t really a neural network library–so I\nfind myself, for example, training UMAP models using the neural network interface\nrather than the CPU one even though I’d prefer the other. Most of these rely on CUDA to access GPUs. (When I said I don’t know what I’m\ntalking about–this is the core of it.) If you\nwant to do programming on these platforms, you increasingly boot up a cloud\nserver and run heavy-duty models there.  Cuda configuration is a pain, and the\nodds are decent your home machine doesn’t have a GPU anyway. If you want to\nrun everything in the cloud, this is fine–Google just gives away TPUs for free.\nBut doing a group-by/apply/summarize on a few million rows, this is overkill;\nand while cloud compute is pretty cheap compared to your home laptop,\ncloud storage is crazy expensive. Digital Ocean charges me like a hundred dollars a year\njust to keep up the database backing RateMyProfessor; for the work I do\non several terabytes of data from the HathiTrust, I’d be lost without a university\ncluster and the 12TB hard drive on my desk at home. But I want these operations to run faster. When I started using webgl to make charts in Javascript, I was completely blown\naway what it could do. I’m used to sitting around waiting for ggplot to render even a few thousand points.\nI’m used to polygon operations in geopandas being long and expensive. I’m used\nto getting up to get some tea when I want to load a geojson file. But I could use javascript to generate millions of points in random\npolygons from primitive triangles in barely any time; and then using regl\nit can animate fast enough to make seamless zooming reasonable. Here, for example,\nis every single vote (excluding absentee) in New York City precincts in\nthe 2020 election. (Hopefully this embed from Observable loads…\nbut if it doesn’t, well, that’s the kind of the point, too. I’m making you click below to avoid\nclobbering people on phones.) Load iframe Digging into the weeds to make more elaborate visualizations like this, I can\nsee why. Apache Arrow exposes an extremely low level model of the data you work\nwith, that encourages you to think a lot about both the precise schema and\nthe underlying types. In Python, I’ve gotten used to this kind of work in numpy;\nin R, I’ve only ever done a little bit a bit twiddling. But in modern JS, binary\narray buffers are built right into the language. When I started tinkering with\nJS, I thought of it as slow; but web developers are far more obsessive about speed\nthan any other high-level, dynamically typed language I’ve seen.\nThe profiling tools built into Chrome are incredibly powerful; and Google,\nespecially, has made a huge investment in making JS run incredibly quickly because\nthere’s huge money in frictionless web experience. Sure, lots of websites are\nslow because they come with megabyte-sized React installations and casual bloat;\nsure, the DOM is slow to work with. But Javascript itself is fast. In my first few years teaching digital humanities, probably the least thankful\ntask was helping students manage their local Java installations so they could\nrun Mallet, the best implementation of topic-modeling algorithm out there. Now,\nwe usually use slower and inferior implementations in gensim, structural topic\nmodels, and the like.\n(For an interesting discussion from Ted Underwood and Yoav Goldberg of how\ninferior results in gensim and sklearn came to displace mallet, see the Twitter\nthreads here.)\nBut as David Mimno, who keeps Mallet running, says, Javascript works much\nfaster. Finally, integrate algorithms with interface. The browser is a high performance computing environment (JavaScript is MUCH faster than Python) embedded in an excellent interactive graphics environment. Plus thereâs a code environment hidden underneath! Print those variables! And while Javascript has a reputation as a terrible language, the post ES2015\niterations have made it in many cases relatively easy to program with.\nMaps, sets, for ... of ... all work much like you’d expect (unlike the days\nwhen I spent a couple hours hunting out a rarely occuring bug in one data\nvisualization that turned out to occur when I was making visualizations of\nwordcounts that included the word constructor somewhere in the vocabulary);\nand many syntactic features like classes, array destructuring, and arrow\nfunction notation\nare far more pleasant than their Python equivalents. (Full disclosure–even\nafter a decade in the language, I still find Python’s whitespace syntax\ngimmicky and at heart just don’t like the language. But that’s a post for\nanother day.) And if javascript is fast, WebGL is just bonkers in what it can do.\nWant to lay out two million points in a peano curve in a few milliseconds?\nNo problem–you can even regenerate every single frame. Load iframe And WebGL uses floating-point buffers that are the same as those in Apache Arrow, so you\ncopy blocks of data straight from disk (or the web) into the renderers without even\nhaving to do that (still fast) javascript computation. It’s difficult, and\neasy to do wrong. (I’ve found regl pitched at the perfect level of abstraction,\nbut I still occasionally end up allocating thousands of buffers on the GPU every\nframe where I meant to only create one persistent one). In online cartography, protobuffer-based vector files do something similar in\nlibraries like mapbox.gl and deck.gl. The overhead of JSON-based formats\nfor working with cartographic data is hard to stomach once you’ve seen how\nfast, and how much more compressed, binary data can be. In working with WebGL, I’ve seen just how fast it can be. For things like\narray smoothing, counting of points to apply complicated numeric filters, and\ngroup-by sums, it’s possible to start applying most of the elements of the relational\nalgebra on data frames in a fully parallelized form. But I’ve held back from doing so in any but the most ad-hoc situations because\nWebGL is also terrible for data computing. I would never tell anyone to learn it, right now,\nunless they completely needed to. Attribute buffers can only be floats,\nso you need to convert all integer types before posting. In many situations data\nmay be downsized to half precision points, and double-precision floating points\nare so difficult that there are entire rickety structures built to support them at great cost\nSupport for texture types\nvaries across devices (Apple ones seem to pose special problems), so people I’ve\nlearned from like Ricky Reusser go to great lengths\nto support various fallbacks. And things that are essential for data programming,\nlike indexed lookup of lists or for loops across a passed array, are nearly impossible.\nI’ve found writing complex shaders in WebGL fun, but doing so always involves\nabusing the intentions of the system. But the last two pieces of the puzzle are lurking on the horizon. Web Assembly–\nwasm files–give another way to write things for the javascript virtual machine\nthat can avoid the pitfalls of Javascript being a poorly designed language.\nA few projects that are churning along in Rust hold the promise of making in-browser\ncomputation even faster. (If I were going to go all-in on a new programming language\nfor a few months right now, it would probably be Rust; in writing webgl programs\nI increasingly find myself doing the equivalent of writing my own garbage collectors,\nbut as a high-level guy I never learned enough C to really know the basic concepts.)\nBack in the 2000s, the python and R ecosystems were littered with packages\nthat relied on the Java virtual machine in various ways. In the 2010s, it felt\nto me like they shifted to underlying C/C++ dependencies. But given how much\neffort is going into it, I think we’ll start to see things use the Javascript\nVirtual Machine more and more. When I want to use some of D3 spherical projections\nin R, that’s how I call them; and Jerome Ooen’s V8 package (for running the JSVM,\nor whatever we call it) is approaching the same level of downloads as the\nmore venerable rJava. I suspect almost all of this is running just Javascript.\nIf it starts becoming a realistic way to run pre-compiled\nRust and C++ binaries on any system… that’s interesting. The last domino is a little off, but could be titanically important. WebGL is\nslowly dying, but the big tech companies have all gotten together to create\nWebGPU as the next-generation standard for talking to\nGPUs from the browser. It builds on top of the existing GPU interfaces for\nspecific devices (Apple, etc.) like\nVulkan and Metal, about which I have rigorously resisted learning anything. WebGPU will replace WebGL for fast in-browser graphics. But the capability to\ndo heavy duty computation in WebGL is so tantalizing that some lunatics\nhave already begun to do it. The stuff that goes on into Reusser’s work]\nis amazing; check out\nthis notebook about “multiscale Turing patterns” that creates gorgeous images\nhalfway between organic blobs and nineteenth-century endplates I haven’t read the draft WebGPU spec carefully, but it will certainly allow a more\nrobust way to handle things. There is already at least one linear algebra\nlibrary (i.e., BLAS) for WebGPU out there.\nI can only imagine that support for more data types will make many simple\ngroup-by-filter-apply functions plausible entirely in GPU-land on any computer\nthat can browse the web. When I started in R back in 2004, I spent hours tinkering with SQL backing for\nwhat seemed at the time like an enormous dataset: millions of rows giving\ndecades of data about student majors by race, college, gender, and ethnicity.\nI’d start a Windows desktop cranking out charts before I left the office at night,\nand come back to work the next morning to folders of images. Now, it’s feasible\nto send an only-slightly-condensed summary of 2.5 million rows for in-browser work and the whole dataset could easily fit in GPU memory.\nIn general, the distinction between generally available GPU memory (say, 0.5 - 4GB) and\nRAM (2-16GB) is not so massive that we won’t be sending lots of data there.\nData analysis and shaping is generally extremely parallelizable. Once this bundle gets rolling, it will much faster and more convenient than\npython/R, and in many cases it will be able to run with zero configuration.\nThe Arquero library,\nintroduced last year, already brings most of the especially important features\nof the dplyr or pandas API into observable at a nearly comparable speed. With\ntighter binary integration or a different backend, it–or something like it–\ncould easily become the basic platform for teaching the non-major introduction\nto data science course all of the universities are starting to launch. Even\nif it didn’t, the vast superiority of Javascript over R/Python for both\nvisualization speed (thanks to GPU integration) and interface (thanks to\nthe uniquity of HTML5) means that people will increasinly bring their own\ndata to websites for initial exploration first, and may never get any farther.\n(If I were going to short public companies based on the contents of these speculations,\nI’d start with NVidia–whose domination of the GPU space is partially dependent\non CUDA being the dominant language, not WebGPU, and ESRI, which is floundering\nas it tries to make desktop software that does what web browsers do easily.) Once these things start getting fast, the insane overhead of parsing CSV and JSON,\nand the loss of strict type definitions that they come with, will be far more onerous.\nSomething–I’d bet on parquet, but there are are possibilities involving arrow, HDF5,\nORC, protobuffer, or something else–will emerge as a more standard binary interchange format. So–this is the theory–the data programming languages in R and Python are going to rely on that. Just\nas they wrap Altair and they wrap HTML click elements, you’ll start finding\nmore and more that the package/module that seems to just work, and quickly,\nthat the 19-year-olds gravitate towards, runs on the JSVM. There will be\nstrange stack overflow questions in which people realize that they have\nan updated version of V8 installed which needs to be downgraded for some particular\npackage. There will python programs that work everywhere but mysteriously fail on some\nlow-priced laptops using a Chinese startup’s GPU. And there will be things\nthat almost entirely avoid the GPU because they’re so damned complicated to\nimplement that the Rust ninjas don’t do the full text, and which–compared to\nthe speed we see from everything else–come to be unbearable bottlenecks.\n(From what I’ve seen, Unicode regular expressions and non-spherical map projections seem to be a likely candidate here.) But it will also raise the question of why we should bother to continue in\nR and Python at all. Javascript is faster, and will run anywhere, universally,\nwithout the strange overhead of binder notebooks and the cost of loading data\nin the cloud. WASM ports of these languages that run inside the JSVM\nwill help, but ultimately get strange. (Will you write python code that gets\ntranspiled in the browser to WASM, and then invokes its own javascript emulator\nto build an altair chart?) Beats me! But I’ve already started sharing elementary data exercises for classes\nusing observablehq, which provides a far more coherent\napproach to notebook programming than Jupyter or RStudio. (If you haven’t\ntried it–among many, many other things, it parses the dependency relations between cells in a notebook\ntopologically and\navoids the incessant state errors that infect expert and–especially–novice\nprogramming in Jupyter or Rstudio.) And if you want to work with data\nrather than write code, it is almost as refreshing as the moment in computer history it\ntries to recapitulate, the shift from storing business data in COBOL to\nrunning them in spreadsheets. The tweet above that forms of the germ of this\nrant has just a single, solitary like on it; but it’s from Mike Bostock, the creator\nof D3 and co-founder of Observable, and that alone is part of the reason I\nbothered to write this whole thing up. The Apache Arrow platform I keep rhapsodizing about\nis led by Wes McKinney, the creator of pandas, who views it as the germ of a faster, better pandas2,\nfrom a position initially sponsored by RStudio and subsequently with funding\nfrom Nvidia. Speculative as this all\nis, it’s also–aside from massive neural-network gravitational of the tensorflow/torch solar systems–\nwhere the tools that become hegemonic in the last decade are naturally drifting.\n(Not to imply that Javascript is anywhere near the top of the Arrow project’s priority list, BTW. It isn’t.)\nI wish more of the data analysts, not just the insiders, saw this coming, or were excited that it is. As I said, I’ve been doing some of this programming since 2003 or so, and been\nputting in my regular rounds most days since 2010. In that time I’ve come to see\nthat I what I want to see most–fully editable, universally runnable, data\nanalysis on open data–is not a universal code. Some people just want\nstatic charts. Some people want to hide their data. Most readers don’t want\nto tweak the settings. And everyone looks down on people who like Javascript.\nBut it’s also the case that the web was first built\nin the 90s to share complicated academic work and make it editable by its\nreaders. Even if most of academia and much of the media is devoted to one-way\nflows of information, and much of the post-social media Internet is a blazing hellscape,\nI’m excited about these shifts in the landscape\nprecisely because they hold out the possibility that some portion of the Web\nmight actually live up to its promise of making it easier to think through ideas. I am a digital historian and Director of Digital Humanities at NYU. Â© 2019 · \n\n    Powered by the\n    Academic theme for\n    Hugo."
  },
  {
    "url": "https://arrow.apache.org/docs/js/",
    "title": "JavaScript docs — Apache Arrow v21.0.0",
    "content": "Stub page for the JavaScript docs; actual source is located in js/ sub-directory. Â© Copyright 2016-2025 Apache Software Foundation.\nApache Arrow, Arrow, Apache, the Apache feather logo, and the Apache Arrow project logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries. Created using Sphinx 6.2.0. Built with the PyData Sphinx Theme 0.16.1."
  },
  {
    "url": "https://uwdata.github.io/arquero/",
    "title": "Arquero | arquero",
    "content": "Arquero is a JavaScript library for query processing and transformation of array-backed data tables. Following the relational algebra and inspired by the design of dplyr, Arquero provides a fluent API for manipulating column-oriented data frames. Arquero supports a range of data transformation tasks, including filter, sample, aggregation, window, join, and reshaping operations. To get up and running, start with the Introducing Arquero tutorial, part of the Arquero notebook collection. Have a question or need help? Visit the Arquero GitHub repo or post to the Arquero GitHub Discussions board. Arquero is Spanish for “archer”: if datasets are arrows, Arquero helps their aim stay true. 🏹 Arquero also refers to a goalkeeper: safeguard your data from analytic “own goals”! 🥅 ✋ ⚽ The core abstractions in Arquero are data tables, which model each column as an array of values, and verbs that transform data and return new tables. Verbs are table methods, allowing method chaining for multi-step transformations. Though each table is unique, many verbs reuse the underlying columns to limit duplication. To use in the browser, you can load Arquero from a content delivery network: Arquero will be imported into the aq global object. The default browser bundle also includes the Flechette library for processing Apache Arrow data. Alternatively, you can build and import arquero.min.js from the dist directory, or build your own application bundle. When building custom application bundles for the browser, the module bundler should draw from the browser property of Arquero’s package.json file. For example, if using rollup, pass the browser: true option to the node-resolve plugin. Arquero uses modern JavaScript features, and so will not work with some outdated browsers. To use Arquero with older browsers including Internet Explorer, set up your project with a transpiler such as Babel. First install arquero as a dependency, for example via npm install arquero --save.\nArquero assumes Node version 18 or higher.\nAs of Arquero version 6, the library uses type module and should be loaded using ES module syntax. Import using ES module syntax, import all exports into a single object: Import using ES module syntax, with targeted imports: Dynamic import (e.g., within a Node.js REPL): To build and develop Arquero locally:"
  },
  {
    "url": "https://pbeshai.github.io/tidy/",
    "title": "tidy.js",
    "content": "Your Docusaurus site did not load properly. A very common reason is a wrong site baseUrl configuration. Current configured baseUrl = /tidy/ We suggest trying baseUrl = Tidy up your data with JavaScript tidy.js prioritizes making your data transformations readable, so future you and your teammates can get up and running quickly. Inspired by dplyr and the tidyverse in R, tidy.js is built using battle-tested verbs that can handle any data wrangling need. No wrapper classes needed — all tidy.js needs is an array of plain old-fashioned JS objects to get started. Here's a quick jumping off point to see the API for all the functions provided by tidy.js."
  },
  {
    "url": "https://observablehq.com/@observablehq/plot",
    "title": "Plot | The JavaScript library for exploratory data visualization",
    "content": "Create expressive charts with concise code Plot doesn’t have chart types. Instead, it has layered geometric shapes such as bars, dots, and lines. Scales map an abstract value such as time or temperature to a visual value such as position or color. Derive data on-the-fly while plotting, say to bin quantitative values or compute a rolling average. Small multiples facilitate comparison by repeating a plot across partitions of data. Plot supports GeoJSON and D3’s spherical projection system for geographic maps. Plot is built by the same team as D3. If you know some D3, you’ll be right at home with Plot. Visit D3 With Observable’s chart cell, quickly create plots with a GUI, then eject to code to customize. Try chart cell Plot is developed by Observable, the platform for collaborative data analysis. Visit Observable"
  },
  {
    "url": "https://vega.github.io/vega-lite/",
    "title": "A High-Level Grammar of Interactive Graphics | Vega-Lite",
    "content": "Vega-Lite is a high-level grammar of interactive graphics. It provides a concise, declarative JSON syntax to create an expressive range of visualizations for data analysis and presentation. Vega-Lite specifications describe visualizations as encoding mappings from data to properties of graphical marks (e.g., points or bars).\n    The Vega-Lite compiler automatically produces visualization components including axes, legends, and scales.\n    It determines default properties of these components based on a set of carefully designed rules.\n    This approach allows Vega-Lite specifications to be concise for quick visualization authoring, while giving user control to override defaults and customize various parts of a visualization.\n    As we also designed Vega-Lite to support data analysis, Vega-Lite supports both data transformations (e.g., aggregation, binning, filtering, sorting) and visual transformations (e.g., stacking and faceting).\n    Moreover, Vega-Lite specifications can be composed into layered and multi-view displays, and made interactive with selections.\n  \n\nGet startedLatest Version: 6.2.0\nTry online Compared to Vega, Vega-Lite provides a more concise and convenient form to author common visualizations. As Vega-Lite can compile its specifications to Vega specifications, users may use Vega-Lite as the primary visualization tool and, if needed, transition to use the lower-level Vega for advanced use cases. Check out the documentation and take a look at our example gallery. Follow us on Bluesky to stay informed about updates. With Vega-Lite, we can start with a bar chart of the average monthly precipitation in Seattle, overlay a rule for the overall yearly average, and have it represent an interactive moving average for a dragged region. Next step Vega-Lite is used by thousands of data enthusiasts, developers, journalists, data scientists, teachers, and researchers across many organizations. Here are some of them. Learn about integrations on our ecosystem page. The development of Vega-Lite is led by the alumni and members of the University of Washington Interactive Data Lab (UW IDL), including Kanit “Ham” Wongsuphasawat (now at Databricks), Dominik Moritz (now at CMU / Apple), Arvind Satyanarayan (now at MIT), and Jeffrey Heer (UW IDL). Vega-Lite gets significant contributions from its community. Please see the contributors page for the full list of contributors."
  },
  {
    "url": "https://d3js.org",
    "title": "D3 by Observable | The JavaScript library for bespoke data visualization",
    "content": "The JavaScript library for bespoke data visualization Create custom dynamic visualizations with unparalleled flexibility Create, update, and animate the DOM based on data without the overhead of a virtual DOM. Encode abstract data into visual values such as position, size, and color. Explain position encodings with axes. Render arcs, areas, curves, lines, links, pies, stacks, symbols… and any geometric primitive you might need to visualize data. Facilitate exploration with reusable interactive behaviors, including panning, zooming, brushing, and dragging. Treemaps, trees, force-directed graphs, Voronoi, contours, chords, circle-packing… a library of layout algorithms at the ready. More spherical projections than you can shake a stick at, with arbitrary aspects, adaptive sampling, and flexible clipping. CSV parsing, localized date parsing and formatting, color spaces, calendar math, statistics, and can I stop listing features now? The D3 team also builds Observable Plot, a high-level API for quick charts built on top of D3. Try Observable Plot D3 is developed by Observable, the platform for collaborative data analysis. Visit Observable"
  },
  {
    "url": "https://towardsdatascience.com/tag/data-analysis/",
    "title": "Data Analysis | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. A waterfall chart can be a powerful tool for conveying information. But it has some… Why are there so many hotels named after cities they are not in? Follow along… My lesson of how blindly over-controlling for noise can erase the effects you are measuring When you have a drill-through page that can be called from multiple pages, it could… Discover how POSET indicators transform data into coherent scoring systems, enabling meaningful comparisons while preserving… From architectural design to food security. Is there a way to use the out-of-the-box features of Power BI to be IBCS… Deep work, trends, data, and research Understand how to use Window Functions to perform calculations without losing details Playbook on how data analysts can become data scientists Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/tag/data-visualization/",
    "title": "Data Visualization | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Mariya Mansurova explains how hands-on learning, agentic AI, and engineering habits shape her writing and… How to evaluate classification models and understand which metric matters the most. An argument for human-led analysis (at least for now) Data storytelling can enlighten—but it can also deceive. When persuasive narratives meet biased framing, cherry-picked… Why are there so many hotels named after cities they are not in? Follow along… Learn three easy steps for gaining an intelligent picture for any project by using the… Using the Elastic Beanstalk service Learn how you can improve your machine learning models using effective data cleaning Analyse dynamic signals in a computer vision pipeline in Python using OpenCV and Rerun How to find machine learning projects that will get you hired. Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/tag/javascript/",
    "title": "JavaScript | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. This week, we celebrate recent articles that resonated the most with our community. Can Python really replace JavaScript for web development? Designing asynchronous pipelines for efficient data processing A step-by-step guide to training a BlazeFace model, from the Python training pipeline to the… Part of a series on how modern AI and other technologies could assist more efficient… From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… And why it feels like I’m switching to the Dark Side Things you should keep in mind before participating in your first #30DayChartChallenge and why you… To what extent are countries adopting data policies and systems for the public good? Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/tag/visualization/",
    "title": "Visualization | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Building a tool to interactively visualize the forward pass of any Pytorch model from within… Using the streamlit Python library Transform boring default Matplotlib line charts into stunning, customized visualizations How pre-attentive processing, Gestalt theory, and visual data encoding inform data design decisions So that it is understandable and engaging to (almost) everyone Simplify Geodata Extraction from OpenStreetMaps via the Overpass API How to identify and visualise clusters in knowledge graphs Create Visualizations at the Level of Leading Newspapers Discover how AI tools can transform intricate concepts into clear, practical frameworks and diagrams An overview of topic modeling global research through the OpenAlex API and visualizing results Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Ftowardsdatascience.com%2Fjavascript-for-data-analysis-2e8e7dbf63a7%2F&title=JavaScript%20for%20Data%20Analysis",
    "title": "LinkedIn Login, Sign in | LinkedIn",
    "content": "Stay updated on your professional world. By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Click on the link to sign in instantly to your LinkedIn account. If you don’t see the email in your inbox, check your spam folder. Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. LinkedIn\n              \n              © 2025"
  },
  {
    "url": "https://towardsdatascience.com/evaluating-cinematic-dialogue-which-syntactic-and-semantic-features-are-predictive-of-genre-2c69a71af6e2/",
    "title": "Evaluating Cinematic Dialogue - Which syntactic and semantic features are predictive of genre? | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. This article explores the relationship between a movie's dialogue and its genre, leveraging domain-driven data analysis and informed… From fragmented speech in thrillers to expletive-laden exchanges in action movies, can we guess a movie’s genre simply by knowing its semantic and syntactic characteristics in the dialogue? If so, which ones? We will investigate whether or not the nuanced dialogue patterns within a screenplay – its lexicon, structure, and pacing – can be powerful predictors of genre. The focus here is twofold: to leverage syntactic and semantic script characteristics as predictive features and to underscore the significance of informed feature engineering. One of the primary gaps in many data science courses is the lack of emphasis on domain expertise and feature generation, engineering, and selection. Many courses also provide students with pre-existing datasets, and sometimes, these datasets are already cleaned. Moreover, in the workplace, the rush to produce results often overshadows the process of hypothesizing and validating predictive features, leaving little room for domain-specific exploration and understanding. In my own experience outlined in \"Using Multi-Task and Ensemble Learning to Predict Alzheimer’s Cognitive Functioning,\" I witnessed the positive impact of informed feature engineering. Researching known predictors of Alzheimer’s allowed me to question the initial task and data, ultimately leading to the inclusion of key features during modeling. In this article, I delve into a project that examines movie dialogue to illustrate my approach to research and feature extraction. The focus will be on identifying and analyzing textual, semantic, and syntactic elements within film dialogue, investigating how they interrelate, and evaluating their capacity to accurately predict a movie’s genre. I like to start every project by conducting a literature review. I begin by jotting down relevant concepts and questions to guide my review. This initial phase is crucial and, depending on the time I have, I intentionally steer clear of research directly related to the modeling problem at hand. The goal is to understand the broader context and seek out supplemental information first. This strategy helps in cultivating an unbiased understanding of the subject matter, ensuring that my approach to the problem is informed, yet not prematurely narrowed by the solutions and methodologies already explored by others. There is a body of literature that explores the interplay between natural dialogue and our emotions. Screenwriters capture an emotion or mood by capitalizing on textual and syntactical relationships. These vary across genres since different moods are associated with different genres. We will extract and evaluate the 4 characteristics listed below. In each section, I’ll explain the rationale: The dataset used here is the Cornell Movie-Dialogs Corpus (MIT License) from Kaggle, which was originally retrieved from the ConvoKit toolkit (Chang et al., 2020). This is comprised of over 300k spoken lines ** across ~220k conversational exchanges derived from 61**7 different movies. We’ll begin by loading data using the movie_lines.txt file. The columns are split by +++$+++so this will be used as the separator to split each line, extract the columns, and read the data into a data frame. I used spaCy – an open-source natural language processing library written in Python and Cython – to process the text. This included cleaning contractions, removing punctuation, and lemmatizing words. In suspense movies, dialogue is often sparse, showcasing the link between syntax and emotions. When characters are in states of terror, their speech tends to be concise, while nervousness often leads to longer utterances (i.e. rambling), a trait more commonly seen in comedies. Therefore, we will examine the length attributes of each line in the corpus. In this section, we’ll take a look at: In the boxplot and the statistics data frame above, we see that: Less than half of the script lines maintain more than 1 sentence. This informs us that each script line is short, and should be framed accordingly. The metrics mentioned above were calculated on a ‘per line’ basis within the movie script data. In the next section, we shift our focus to explore the average length of lines per movie, allowing us to examine variations in word length at the movie level. The \"Length Features Statistics DataFrame\" figure shows that individual lines in scripts range from 0 to 582 words, with a median of 7 words, which suggests a high degree of variability in dialogue density on a line-by-line basis. In contrast, the aggregated movie data shows a much narrower range, with a maximum average of 38.69 words per line, indicating that while individual lines can be extremely verbose or concise, movies tend to balance out to a moderate density of words. With over 39% of script lines containing more than one sentence, the per-line analysis indicates a tendency towards compound or complex sentences. However, the tighter standard deviation in the movie averages (0.29 for sentences) suggests a consistency in narrative rhythm across different films, aiming for a steady pace in dialogue delivery. The contrast between the median length of individual lines (7 words) and the average across movies (11.36 words) implies that screenwriters might often intersperse shorter lines of dialogue with longer monologues or exchanges. This technique could be a deliberate choice to create dynamic interactions between characters, keep the audience engaged, and ensure that each movie has its unique tempo and style. The histograms show a right-skewed distribution, with a central tendency for movies to feature lines averaging 7–13 words. This skewness is indicative of a minority of films with unusually long lines, which heavily influence the overall average. After outliers are excluded, the bimodal distribution for words per line becomes more evident, suggesting that there are two common line lengths in scripts. This observation is interesting as it could reflect different styles or genres within the corpus. The distribution of sentences per line appears to be approximately normal, with a negligible right skew, indicating a consistent sentence structure across screenplays. There are various ways to represent a heightened state of emotion in a script. One of which is to use an exclamation point (!) for emphasis and another is to use CAPITALIZATION FOR EMPHASIS. We’ll look at the presence of both and see if there’s a correlation with the overarching sentiment. A hyphen placed at the end of a character’s dialogue (-) may signify an interruption in their speech or an abrupt pause in the character’s thinking (e.g., the character has an epiphany). It can also convey fragmented speech. I had no prior knowledge or intuition about the relationship between the presence of questions in a script and other features. However, the proportion of questions is easily measurable, and it could be intriguing to explore whether any patterns can be detected. Below, we see that the proportion of lines with questions, indicated at 31.4%, suggests a strong preference for interactive dialogue within movies. This is substantially higher than the proportion of lines with exclamations, at 8.9%, which could indicate that while intense emotional expressions are present, they are less frequent than interrogative exchanges. The boxplot for the count of all-caps words reveals that the use of capitalized words is not common, suggesting that screenwriters may prefer subtler methods of conveying emphasis in dialogue rather than relying on text formatting. While questions are more common, the range of usage varies widely among movies, potentially reflecting different genres or directorial styles. For example, a thriller may have more questions built into the dialogue to maintain suspense, whereas a comedy may use exclamations to highlight punchlines. The histogram for lines that end with a hyphen shows a significant skew towards a lower proportion, indicating that lines ending with a hyphen are relatively uncommon in movie scripts. This could suggest that interrupted dialogue or sentences leading into actions (which are often denoted by hyphens) are used sparingly, perhaps to maintain the flow of dialogue or to avoid overusing a device that might otherwise lose its impact. Part of speech helps us understand the grammatical function of a word in a sentence. For instance, genres like historical or biographical films are often flooded with proper nouns, making the tracking of these and other common tags potentially revealing. According to \"Judging Screenplays by Their Coverage\" by Stephen Follows and Josh Cockcroft, \"swear words (are) not spread equally across all scripts […] Comedies are the sweariest, beating Action and Horror scripts by a tiny margin (and) the genres featuring the lowest levels of swearing are Family, Animated and Faith-based scripts\" (42). We’ll start by taking a look at the most frequent tags from the text by flattening the text, taking a sample, and using SpaCy for POS tagging. Overall, nouns are by far the most common parts of speech, with adjectives and verbs maintaining relatively similar counts. Adverbs are the rarest part of speech for our movies. I chose to display all four histograms on this plot because it highlights a clear differentiation in the usage of various parts of speech within movie dialogues. Nouns dominate the linguistic landscape, occupying 40% to 60% of the dialogue whereas adverbs range anywhere between 0 to 10%. This prevalence underlines the concrete and tangible nature of film narratives, which often rely on specific nouns to anchor the conversation and set scenes. Adverbs, conversely, appear infrequently, suggesting that movie dialogue may favor direct and concise language over descriptive or qualifying phrases. We’ll detect profanity using the ‘badwords.txt’ from profanityfilter. While most movie lines are devoid of profanity, there is a significant presence of it in certain scripts, with a few reaching a proportion as high as 0.37. This might reflect the genre, setting, or character development choices, where profanity is used to add realism, and intensity, or to delineate characters’ personalities. We’ll utilize two sentiment analysis models: NLTK Vader, which is quick but uses a basic rule-based approach, and Flair, which is more accurate but computationally intensive. NLTK Vader assigns sentiment scores based on individual words and may be biased by neutral words even in the presence of strong negative words, making it less precise. It also struggles to identify sarcasm or context nuances. Flair is an embedding-based model which enables it to capture context. Words with similar vector representations are often used in the same context. The downside to using this approach is that it’s significantly slower than the naive, rules-based approach. The NLTK model took ~ 4 minutes to run while this model took ~3 hours to run. \"The Pearson correlation evaluates the linear relationship between two continuous variables. A relationship is linear when a change in one variable is associated with a proportional change in the other variable. The Spearman correlation evaluates the monotonic relationship between two continuous or ordinal variables. In a monotonic relationship, the variables tend to change together, but not necessarily at a constant rate.\" (source) In our analysis, we will use the Spearman correlation coefficient to identify a monotonic relationship between all values. Below displays only the significant correlations where the p-value for the Spearman correlation is less than 0.05. I expected to find some significant correlations, such as those between the average number of words and the average number of sentences or the average number of uppercase words. I’d also anticipated the following correlations: There were a few interesting observations: A significant positive correlation between the average number of words and the use of proper nouns (prop_noun) may also indicate that more complex dialogues include more specific references to entities or names, which could be characteristic of certain genres like science fiction or fantasy with complex world-building. As noted above, I was quite surprised to see a correlation between questions and profanity yet no relationship between exclamation marks and profanity. Therefore, I decided to plot out a slope graph to see if we could uncover any relationship there. Interestingly enough, despite there being no significant correlation between the proportion of exclamation and the proportion of profanity, it appears that the most significant jump between the proportion of profanity occurs from dialogue with no exclamation marks to dialogue with exclamation marks. I am going to fast-forward through this last part and provide a brief overview of the modeling process and performance. However, please feel free to let me know if you’d like a more in-depth exploration of the modeling work done here and I’ll release a part two 🙂 Here we are building a classifier to predict the genre of drama. To expedite the modeling phase, we utilized LazyPredict, an AutoML Python package that applies all of the common machine learning algorithms to a dataset and presents common metrics based on the task. We then performed hyperparameter tuning on the first 4 models: Classically, hyperparameter sweeps are run via grid search (brute force), where all possible combinations of hyperparameters are empirically evaluated for optimization. Given that the number of trials grows exponentially with every new hyperparameter, this is usually non-feasible. Another approach, random search, randomly combines hyperparameters, reaching a local optimum more efficiently than grid search if all combinations are not exhausted. Instead of either of these options, I will utilize Bayesian Optimization. This method constructs a Gaussian process to model the black-box function and search space. The overarching advantage is that we are converging to a local solution (like any ML model does) rather than shooting simply trying out different hyperparameters. The F1 score, a harmonic mean of precision and recall, serves as a key indicator of our model’s performance. Precision reflects the model’s reliability in correctly identifying a movie as belonging to the ‘drama’ genre, while recall measures the model’s ability to capture all relevant instances of drama movies. Considering the constraints, such as the absence of a fully developed pipeline for filtering low variance columns, addressing potential multi-collinearity, and a more extensive feature engineering process, the model demonstrated reasonable effectiveness. The following section will highlight the features that were most important for the model. This article was mainly focused on the process of feature generation and analyzing the data within the context of screenplays. However, if I wanted to work more on modeling, I’d focus on feature engineering, examine the effects of multi-collinearly, and spend more time on model selection. More specifically, I would: I hope you enjoyed this analysis and that this article showcased the potential of tailoring analyses to the unique characteristics of a field. While I focused on cinematic dialogue, the principles of domain-driven data analysis and modeling are universal. I encourage you to research your chosen domain, remain curious, and get creative with feature engineering during your next modeling task. I would also love to hear about your own experiences with interesting domain-driven analyses so feel free to write a comment here or email me at [email protected]. Thanks! Cornell Movie-Dialogs Corpus. Retrieved from https://www.kaggle.com/\nrajathmc/cornell-moviedialog-corpus/kernels. (originally retrieved from ConvoKit). Follows, S. (2019). Judging screenplays by their coverage. Retrieved from https://stephenfollows.com/wp-content/uploads/2019/01/Judging\nScreenplaysByTheirCoverage_StephenFollows_c.pdf Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Feature engineering, structuring unstructured data, and lead scoring An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/category/cinema/",
    "title": "Cinema | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. This article explores the relationship between a movie’s dialogue and its genre, leveraging domain-driven data… Plotting the ebbs and flows of cinema trends Practical insights on how to generate movie framelines and many other chromatic data-viz plots A colorful, data-driven journey across the chromatic world of movies Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/christabellecp/",
    "title": "Christabelle Pabalan, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. An article on the most common LLM development challenges I’ve encountered, effective mitigation strategies, and… A prompt template containing prompting techniques that have worked for me on over a dozen… Addressing the long tail problem and enhancing the recommendation experience for users on the Headspace… Network Analysis Spreading Information About Disease Prevention Imagine you’re a public health official tasked with… This article explores the relationship between a movie’s dialogue and its genre, leveraging domain-driven data… Refine hastily written code your future self will thank you for with ChatGPT and autoformatters. A tutorial on how to create slopegraph visualizations for assessing technological trend shifts, such as… An invitation to identify your repetitive EDA tasks and create an automated workflow, illustrated through… An examination of the factors that contribute to a good recommendation and long-term user retention Personal Tales Into Data Science In one of my previous articles, I detailed my experience… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/building-a-data-platform-in-2021-b759f6470426/",
    "title": "Building a Data Platform in 2021 | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. How to build a modern, scalable data platform to power your analytics and data science projects. Table of Contents: — The Platform — Integration — Data Warehouse — Transformation — Presentation — Transportation — Closing You know how the saying goes – \"There’s more than one way to skin a cat\" This is a tough metaphor for me to use as a proud cat parent, but the sentiment has never been more accurate when it comes to data in the 21st century. While it’s true that you can solve most of your data problems with a spreadsheet, a python script or a terminal command, problems emerge quickly as you start to consider scale, speed and consistency. Furthermore, the range of tools and processes in the data landscape has stifled collaboration and driven specialization in tool sets, rather than fostering a deep understanding of core data science concepts such as statistics, data modeling and effective visualization. Lucky for us, a consistent framework has begun to emerge. The nouveau approach to building a data platform is part do-it-yourself and part do-it-for-me. It involves quilting together managed services and engineering enough flexibility in your platform to anticipate the unknown. If done correctly, this modern infrastructure allows data professionals to focus on solving complex problems with math and science, rather than facilitating archaic processes that revolve around administration and documentation. One of the key concepts in this approach to building a modern data platform is modularization. There is no one vendor or technology that currently has domain over the entire data landscape, despite what clever marketing and sales campaigns present. Therefore, understanding each component is key to piecing together the right solutions for your specific project … The components are as follows: Let’s assume that the source component is obvious. Sources of data come in many shapes and sizes, and the integration layer should be pliable enough to account for all of them. On the DYI spectrum of this component are popular tools such as Airflow, which many companies use to build robust, end to end data pipelines. Other Apache offerings such as Kafka offer a more event-based approach to data integration and can be used in combination with Airflow to extend custom data pipelines even further. Managed services have come a long way in the integration space. Aside from enterprise grade versions of the aforementioned Apache projects such as Astronomer (Airflow) and Confluent (Kafka), there are several leaders in this space that offer flexibility, but are opinionated enough to help accelerate development in a meaningful way. From an event based perspective, Segment is the undeniable leader, while solutions such as Fivetran have emerged as the de facto solution for more traditional ETL/ELT based data integrations. Possibly the most ambiguous and most critical component of modern data platforms is the data warehouse. This is in part because legacy database technologies such as SQL Server, Postgres and MySQL are still extremely effective. However, the dominance of newcomers like Snowflake have highlighted clear path for the future. Cloud based data warehouses such as Snowflake, RedShift and BigQuery offer a multitude of benefits over their predecessors in the way they store, access and manage data. Regardless of which cloud based data warehouse you choose for your situation, the concept of partitioning this warehouse into different functional layers is still an evolving concept. Best practice is starting to emerge that suggests at least two distinct \"zones\" of your data warehouse; one that stores raw/unstructured data and another that stores normalized/transformed data. There is much room for debate on this topic, but the general benefit to having these two distinct zones is the ability to effectively manage ever-changing rules that transform raw data into digestible information. If the data warehouse component is the most critical piece of the modern data stack, then the transformation component is the most overlooked. Most projects tend to disperse transformations across business tools, visualization platforms and manual artifacts like spreadsheets, but centrally managing data transformations is a clearly identifiable attribute of mature data organizations. The idea of efficiently managing transformations first started to manifest in the mainstream as the battle between ETL and ELT. While it may seem pedantic, the simple reorganization of letters in a common acronym ushered in a brand new era that allowed non-data people to participate in building data products. This paradigm shift also gave new life to concepts like data governance and MDM, which rely heavily on input from business stakeholders. From a DIY perspective, Python reigns supreme, as it can easily manage simple SQL/task based transformations with modules like SQLAlchemy and Airflow, and is tailor made for more complex machine learning transformations fueled by Tensorflow, Scikit-learn and many more. From a managed service perspective it’s hard to find a better product than dbt. While it’s true that all of the major cloud providers (AWS, Microsoft, Google) have their own set of tools to manage transformations on their platform, dbt appears to be ahead of the pack from a platform agnostic standpoint. Up until now most of the components we have discussed are pure infrastructure. While it’s true most data analysts, engineers and scientists will be consuming content from the data warehouse and transformation components, the bulk of end users won’t see anything until it hits a dashboard in the presentation layer. The presentation component is a candidly vast category. Who’s to say that a Jupyter Notebook, which also includes elements of transformation, can’t also be used as a presentation tool? After all, Databricks has been very successful using this strategy as they seem poised to be one of the next big tech IPOs of the roaring 20s. From a historical perspective, visualization tools have dominated both the transformation and presentation categories, with tools like Looker, Power BI, Qlik, Sisense and Tableau proving that managing transformations and building beautiful visualizations are not mutually exclusive concepts. As the data stack continues to evolve I believe the champions in the presentation space will be those that double down on visualization capabilities and rely less on transformative ability. As organizations integrate more sources of data and data volumes exponentially increase, managing transformations at the presentation level will not only present a challenge but will create ill-defined information and inaccurate analysis. The consideration of a transportation component is what makes this approach uniquely modern. In the past it was acceptable that end users would consume information through dashboards and external analytics tools, but it is becoming increasingly apparent that unless data professionals can get their insights back into systems of record, their work may be all for naught. Sometimes referred to as \"embedded analytics\", the concept of data transportation is simple as it bridges the gap between data tools and systems of record (i.e customer relationship management, marketing automation, and customer success platforms) However, there are few managed services that have emerged to solve this problem effectively and even the ones that have emerged are still actively developing. Companies like Hightouch, Census and Syncari seem to be the first ones through the wall and are likely the only option for most projects unless they have copious amounts of developer resources and experience in automating information exchange. Even as I write this the data landscape is changing. Concepts around data platform observability and security are quickly becoming en vogue and companies are materializing overnight to solve these problems. With that in mind, understand that flexibility and agnosticism are the main takeaways from this message. Although it will happen, I am willing to bet that it will take several years before one vendor distills the entire data stack into one unified platform. So take this framework into the future with the understanding that you will have to change your thinking and accept new ideas on a daily basis. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… With demos, our new solution, and a video Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/category/data-science/analytics/",
    "title": "Analytics | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Retraining is easy; knowing when not to is the real challenge. In machine learning, performance… Explore the shift from static reports to intelligent apps with our first ebook. Learn the key concepts and reports of Google Analytics while practising with the platform A brief article on the Solar Cycles: data analysis and time series forecasting for the… Turning complex ML models into simple, interpretable rules with Human Knowledge Models for actionable insights… Your essential machine learning checklist to excel as a data scientist in analytics How can numerical user metrics be transformed into a personalized assessment of whether this behavior… Transform boring default Matplotlib line charts into stunning, customized visualizations Solving the knapsack problem In depth SQL code for creating your own statistical test design Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/data-dave/",
    "title": "Dave Melillo, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. How to build a modern, scalable data platform to power your analytics and data science… How to build a modern, scalable data platform to power your analytics and data science… Automate everything until only the fun stuff is left. Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/introduction-to-data-visualization-in-python-89a54c97fbed/",
    "title": "Introduction to Data Visualization in Python | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. How to make graphs using Matplotlib, Pandas and Seaborn Data visualization is the discipline of trying to understand data by placing it in a visual context so that patterns, trends and correlations that might not otherwise be detected can be exposed. Python offers multiple great graphing libraries that come packed with lots of different features. No matter if you want to create interactive, live or highly customized plots python has an excellent library for you. To get a little overview here are a few popular plotting libraries: In this article, we will learn how to create basic plots using Matplotlib, Pandas visualization and Seaborn as well as how to use some specific features of each library. This article will focus on the syntax and not on interpreting the graphs, which I will cover in another blog post. In further articles, I will go over interactive plotting tools like Plotly, which is built on D3 and can also be used with JavaScript. In this article, we will use two datasets which are freely available. The Iris and Wine Reviews dataset, which we can both load in using pandas read_csv method. Matplotlib is the most popular python plotting library. It is a low-level library with a Matlab like interface which offers lots of freedom at the cost of having to write more code. To install Matplotlib pip and conda can be used. Matplotlib is specifically good for creating basic graphs like line charts, bar charts, histograms and many more. It can be imported by typing: To create a scatter plot in Matplotlib we can use the scatter method. We will also create a figure and an axis using plt.subplots so we can give our plot a title and labels. We can give the graph more meaning by coloring in each data-point by its class. This can be done by creating a dictionary which maps from class to color and then scattering each point on its own using a for-loop and passing the respective color. In Matplotlib we can create a line chart by calling the plot method. We can also plot multiple columns in one graph, by looping through the columns we want and plotting each column on the same axis. In Matplotlib we can create a Histogram using the hist method. If we pass it categorical data like the points column from the wine-review dataset it will automatically calculate how often each class occurs. A bar chart can be created using the bar method. The bar-chart isn’t automatically calculating the frequency of a category so we are going to use pandas value_counts function to do this. The bar-chart is useful for categorical data that doesn’t have a lot of different categories (less than 30) because else it can get quite messy. Pandas is an open source high-performance, easy-to-use library providing data structures, such as dataframes, and data analysis tools like the visualization tools we will use in this article. Pandas Visualization makes it really easy to create plots out of a pandas dataframe and series. It also has a higher level API than Matplotlib and therefore we need less code for the same results. Pandas can be installed using either pip or conda. To create a scatter plot in Pandas we can call <dataset>.plot.scatter() and pass it two arguments, the name of the x-column as well as the name of the y-column. Optionally we can also pass it a title. As you can see in the image it is automatically setting the x and y label to the column names. To create a line-chart in Pandas we can call <dataframe>.plot.line(). Whilst in Matplotlib we needed to loop-through each column we wanted to plot, in Pandas we don’t need to do this because it automatically plots all available numeric columns (at least if we don’t specify a specific column/s). If we have more than one feature Pandas automatically creates a legend for us, as can be seen in the image above. In Pandas, we can create a Histogram with the plot.hist method. There aren’t any required arguments but we can optionally pass some like the bin size. It’s also really easy to create multiple histograms. The subplots argument specifies that we want a separate plot for each feature and the layout specifies the number of plots per row and column. To plot a bar-chart we can use the plot.bar() method, but before we can call this we need to get our data. For this we will first count the occurrences using the value_count() method and then sort the occurrences from smallest to largest using the sort_index() method. It’s also really simple to make a horizontal bar-chart using the plot.barh() method. We can also plot other data then the number of occurrences. In the example above we grouped the data by country and then took the mean of the wine prices, ordered it, and plotted the 5 countries with the highest average wine price. Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for creating attractive graphs. Seaborn has a lot to offer. You can create graphs in one line that would take you multiple tens of lines in Matplotlib. Its standard designs are awesome and it also has a nice interface for working with pandas dataframes. It can be imported by typing: We can use the .scatterplot method for creating a scatterplot, and just as in Pandas we need to pass it the column names of the x and y data, but now we also need to pass the data as an additional argument because we aren’t calling the function on the data directly as we did in Pandas. We can also highlight the points by class using the hue argument, which is a lot easier than in Matplotlib. To create a line-chart the sns.lineplot method can be used. The only required argument is the data, which in our case are the four numeric columns from the Iris dataset. We could also use the sns.kdeplot method which rounds of the edges of the curves and therefore is cleaner if you have a lot of outliers in your dataset. To create a histogram in Seaborn we use the sns.distplot method. We need to pass it the column we want to plot and it will calculate the occurrences itself. We can also pass it the number of bins, and if we want to plot a gaussian kernel density estimate inside the graph. In Seaborn a bar-chart can be created using the sns.countplot method and passing it the data. Now that you have a basic understanding of the Matplotlib, Pandas Visualization and Seaborn syntax I want to show you a few other graph types that are useful for extracting insides. For most of them, Seaborn is the go-to library because of its high-level interface that allows for the creation of beautiful graphs in just a few lines of code. A Box Plot is a graphical method of displaying the five-number summary. We can create box plots using seaborns sns.boxplot method and passing it the data as well as the x and y column name. Box Plots, just like bar-charts are great for data with only a few categories but can get messy really quickly. A Heatmap is a graphical representation of data where the individual values contained in a matrix are represented as colors. Heatmaps are perfect for exploring the correlation of features in a dataset. To get the correlation of the features inside a dataset we can call <dataset>.corr(), which is a Pandas dataframe method. This will give us the correlation matrix. We can now use either Matplotlib or Seaborn to create the heatmap. Matplotlib: To add annotations to the heatmap we need to add two for loops: Seaborn makes it way easier to create a heatmap and add annotations: Faceting is the act of breaking data variables up across multiple subplots and combining those subplots into a single figure. Faceting is really helpful if you want to quickly explore your dataset. To use one kind of faceting in Seaborn we can use the FacetGrid. First of all, we need to define the FacetGrid and pass it our data as well as a row or column, which will be used to split the data. Then we need to call the map function on our FacetGrid object and define the plot type we want to use, as well as the column we want to graph. You can make plots a lot bigger and more complicated than the example above. You can find a few examples here. Lastly, I will show you Seaborns pairplot and Pandas scatter_matrix , which enable you to plot a grid of pairwise relationships in a dataset. As you can see in the images above these techniques are always plotting two features with each other. The diagonal of the graph is filled with histograms and the other plots are scatter plots. Scraping Reddit data Data visualization is the discipline of trying to understand data by placing it in a visual context so that patterns, trends and correlations that might not otherwise be detected can be exposed. Python offers multiple great graphing libraries that come packed with lots of different features. In this article, we looked at Matplotlib, Pandas visualization and Seaborn. If you liked this article consider subscribing on my Youtube Channel and following me on social media. The code covered in this article is available as a Github Repository. If you have any questions, recommendations or critiques, I can be reached via Twitter or the comment section. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… With demos, our new solution, and a video Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/category/data-science/",
    "title": "Data Science | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Three pillars that shaped my first year in data science management - prioritization, empowerment, and recognition A waterfall chart can be a powerful tool for conveying information. But it has some… A common misconception about the working state of code in data, AI or software engineering… Generative Molecular Design (Part 1): common molecular representations in data science. Retraining is easy; knowing when not to is the real challenge. In machine learning, performance… How to evaluate classification models and understand which metric matters the most. In today’s fast-paced, distraction-heavy world, data literacy isn’t just about understanding charts or analyzing numbers—it’s… Why you should read this article Most data scientists whip up a Jupyter Notebook, play… Learn how to automate secure AWS infrastructure using Terraform — including VPC, public/private subnets, a… “Will that break a query folding?” “Does your query fold?”… Maybe someone asked you those… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/gilberttanner/",
    "title": "Gilbert Tanner, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. When thinking of data science and machine learning two programming languages, Python and R, immediately… Nobody can know everything but with help, we can overcome every obstacle. That’s exactly the… What is local model interpretation and how can it be used to gain insights from… What features are important and why Speeding up machine learning models in a small form factor Regardless of what problem you are solving an interpretable model will always be preferred because… Segment your images using the FastAI deep learning library Create deep learning models without writing code Learn how to work with multi-label data Use the FastAI deep learning library to classify images Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/seven-key-features-you-should-know-for-creating-professional-visualizations-with-plotly-f89558de5d0c/",
    "title": "Seven Key Features You Should Know for Creating Professional Visualizations with Plotly | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Create Visualizations at the Level of Leading Newspapers We are used to seeing interactive visualizations in online newspapers, and often we might wonder what tools data journalists use to create these professional-looking visualizations. The reality is that no special software is needed to create this type of visualizations; most interactive visualization libraries in Python are highly customizable, allowing for the creation of high-quality visualizations from a design perspective. In this article, we will show step-by-step how to customize an interactive visualization created in Plotly to significantly improve its design with just a few tweaks. For this article, we will use a simple visualization, such as the temporal evolution of a variable. The data to be visualized corresponds to the German population from the year 1800 to 2020. First, we will create the simplest possible visualization of the data with Plotly, without any customization. Subsequently, we will explain all the customizations used step by step, and the code for the improved visualization will be provided at the end of the article. The data used in the visualization can be obtained from the following page. Population of Germany 1800-2020 | Statista You can download the file with the data and run the code from this article on your computer to obtain the same results as shown in this article. Additionally, you can continue customizing to further improve the visualization. First, we download the data and store it in a Pandas DataFrame. We start with a simple visualization created in Plotly; in this case, the visualization consists of a line graph, with no additional customization. The goal of this article is to beautify the original visualization to make it look more professional, with a style similar to those visualizations you can find in newspapers such as the Financial Times. Next, we will explain step by step each technique that has been applied to transform the visualization and make it more visually appealing. Direct labels are a simple method to highlight specific information in a visualization. In this example, the goal is to highlight the beginning and the end of the German population over a period, from 1800 to 2020. Information for the remaining points can be observed using hoverovers. As shown in the code below, direct labels are implemented through annotations at the points of interest. Additionally, the y-axis labels have been removed to avoid overloading the visualization, as the values can be obtained from both the direct labels and also the hoverovers. When we create a visualization that reflects a temporal evolution, highlighting certain events on the graph is sometimes interesting to provide context. Sometimes, these events occur on a specific day and are therefore visualized using a vertical line; however, in other cases, the event occurs over a period of time and should be visualized using a shaded area. Visualizations in newspapers often have a title and a subtitle. One of them usually explains what is shown in the visualization, while the other conveys the key insight that the visualization provides. Annotations are used to create subtitles in the graph, as shown in the code below. The important part will be adjusting the x and y coordinates to place the annotation below the title. Another important aspect is to create the subtitles in a smaller font than the title, as shown in the visualization. Another feature that many visualizations in newspapers or magazines typically have is information about the data source, placed in a footer below the graph. This is done in the same way as the subtitle, using annotations. As before, finding the appropriate x and y coordinates, to correctly place the annotation, will be necessary. Another important aspect is the font size, which for the footer will generally be smaller than the one used for the subtitle. The default hoverovers are simply a tuple with the x and y values, as shown in the original image at the beginning of the article. These can be customized to create more detailed hoverovers. In this case, a hoverover with two lines has been created: one indicating the year and the other the population. This hoverover is much more professional than the one that was created by default. The default font in Plotly is Open Sans. This font looks very professional; however, there may be times when you need to customize your visualization with a different font type. I am a big fan of rounded fonts mainly because I think they look both professional and modern. For this reason, I have decided to implement a font called Rubik in the visualization. Poppins, Aptos, and Rubik are in my view, the most modern and professional fonts for visualizations. You can also try other fonts, depending on the needs or preferences of your particular project. One of the key principles for designing a good visualization is to use ink only to represent data. Therefore, a good visualization design should avoid the unnecessary use of ink. Following this guideline, we can implement changes in the visualization such as removing the background or the grids, as these elements do not provide any additional information. The following code shows all the previously implemented changes. I invite you to try additional changes according to your preferences. The truth is that Plotly is one of the Python libraries that offers higher quality and highly customizable visualizations. In addition, these visualizations can easily be used in web apps, for example, those made with Streamlit, or even embedded in Medium articles. In this article, we have covered seven features you can use to create professional visualizations, similar to those you might see in newspapers and online publications, using Plotly. With these simple methods, you can make your visualizations look much more professional. In the given example, they have been applied to a line chart. Still, the truth is that these features can be applied to a wide variety of visualizations, significantly improving their quality. I encourage you to put them into practice in your visualizations! Thanks for reading, Amanda Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. This article explores the relationship between a movie’s dialogue and its genre, leveraging domain-driven data… How to build a modern, scalable data platform to power your analytics and data science… How to make graphs using Matplotlib, Pandas and Seaborn Our weekly selection of must-read Editors’ Picks and original features Our weekly selection of must-read Editors’ Picks and original features Striking the balance between efficiency and engagement of your data-driven stories Insights from the NYC Public Hospital System’s Price Lists Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/amandaiglesiasmoreno/",
    "title": "Amanda Iglesias Moreno, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. A step-by-step guide to automating Jupyter Notebook execution and report generation using Python Learn how to create custom bump charts in Python using Plotly for data visualization Learn how to create custom waffle charts in Python using Plotly for data visualization Explore Overpass, Geoapify, Distancematrix.ai, Amadeus, and Mapillary for Advanced Mapping and Location Data Simplify Geodata Extraction from OpenStreetMaps via the Overpass API Create interactive calendars with heatmaps using Plotly Create Visualizations at the Level of Leading Newspapers In today’s market, buying electric cars represents an important challenge and a purchase decision that… Reflections on 4+ Years of Publishing Programming Articles Unlocking efficient text classification with pre-trained models: a case study using OpenAI’s GPT-3.5-turbo Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/global-data-barometer-whats-the-current-state-of-open-data-in-the-world-2841e80503c2/",
    "title": "Global Data Barometer: What's the Current State of Open Data in the World? | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. To what extent are countries adopting data policies and systems for the public good? I recently read a book on the adoption of open data policies in cities around the world. It’s called Beyond Transparency (publicly available at this link) and it consists of case studies showing the successes and obstacles of the Open Data policies in the early 2010s. As the title hints at, providing accessible and free datasets is a move towards a more transparent government, but not only. The book talks about how this data spurred innovation, improved government efficiency and encouraged new civic habits, such as more citizen participation. Data professionals, including engineers and data scientists, also formed their own solutions around these new datasets, building better models and apps. For some examples of these new civic tech ecosystems, check out the Building a Smarter Chicago chapter and the analytics work that arose from websites like Data SF or Chicago Data Portal! Another personal favorite chapter: The Data-Driven City, on how collecting 311 calls allows NYC to model emergency services and resource allocation. Since the 2010s, hundreds of countries have passed their own open data laws. The Global Data Barometer (license: Creative Commons Attribution 4.0) measures the state of open data in 109 of them, answering \"To what extent are countries managing data for the public good?\" The study uses a mix of quantitative indicators and qualitative descriptions to provide a clear picture of how the world is doing in terms of open data. This is an incredibly thorough work so I think it’s interesting to spend some time exploring part of it with this story. Quick note: for the story, I’m using Observable notebooks (JS-based) to do the visualizations and Jupyter for the data wrangling; links to both at the very end. Let’s get started! The overall countries’ index ranges from 0 (non-existent) to 100 (exhibiting best practices). The chart above shows quite a bit of spread. The highest scoring country (70) is the United States and the lowest scoring (10) is Turkmenistan, whereas the mean score is 38.51. This index is generated by examining each country’s open data practices across 4 fronts or \"pillars\": governance, capabilities, availability and use and impact. For each of these pillars, the countries provide information on existence of a particular element (e.g. a data protection framework), elements (quality related features and open data features), and extent (the limitations and applicability of a specific framework across a country). The study also tracks secondary indictors for each pillar, each of which was scored out of 100. Therefore, an overall index score of 100 would represent a type of \"normative ideal\" across all these primary and secondary indicators. Let’s focus on the countries whose index is around average (35 to 45). There’s 20 of them, including Albania and Kosovo, as well as countries from all over the world: Jamaica, Kazakhstan, Paraguay, Philippines, Peru, Thailand and South Africa among others. Despite overall index similarity, when looking into their mean scores across modules, their performance is not nearly as consistent. The Barometer compiles thematic or module scores in addition to pillars, and the 7 modules look into open data practices in the following areas: Health and COVID-19, Land, Public Finance, Procurement, Climate Action, Political Integrity and Company Information. Here’s a view at the Climate Action, Political Integrity and Procurement modules for these 20 countries: Many countries have inconsistencies: Making progress on all of these modules leads to a more transparent and efficient government. For instance, having better procurement data allows citizens to analyze where and how money gets spent in all stages of a project or push for more equitable allocations. Similarly, having transparent lobbying data and asset declarations allows for more accountability for political integrity. Climate information allows the public to stay informed on aspects such as biodiversity, emissions or vulnerabilities. Governance is one of major pillars of the dataset. The questions it answers involve assessing the state of policies and frameworks guiding how data is protected and managed. The research on governance tracks a couple of indicators: \"Open data policy\" being one of them, and \"data protection\", \"data sharing frameworks\", \"data management\" some of the others. Here’s how countries score on these governance indicators: A lot of countries are performing moderately well on metrics like Data Protection, Open Data Policy or Data Management. Around 92% of the surveyed policies have a common definition of open data, 72% of countries have some type of data management framework in place, and 90% of them offer data protection regulations (report here). Many of the countries offer partial or complete regulations on issues of data consent, rights of redress and access or correction. While forms of data governance frameworks exist, Global Open Data shows that countries still have severe limitations. For instance, only 24% of frameworks address issues of location information and only 31% address algorithmic decision making (also on the report). Most of these countries are in Europe and North America: these two regions comprise 17 out of the 23 countries answering \"Yes\" to \"Frameworks explicitly cover the protection of location-related data\", and 20 of the 31 whose \"framework addresses algorithmic decision making\". The last two indicators, Accessibility and Language coverage, evaluate the regulations for ensuring the data is accessible to people with disabilities and available in each country’s official language(s). The later is particularly important for countries with many such languages, but nevertheless also fractured: 13 of the 109 countries achieve a score of 100 on this category (possessing a framework with the force of law). The COVID-19 pandemic tested many of these data systems, especially those at the local level. The study measures data capacities not only on the availability of vaccination data, but also on real-time healthcare data (e.g. ICU beds) and vital statistics. Vital statistics include birth and mortality information, historical spans and how locally available this data is within a country. Here’s a heat map of how countries are doing: Vaccination data was largely available in most of the countries in the dataset, however not without its issues. Only about 50% of available datasets were broken down by age, and around 33% were disaggregated by sex (report statistics). Furthermore, real-time healthcare data was only available in around 50% of countries and even fewer published information on the number of available beds. For some of these countries, this type of data was only made available for the first time during the pandemic, which allows for countries to start building on it for better healthcare reporting in the future. And how easy was it for users to explore the data? A deeper look reveals that 61 out of the 109 countries didn’t offer official open tools that allowed citizens to access Vital Statistics data. Similarly, 63 out of the 109 didn’t provide official and accessible COVID-19 vaccination data. 57 countries also did not offer machine-readable data (such as CSV), which is important for easy distribution and reproduction. A major takeaway of the report was the relatively lacking open data environments at global level (recall the average overall index: 38/100). As we already saw in this story, having fully formed laws around open data is simply essential. In addition to the research being done by Global Data Barometer, there’s other repos that track new legislation around open data issues around the world. The State of Open Data by The Gov Lab is one of them. So, take a look at it for laws by sector or type of collaboration! Broadly, though, the Global Data Barometer report illuminated the practical challenges in adopting these laws, including data gaps or inaccessible or unavailable data. One striking example we looked at was the publication and management of health data, especially in emergency situations such as COVID-19 where having timely information is crucial. This example, however, also showed how new challenges can also spur data to promote transparency and allow citizens to stay informed which is very promising! Overall, the report points at very specific areas for each country to focus on, offering context-specific strategies for better data collection while still providing a big picture view of the current challenges with open data. Here are the notebooks (Jupyter and Observable), Thanks for reading! Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… With demos, our new solution, and a video Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/deabardhoshi/",
    "title": "Dea Bardhoshi, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Efficient geospatial manipulations for OSM map data Optimizing queries, improving runtimes and the geospatial data science applications To what extent are countries adopting data policies and systems for the public good? Using spatial data science to model populations + analysing educational equity in Tirana. Quantifying urban resilience in Tirana: power law distributions, self-organizing cities and sustainable population dynamics Analyzing street names according to historical periods and occupations! Data visualizations of street names in Tirana, Albania. I have recently been interested in making highly customizable visualizations. Here is one example of… Using unsupervised learning to model the optimal urban places for bike-share services Using data visualization principles to fix misleading and uninformative charts Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/charting-the-final-frontier-completing-the-30daymapchallenge-odyssey-bb68d4ab1285/",
    "title": "Charting the Final Frontier: Completing the #30DayMapChallenge Odyssey | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable Plot As the #30DayMapChallenge draws to a close, I am struck by the power of community and collaboration. Each day brought new themes, each theme brought new insights, and along the way, a tapestry of geo-visual stories emerged. The challenge was more than a test of skill; it was a celebration of shared knowledge and collective creativity. For me, the #30DayMapChallenge was more than just a commitment to craft a map a day; it was an exploration into the heart of data storytelling. Through this challenge, I have stitched together the fragmented stories of migration, intricate networks of transportation, and the pulse of urban life with every map I designed, and I am excited to share these with you. Along the way, I learned so much about geo-visualizations. Continuing the practice from the #30DayChartChallenge, I harnessed the simplicity of Observable Plot to breathe life into data. In my previous article, I shared my favorite visualizations from the first half of this challenge. In this concluding article, I will unveil some of the remaining collection of my geo-visual creations, each a unique blend of data, design, and discovery. You can find all of the visualizations along with data source and code in my #30DayMapChallenge collection. All images in this article were created by the author. Leveraging the detailed sighting records from ala.org.au, I sought to showcase Australia Numbat sightings through an intricate depiction of the habitats of one of Australia’s unique marsupials. In creating this map, the challenge lay in representing the spatial data in a manner that was both visually appealing and scientifically accurate. To achieve this, I superimposed the recorded sightings onto an outline of Australia, with bright points indicating the presence of numbats. The result is an informative and compelling illustration of the numbat’s now sparse distribution, allowing for quick visual assessment of population density and geographical spread. The background image of a numbat sourced from wikipedia.org was superimposed beautifully with the map of Australia to make the story even more appealing, powerful and connected. The comprehensive datasets from databank.worldbank.org provided a global perspective on migration, allowing me to thread together the routes that span continents and cultures. In this image, I captured the \"Flow\" theme with the fluidity and dynamics of global migration patterns from the United States to the top 10 countries in the year 2000. It uses arcing lines to represent the movement of people across the planet, with each curve originating from the US and reaching out to various countries around the world. This visualization not only reflects the physical movement from one nation to another but also encapsulates the transfer of cultures, ideas, and connections that are inherent to the migration process. For this theme I wanted to create stark visualization of traffic collisions in Seattle with data derived from a capstone project for an IBM course. I used a raster graphics approach to effectively plot each incident within the city’s confines, which inherently involves breaking down the image into a grid of pixels or points of color. Each collision event is represented as a pixelated dot, which emphasizes the granularity of the data. This method allows for the representation of a high level of detail and the creation of a density map that can reveal patterns not immediately apparent in other forms of data visualization. In the image series above, I explored the theme of \"Raster\" through a fascinating lens, displaying various interpolation methods available through Observable Plot, applied to Seattle traffic collision data. Each panel represents a distinct approach – default, nearest, barycentric, and random-walk – creating a spectrum of visuals from a singular dataset, each affecting the spatial distribution and visual texture of the data. The ‘default’ maintains the raw granularity of the data, while ‘nearest’ introduces a more segmented, almost cubist distortion. ‘Barycentric’ offers a prism-like refraction of the data points, and ‘random-walk’ creates a diffused, almost impressionistic, effect. It definitely felt like I created accidental art. In this image I wanted to present a sleek and minimalist portrayal of diamond production across the globe, symbolized by luminous points on a dark world map, with data from theglobaleconomy.com. To create a 3D effect, I plotted bright spots to mark significant locations of diamond extraction, shining against the stark black background of the world map, encapsulating the precious nature of diamonds themselves. For an added luminescent effect for these spots, I underlaid them with a density visualization. The rotation of the globe further enhances the 3D effect by the appearance and disappearance of these diamond production spots. I wanted to show a monochromatic interpretation of migration trends in Washington State from 2016 to 2020, using data from the United States Census Bureau. This choice of black and white not only conforms to the aesthetic theme but also serves to emphasize the dichotomy between the influx and outflow of people – white for net positive migration and black for net negative. The bold, contrasting spikes across the map are a graphical representation of the state’s demographic ebb and flow. Their varying lengths and color indicate the scale and direction of migration, with the two-tone scheme providing a clear visual differentiation that enhances comprehension. By creating a shadow for these spikes and the boundary of the States, I wanted to give it a 3D effect in addition to the monochromatic theme. The image is a striking visual tribute to \"My favorite…\" theme, showcasing the intricate and extensive railway network of Switzerland, which is renowned as the densest in Europe. I love how amazingly powerful the European rail network is which is certainly my favorite way to travel. Another favorite call out in this visualization for me is the type – Raster. Using a raster plot, this map truly makes the marvel of engineering and organization that the Swiss rail network represents shine through like a starry night. I learned how to make this during the #30DayMapChallenge, and fell in love with how powerful this type of plot can be to represent an intricate system of points. These maps have been more than a daily exercise; they have been a lens through which the complexity and beauty of our world are magnified. The second half of the challenge was as riveting as the first, each map a deeper foray into the confluence of data and design, story and space. From the endangered species to the veins of international migration, and from the movement of people to the tracks that guide them, the #30DayMapChallenge has been a profound exploration into how we visualize and interpret the spaces we inhabit. To those who have followed along, your curiosity and engagement have been the wind in the sails of this endeavor. For those who may tread this path in the future: let each theme challenge you, the community uplift you, and may every map you create be a landmark in your own journey of discovery. All the visualizations, complete with their underlying code and data, are available in my Observable collection for those eager to embark on their own mapping adventures. If you’d like, find me on Linkedin. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. To what extent are countries adopting data policies and systems for the public good? From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… Tutorial on how to use WebSockets to build real-time APIs in Go A tutorial stripping down low-level code that you can edit and run in your browser… Image from Unsplash AWS has a variety of services in ML/AI, Robotics, and IoT that… Lack of experience, rushed deadlines, or missed code reviews are just a few factors that… Photo by Mark Harpur on Unsplash There are adequate choices on the tools and workflows… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/category/javascript/",
    "title": "JavaScript | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Designing asynchronous pipelines for efficient data processing A step-by-step guide to training a BlazeFace model, from the Python training pipeline to the… Part of a series on how modern AI and other technologies could assist more efficient… From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… And why it feels like I’m switching to the Dark Side Things you should keep in mind before participating in your first #30DayChartChallenge and why you… To what extent are countries adopting data policies and systems for the public good? Enhance code quality with functions including Debounce, Once, and Memoize, all the way to Pipe,… Use the SOLID principles to design readable, scalable, and maintainable software Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/author/menghani-deepsha/",
    "title": "Deepsha Menghani, Author at Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. A detailed walkthrough on transforming simple chatbots into sophisticated AI assistants with long-term memory and… Step-by-Step Integration of AI Chatbots into Shiny for Python Applications: From API Setup to User… Step by step guide to EDA, feature engineering, cross validation and model comparison with tidymodels,… Step-by-step guide to implementing cross-validation, feature engineering, and model evaluation with XGBoost in Tidymodels Exploring the Differences and Use Cases of Shiny Core and Shiny Express for Python Guide to using the standardized syntax within Tidymodels to build and compare various models and… Step-by-step guide to creating “Who is the Goodest Doggy” Application with Shiny for Python, from… In-depth guide to learning how to build Shiny applications using modules. From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/navigating-the-cartographic-challenge-halfway-through-the-30daymapchallenge-ee96e02aaf95/",
    "title": "Navigating the Cartographic Challenge: Halfway Through the #30DayMapChallenge | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. From Haunted Locales to Political Patterns: A Mapping Journey Through Data and Design with Observable Plot Every map tells a story, and every story brings us closer to understanding our world. Visualizing the vastness of the world is no small feat. But to get a step closer to it, this November, I am participating in the #30DayMapChallenge. I have been enamored by geo visualizations from afar and dabbled a little myself, but I wanted to go deeper and learn how to build beautiful maps that tell stories. Hence, I participated in the #30DayMapChallenge. The #30DayMapChallenge is a community-driven event that unfolds each November. The idea is to create maps around different daily themes using the hashtag #30DayMapChallenge. There are no restrictions on the tools, technologies, or data used behind the maps. This challenge has provided the perfect opportunity for me to dive into the world of mapping (pun intended). We’re halfway through November, and my favorite part of this challenge has been learning from and being inspired by the community while challenging my creativity each day. In this article, I will share some of my favorite geo visualizations thus far that I’ve conjured up using Observable Plot, a JavaScript library for exploratory data visualization. You can find all of the visualizations along with data source and code in my #30DayMapChallenge collection. I used Observable Plot because I fell in love with the simplicity and ease of using it during the #30DayChartChallenge I participated in earlier this year. All images in this article were created by the author. October is over, but the hauntings are not. For this theme, I chose to plot haunted places in the United States. I sourced the data from the data.world website, one of my favorite sources for dataset discovery. This dataset includes the latitude and longitude of every reported haunted location, making it perfect for plotting as \"points.\" I used the Albers projection to plot the map of the USA and gave it a black background for a striking contrast effect. The main goal for this plot was to not only draw points at each location but to create a sort of glowing effect to align with my \"haunting\" theme. To achieve this, I layered each reported location with three points stacked on top of each other. First, a large \"red\" point with minimal opacity, then an \"orange\" point with a smaller radius and higher opacity, and finally, a \"white\" point with the smallest radius and highest opacity. These layered points created the effect of a white point with a fiery aura and a glowing effect. While I don’t think this exact combination would create a similar effect on every plot, the closeness of all points in this plot created the perfect effect I was looking to achieve. This plot definitely added many East Coast locations to my list of future travels, perhaps in this life or maybe as a ghost. My favorite way to navigate this country is by the Amtrak system, so for the theme of navigation, I decided to plot the Amtrak stations for the state with the most stations, California, with data from data.world. In visualizing California’s Amtrak station network, I aimed to highlight the connectivity and reach of rail travel within the Golden State. I chose a Mercator projection to accurately represent the network at the state level, with a color gradient background transitioning from the Pacific to the Southwest. Each station is marked by a ‘point,’ with major hubs distinguished by larger, more prominent markers. To further convey the spread and reach of the rail system, I overlaid the points with a Voronoi network diagram. This map serves not only as a tool for potential travelers but also as a testament to the infrastructure that facilitates movement across this vast state. Tackling Africa’s theme, I zoomed in on Uganda’s reported water sources, with data from the Water Point Data Exchange via TidyTuesday. The data provided a comprehensive list of water points, which I plotted using mercator projection to give an accurate representation of their distribution. The map’s dark background allows the water source points to stand out, drawing attention to regions with higher water source density. I plotted these locations as a Raster map to add a layer of texture. This map doesn’t just show locations; it tells a story of access, where the clustering of points can indicate areas of potential water stress or abundance. The US House Election Results map is a compact visualization of over five decades of political tides across the United States. I sourced the data from the MIT Election Data and Science Lab via TidyTuesday, providing detailed voting statistics by state. The challenge was to present this extensive dataset in a way that was both informative and engaging. I opted for a grid layout, with each state’s voting trends represented by mini bar charts that allow for immediate visual comparison across years and states. The color-coding is straightforward – blue for Democrats, red for Republicans, and grey for others – to provide clear insights into the political landscape at a glance. This map functions as a visual history of American politics, showing shifts and patterns that might not be apparent from numbers alone. For this theme, I found a powerful dataset sourced from movebank.org via data.world about Turkey vulture migration in South America. For this data visualization, I aimed to capture the dynamic nature of avian movement. The data included timestamped locations of tagged vultures, which allowed me to plot their journey over time. I chose a dark background to symbolize the vast area covered, with migration paths highlighted by a color gradient that easily allows the eyes to track the pattern. This map isn’t static; it’s designed to represent the ebb and flow of migration, with larger, more concentrated points possibly indicating key resting areas. It offers a glimpse into the patterns of nature and serves as a tool for conservationists tracking the health of these migratory routes. This is certainly one of my favorite animated visualizations I have created because of how easily it facilitates the observation of a clear pattern of migration. There is so much more that can be done with this data, and I feel I have barely scratched the surface. The choropleth map of Political Rights in Africa for 2020 is a sobering depiction of the continent’s diverse political landscape. Using data sourced from Freedom House via TidyTuesday, I aimed to illustrate the varying levels of political rights across the continent. The map uses shades of green to represent the gradations in political rights, with darker hues indicating greater freedom. This color choice is symbolic, with green being associated with ‘proceed’ in many cultures, suggesting areas where political rights are advancing. Beyond its aesthetic value, this map serves as an analytical tool, presenting a snapshot of a complex and evolving political climate. As we reach the midpoint of the #30daymapchallenge, it’s the perfect moment to pause and reflect on the journey thus far. Crafting maps each day, rather than charts, has been a different kind of exploration – one that merges the realms of data and geography, story and symbol. As I continue the second half of the challenge, I’m excited to delve deeper into the art of mapping, armed with the lessons learned so far. To future challengers: let each theme inspire you, learn from the community, and treat each map as a step in your data visualization journey. Once the full challenge is completed, I look forward to sharing an even richer collection of visualizations and the insights they have granted me. Until then, I encourage readers to dive into these datasets, to create and share their own interpretations, and to join in this global challenge of mapping. You can find the code and data to reproduce all visualizations in this article in my Observable collection. If you’d like, find me on Linkedin. Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. To what extent are countries adopting data policies and systems for the public good? From Urban Collisions to Global Connections: Unveiling the Full Spectrum of Geo-Visual Storytelling with Observable… Tutorial on how to use WebSockets to build real-time APIs in Go A tutorial stripping down low-level code that you can edit and run in your browser… Image from Unsplash AWS has a variety of services in ML/AI, Robotics, and IoT that… Lack of experience, rushed deadlines, or missed code reviews are just a few factors that… Photo by Mark Harpur on Unsplash There are adequate choices on the tools and workflows… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://bsky.app/profile/towardsdatascience.com",
    "title": "@towardsdatascience.com on Bluesky",
    "content": "This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is.\n    Learn more about Bluesky at bsky.social and atproto.com.\n    \nProfile\nTowards Data Science\ntowardsdatascience.com\ndid:plc:zcumilxztfgl3var46mgznnt\nThe world's leading publication for data science and artificial intelligence professionals.\n\nWebsite 🌐 towardsdatascience.com\nSubmit an Article ✍️ https://contributor.insightmediagroup.io\nSubscribe to our Newsletter 📩 https://bit.ly/TDS-Newsletter Learn more about Bluesky at bsky.social and atproto.com.\n    \nProfile\nTowards Data Science\ntowardsdatascience.com\ndid:plc:zcumilxztfgl3var46mgznnt\nThe world's leading publication for data science and artificial intelligence professionals.\n\nWebsite 🌐 towardsdatascience.com\nSubmit an Article ✍️ https://contributor.insightmediagroup.io\nSubscribe to our Newsletter 📩 https://bit.ly/TDS-Newsletter Towards Data Science towardsdatascience.com did:plc:zcumilxztfgl3var46mgznnt The world's leading publication for data science and artificial intelligence professionals.\n\nWebsite 🌐 towardsdatascience.com\nSubmit an Article ✍️ https://contributor.insightmediagroup.io\nSubscribe to our Newsletter 📩 https://bit.ly/TDS-Newsletter"
  },
  {
    "url": "https://towardsdatascience.com/about-towards-data-science-d691af11cc2f/",
    "title": "About Towards Data Science | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Our goal is to support the growth of data science and machine learning professionals Towards Data Science started in 2016 as a community-powered hub where data science and machine learning practitioners can share their knowledge and ideas with their peers – regardless of their location or background. This fundamental goal remains the same today. Over the years, we’ve grown into a publication with hundreds of active authors, a team of editors that ensure the quality of the articles we publish, and millions of readers and followers across our main site, newsletters, and social channels. We’re thrilled to be a leading hub for learning about data science, ML, AI, and adjacent topics, especially as developments in these fields affect an increasing number of people, communities, and societies around the world. We’re equally proud to be a welcoming and inclusive community, where authors and readers from all over the world, representing a wide range of opinions and experience levels, come together with a shared sense of mutual respect and a focus on professional and intellectual growth. In 2024, Insight Media Group acquired the Towards Data Science publication. The following year we launched our own self-hosted website and contributor platform, enabling us to host and pay contributors. We’ve evolved in many ways since our early days, and we’re sure that we’ll continue to grow alongside our community. What will never change is our mission to share the best writing we can find on the topics we cover. Thank you for joining us on this journey. For those of you who’d like to become more actively involved with our publication, we’re always happy to consider work from new authors who have insight and expertise to share around our core focus areas: data science, machine learning, AI, and related programming topics. You’ll find all the details and guidelines about submitting your work on the Write for Towards Data Science page. If you have any questions, feel free to reach out at [email protected]. If you’re interested in advertising opportunities, learn more here. For general questions and inquiries, you can contact us at [email protected]. Ludovic Benistant Publisher. Linkedin. [email protected] Ben Huberman Director of Content Operations. Linkedin. [email protected] Anne Bonner Editor. Linkedin. [email protected] Written By Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://contact.towardsdatascience.com/advertise-with-towards-data-science",
    "title": "Advertise with Towards Data Science",
    "content": "The best way to reach the data science, AI, and ML community Since 2016, TDS has been a community-powered hub where data science practitioners can share knowledge and ideas with their peers. Over the years, we’ve grown into a trusted publication with a thriving ecosystem of experts, a team of editors that ensure the quality of the content we publish, and millions of readers and followers.Reach decision-makers, practitioners, and learners in AI, machine learning, data engineering and analytics with strategic advertising that places your brand at the center of meaningful conversations. Our tailored marketing solutions ensure your message resonates with the right audience—driving engagement, influence, and results through original articles, ebooks, native advertising, display media, newsletter placements, social media amplification and so much more. Don’t miss the opportunity to connect with today's leaders and the next generation of data experts! Copyright © 2025 Insight Media Group, LLC. All Rights Reserved."
  },
  {
    "url": "https://towardsdatascience.com/privacy-policy/",
    "title": "Privacy Policy | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Insight Media Group Privacy Policy Effective Date: February 3, 2025 This Privacy Policy describes how we, Insight Media Group, LLC (referred to as “we” “our” or “us”), collect, use, and share your personal information. This Privacy Policy applies to any publication we offer as well as to information that we collect when you visit our websites (which include, but are not limited to: https://roadmap.sh/, https://towardsdatascience.com/, and www.thenewstack.io), interact with the features of our websites (including downloading eBooks, listening to content, submit contributions, sign up to attend events, fill out surveys or sign up for our newsletter), or when you interact with us through our email, advertising, or social media campaigns (collectively, our “Services”). By accessing or using our Services, you agree to this Privacy Policy. This Privacy Policy may change from time to time, please see Changes to this Privacy Policy below for more information. This Privacy Policy does not apply to employees or job applicants. Our Services are not intended for children under the age of 16. We do not knowingly collect personal information from children under 16.  If we learn that we have collected personal information from individuals under the age of 16, we will delete that information. Information We Collect We may collect the following types of personal information in the following ways: We collect the following categories of personal information and share personal information with the following categories of third parties: *Third parties to whom we sell or share your information, which is described further in the Right to Opt-Out of the Sale or Sharing of Personal Information to Third Parties subsection further below. Cookies & Automatic Tracking Technologies Like many websites and online Services, we use cookies, pixels, and other automatic tracking technologies to collect information about your use of our Services. Cookies are alphanumeric identifiers in the form of data files that are inserted and stored by your web browser on your computer’s hard drive. Through cookies or similar tracking technologies, we collect usage information like your interactions with our Services. We may associate this information with your IP address or other ID (e.g., device ID). Through automated tracking technology, we collect browser type and setting, time/date, language preferences, and device information, crash data, usage information, transaction information, links clicked, pages or content viewed, emails opened, and other similar kinds of information. We use this information to identify you and to analyze how you use our websites, to improve your experience, to remember your settings, to evaluate the performance of content, advertisements, or marketing campaigns, to tailor content, advertising or marketing to you, to evaluate or analyze trends, to help our advertising or affiliate program partners provide content or advertising that is relevant to you, to administer contests, sweepstakes, or promotions, for operational or security purposes such as to administer and deliver the websites or Services, or to improve our websites or Services. You can set your browser to refuse all or some browser cookies, or to alert you when cookies are being set. Social Media Through our Services, we may provide you with access to social media pages via icons on our websites, or you may interact with our social media pages. We also may use automatic tracking technology made available by third party social media platforms on our website. When you interact with us through social media, we receive the personal information that you share with us. If you click on a widget or social media feature, like the Twitter “tweet” button or Facebook “like” button, the third-party provider receives certain identifiers (such as device ID or IP address), internet or network activity, or other information you provide to us. They may receive this information from their own automatic tracking technologies or from our automatic tracking technologies. We use information collected from social media platform providers to allow you to use the features of the social media platform (e.g., to share content with your network), to track your engagement with our Site, our social media pages, or content (such as “likes”, “shares” etc.), to understand overall engagement, to communicate with you, to analyze trends and usage patterns, or to help us improve our website and the Services. We also use third-party social media tracking technologies, such as the LinkedIn and Meta pixel. These tracking technologies share information with the applicable social media platform provider, and they may use this information, and combine it with other information they collect about you (including to create profiles). As with other third-party websites, the information that you share with the social media platforms will be governed by the privacy policies and terms of service of the providers of the social media platform and not by the policies and procedures we describe here. How We Use the Information We Collect We use the personal information that we collect or receive: How We Share Your Personal Information We may share the personal information that we collect via the Services as follows: European Data Privacy Rights Data Subject Rights If you are a resident of the European Economic Area (“EEA”), the United Kingdom (“UK”), or Switzerland, you are entitled to certain rights. These rights include: To submit a request to exercise your rights, please contact us at [email protected]. If you submit a request related to the Services, we will direct you to submit a request to exercise your rights directly with our customer. We may have a reason under the law why we do not have to comply with your request or may comply with it in a more limited way than you anticipated. If we do, we will explain that to you in our response. We process personal information, or “Personal Data” as that term is defined in the EU General Data Protection Regulation, on the following legal bases: (1) with your consent; (2) as necessary to perform our agreement to provide Services; and (3) as necessary for our legitimate interests where those interests do not override your fundamental rights and freedom related to data privacy. We may also process personal information as necessary to comply with legal obligations. Users that reside in the EEA, UK, or Switzerland have the right to lodge a complaint about our data collection and processing actions with the supervisory authority concerned. Contact details for data protection authorities are available here. Cross-border Transfer of Data If you use our Services outside of the United States, you understand that we may collect, process, and store your information in the United States and other countries. The laws in the U.S. regarding information may be different from the laws of your state or country. Any such transfers will comply with safeguards as required by relevant law. To facilitate data transfers from the EU to other countries, we make use of the European Commission-approved standard contractual data protection clauses, the Data Privacy Framework, or other appropriate legal mechanisms to safeguard the transfer. Our standard contractual data protection clauses are available upon request via the email listed in the Contact Us section below. U.S. State Data Privacy Rights If you are a resident of a state that has passed data protection legislation, including California, Colorado, Connecticut, Delaware, Indiana, Montana, New Hampshire New Jersey, Oregon, Tennessee, Texas, Utah, Virginia, Minnesota or any other state that has passed similar legislation, you may have additional rights regarding our use of your personal information. The rights outlined in this section do not apply to information exempted under applicable state privacy law. Further, the rights described in this Section are not absolute, are subject to exceptions and limitations, and may not be afforded to residents of all states. In certain cases, we may decline requests to exercise these rights where permitted by law. Persons with disabilities may obtain this notice in alternative format upon request by contacting us at [email protected]. U.S. Privacy Rights To the extent you are provided additional privacy rights in the state you reside, you have the following rights with respect to the information that we collect (in each case, subject to applicable law): Right to Opt-Out of the Sale or Sharing of Personal Information to Third Parties You have the right to opt-out of our sale of your personal information or our sharing your personal information for behavioral advertising purposes. We use third-party analytics tools on our websites for cross-contextual behavioral tracking, please refer to the Information We Collect section above for more information. To exercise your right to opt-out of the sale of your personal information, please refer to our “Controlling Cookie Preferences” section.  If you have additional questions, please contact us using the information provided in the Contact Us section below. Exercising Your State Privacy Rights If you reside in a state that provides specific data privacy rights, you can exercise those rights by contacting us via phone or email as indicated in the Contact Us section below. Your rights may only be exercised by you or by your designated agent. You may submit a request to know twice within a 12-month period unless applicable data privacy law grants you additional rights. Your request must include enough information to allow us to reasonably verify that you are the person about whom we collected personal information or an authorized representative, which may include: (1) verifying your account information if you have an account with us; or (2) requesting two forms of identification that are reliable for verification purposes, unless the request includes sensitive information and, in which case, we may require additional verification. The information included in your request must allow us to properly understand, evaluate, and respond to it. We cannot respond to your request if we cannot verify your identity or authority to make the request and confirm that the personal information relates to you. If we cannot verify your identity or authority, we will not fulfill your request. We will only use personal information provided in the request to verify the requestor’s identity or authority to make it. You may submit a request through a designated agent. You must instruct that agent that they will need to state that they are acting on your behalf when making the request, have reasonably necessary documentation, and be prepared to provide the necessary personal information to properly verify your request. We will acknowledge receipt of your request. We will provide a substantive response within 45 calendar days or inform you of the reason and extension period (up to a total of 90 days) in writing. You may appeal our decision with respect to a request you have submitted by contacting us using the Contact Us section below. California Shine the Light Disclosure Under California Civil Code sections 1798.83- 1798.84, California residents are entitled to ask for a notice identifying the categories of personal customer data which we share with our affiliates and/or third parties for their own direct marketing purposes and providing contact information for such affiliates and/or third parties. If you are a California resident and would like a copy of this notice, please submit a written request to the address in the Contact Us section below. Nevada Resident Privacy Rights If you are a resident of Nevada, you have the right to opt-out of the sale of certain information to third parties who intend to license or sell that information. If we sell your information, you can exercise this right by contacting us by submitting a communication as directed in the Contact Us section below. If the request is via email, please include the subject line “Nevada Do Not Sell Request” and providing us with your name and the email address. Data Retention We retain your personal information for as long as we have an ongoing legitimate business need to do so (for example, to provide you with a service you have requested or to comply with applicable legal, tax, or accounting requirements). The criteria used to determine appropriate retention period for personal information include: Security We take security of your information seriously and take precautions, including technical, administrative, and physical measures, to safeguard your information collected through the Services. However, no method of online data transfer, processing or storage is 100% secure, and we do not guarantee the security of your personal information. Links The Services may contain links to third party sites and Services. We do not endorse and are not responsible for the privacy practices or the content of these third-party sites or Services. We exercise no control over how your information is stored, maintained, or displayed by third parties or on third-party sites. Your Choices We strive to provide you with choices regarding the personal information you provide to us. We have created mechanisms to provide you with the following control over your information: Changes to this Privacy Policy We may change this Privacy Policy at any time. If we change this Privacy Policy, we will notify you of the changes by posting them on this Privacy Policy under the Change Log section, by providing a pop-up notification, or by providing you with notice of the modification by email. We will indicate when such changes are effective by updating the effective date at the top of this Privacy Policy. By continuing to access or use the Services after we have posted a modification or have provided you with notice of a modification, you indicate your agreement to the modified Privacy Policy. If the modified Privacy Policy is not acceptable to you, please do not use the Services. Contact Us To contact us you may email us at [email protected] . You may also submit a request to the following address or call us at: For questions about our privacy practices, contact us via: Mail: Insight Media Group, LLC 548 Market St, PMB 42061 San Francisco, CA 94104 Email: [email protected] Phone Number: (844) 503-3213 Change Log Privacy Policy implemented as of February 3, 2025 Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://towardsdatascience.com/website-terms-of-use/",
    "title": "Website Terms of Use | Towards Data Science",
    "content": "Publish AI, ML & data-science insights to a global community of data professionals. Last Modified: February 7, 2025 Acceptance of the Terms of Use These terms of use are entered into by and between you and Insight Media Group, LLC (“TDS,” “we,” or “us” or “our”). The following terms and conditions, together with any documents they expressly incorporate by reference (collectively, “Terms of Use“), govern your access to and use of https://towardsdatascience.com, including any content, functionality, and services offered on or through https://towardsdatascience.com (the “Website“), whether as a guest or a registered user. Please read these Terms of Use carefully before you start to use the Website. By using the Website or by clicking to accept or agree to the Terms of Use when this option is made available to you, you accept and agree to be bound and abide by these Terms of Use  and our Privacy Policy, found at https://towardsdatascience.com/privacy-policy/, and incorporated herein by reference. In the event of a conflict between these Terms of Use and the Privacy Policy, the Privacy Policy shall govern. If you do not want to agree to these Terms of Use or the Privacy Policy, you must not access or use the Website. This Website is offered and available to users who are 13 years of age or older. If you are under the age of 13, you may not access or use the Website. Changes to the Terms of Use TDS may revise and update these Terms of Use from time to time in our sole discretion. All changes are effective immediately when we post them and apply to all access to and use of this Website thereafter. Your continued use of the Website following the posting of revised Terms of Use means that you accept and agree to the changes. You are expected to check this page from time to time so you are aware of any changes, as they are binding on you. Accessing the Website and Account Security TDS reserves the right to withdraw or amend the Website, and any service or material we provide on the Website, in our sole discretion without notice. TDS will not be liable, if for any reason, all or any part of this Website is unavailable at any time or for any period. From time to time, TDS may restrict user access, including registered user access, to some parts of this Website or the entire Website. You are responsible for both: To access this Website or some of the resources it offers, you may be asked to provide certain registration details or other information. It is a condition of your use of this Website that all the information you provide on this Website is correct, current, and complete. You agree that all information you provide to register with this Website or otherwise, including, but not limited to, through the use of any interactive features on the Website, is governed by our Privacy Policy at https://towardsdatascience.com/privacy-policy/, and you consent to all actions we take with respect to your information consistent with our Privacy Policy. If you choose, or are provided with, a username, password, or any other piece of information as part of our security procedures, you must treat such information as confidential, and you must not disclose it to any other person or entity. You also acknowledge that your account is personal to you and you agree not to provide any other person with access to this Website or portions of it using your username, password, or other security information. You agree to notify TDS immediately of any unauthorized access to or use of your username or password or any other breach of security. You also agree to ensure that you exit from your account at the end of each session. You should use particular caution when accessing your account from a public or shared computer so that others are not able to view or record your password or other personal information. TDS has the right to disable any username, password, or other identifier, whether chosen by you or provided by us, at any time in our sole discretion for any or no reason, including if, in our opinion, you have violated any provision of these Terms of Use. Intellectual Property Rights The Website, the articles, and its entire contents, features, and functionality (including but not limited to all information, software, text, displays, images, video, and audio, and the design, selection, and arrangement thereof) are owned by TDS, its licensors, or other providers of such material and are protected by United States and international copyright, trademark, patent, trade secret, and other intellectual property or proprietary rights laws. These Terms of Use permit you to use the Website for your personal, non-commercial use only. You must not reproduce, distribute, modify, create derivative works of, publicly display, publicly perform, republish, download, store, or transmit any of the material on this Website. If you print, copy, modify, download, or otherwise use or provide any other person with access to any part of this Website in breach of these Terms of Use, your right to use the Website will immediately be revoked and you must, at TDS’s option, return or destroy any copies of the materials you have made. No right, title, or interest in or to the Website or any content on the Website is transferred to you, and all rights not expressly granted are reserved by TDS. Any use of the Website not expressly permitted by these Terms of Use is a breach of these Terms of Use and may violate copyright, trademark, and other laws. Trademarks The Company name, the terms Towards Data Science, and all related names, logos, product and service names, designs, and slogans are trademarks of the Insight Media Group, LLC, TDS or its affiliates or licensors. You must not use such marks without the prior written permission of the TDS. All other names, logos, product and service names, designs, and slogans on this Website are the trademarks of their respective owners. Prohibited Uses You may use this Website only for lawful purposes and in accordance with these Terms of Use. You agree not to use this Website: Additionally, you agree not to: Comments and Messages This Website may allow you to leave comments or send message other visitors or the public (collectively, “Interactive Services“) that allow users to post, submit, publish, display, or transmit to other users or other persons (collectively, “Posts“) on or through this Website (excluding content that is submitted to TDS which is governed by the TDS Author Terms). All Posts must comply with these Terms of Use. Any Post you post to the Website will be considered non-confidential and non-proprietary. By providing any Posts, you grant Insight Media Group, LLC and our affiliates and service providers, and each of their and our respective licensees, successors, and assigns the right to use, reproduce, modify, perform, display, distribute, and otherwise disclose to third parties any such material for any purpose. You understand and acknowledge that you are responsible for any Posts you submit or contribute, and you, not TDS, have full responsibility for such content, including its legality, reliability, accuracy, and appropriateness. TDS is not responsible or liable to any third party for the content or accuracy of any Posts posted by you or any other user of the Website. Monitoring and Enforcement; Termination TDS has the right to: Without limiting the foregoing, TDS has the right to cooperate fully with any law enforcement authorities or court order requesting or directing us to disclose the identity or other information of anyone posting any materials on or through the Website. YOU WAIVE AND HOLD HARMLESS TDS AND ITS AFFILIATES, LICENSEES, AND SERVICE PROVIDERS FROM ANY CLAIMS RESULTING FROM ANY ACTION TAKEN BY ANY OF THE FOREGOING PARTIES DURING, OR TAKEN AS A CONSEQUENCE OF, INVESTIGATIONS BY EITHER TDS OR LAW ENFORCEMENT AUTHORITIES. However, TDS does not undertake to review all material before it is posted on this Website and cannot ensure prompt removal of objectionable material after it has been posted. Accordingly, we assume no liability for any action or inaction regarding transmissions, communications, or content provided by any user or third party. TDS has no liability or responsibility to anyone for performance or nonperformance of the activities described in this section. Content Standards These Content Standards apply to any and all Posts and use of Interactive Services. Posts must in their entirety comply with all applicable federal, state, local, and international laws and regulations. Without limiting the foregoing, Posts must not: Copyright Infringement In accordance with the Digital Millennium Copyright Act (“DMCA”) and other applicable law, TDS has adopted a policy of terminating, in appropriate circumstances, any user who is deemed to be a repeat infringer. TDS may also, in our sole discretion, limit access to the Website or Interactive Services if you infringe any intellectual property rights of others, whether or not you are considered a repeat infringement. If you believe that any content made available on the Website infringes upon any copyright which you own or control, you may file a notification of such infringement with our designated copyright agent as set forth below. ‍Towards Data Science 548 Market Street, PMB 50938 San Francisco, CA, US E-Mail Address: [email protected]. Your written notice must: (a) contain your physical or electronic signature; (b) identify the copyrighted work alleged to have been infringed; (c) identify the allegedly infringing material in a sufficiently precise manner to allow us to locate that material; (d) contain adequate information by which we can contact you (including postal address, telephone number, and email address); (e) contain a statement that you have a good faith belief that use of the copyrighted material is not authorized by the copyright owner, the copyright owner’s agent, or the law; (f) contain a statement that the information in the written notice is accurate; and (g) contain a statement, under penalty of perjury, that you are authorized to act on behalf of the copyright owner. Please do not send notices or inquiries unrelated to alleged copyright infringement to our designated copyright agent. Please see 17 U.S.C. §512(c)(3) for the requirements of a proper notification. If you believe in good faith that someone has wrongfully filed a notice of copyright infringement against you, the DMCA permits you to send us a counter-notice. Notices and counter-notices must meet the current statutory requirements imposed by the DMCA. If you knowingly misrepresent in your notification that the material or activity is infringing, you will be liable for any damages, including costs and attorneys’ fees, incurred by us or the alleged infringer as the result of our relying upon such misrepresentation in removing or disabling access to the material or activity claimed to be infringing. Reliance on Information Posted The information presented on or through this Website is made available solely for general information purposes. TDS does not warrant the accuracy, completeness, or usefulness of this information. Any reliance you place on such information is strictly at your own risk. TDS disclaims all liability and responsibility arising from any reliance placed on such materials by you or any other visitor to this Website, or by anyone who may be informed of any of its contents. This Website includes content provided by third parties, including materials provided by other users, bloggers, and third-party licensors, syndicators, aggregators, and/or reporting services. All statements and/or opinions expressed in these materials, and all articles and responses to questions and other content, other than the content provided by TDS, are solely the opinions and the responsibility of the person or entity providing those materials. These materials do not necessarily reflect the opinion of TDS. We are not responsible, or liable to you or any third party, for the content or accuracy of any materials provided by any third parties. Changes to the Website TDS may update the content on this Website from time to time, but its content is not necessarily complete or up-to-date. Any of the material on this Website may be out of date at any given time, and TDS is under no obligation to update such material. Information About You and Your Visits to the Website All information TDS collects on this Website is subject to our Privacy Policy https://towardsdatascience.com/privacy-policy/. By using this Website, you consent to all actions taken by TDS with respect to your information in compliance with the Privacy Policy. Linking to the Website and Social Media Features You may link to the TDS homepage, provided you do so in a way that is fair and legal and does not damage our reputation or take advantage of it, but you must not establish a link in such a way as to suggest any form of association, approval, or endorsement on our part without our express written consent. This Website may provide certain social media features that enable you to: You may use these features solely as they are provided by us and solely with respect to the content they are displayed with, and otherwise in accordance with any additional terms and conditions we provide with respect to such features. Subject to the foregoing, you must not: The website from which you are linking, or on which you make certain content accessible, must comply in all respects with the Content Standards set out in these Terms of Use. You agree to cooperate with TDS in causing any unauthorized framing or linking immediately to stop. TDS reserves the right to withdraw linking permission without notice. TDS may disable all or any social media features and any links at any time without notice in our discretion. Links from the Website If the Website contains links to other sites and resources provided by third parties, these links are provided for your convenience only. This includes links contained in advertisements, including banner advertisements and sponsored links. TDS has no control over the contents of those sites or resources and accept no responsibility for them or for any loss or damage that may arise from your use of them. If you decide to access any of the third-party websites linked to this Website, you do so entirely at your own risk and subject to the terms and conditions of use for such websites. Disclaimer of Warranties TO THE FULLEST EXTENT PROVIDED BY LAW, TDS WILL NOT BE LIABLE FOR ANY LOSS OR DAMAGE CAUSED BY YOUR USE OF THE WEBSITE OR ANY SERVICES OR ITEMS OBTAINED THROUGH THE WEBSITE OR YOUR DOWNLOADING OF ANY MATERIAL POSTED ON IT, OR ON ANY WEBSITE LINKED TO IT. YOUR USE OF THIS WEBSITE, ITS CONTENT, AND ANY SERVICES OR ITEMS OBTAINED THROUGH THIS WEBSITE IS AT YOUR OWN RISK. THIS WEBSITE, ITS CONTENT, AND ANY SERVICES OR ITEMS OBTAINED THROUGH THE WEBSITE ARE PROVIDED ON AN “AS IS” AND “AS AVAILABLE” BASIS, WITHOUT ANY WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED OR OTHERWISE, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR PARTICULAR PURPOSE. NEITHER TDS NOR ANY PERSON ASSOCIATED WITH TDS MAKES ANY WARRANTY OR REPRESENTATION WITH RESPECT TO THE COMPLETENESS, SECURITY, RELIABILITY, QUALITY, ACCURACY, OR AVAILABILITY OF THIS WEBSITE. WITHOUT LIMITING THE FOREGOING, NEITHER TDS NOR ANYONE ASSOCIATED WITH TDS REPRESENTS OR WARRANTS THAT THIS WEBSITE, ITS CONTENT, OR ANY SERVICES OR ITEMS OBTAINED THROUGH THIS WEBSITE WILL BE ACCURATE, RELIABLE, ERROR-FREE, OR UNINTERRUPTED, THAT DEFECTS WILL BE CORRECTED, THAT OUR SITE OR THE SERVER THAT MAKES IT AVAILABLE ARE FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS, OR THAT THIS WEBSITE OR ANY SERVICES OR ITEMS OBTAINED THROUGH THIS WEBSITE WILL OTHERWISE MEET YOUR NEEDS OR EXPECTATIONS. THE FOREGOING DOES NOT AFFECT ANY WARRANTIES THAT CANNOT BE EXCLUDED OR LIMITED UNDER APPLICABLE LAW. Limitation on Liability TO THE FULLEST EXTENT PROVIDED BY LAW, IN NO EVENT WILL TDS, ITS AFFILIATES, OR THEIR LICENSORS, SERVICE PROVIDERS, EMPLOYEES, AGENTS, OFFICERS, OR DIRECTORS BE LIABLE FOR DAMAGES OF ANY KIND, UNDER ANY LEGAL THEORY, ARISING OUT OF OR IN CONNECTION WITH YOUR USE, OR INABILITY TO USE, THIS WEBSITE, ANY WEBSITES LINKED TO IT, ANY CONTENT ON THIS WEBSITE OR SUCH OTHER WEBSITES, INCLUDING ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES, INCLUDING BUT NOT LIMITED TO, PERSONAL INJURY, PAIN AND SUFFERING, EMOTIONAL DISTRESS, LOSS OF REVENUE, LOSS OF PROFITS, LOSS OF BUSINESS OR ANTICIPATED SAVINGS, LOSS OF USE, LOSS OF GOODWILL, LOSS OF DATA, AND WHETHER CAUSED BY TORT (INCLUDING NEGLIGENCE), BREACH OF CONTRACT, OR OTHERWISE, EVEN IF FORESEEABLE. THE FOREGOING DOES NOT AFFECT ANY LIABILITY THAT CANNOT BE EXCLUDED OR LIMITED UNDER APPLICABLE LAW. Indemnification You agree to defend, indemnify, and hold harmless TDS, its affiliates, licensors, and service providers, and its and their respective officers, directors, employees, contractors, agents, licensors, suppliers, successors, and assigns from and against any claims, liabilities, damages, judgments, awards, losses, costs, expenses, or fees (including reasonable attorneys’ fees) arising out of or relating to your violation of these Terms of Use or your use of this Website, including, but not limited to, your Posts, any use of this Website’s content, services, and products other than as expressly authorized in these Terms of Use, or your use of any information obtained from this Website. Governing Law and Jurisdiction All matters relating to this Website and these Terms of Use, and any dispute or claim arising therefrom or related thereto (in each case, including non-contractual disputes or claims), shall be governed by and construed in accordance with the internal laws of the State of New York without giving effect to any choice or conflict of law provision or rule (whether of the State of New York or any other jurisdiction). Any legal suit, action, or proceeding arising out of, or related to, these Terms of Use or this Website shall be instituted exclusively in the federal courts of the United States or the courts of the State of New York, although TDS  retains the right to bring any suit, action, or proceeding against you for breach of these Terms of Use in your country of residence or any other relevant country. You waive any and all objections to the exercise of jurisdiction over you by such courts and to venue in such courts. Arbitration At TDS’s sole discretion, it may require you to submit any disputes arising from these Terms of Use or use of this Website, including disputes arising from or concerning their interpretation, violation, invalidity, non-performance, or termination, to final and binding arbitration under the Rules of Arbitration of the American Arbitration Association applying New Yokr law. Limitation on Time to File Claims ANY CAUSE OF ACTION OR CLAIM YOU MAY HAVE ARISING OUT OF OR RELATING TO THESE TERMS OF USE OR THIS WEBSITE MUST BE COMMENCED WITHIN ONE (1) YEAR AFTER THE CAUSE OF ACTION ACCRUES; OTHERWISE, SUCH CAUSE OF ACTION OR CLAIM IS PERMANENTLY BARRED. Waiver and Severability No waiver by TDS of any term or condition set out in these Terms of Use shall be deemed a further or continuing waiver of such term or condition or a waiver of any other term or condition, and any failure of TDS to assert a right or provision under these Terms of Use shall not constitute a waiver of such right or provision. If any provision of these Terms of Use is held by a court or other tribunal of competent jurisdiction to be invalid, illegal, or unenforceable for any reason, such provision shall be eliminated or limited to the minimum extent such that the remaining provisions of the Terms of Use will continue in full force and effect. Entire Agreement The Terms of Use, and our Privacy Policy constitute the sole and entire agreement between you and TDS regarding this Website and supersede all prior and contemporaneous understandings, agreements, representations, and warranties, both written and oral, regarding this Website. Your Comments and Concerns This website is operated by Insight Media Group, LLC. All notices of copyright infringement claims should be emailed to us at [email protected]. Contact Us Towards Data Science 548 Market Street PMB 50938 San Francisco, CA, US Google Voice: 707-653-5505 Toll-free: (844) 503-3215 Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Main_Page",
    "title": "Wikipedia, the free encyclopedia",
    "content": "2020 Missouri Amendment 2, also called the Medicaid Expansion Initiative, was a ballot measure to amend the Constitution of Missouri to expand Medicaid under the Affordable Care Act. The initiative was on the August 4, 2020, primary ballot and passed with 53.27% of the vote. Following Medicaid expansion initiatives in other states, Republican lawmakers in Nebraska and Utah added work requirements to their states' expansions; supporters aimed to prevent this by proposing state constitutional amendments for future Medicaid expansion initiatives. The measure was supported most in urban areas and opposed in rural areas. After a delay due to a lack of funding from the Missouri General Assembly and resulting litigation, the initiative was slowly implemented in October 2021. Republican lawmakers attempted to roll back the program and add a work requirement through a state constitutional amendment, which failed after the United States Supreme Court prevented its implementation. (Full article...) August 4 The Swedish pop group Tages released six studio albums and 26 singles in their home country during their existence from 1963 to 1970. Their professional career began during the summer of 1964, when they won a contest awarding them a recording contract with Platina Records, an independent record label. Their debut single, \"Sleep Little Girl\", was released in October 1964 and became a large hit in Sweden. The band's debut album, Tages, was released in November 1965, reaching the top 10 of the Finnish Albums Charts. The band's fourth and fifth albums, Contrast and Studio (both 1967), were released by Parlophone, whereas their sixth and final album, The Lilac Years (1969), was released through Fontana Records. The Lilac Years and the band's final three singles were released under the name Blond, which was considered more internationally viable by their management. (Full list...) The Cheat is a 1923 American silent drama film produced by Famous Players–Lasky and distributed by Paramount Pictures. It is a remake of Cecil B. DeMille's 1915 film The Cheat, using the same script by Hector Turnbull and Jeanie MacPherson. The remake stars Pola Negri and was directed by George Fitzmaurice,  and tells the story of Carmelita De Cordoba, a beautiful young South American woman who has been betrothed by her stern father to Don Pablo, whom she despises, and then meets and falls in love with Dudley Drake, a New York City broker. With no known prints of The Cheat remaining, it is considered a lost film, although there is an extant version in novel form, written in the same year as the film by Russell Holman, a Paramount Pictures employee. This color lithograph poster was produced in 1923 by Paramount to promote The Cheat, and depicts Negri as Carmelita with Charles de Rochefort as Claude Mace, an art swindler masquerading as the East Indian prince Rao-Singh. Poster credit: Paramount Pictures; restored by Ezarate Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects: This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Contents",
    "title": "Wikipedia:Contents - Wikipedia",
    "content": "Easily explore Wikipedia using the topic links below. You can also search directly using the search bar. All section headers are clickable for quick navigation. Wikipedia's content is divided into broad subject areas: Topics Types Places, people and times Indices"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Portal:Current_events",
    "title": "Portal:Current events - Wikipedia",
    "content": "Armed conflicts and attacks Business and economy Disasters and accidents International relations Armed conflicts and attacks Disasters and accidents Law and crime Armed conflicts and attacks Disasters and accidents Law and crime Politics and elections Sports Armed conflicts and attacks Business and economy International relations Law and crime Armed conflicts and attacks Arts and culture Disasters and accidents Health and environment International relations Law and crime Politics and elections Science and technology Armed conflicts and attacks Disasters and accidents International relations Science and technology Armed conflicts and attacks Disasters and accidents International relations Law and crime Politics and elections"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:Random",
    "title": "David Mendoza (footballer) - Wikipedia",
    "content": "David Bernardo Mendoza Ayala (born 10 May 1985), known as David Mendoza, is a Paraguayan football defender who currently plays for Guaireña. This biographical article related to a football midfielder from Paraguay is a stub. You can help Wikipedia by expanding it."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:About",
    "title": "Wikipedia:About - Wikipedia",
    "content": "Wikipedia is a free online encyclopedia that anyone can edit, and millions already have. Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information. Written collaboratively by volunteers known as Wikipedians, Wikipedia articles can be edited by anyone with Internet access, except in limited cases in which editing is restricted to prevent disruption or vandalism. Since its creation on January 15, 2001, it has grown into the world's largest reference website, attracting over a billion visitors each month. Wikipedia currently has more than sixty-five million articles in more than 300 languages, including 7,034,172 articles in English, with 107,267 active contributors in the past month. Wikipedia's fundamental principles are summarized in its five pillars. While the Wikipedia community has developed many policies and guidelines, new editors do not need to be familiar with them before they start contributing. Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies, including being verifiable by published reliable sources. Contributions based on personal opinions, beliefs, or personal experiences, unreviewed research, libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits. Wikipedia differs from printed references in important ways. Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism. Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes. For over 24 years, editors have volunteered their time and talents to create history's most comprehensive encyclopedia while providing references and other resources to researchers worldwide (see Researching with Wikipedia). In summary, Wikipedia has tested the wisdom of the crowd since 2001 and has found that it succeeds. To start editing simply click the Edit or Edit source button, or the pencil icon , at the top of any non-protected Wikipedia page or section."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Contact_us",
    "title": "Wikipedia:Contact us - Wikipedia",
    "content": "Introduction Readers\nHow to report a problem with an article, or find out more information. Article subjects\nProblems with articles about you, your company, or somebody you represent. Licensing\nHow to copy Wikipedia's information, donate your own, or report unlicensed use of your information. Donors\nFind out about the process, how to donate, and information about how your money is spent. Press and partnerships\nIf you're a member of the press looking to contact Wikipedia, or have a business proposal for us. Back to main page Thank you for your interest in contacting Wikipedia. Before proceeding, some important disclaimers: The links on the left should direct you to how to contact us or resolve problems. If you cannot find your issue listed there, you can email helpful, experienced volunteers at info-enwikimedia.org. Please refrain from emailing about disagreements with content; they will not be resolved via email."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Help:Contents",
    "title": "Help:Contents - Wikipedia",
    "content": "This page provides help with the most common questions about Wikipedia. Use the search box below, or browse the Help menu or the Help directory to search Wikipedia's help pages. For interactive assistance related to using and editing Wikipedia, see the help desk and the Teahouse. The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia. For simple searches, there is a search bar at the top of every page. Type what you are looking for in the box. Suggested matches will appear in a dropdown list. Select any page in the list to go to that page. Or, select the \"Search\" button, or press ↵ Enter, to go to a full search result. For advanced searches, see Help:Searching. There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations. For mobile access, press the mobile view link at the very bottom of every desktop view page. Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup. Be bold in improving articles! When adding facts, please provide references so others may verify them. If you are affiliated with the article subject, please see our conflict of interest guideline. The simple guide to vandalism cleanup can help you undo malicious edits. If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox. If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help. If you spot a problem with an article, you can fix it directly, by clicking on the \"Edit\" link at the top of that page. See the \"edit an article\" section of this page for more information. If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a \"Talk\" link at the beginning of every article page. You can contact us. If it's an article about you or your organization, see Contact us – Subjects. Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article. Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations). For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process. Answers to common problems can be found at frequently asked questions. Or check out where to ask questions or make comments. New users should seek help at the Teahouse if they're having problems while editing Wikipedia. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able. Or ask for help on your talk page and a volunteer will visit you there! You can get live help with editing in the help chatroom. For help with technical issues, ask at the Village pump. If searching Wikipedia has not answered your question (for example, questions like \"Which country has the world's largest fishing fleet?\"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need. Articles created without a category should be tagged with a maintenance tag. Use the {{Uncategorized}} tag to put articles in a maintenance category. Optionally add a date parameter like {{Uncategorized|date=August 2025}} to put articles in by-date maintenance categories. Such tagged articles are found at Uncategorized pages. You can try to categorize articles yourself. One useful technique is to follow links in the article to other similar articles and see how they are categorized, so you know what to copy. Search Frequently Asked Questions Search the help desk archives"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Help:Introduction",
    "title": "Help:Introduction - Wikipedia",
    "content": "Wikipedia is made by people like you. Get started\nPolicies and Guidelines Editing\nReferencing\nImages\nTables Editing\nReferencing\nImages\nTables Talk pages\nNavigating WikipediaManual of StyleConclusion View all as single page For more training information, see also:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Community_portal",
    "title": "Wikipedia:Community portal - Wikipedia",
    "content": "This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory. For a listing of ongoing discussions and current requests, see the Dashboard. Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements. Yearly or infrequent events Monthly or continuous events Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.\nPlease include your signature when adding a listing here. Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available. Weekly highlight Updates for editors Updates for technical contributors Meetings and events Tech news prepared by Tech News writers and posted by bot • Contribute • Translate • Get help • Give feedback • Subscribe or unsubscribe. Discussions in the following areas have requested wider attention via Requests for comment: You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.) Help counter systemic bias by creating new articles on important women. Help improve popular pages, especially those of low quality. This week's article for improvement is: Housing Previous selections:\nHistory of hide materials ·\nHuman behavior ·\nEternal Sunshine of the Spotless Mind This week's backlog of the week is: Category:Wikipedia pages about a contentious topic mislabelled as protected When you create an article through Wikipedia's Articles for Creation process, \nit creates a draft in the Drafts area. The purpose of AfC process is to help new editors learn how to write better articles. If accepted, your draft can be a valuable contribution to the encyclopedia. Wikipedia is over 17 years old and has well over five million articles. The vast majority of those articles never went through AfC which is only a few years old. AfC works as a peer review process in which registered editors can either help create an article submitted or decline the article because it is unsuitable for Wikipedia. To nominate an existing draft or user sandbox for review at Articles for Creation, add the code {{subst:submit}} to the top of the draft or sandbox page. The AfC process allows others to review the draft when you are ready, and also to create the article for you, if it is suitable."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard",
    "title": "Wikipedia:File upload wizard - Wikipedia",
    "content": "Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding. Uploads to Wikimedia Commons Upload a non-free file Uploads locally to the English Wikipedia; must comply with the non-free content criteria You do not have JavaScript enabled Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript. You are not currently logged in. Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again. Your account has not become confirmed yet. Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it. You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there. Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed. Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for # < > [ ] | : { } /  and ~~~. Your filename has been modified to avoid these. Please check if it is okay now. The filename you chose seems to be very short, or overly generic. Please don't use: A file of this name already exists on Commons! If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used. This should not be done, except in very rare exceptional cases. Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead. A file of this name already exists. If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to: It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, Wikimedia Commons.\nFiles uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. Please consider uploading your file on Commons. However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here. Please note that by \"entirely self-made\" we really mean just that. Do not use this section for any of the following: Editors who falsely declare such items as their \"own work\" will be blocked from editing. Use this only if there is an explicit licensing statement in the source. The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this. If the source website doesn't say so explicitly, please do not upload the file. Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it. Please remember that you will need to demonstrate that: This file will be used in the following article: Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the \"http://en.wikipedia.org/...\" URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. Example – article okay. This article doesn't exist! The article Example could not be found. Please check the spelling, and make sure you enter the name of an existing article in which you will include this file. If this is an article you are only planning to write, please write it first and upload the file afterwards. This is not an actual encyclopedia article! The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc. Please upload this file only if it is going to be used in an actual article. If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that. This is a disambiguation page! The page Example is not a real article, but a disambiguation page pointing to a number of other pages. Please check and enter the exact title of the actual target article you meant. If neither of these two statements applies, then please do not upload this image. This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like. In view of this, please explain how the use of this file will be minimal. Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then: Please don't upload it. Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader. In particular, please don't upload: If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you. This is the data that will be submitted to upload: Your file is being uploaded. This might take a minute or two, depending on the size of the file and the speed of your internet connection. Once uploading is completed, you will find your new file at this link: File:Example.jpg Your file has been uploaded successfully and can now be found here: File:Example.jpg Please follow the link and check that the image description page has all the information you meant to include. If you want to change the description, just go to the image page, click the \"edit\" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version. To insert this file into an article, you may want to use code similar to the following: If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the \":\" after the initial brackets!): See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages. Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:SpecialPages",
    "title": "Special pages - Wikipedia",
    "content": "This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators). You can also see what message names are used on a page by adding ?uselang=qqx to the end of its URL, e.g. https://en.wikipedia.org/wiki/Special:SpecialPages?uselang=qqx will show (specialpages-summary) in place of this message, which allows you to find MediaWiki:Specialpages-summary. For an index of special pages, see Help:SpecialPages."
  },
  {
    "url": "https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en",
    "title": "Make your donation now - Wikimedia Foundation",
    "content": "Thank you for considering a donation to the Wikimedia Foundation. We invite you to reflect on the number of times you visited Wikipedia in the last year. If the knowledge you gained here was valuable, please join the 2% of readers who donate. Any amount helps: $5, $20, $50, or whatever feels right to you today. The internet we were promised—a place of free, collaborative, and accessible knowledge—is under constant threat. On Wikipedia, volunteers work together to create and verify the pages you rely on, supported by tools that undo vandalism within minutes, ensuring the information you seek is trustworthy. If Wikipedia has given you useful knowledge this year, please give back. There are no small contributions: every edit counts, every donation counts. Thank you. Technology: Servers, bandwidth, maintenance, development. Wikipedia is one of the top 10 websites in the world, and it runs on a fraction of what other top websites spend. People and Projects: The other top websites have thousands of employees. Wikimedia Foundation has about 700 staff and contractors to support a wide variety of projects, making your donation a great investment in a highly-efficient not-for-profit organization. The Wikimedia Foundation is an international non-profit organization that supports local and independent associations around the world. Our tax-exempt status varies according to the laws of each country. Donations to the Wikimedia Foundation are likely not tax-deductible outside the USA. If you have any questions about tax exemptions or reductions, we invite you to contact donate@wikimedia.org. We do not sell or trade your information to anyone. By donating, you agree to share your personal information with the Wikimedia Foundation, the nonprofit organization that hosts Wikipedia and other Wikimedia projects, and its service providers pursuant to our donor policy. Wikimedia Foundation and its service providers are located in the United States and in other countries whose privacy laws may not be equivalent to your own. For more information please read our donor policy. For recurring donors, fixed monthly payments will be debited by the Wikimedia Foundation on the monthly anniversary of the first donation, until such time as you notify us to discontinue them. Donations initiated on the 29, 30, or 31 of the month will recur on the last day of the month for shorter months, as close to the original date as possible. For questions, please contact donate@wikimedia.org."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:MyContributions",
    "title": "User contributions for 37.61.125.86 - Wikipedia",
    "content": "This user or IP address is currently globally blocked.\nIf the block is marked as locally disabled, this means that it applies on other sites, but a local administrator has decided to disable it on this wiki.\nThe global block log entry is provided below for reference:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:MyTalk",
    "title": "User talk:37.61.125.86 - Wikipedia",
    "content": "People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using. Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address."
  },
  {
    "url": "https://af.wikipedia.org/wiki/Diepleer",
    "title": "Diepleer - Wikipedia",
    "content": "Diepleer is deel van 'n breër familie van masjienleermetodes, wat gebaseer is op kunsmatige neurale netwerke met voorstellingsleer. Die byvoeglike naamwoord \"diep\" in diepleer verwys na die gebruik van veelvuldige lae in die netwerk. Metodes wat gebruik word kan óf onder toesig, semi-toesig óf sonder toesig wees.[1] Diepleer argitekture soos diep neurale netwerke, diep geloof netwerke, diep versterking leer, herhalende neurale netwerke, konvolusionele neurale netwerke en transformators is toegepas op velde insluitend rekenaarvisie, spraakherkenning, natuurlike taalverwerking, masjienvertaling, bioinformatika, medisyne ontwerp, mediese beeldanalise, klimaatwetenskap, materiaalinspeksie en bordspeletjieprogramme, waar hulle resultate opgelewer het wat vergelykbaar is met en in sommige gevalle menslike deskundige prestasie oortref.[2][3][4] Kunsmatige neurale netwerke (KNN'e) is geïnspireer deur inligtingverwerking en verspreide kommunikasienodes in biologiese stelsels. KNN'e het verskillende verskille van biologiese breine. Spesifiek, kunsmatige neurale netwerke is geneig om staties en simbolies te wees, terwyl die biologiese brein van die meeste lewende organismes dinamies (plasties) en analoog is.[5][6] Diepleer is 'n klas masjienleeralgoritmes wat[7](pp199–200)  veelvuldige lae gebruik om geleidelik hoërvlakkenmerke uit die rou insette te onttrek. Byvoorbeeld, in beeldverwerking kan onderste lae rande identifiseer, terwyl hoër lae die konsepte kan identifiseer wat relevant is vir 'n mens, soos syfers of letters of gesigte. Vanuit 'n ander hoek om diepleer te beskou, verwys diepleer na \"rekenaar-simuleer\" of \"outomatiseer\" menslike leerprosesse vanaf 'n bron (bv. 'n beeld van honde) na 'n aangeleerde voorwerp (honde). Daarom maak 'n idee wat geskep word as \"dieper\" leer of \"diepste\" leer[8] sin. Die diepste leer verwys na die ten volle outomatiese leer van 'n bron na 'n finale geleerde objek. 'n Dieper leer verwys dus na 'n gemengde leerproses: 'n menslike leerproses van 'n bron na 'n aangeleerde semi-objek, gevolg deur 'n rekenaarleerproses van die menslike aangeleerde semi-objek tot 'n finale geleerde objek."
  },
  {
    "url": "https://ar.wikipedia.org/wiki/%D8%AA%D8%B9%D9%84%D9%85_%D9%85%D8%AA%D8%B9%D9%85%D9%82",
    "title": "تعلم متعمق - ويكيبيديا",
    "content": "التعلّم المُتعمّق أو التعلّم العميق[1] هو مجال بحث جديد يتناول إيجاد نظريات وخوارزميات تتيح للآلة أن تتعلم بنفسها عن طريق محاكاة الخلايا العصبية في جسم الإنسان.[2][3] وأحد فروع العلوم التي تتناول علوم الذكاء الاصطناعي.[4] يعد من فرع من فروع علوم التعلم الآلي،[5] تركز معظم أبحاث التعلم المتعمق على إيجاد أساليب استنباط درجة عالية من المتجردات بتحليل مجموعة بيانات ضخمة، [6][7] باستخدام متحولات خطية وغير خطية.[8][9] تُشير صفة \"عميق\" إلى استخدام طبقاتٍ مُتعددةٍ (تتراوح من ثلاث طبقاتٍ إلى عدة مئاتٍ أو آلاف) في الشبكة.[10] يُمكن أن تكون الأساليب المُستخدمة إما مُراقبةً أو شبه مُراقبةٍ أو غير مُراقبة.[11][12] تشمل بعض بنى شبكات التعلم العميق الشائعة الشبكات المُتصلة بالكامل، وشبكات المعتقدات العميقة، والشبكات العصبية الالتفافية، والشبكات العصبية المُتكررة، والشبكات التوليدية التنافسية، والمُحولات، وحقول الإشعاع العصبي.[13] طُبقت هذه البنى على مجالاتٍ تشمل الرؤية الحاسوبية، والتعرف على الكلام، ومعالجة اللغة الطبيعية، والترجمة الآلية، والمعلوماتية الحيوية، وتصميم الأدوية، وتحليل الصور الطبية، وعلم المناخ، وفحص المواد، وبرامج ألعاب الطاولة، حيث أنتجت نتائج مُقارنةً بِأداء الخبراء البشرِ، وفي بعض الحالات تتجاوزُه.[14][15] استُلهِمَت الأشكال المُبكرة من الشبكات العصبية من مُعالجة المعلومات وعُقَد الاتصال المُوزعة في الأنظمة البيولوجية، وخاصةً الدماغ البشري. ومع ذلك، لا تهدف الشبكات العصبية الحالية إلى نمذجة وظيفة الدماغ لِلكائنات الحية، ويُنظر إليها عمومًا على أنها نماذج ذات جودةٍ مُنخفضةٍ لِهذا الغرض.[16] يمكن توصيف أي كائن بطرق عديدة متنوعة. مثلا، يمكن توصيف صورة ما على اساس متجهي لدرجة الضياء في كل وحدة بكسل أو بطريقة متجردة على اساس مجموع الحواف والمناطق التي تشكل الصورة. هناك العديد من الاساليب الأخرى التي يمكن استعمالها لتوصيف هذه الصورة. وتشير الدراسات ان بعض هذه الاساليب هي أفضل من غيرها في تبسيط تعلم الآلة (مثل ملاحظة الوجه أو ملاحظة التعابير).[17] ومن الاهداف المتوقعة في دراسة التعلم المتعمق هو استبدال ميزات التعلم الالي التي يتم تحديدها بشريا بميزات يتم انتاجها بواسطة الآلة نفسها عن طريق خوارزميات فعالة في استنباط الميزات بصورة ألية أو نصف آلية.[18] تعتمد ابحاث التعلم المتعمق على الاكتشافات في علوم الاعصاب بشكل كبير وخاصة في مجال فهم العمليات الترميزة التي يقوم بها النظام العصبي في تحديد العلاقات المختلفة بين المحفزات والنشاطات الدماغية.[19] تستند مُعظم نماذج التعلم العميق الحديثة إلى شبكات عصبية مُتعددة الطبقات مثل الشبكات العصبية التلافيفية والمُحولات، على الرغم من أنها قد تتضمن أيضًا صيغًا اقتراحيةً أو مُتغيرات كامنةً مُنظمةً في طبقات في نماذج توليدية عميقة مثل العُقَد في شبكات الاعتقاد العميق وآلات بولتزمان العميقة.[20] بشكل أساسي، يُشير التعلم العميق إلى فئة من خوارزميات التعلم الآلي حيث يتم استخدام تسلسل هرمي من الطبقات لِتحويل بيانات الإدخال إلى تمثيل أكثر تجريدًا وتركيبًا بشكل طفيف. على سبيل المثال، في نموذج التعرف على الصور، قد يكون الإدخال الخام عبارةً عن صورة (مُمثلةٌ كمُوتر من البكسلات). قد تُحاول طبقة التمثيل الأولى تحديد الأشكال الأساسية مثل الخطوط والدوائر، وقد تُؤلف طبقة التمثيل الثانية ترتيبات الحواف وتُشفرها، وقد تُشفر طبقة التمثيل الثالثة أنفًا وعينين، وقد تُدرك طبقة التمثيل الرابعة أن الصورة تحتوي على وجه. الأهم من ذلك، أن عملية التعلم العميق يُمكنُها تعلم الميزات التي يجب وضعُها على النحو الأمثل في أي مستوى بمفردها. قبل التعلم العميق، غالبًا ما كانت تقنيات التعلم الآلي تنطوي على هندسة ميزات يدوية لِتحويل البيانات إلى تمثيل أكثر مُلاءمةً لِخوارزمية التصنيف لِلعمل عليها. في نهج التعلم العميق، لا يتم تصميم الميزات يدويًا، ويَكتشف النموذج تمثيلات الميزات المُفيدة من البيانات تلقائيًا. هذا لا يُلغي الحاجة إلى الضبط اليدوي؛ على سبيل المثال، يُمكن أن تُوفر أعدادٌ مُتفاوتةٌ من الطبقات وأحجام الطبقات درجات مُختلفةً من التجريد.[21][22] تُشير كلمة \"عميق\" في \"التعلم العميق\" إلى عدد الطبقات التي يتم من خلالِها تحويل البيانات. بِشكل أكثر دقةً، تتمتع أنظمة التعلم العميق بِعمق كبير في مسار تخصيص الرصيد (CAP). مسار تخصيص الرصيد هو سلسلة التحولات من الإدخال إلى الإخراج. يصف مسار تخصيص الرصيد الاتصالات السببية المُحتملة بين الإدخال والإخراج. لِشَبكة عصبية للتغذية الأمامية، فإن عمق مسارات تخصيص الرصيد هو عمق الشبكة وهو عدد الطبقات المخفية زائد واحد (حيث يتم تحديد معلمات طبقة الإخراج أيضًا). بالنسبة لِلشبكات العصبية المُتكررة، التي قد ينتشر فيها إشارةٌ عبر طبقة أكثر من مرة، فإن عمق مسار تخصيص الرصيد غير محدود مُحتملًا.[23] لا يوجد حدٌ مُتفقٌ عليه عالميًا للعمق يفصل التعلم الضحل عن التعلم العميق، لكن مُعظم الباحثين يتفقون على أن التعلم العميق ينطوي على عمق مسار تخصيص الرصيد أعلى من اثنين. لقد ثبت أن مسار تخصيص الرصيد ذي العمق اثنين هو مُقربٌ شاملٌ بِمعنى أنه يُمكنُه مُحاكاة أي دالة.[24] بعد ذلك، لا تُضيف المزيد من الطبقات إلى قدرة مُقرب الدالة لِلشبكة. النماذج العميقة (مسار تخصيص الرصيد > اثنين) قادرةٌ على استخراج ميزات أفضل من النماذج الضحلة، وبالتالي، تُساعد الطبقات الإضافية في تعلم الميزات بفعالية. يُمكن بناء بنى التعلم العميق باستخدام خوارزمية طبقة تلو الأخرى.[25] يُساعد التعلم العميق على فك تشابك هذه التجريدات واختيار الميزات التي تُحسن الأداء.[4] يُمكن تطبيق خوارزميات التعلم العميق على مهام التعلم غير المُراقب. تُعد هذه فائدةً مهمةً لأن البيانات غير المُعلمة أكثر وفرةً من البيانات المُعلمة. من أمثلة البُنى العميقة التي يُمكن تدريبُها بطريقة غير مُراقبة شبكات الاعتقاد العميق.[4][26] تم تقديم مُصطلح التعلم العميق إلى مجتمع التعلم الآلي بواسطة رينا ديختر في عام 1986، [27] وإلى الشبكات العصبية الاصطناعية بواسطة إيغور آيزنبرغ وزملائه في عام 2000، في سياق الخلايا العصبية ذات عتبة القيمة المنطقية.[28][29] على الرغم من أن تاريخ ظهورِه يبدو أكثر تعقيدًا.[30] تفسر الشبكات العصبية العميقة عمومًا من حيث مبرهنة التقريب العام [31][32] أو استدلال بايزي.[33] تتعلق نظرية التقريب الشاملة الكلاسيكية بقدرة الشبكات العصبية للتغذية الأمامية ذات طبقة واحدة مخفية ذات حجم محدود على تقريب الدوال المستمرة.[32][34] في عام 1989، نشر جورج سيبيكو أول دليل لدوال التنشيط السينية، [35] وتم تعميمه على بنى متعددة الطبقات للتغذية الأمامية في عام 1991 بواسطة كورت هورنيك.[31] أظهر العمل الأخير أيضًا أن التقريب الشامل ينطبق أيضًا على دوال التنشيط غير المحدودة مثل وحدة كونييهيكو فوكوشيما الخطية المصححة.[36][37] تتعلق نظرية التقريب الشاملة للشبكات العصبية العميقة بقدرة الشبكات ذات العرض المحدد ولكن يسمح للعمق بالنمو. أثبت لو وآخرون [38] أنه إذا كان عرض شبكة عصبية عميقة ذات تنشيط وحدة خطية مصححة أكبر بشكل صارم من بعد الإدخال، فإن الشبكة يمكنها تقريب أي دالة قابلة للتكامل للوبيغ؛ إذا كان العرض أصغر من أو يساوي العرض بعد الإدخال، فإن الشبكة العصبية العميقة ليست مقربًا شاملًا. يشتق التفسير الاحتمالي، [39] من مجال التعلم الآلي. ويتميز بالاستدلال، [40] بالإضافة إلى مفاهيم التحسين للتدريب والاختبار، والمتعلقة بالتأقلم والتعميم على التوالي. وبشكل أكثر تحديدًا، يأخذ التفسير الاحتمالي في الاعتبار اللاخطية التنشيطية كدالة توزيع تراكمية.[39] أدى التفسير الاحتمالي إلى إدخال التسرب كمنظم في الشبكات العصبية. تم تقديم التفسير الاحتمالي من قبل باحثين بما في ذلك هوبفيلد وويدرو وناريندرا، وتم تعميمه في دراسات استقصائية مثل تلك التي أجراها كريستوفر بيشوب.[39][41] هناك نوعان من الشبكات العصبية الاصطناعية (ANN): الشبكة العصبية للتغذية الأمامية (FNN) أو المدرك المتعدد الطبقات (MLP) والشبكات العصبية المتكررة (RNN). تحتوي الشبكات العصبية المتكررة على دورات في هيكل اتصالها، بينما لا تحتوي الشبكات العصبية للتغذية الأمامية على ذلك. في عشرينيات القرن الماضي، ابتكر فيلهلم لينز وإرنست إيسينغ نموذج إيزينج [42][43] الذي هو في الأساس بنية شبكة عصبية متكررة غير متعلمة تتكون من عناصر عتبة تشبه الخلايا العصبية. في عام 1972، جعل شونيتشي أماري هذه البنية قابلةً للتكيف.[44][45] أعاد جون هوبفيلد نشر شبكته العصبية المتكررة المتعلمة في عام 1982.[46] نشر كاورو ناكانو شبكات عصبية متكررة مبكرةً أخرى في عام 1971.[47][48] في عام 1948، أنتج آلان تورينج أعمالًا حول \"الآلات الذكية\" لكنها لم تنشر في حياته، [49] وأحتوت على \"أفكار متعلقة بالتطور الاصطناعي وتعلم الشبكات العصبية المتكررة\".[45] اقترح فرانك روزنبلات (1958) [50] المدرك، وهو شبكة عصبية للتغذية الأمامية ذات ثلاث طبقات: طبقة إدخال، وطبقة مخفية ذات أوزان عشوائية لم تتعلم، وطبقة إخراج. نشر لاحقًا كتابًا في عام 1962 قدم أيضًا متغيرات وتجارب حاسوبية، بما في ذلك إصدار يحتوي على مدركات ذات أربع طبقات \"مع شبكات متكيفة قبل النهائية\" حيث تتعلم الطبقتان الأخيرتان الأوزان (هنا ينسب الفضل إلى إتش. دي. بلوك وبي. دبليو. نايت).[51]:القسم 16: يشير الكتاب إلى شبكة سابقة بواسطة آر. دي. جوزيف (1960) [52] \"مكافئة وظيفيًا لتغير\" في هذا النظام ذي الأربع طبقات (يذكر الكتاب جوزيف أكثر من 30 مرة). هل ينبغي إذن اعتبار جوزيف منشئ المدركات المتعددة الطبقات المتكيفة المناسبة مع وحدات مخفية متعلمة؟ لسوء الحظ، لم تكن خوارزمية التعلم وظيفية، وسقطت في طي النسيان. كانت أول خوارزمية تعلم عميق عاملة هي طريقة المجموعة لمعالجة البيانات، وهي طريقة لتدريب الشبكات العصبية العميقة بشكل تعسفي، والتي نشرها أليكسي إيفاخنينكو[الإنجليزية] ولابا في عام 1965. لقد اعتبروها شكلاً من أشكال الانحدار متعدد الحدود، [53] أو تعميمًا لمدرك روزنبلات.[54] وصفت ورقة بحثية عام 1971 شبكةً عميقةً ذات ثماني طبقات تم تدريبها بهذه الطريقة، [55] والتي تستند إلى التدريب طبقةً تلو الأخرى من خلال تحليل الانحدار. يتم تقليم الوحدات المخفية الزائدة باستخدام مجموعة تحقق منفصلة. نظرًا لأن دوال تنشيط العقد هي متعددات حدود كولموغوروف-غابور، فقد كانت هذه أيضًا أول الشبكات العميقة ذات وحدات ضربية أو \"بوابات\".[45] نشر شونيتشي أماري أول مدرك متعدد الطبقات للتعلم العميق مدرب بواسطة الانحدار التدريجي العشوائي [56] في عام 1967.[57] في التجارب الحاسوبية التي أجراها سايتو، الطالب لدى أماري، تعلم مدرك متعدد الطبقات من خمس طبقات ذو طبقتين قابلتين للتعديل تمثيلات داخليةً لتصنيف فئات الأنماط غير القابلة للفصل خطيًا.[45] جعلت التطورات اللاحقة في الأجهزة وضبط المعلمات الفائقة من الانحدار التدريجي العشوائي من طرف إلى طرف تقنية التدريب المهيمنة حاليًا. في عام 1969، قدم كونيهيكو فوكوشيما دالة تنشيط ReLU (وحدة التقويم الخطية) تابع التفعيل.[36][45] أصبح المقوم هو دالة التنشيط الأكثر شيوعًا للتعلم العميق.[58] بدأت بنى التعلم العميق للشبكات العصبية التلافيفية (CNNs) مع طبقات تلافيفية وطبقات للتقليل من العينات مع نيوكونييترون الذي قدمه كونيهيكو فوكوشيما في عام 1979، على الرغم من عدم تدريبه بواسطة الانتشار العكسي.[59][60] الانتشار العكسي هو تطبيق فعال لقاعدة السلسلة التي اشتقها غوتفريد فيلهلم لايبنتس في عام 1673 [61] على شبكات العقد القابلة للاشتقاق. تم تقديم مصطلح \"أخطاء الانتشار العكسي\" في الواقع في عام 1962 بواسطة روزنبلات، [51] لكنه لم يكن يعرف كيفية تطبيق ذلك، على الرغم من أن هنري جيه كيلي كان لديه مقدمة مستمرة للانتشار العكسي في عام 1960 في سياق نظرية التحكم.[62] نشر الشكل الحديث للانتشار العكسي لأول مرة في أطروحة الماجستير لسيبو ليناينما (1970).[45][63][64] أعاد جي. إم. أوستروفسكي وآخرون نشرها في عام 1971.[65][66] طبق بول ويربوس الانتشار العكسي على الشبكات العصبية في عام 1982 [67] (أطروحة الدكتوراه الخاصة به لعام 1974، التي أعيد طبعها في كتاب عام 1994، [68] لم تصف الخوارزمية بعد [66]). في عام 1986، عمم ديفيد إي روميلهارت وآخرون الانتشار العكسي لكنهم لم يستشهدوا بالعمل الأصلي.[69][70] تمّ تقديم الشّبكة العصبية ذات التّأخير الزمنيّ (TDNN) في عام 1987 بواسطة ألكسندر وايبل لتطبيق الشّبكات العصبية التلافيفية على التعرّف على الفونيم. استخدمت التّلافيف ومشاركة الوزن والانتشار العكسيّ.[71][72] في عام 1988، طبّق وي تشانغ شبكةً عصبيةً تلافيفيةً مدرّبةً بالانتشار العكسيّ على التعرّف على الحروف الأبجدية.[73] في عام 1989، ابتكر يان ليكون وآخرون شبكةً عصبيةً تلافيفيةً تسمّى LeNet للتعرّف على الرموز البريدية المكتوبة بخطّ اليد على البريد. استغرق التّدريب 3 أيام.[74] في عام 1990، طبّق وي تشانغ شبكةً عصبيةً تلافيفيةً على أجهزة الحوسبة البصرية.[75] في عام 1991، تمّ تطبيق شبكة عصبية تلافيفية على تجزئة كائن الصّورة الطبية [76] واكتشاف سرطان الثّدي في صور الثّدي بالأشعّة السّينية.[77] تمّ تطبيق LeNet-5 (في 1998)، وهي شبكة عصبية تلافيفية من 7 مستويات بواسطة يان ليكون وآخرون، تصنّف الأرقام، بواسطة العديد من البنوك للتعرّف على الأرقام المكتوبة بخطّ اليد على الشّيكات المرقمنة في صور بدقّة 32 × 32 بكسل.[78] تمّ تطوير الشّبكات العصبية المتكرّرة (RNN) [42][44] بشكل أكبر في الثمانينيات. يتمّ استخدام التّكرار لمعالجة التّسلسل، وعندما يتمّ فكّ شبكة متكرّرة، فإنّها تشبه رياضيًا طبقةً تغذيةً أماميةً عميقة. وبالتّالي، فإنّ لها خصائص وقضايا متشابهة، وكان لتطوّراتها تأثيرات متبادلة. في الشّبكات العصبية المتكرّرة، كان هناك عملان مؤثّران مبكّران هما شبكة جوردان في 1986، [79] وشبكة إلمان في 1990، [80] التي طبّقت الشّبكات العصبية المتكرّرة لدراسة المشكلات في علم النفس المعرفي. في الثمانينيات، لم يكن الانتشار العكسيّ يعمل بشكل جيّد مع التعلّم العميق ذي مسارات تخصيص الرّصيد الطويلة. للتغلّب على هذه المشكلة، في عام 1991، اقترح يورغن شميدهوبر تسلسلًا هرميًا من الشّبكات العصبية المتكرّرة مدرّبةً مسبقًا مستوىً واحدًا في كلّ مرّة بواسطة التعلّم الذّاتيّ الإشراف حيث تحاول كلّ شبكة عصبية متكرّرة التنبّؤ بإدخالها التّالي، وهو الإدخال التّالي غير المتوقّع للشّبكة العصبية المتكرّرة أدناه.[81][82] يستخدم هذا \"الضّاغط التّاريخيّ العصبيّ\" التّشفير التنبّئيّ لتعلّم التّمثيلات الدّاخلية على نطاقات زمنية متعدّدة التنظيم الذّاتيّ. يمكن أن يسهّل هذا بشكل كبير التعلّم العميق في المراحل التّالية. يمكن دمج التسلسل الهرميّ للشّبكات العصبية المتكرّرة في شبكة عصبية متكرّرة واحدة، عن طريق تقطير شبكة تجميع ذات مستوىً أعلى إلى شبكة تشغيل آليّ ذات مستوىً أدنى.[81][82] في عام 1993، حلّ ضّاغط تاريخيّ عصبيّ مهمّة \"تعلّم عميق جدًا\" تطلّبت أكثر من 1000 طبقة لاحقة في شبكة عصبية متكرّرة تمّ نشرها بمرور الوقت.[83] يشير الحرف \"P\" في ChatGPT إلى مثل هذا التّدريب المسبق. طبّقت أطروحة دبلوم سيب هوشريتر عام 1991 [84] الضّاغط التّاريخيّ العصبيّ، [81] وحدّدت وحلّلت مشكلة التّدريج المتلاشي.[84][85] اقترح هوشريتر اتّصالات متبقيةً متكرّرةً لحلّ مشكلة التّدريج المتلاشي. أدّى هذا إلى ظهور ذاكرة المدى الطويل القصيرة (LSTM)، التي نشرت في عام 1995.[86] يمكن لذاكرة المدى الطويل القصيرة تعلّم مهامّ \"التعلّم العميق جدًا\" [23] مع مسارات تخصيص رصيد طويلة تتطلّب ذكريات أحداث وقعت قبل آلاف الخطوات الزمنية المنفصلة. لم تكن ذاكرة المدى الطويل القصيرة هي البنية الحديثة بعد، والتي تطلّبت \"بوابة نسيان\"، تمّ تقديمها في عام 1999، [87] والتي أصبحت بنية الشّبكة العصبية المتكرّرة القياسية. في عام 1991، نشر يورغن شميدهوبر أيضًا شبكات عصبيةً متنافسةً تتنافس مع بعضها البعض على شكل لعبة ذات مجموع صفريّ، حيث يكون ربح شبكة واحدة هو خسارة الشّبكة الأخرى.[88][89] الشّبكة الأولى هي نموذج توليديّ يصوّر توزيع احتمالية على أنماط الإخراج. تتعلّم الشّبكة الثّانية عن طريق الانحدار التّدريجيّ التنبّؤ بردود فعل البيئة على هذه الأنماط. سميّ هذا بـ \"الفضول الاصطناعيّ\". في عام 2014، تمّ استخدام هذا المبدأ في الشّبكات التوليدية المتنافسة (GANs).[90] خلال 1985-1995، تمّ تطوير العديد من البنى والطرق بواسطة تيري سيجنوفسكي وبيتر دايان وجيوفري هينتون مستوحاةً من الميكانيكا الإحصائية، مثل آلة بولتزمان، [91] وآلة بولتزمان المقيّدة، [92] وآلة هيلمهولتز، [93] وخوارزمية الاستيقاظ والنّوم.[94] صمّمت هذه للتعلّم غير المراقب لنماذج توليدية عميقة. ومع ذلك كانت هذه أكثر تكلفةً من الناحية الحسابية مقارنةً بالانتشار العكسيّ. كانت خوارزمية تعلّم آلة بولتزمان التي نشرت في عام 1985، شائعةً لفترة وجيزة قبل أن تطغى عليها خوارزمية الانتشار العكسيّ في عام 1986. (ص 112 [95]). أصبحت شبكة عام 1988 حالةً فنيةً في التنبّؤ ببنية البروتين، وهو تطبيق مبكّر للتعلّم العميق على المعلوماتية الحيوية.[96] تمّ استكشاف التعلّم الضحل والعميق (على سبيل المثال، الشّبكات المتكرّرة) للشّبكات العصبية الاصطناعية للتعرّف على الكلام لسنوات عديدة.[97][98][99] لم تتفوّق هذه الطرق أبدًا على تقنية نموذج الخليط/نظرية ماركوف المخفية (GMM-HMM) الدّاخلية غير الموحّدة المصنوعة يدويًا والقائمة على نماذج توليدية للكلام مدرّبةً بشكل تمييزيّ.[100] تمّ تحليل الصّعوبات الرّئيسية، بما في ذلك تناقص التّدريج [70] وهيكل الارتباط الزمنيّ الضعيف في نماذج التنبّؤ العصبية.[101][102] من الصّعوبات الإضافية نقص بيانات التّدريب وقدرة الحوسبة المحدودة. ابتعد معظم باحثي التعرّف على الكلام عن الشّبكات العصبية لمتابعة النّمذجة التّوليدية. كان الاستثناء في معهد ستانفورد للأبحاث في أواخر التسعينيات. بتمويل من وكالة الأمن القومي الأمريكية وداربا، أجرت وكالة الأمن القومي بحثًا في التعرّف على الكلام والمتحدّث. أفاد فريق التعرّف على المتحدّث بقيادة لاري هيك بنجاح كبير مع الشّبكات العصبية العميقة في معالجة الكلام في معيار المعهد الوطني للمعايير والتقانة لعام 1998، [103][104] وهو يمثّل أوّل تطبيق صناعيّ رئيسيّ للتعلّم العميق.[105] تمّ استكشاف مبدأ رفع الميزات \"الخام\" على التّحسين المصنوع يدويًا بنجاح لأوّل مرّة في بنية المشفّر التّلقائيّ العميق على الطّيف \"الخام\" أو ميزات بنك المرشّح الخطيّ في أواخر التسعينيات، [104] مظهرًا تفوّقه على ميزات ميل-سيبسترال التي تحتوي على مراحل تحويل ثابتة من الأطياف. أنتجت الميزات الخام للكلام، الموجات الصوتية، لاحقًا نتائج ممتازةً على نطاق أوسع.[106] دخلت الشبكات العصبية في حالة ركود، وأصبحت النماذج الأبسط التي تستخدم ميزات يدوية الصنع مخصصةً للمهام مثل مرشحات غابور وآلات متجهات الدعم (SVMs) هي الخيارات المفضلة في التسعينيات والعقد الأول من القرن الحادي والعشرين، بسبب التكلفة الحسابية للشبكات العصبية الاصطناعية ونقص فهم كيفية توصيل الدماغ لشبكاته البيولوجية. [بحاجة لمصدر] في عام 2003، أصبحت ذاكرة المدى الطويل القصيرة قادرةً على منافسة معرفات الكلام التقليدية في مهام معينة.[107] في عام 2006 جمع أليكس غريفز[الإنجليزية] وسانتياغو فرنانديز وفاوستينو غوميز وشميدهوبر بينها وبين التصنيف الزمني للاتصال (CTC) [108] في مجموعات من ذواكر المدى الطويل القصيرة.[109] في عام 2009، أصبحت أول شبكة عصبية متكررة تفوز في مسابقة التعرف على الأنماط، في التعرف على خط اليد المتصل. تعرف على خط اليد في عام 2006، تم تطوير شبكة الاعتقاد العميق للنمذجة التوليدية من خلال منشورات لجيفري هينتون ورسلان سالاخوتدينوف وأوسيندرو وته.[110][111] يتم تدريبها عن طريق تدريب آلة بولتزمان المقيدة، ثم تجميدها وتدريب آلة أخرى فوق الأولى، وهكذا، ثم ضبطها اختياريًا باستخدام الانتشار العكسي المراقب.[112] يمكنها نمذجة توزيعات احتمالات عالية الأبعاد، مثل توزيع \"صور ذاكرة المدى الطويل القصيرة\"، لكن التقارب كان بطيئًا.[113][114][115] بدأ تأثير التعلم العميق في الصناعة في أوائل العقد الأول من القرن الحادي والعشرين، عندما عالجت الشبكات العصبية التلافيفية بالفعل ما يقدر بـ 10% إلى 20% من جميع الشيكات المكتوبة في الولايات المتحدة، وفقًا ليان ليكون.[116] بدأت التطبيقات الصناعية للتعلم العميق على التعرف على الكلام على نطاق واسع حوالي عام 2010. كانت ورشة عمل NIPS لعام 2009 حول التعلم العميق للتعرف على الكلام مدفوعةً بقيود النماذج التوليدية العميقة للكلام، واحتمالية أن تصبح الشبكات العصبية العميقة عمليةً بالنظر إلى الأجهزة الأكثر قدرةً ومجموعات البيانات واسعة النطاق. كان يعتقد أن التدريب المسبق للشبكات العصبية العميقة باستخدام نماذج توليدية لشبكات المعتقدات العميقة (DBN) سيتغلب على الصعوبات الرئيسية للشبكات العصبية. ومع ذلك، تم اكتشاف أن استبدال التدريب المسبق بكميات كبيرة من بيانات التدريب للانتشار العكسي المباشر عند استخدام الشبكات العصبية العميقة ذات طبقات الإخراج الكبيرة المعتمدة على السياق أنتج معدلات خطأ أقل بشكل كبير من نموذج خليط غاوسي (GMM) ونموذج ماركوف المخفي (HMM) الأكثر تطورًا، وكذلك من الأنظمة الأكثر تقدمًا القائمة على النموذج التوليدي.[117] كانت طبيعة أخطاء التعرف التي أنتجها النوعان من الأنظمة مختلفةً بشكل كبير، [118] مما يوفر رؤى فنيةً حول كيفية دمج التعلم العميق في نظام فك تشفير الكلام الحالي عالي الكفاءة في وقت التشغيل الذي نشرته جميع أنظمة التعرف على الكلام الرئيسية.[40][119][120] حفز التحليل في عامي 2009 و2010 - الذي يقارن بين نموذج الخليط ونماذج الكلام التوليدية الأخرى مقابل نماذج الشبكات العصبية العميقة - الاستثمار الصناعي المبكر في التعلم العميق للتعرف على الكلام.[118] تم إجراء هذا التحليل بأداء مقارن (أقل من 1.5% في معدل الخطأ) بين الشبكات العصبية العميقة التمييزية والنماذج التوليدية.[117][118][121] في عام 2010، وسع الباحثون التعلم العميق من TIMIT[الإنجليزية] إلى التعرف على الكلام ذي المفردات الكبيرة، من خلال اعتماد طبقات إخراج كبيرة من الشبكة العصبية العميقة بناءً على حالات نظرية ماركوف المخفية المعتمدة على السياق التي تم إنشاؤها بواسطة أشجار القرار.[119][122][123][124] بدأت ثورة التعلم العميق حول رؤية الحاسوب القائمة على الشبكات العصبية التلافيفية (CNN) ووحدات معالجة الرسومات (GPU). على الرغم من أن الشبكات العصبية التلافيفية المدربة بواسطة الانتشار العكسي كانت موجودةً منذ عقود وتطبيقات وحدات معالجة الرسومات للشبكات العصبية لسنوات، [125] بما في ذلك الشبكات العصبية التلافيفية، [126] كانت هناك حاجة إلى تطبيقات أسرع للشبكات العصبية التلافيفية على وحدات معالجة الرسومات للتقدم في رؤية الحاسوب. في وقت لاحق، مع انتشار التعلم العميق، تم تطوير أجهزة متخصصة وتحسينات للخوارزميات خصيصًا للتعلم العميق.[127] كان التقدم الرئيسي لثورة التعلم العميق هو التقدم في الأجهزة، وخاصةً وحدات معالجة الرسومات. يعود تاريخ بعض الأعمال المبكرة إلى عام 2004.[125][126] في عام 2009 أفاد راينا ومادهافان وأندرو نج عن شبكة معتقدات عميقة بعمق 100 مليون مدربة على 30 وحدة معالجة رسومات من إنفيديا من فئة (بالإنجليزية: GeForce GTX 280)، وهو عرض مبكر للتعلم العميق القائم على وحدة معالجة الرسومات. أفادوا بما يصل إلى 70 مرةً أسرع في التدريب.[128] في عام 2011، حققت شبكة عصبية تلافيفية تسمى دانت (بالإنجليزية: DanNet) [129][130] بواسطة دان سيريسان ويولي ماير وجوناثان ماسكي ولوكا ماريا غامبارديلا ويورغن شميدهوبر لأول مرة أداءً بشريًا خارقًا في مسابقة التعرف على الأنماط المرئية، متفوقةً على الطرق التقليدية بعامل 3.[23] ثم فازت في المزيد من المسابقات.[131][132] كما أظهروا كيف أدى التجميع الأقصى[الإنجليزية] للشبكات العصبية التلافيفية على وحدة معالجة الرسومات إلى تحسين الأداء بشكل كبير.[131][132] في عام 2012، أنشأ أندرو نج وجيف دين شبكةً عصبيةً للتغذية الأمامية تعلمت التعرف على مفاهيم ذات مستوىً أعلى، مثل القطط، فقط من مشاهدة الصور غير المعلمة المأخوذة من مقاطع فيديو يوتيوب.[133] في أكتوبر 2012، فاز ألكسنت بواسطة أليكس كريجفسكي، وإيليا سوتسكيفر، وجيفري هينتون [134] بمسابقة \"إيميج نت\" بهامش كبير على طرق التعلم الآلي الضحلة. تضمنت التحسينات التزايدية الإضافية شبكة VGG-16 بواسطة كارين سيمونيان وأندرو زيسرمان [135] وشبكة جوجل العصبية انسبشن 3[الإنجليزية].[136] ثم امتد النجاح في تصنيف الصور إلى مهمة إنشاء أوصاف (تسميات) للصور الأكثر تحديًا، غالبًا كمزيج من الشبكات العصبية التلافيفية وذاكرة المدى الطويل القصيرة.[137][138][139] في عام 2014، كانت حالة الفن هي تدريب \"شبكة عصبية عميقة جدًا\" مع 20 إلى 30 طبقة.[140] أدى تكديس العديد من الطبقات إلى انخفاض حاد في دقة التدريب، [141] والمعروفة باسم مشكلة \"التدهور\".[142] في عام 2015 تم تطوير تقنيتين لتدريب الشبكات العميقة جدًا وهما شبكة الطرق السريعة[الإنجليزية] في مايو 2015، والشبكة العصبية المتبقية (ResNet) [143] في ديسمبر 2015. في نفس الوقت تقريبًا، بدأ التعلم العميق يؤثر على مجال الفن. من بين الأمثلة المبكرة ديب دريم (2015)، والنقل الأسلوبي العصبي[الإنجليزية] (2015)، [144] وكلاهما كان قائمًا على الشبكات العصبية المدربة مسبقًا لتصنيف الصور، مثل VGG-19. أصبحت الشبكة التوليدية المتنافسة (GAN) بواسطة (إيان جودفيلو وآخرون، 2014) [145] (بناءً على مبدأ الفضول الاصطناعي ليورغن شميدهوبر [88][90]) حالةً فنيةً في النمذجة التوليدية خلال الفترة 2014-2018. تم تحقيق جودة صورة ممتازة بواسطة StyleGAN من إنفيديا (2018) [146] استنادًا إلى شبكة خصومية توليدية بواسطة تيرو كاراس وآخرون.[147] هنا ينمو مولد الشبكة التوليدية المتنافسة من نطاق صغير إلى نطاق كبير بطريقة هرمية. حقق توليد الصور بهذه الشبكة نجاحًا شائعًا، وأثار مناقشات حول التزييف العميق.[148] طغت نماذج الانتشار (2015) [149] على الشبكات التوليدية المتنافسة في النمذجة التوليدية منذ ذلك الحين، مع أنظمة مثل دال-إي 2 (2022) وستيبل ديفيوجن (2022). في عام 2015، تحسن التعرف على الكلام من غوغل بنسبة 49% بواسطة نموذج قائم على LSTM، وجعلوه متاحًا من خلال بحث غوغل الصوتي على الهواتف الذكية.[150][151] يعد التعلم العميق جزءًا من أحدث الأنظمة في مختلف التخصصات، وخاصةً الرؤية حاسوبية والتعرف التلقائي على الكلام (ASR). تحسنت النتائج على مجموعات التقييم المستخدمة بشكل شائع مثل TIMIT (التعرف الكلام) وMNIST (رؤية حاسوبية)، بالإضافة إلى مجموعة من مهام التعرف على الكلام ذات المفردات الكبيرة بثباتة.[117][152] تم استبدال الشبكات العصبية التلافيفية بالذاكرة القصيرة المدى المطولة.[151][153][154][155] لكنها أكثر نجاحًا في رؤية الحاسوب. حصل يوشوا بنجيو، وجيفري هينتون، ويان ليكون على جائزة تورينج لعام 2018 لـ \"الإنجازات المفاهيمية والهندسية التي جعلت الشبكات العصبية العميقة مكونًا أساسيًا في الحوسبة.\" [156] تعد الشبكات العصبية الاصطناعية (ANNs) أو الأنظمة المتصلة أنظمة حوسبة مستوحاةً من الشبكات العصبية البيولوجية التي تشكل أدمغة الحيوانات. تتعلم هذه الأنظمة (تحسن قدرتها تدريجيًا) على أداء المهام من خلال مراعاة الأمثلة، عمومًا دون برمجة مخصصة للمهام. على سبيل المثال، في التعرف على الصور، قد تتعلم تحديد الصور التي تحتوي على قطط من خلال تحليل صور أمثلة تم تصنيفها يدويًا على أنها \"قطة\" أو \"ليست قطة\" واستخدام النتائج التحليلية للتعرف على القطط في الصور الأخرى. لقد وجدت معظم استخدامها في التطبيقات التي يصعب التعبير عنها باستخدام خوارزمية حاسوب تقليدية باستخدام البرمجة القائمة على القواعد.[157] تعتمد الشبكة العصبية الاصطناعية على مجموعة من الوحدات المتصلة تسمى الخلايا العصبية الاصطناعية، (مماثلة للخلايا العصبية البيولوجية في الدماغ البيولوجي). يمكن أن يرسل كل اتصال (مشبك) بين الخلايا العصبية إشارةً إلى خلية عصبية أخرى. يمكن للخلية العصبية المستقبلة (بعد المشبكية) معالجة الإشارة (الإشارات) ثم إرسال إشارة إلى الخلايا العصبية التالية المتصلة بها. قد يكون للخلايا العصبية حالة، يمثلها عمومًا أعداد حقيقية، عادةً ما بين 0 و1. قد يكون للخلايا العصبية والمشابك أيضًا وزن يتغير مع تقدم التعلم، مما قد يزيد أو يقلل من قوة الإشارة التي ترسلها إلى أسفل التيار.[158] عادةً ما يتم تنظيم الخلايا العصبية في طبقات. قد تجري طبقات مختلفة أنواعًا مختلفةً من التحويلات على مدخلاتها. تنتقل الإشارات من الطبقة الأولى (الإدخال) إلى الطبقة الأخيرة (الإخراج)، ربما بعد اجتياز الطبقات عدة مرات.[159] كان الهدف الأصلي من نهج الشبكة العصبية هو حل المشكلات بنفس الطريقة التي يعمل بها الدماغ البشري. بمرور الوقت، ركز الاهتمام على مطابقة قدرات عقلية محددة، مما أدى إلى انحرافات عن علم الأحياء مثل الانتشار العكسي أو الانتشار الخلفي، أو تمرير المعلومات في الاتجاه المعاكس وتعديل الشبكة لتعكس تلك المعلومات.[160] تم استخدام الشبكات العصبية في مجموعة متنوعة من المهام، بما في ذلك رؤية الحاسوب والتعرف على الكلام، والترجمة الآلية، وتصفية الشبكات الاجتماعية، ولعب ألعاب الطاولة، وألعاب الفيديو، والتشخيص الطبي.[161][162] اعتبارًا من عام 2017، تحتوي الشبكات العصبية عادةً على عدد قليل من الآلاف إلى عدد قليل من ملايين الوحدات وملايين الاتصالات. على الرغم من أن هذا العدد أقل بعدة مراتب من عدد الخلايا العصبية في الدماغ البشري، فإن هذه الشبكات يمكنها أداء العديد من المهام على مستوىً يتجاوز مستوى البشر (على سبيل المثال، التعرف على الوجوه، أو لعب \"غو\").[163] الشبكة العصبية العميقة (DNN) هي نموذج حاسوبي مستوحى من بنية الدماغ، يتألف من طبقات متعددة من الوحدات الحسابية المترابطة، تمتد بين طبقة الإدخال وطبقة الإخراج.[20][23] ورغم تعدد أنواع هذه الشبكات، إلا أنها تتشارك في مكونات أساسية مشتركة، كالخلايا العصبية الاصطناعية والوصلات بينها، والأوزان والانحيازات التي تحدد قوة هذه الوصلات، فضلًا عن الدوال التي تنظم عملية الحساب.[164] على سبيل المثال، يمكن تدريب شبكة عصبية عميقة على تمييز سلالات الكلاب، بحيث عند عرض صورة لكلب عليها، تقوم بحساب الاحتمال النسبي لانتماء هذا الكلب إلى سلالة معينة. ويمكن للمستخدم تعديل هذه الاحتمالات بوضع عتبات محددة، ليحصل في النهاية على تسمية مقترحة للسلالة. كل عملية حسابية من هذه العمليات تعتبر طبقة في الشبكة، ولهذا سميت هذه الشبكات بـ\"العميقة\" نظرًا لاحتوائها على عدد كبير من هذه الطبقات. تستطيع الشبكات العصبية العميقة أن تُمَثِّل علاقات غير خطية معقدة. تولد بنى هذه الشبكات نماذج تركيبية تُعَبَّر فيها عن الكائن على أنه تركيب متعدد الطبقات من عناصر بدائية.[165] تتيح الطبقات الإضافية تكوين ميزات من الطبقات الأدنى، مما يُمكّن من نمذجة بيانات معقدة بوحدات أقل من الشبكة الضحلة ذات الأداء المماثل.[20] على سبيل المثال، ثبت أن كثيرات الحدود المتعددة المتغيرات المفرقة تُقَرَّب بشكل أسهل بكثير باستخدام الشبكات العصبية العميقة مقارنة بالشبكات الضحلة.[166] تشتمل البنى العميقة على العديد من المتغيرات لعدد قليل من الأساليب الأساسية. وقد حققت كل بنية نجاحًا في مجالات محددة. وليس من الممكن دائمًا مقارنة أداء بنى متعددة إلا إذا قُيِّمت على نفس مجموعات البيانات. تُعَدّ الشبكات العصبية العميقة، في الغالب، شبكاتَ تغذية أمامية تسير فيها البيانات في اتجاه واحد، من طبقة الإدخال إلى طبقة الإخراج، دون رجوع. في البداية، تُنشئ هذه الشبكة خريطةً من العُقد العصبية الاصطناعية وتُعيّن قيمًا عدديةً عشوائية، أو ما يُسمى \"أوزانًا\"، للعلاقات فيما بينها. تُضرب هذه الأوزان في قيم المدخلات، وينتج عن ذلك قيمٌ تتراوح بين الصفر والواحد. إذا لم تستطع الشبكة تمييز نمطٍ معين بدقة، تقوم الخوارزمية بتعديل هذه الأوزان.[167] بهذه الطريقة تستطيع الخوارزمية تعزيز تأثير بعض المعلمات، حتى تصل إلى المعالجة الرياضية الصحيحة للبيانات بالكامل.[168][169] تُستخدَم الشبكات العصبية المتكررة التي تتدفق فيها البيانات في اتجاهات متعددة، في تطبيقات نمذجة اللغات.[170][171][172] وتُعد شبكات الذاكرة قصيرة المدى طويلة الأجل ذات كفاءة بالغة في هذا السياق.[173][174] أما الشبكات العصبية التلافيفية (CNNs) فتُستخدم على نطاق واسع في مجال الرؤية الحاسوبية.[175] كما تم توظيفها في نمذجة الإشارات الصوتية لتحقيق التعرف التلقائي على الكلام.[176] كما هو الحال في الشبكات العصبية الاصطناعية، يمكن أن تنشأ العديد من المشكلات في الشبكات العصبية العميقة المدربة بطريقة بسيطة. من بين هذه المشكلات الشائعة الإفراط في التعميم ووقت الحساب الكبير. تميل الشبكات العصبية العميقة إلى الإفراط في التعميم بسبب الطبقات المجردة الإضافية التي تسمح لها بنمذجة العلاقات النادرة في بيانات التدريب. يمكن تطبيق أساليب التنظيم مثل تقليم وحدة إيفاخنينكو، [55] أو اضمحلال الأوزان أو التناثر أثناء التدريب للحد من الإفراط في التعميم.[177] بدلًا من ذلك، يتجاهل تنظيم التسرب وحدات عشوائية من الطبقات المخفية أثناء التدريب. يساعد ذلك في استبعاد العلاقات النادرة.[178] أخيرًا، يمكن زيادة البيانات من خلال أساليب مثل القص وال دوران لزيادة حجم مجموعات التدريب الأصغر وتقليل فرص الإفراط في التعميم.[179] يجب على الشبكات العصبية العميقة أن تأخذ في الاعتبار العديد من معلمات التدريب، مثل الحجم (عدد الطبقات وعدد الوحدات في كل طبقة)، ومعدل التعلم والأوزان الأولية. قد يكون مسح فضاء المعلمات للحصول على المعلمات المثالية غير ممكن بسبب التكلفة من حيث الوقت والموارد الحسابية. تسارع العديد من الحيل الحسابية، مثل التجميع (حساب التدرج على عدة أمثلة تدريب في وقت واحد بدلًا من الأمثلة الفردية)، عملية التدريب.[180] وقد أدت القدرات الحسابية الكبيرة للبنى متعددة النوى (مثل وحدات معالجة الرسومات) إلى تسريع كبير في التدريب، نظرًا لملاءمة هذه البنى الحسابية لحسابات المصفوفات والمتجهات.[181][182] بدلًا من ذلك، قد يبحث المهندسون عن أنواع أخرى من الشبكات العصبية ذات خوارزميات تدريب أبسط وأكثر تقارباً. تعتبر أجهزة التحكم في المفاصل القائمة على نموذج المخيخ (CMAC) أحد هذه الأنواع من الشبكات العصبية. لا تتطلب معدلات تعلم أو أوزانًا أولية عشوائية. يمكن ضمان تقارب عملية التدريب في خطوة واحدة مع دفعة جديدة من البيانات، وتكون التعقيدات الحسابية لخوارزمية التدريب خطية بالنسبة لعدد الخلايا العصبية المشاركة.[183][184] شهدت الأعوام منذ 2010 تطورات متسارعة في خوارزميات التعلم الآلي وأجهزة الحاسوب، مما أفضى إلى أساليب أكثر كفاءة لتدريب الشبكات العصبية العميقة التي تتكون من طبقات متعددة من الوحدات المخفية غير الخطية وطبقة إخراجية ضخمة.[185] وبحلول عام 2019، حلت وحدات معالجة الرسومات، غالبًا مع تحسينات مخصصة للذكاء الاصطناعي، محل وحدات المعالجة المركزية كأداة أساسية للتدريب واسع النطاق للذكاء الاصطناعي السحابي التجاري.[186] وقدّرت أوبن أيه آي الزيادة في مقدار الحساب المطلوب في أكبر مشاريع التعلم العميق، من ألكسنت (2012) إلى ألفا زيرو (2017)، بحوالي 300 ألف ضعف، مع اتجاه لمضاعفة هذا الوقت كل 3.4 أشهر.[187][188] صُممت دوائر إلكترونية خاصة، تُعرف بمعالجات التعلم العميق، لتسريع خوارزميات التعلم العميق. وتتضمن هذه المعالجات وحدات معالجة عصبية في هواتف هواوي المحمولة،[189] وخوادم الحوسبة السحابية مثل وحدات معالجة الموترات في منصة غوغل السحابية.[190] كما قامت شركة سيريبراس سيستمز ببناء نظام متخصص لمعالجة نماذج التعلم العميق الكبيرة، استنادًا إلى أكبر معالج في الصناعة.[191][192] تُعد أشباه الموصلات الرقيقة واعدة لتطوير أجهزة تعلم عميق موفرة للطاقة، حيث تستخدم الهيكل الأساسي نفسه للعمليات المنطقية وتخزين البيانات. وفي عام 2020، نشر ماريغا وآخرون تجاربهم مع مادة قناة نشطة ذات مساحة كبيرة لتطوير أجهزة ودوائر منطقية في الذاكرة تعتمد على ترانزستورات تأثير المجال ذات البوابة العائمة.[193] وفي عام 2021 اقترح جيه فيلدمان وآخرون مسرعًا مدمجًا للأجهزة الضوئية للمعالجة التلافيفية المتوازية.[194] ويحدد المؤلفون ميزتين رئيسيتين للفوتونات المدمجة مقارنة بنظيراتها الإلكترونية: نقل بيانات متواز بشكل هائل من خلال تعدد إرسال الطول الموجي بالتزامن مع أمشاط التردد، وسرعات تعديل بيانات عالية للغاية.[194] ويمكن لنظامهم تنفيذ تريليونات عمليات الضرب والتجميع في الثانية، مما يشير إلى إمكانات الفوتونات المدمجة في تطبيقات الذكاء الاصطناعي الغنية بالبيانات.[194] يُعدُّ التعرف التلقائي على الكلام أوسع نطاقٍ تحقق فيه التعلم العميق نجاحًا مبهرًا. لقد أظهرت شبكات الذاكرة الطويلة قصيرة المدى قدرةً على تعلم مهامّ \"التعلم العميق جدًّا\" [12]، والتي تتضمن فترات زمنية مطولة تمتد لعدة ثوانٍ وتحوي أحداث كلامية متباعدة بآلاف الخطوات الزمنية المتسلسلة، حيث تقابل الخطوة الزمنية الواحدة حوالي 10 مللي ثانية. وقد أظهرت شبكات الذاكرة الطويلة قصيرة المدى ذات بوابات النسيان [174] قدرةً على منافسة أنظمة التعرف على الكلام التقليدية في مهامّ محددة.[195] ارتكز النجاح الأولي في مجال التعرف على الكلام على مهامّ التعرف على نطاقٍ صغير، مثل قاعدة بيانات \"TIMIT\". تحتوي هذه المجموعة البياناتية على 630 متحدثًا يمثلون ثماني لهجات رئيسية للغة الإنجليزية الأمريكية، حيث يقرأ كل متحدث عشر جمل.[196] وقد أتاح حجمها الصغير إمكانية تجربة العديد من التهيئات. والأهم من ذلك، أن مهمتها تتعلق بالتعرف على تسلسل الصوتيات، الأمر الذي يسمح، على عكس التعرف على تسلسل الكلمات، باستخدام نماذج لغة ثنائية بسيطة على مستوى الصوتيات. وقد سمح هذا بتحليل قوة جوانب النمذجة الصوتية في التعرف على الكلام بشكلٍ أيسر. وقد تم تلخيص معدلات الخطأ، بما في ذلك هذه النتائج الأولية التي قيست كنسبة مئوية لمعدلات خطأ الصوتيات (PER)، منذ عام 1991.[197] أدى بروز الشبكات العصبية العميقة في مجالات التعرف على المتحدث في أواخر التسعينات، والتعرف على الكلام حول عامي 2009 و2011، والذاكرة القصيرة المدى المطولة في الفترة الممتدة بين عامي 2003 و2007 إلى تسريع التقدم في ثمانية مجالات رئيسية:[119][121][202] تعتمد جميع أنظمة التعرف على الكلام التجارية الرئيسية على التعلم العميق مثل كورتانا وإكس بوكس، ومترجم سكايب، وأمازون أليكسا، وجوجل ناو، وسيري، وبايدو، وآي فلايتك للبحث الصوتي، ومجموعة من منتجات نيوانس للكلام وغيرها.[3][203][204] تُعد قاعدة بيانات MNIST مجموعة بيانات مرجعية شائعة لتقييم خوارزميات تصنيف الصور. تتألف من أرقام مكتوبة بخط اليد، وتضم 60 ألف عينة تدريبية و10 ألف عينة اختبارية، شأنها شأن قاعدة بيانات TIMIT فإن حجمها المتواضع يتيح للمستخدمين تجربة إعدادات متنوعة. وتتوفر قائمة شاملة بالنتائج التي تحققت على هذه المجموعة.[205] بات التعرف على الصور القائم على التعلم العميق يتفوق على القدرات البشرية، إذ يُنتج نتائج أدق من نظرائه البشر. وقد تحقق هذا الإنجاز لأول مرة في عام 2011 في مجال التعرف على إشارات المرور، ثم تبعه في عام 2014 في مجال التعرف على الوجوه البشرية.[206][207] تفسر المركبات المدربة على التعلم العميق الآن مشاهدات الكاميرا بزاوية 360 درجة.[208] ومن الأمثلة الأخرى على تطبيقات التعلم العميق في هذا المجال، تحليل تشوهات الوجه الجديد (FDNA) الذي يُستخدم في تحليل الحالات الشاذة الوراثية المرتبطة بقاعدة بيانات واسعة من المتلازمات الوراثية.[208] إن التقدم المحرز في مجال التعرف على الصور مرتبط ارتباطًا وثيقًا بالاستخدام المتزايد لتقنيات التعلم العميق في شتى مهام الفنون البصرية. وقد أثبتت الشبكات العصبية العميقة كفاءتها في مجالات عدة، منها على سبيل المثال لا الحصر: استُخدمت الشبكات العصبية لبناء نماذج لغوية منذ مطلع القرن الحادي والعشرين.[168] وقد ساهمت الذاكرة القصيرة المدى المطولة (LSTM) في تطوير الترجمة الآلية ونماذج اللغة بشكل ملحوظ.[169][170][171] ومن التقنيات الأخرى البارزة في هذا المجال التعيين السلبي والتضمين الكلمي.[211] ويمكن اعتبار التضمين الكلمي، كما في نموذج word2vec، بمثابة طبقة تمثيل في بنية التعلم العميق تحول كلمة مفردة إلى تمثيل نقطوي في فضاء متجه يعبر عن علاقتها بالكلمات الأخرى ضمن مجموعة البيانات. ويتيح استخدام التضمين الكلمي كطبقة إدخال في الشبكات العصبية المتكررة (RNN) تحليل الجمل والعبارات اعتمادًا على قواعد متجه تركيبية فعالة. ويمكن النظر إلى هذه القواعد بمثابة قواعد نحوية احتمالية خالية من السياق (PCFG) يتم تنفيذها بواسطة الشبكات العصبية المتكررة.[212] كما يمكن للمشفرات التلقائية المتكررة المبنية على التضمينات الكلمية تقييم تشابه الجمل واكتشاف عمليات إعادة الصياغة.[212] وتوفر بنى الشبكات العصبية العميقة نتائج أفضل في مجالات التحليل التركيبي، [213] وتحليل المشاعر، [214] واسترجاع المعلومات، [215][216] وفهم اللغة المنطوقة، [217] والترجمة الآلية، [169][218] وربط الكيان السياقي، [218] والتعرف على أسلوب الكتابة، [219] والتعرف على الكيان المسمى (تصنيف الرموز المميزة)، [220] وتصنيف النصوص وغيرها.[221] ويعتمد مترجم جوجل (GT) على شبكة ذاكرة قصيرة المدى طويلة (LSTM) كبيرة الحجم تعمل من طرف إلى طرف.[222][223][224] وتستخدم ترجمة جوجل العصبية الآلية (GNMT) أسلوبًا في الترجمة الآلية يعتمد على الأمثلة حيث \"يتعلم النظام من ملايين الأمثلة\".[223][225] ويقوم بترجمة \"الجمل كاملة في وقت واحد، بدلًا من القطع\". ويدعم مترجم جوجل أكثر من مائة لغة.[223] وتشفر الشبكة \"دلالات الجملة بدلًا من مجرد حفظ الترجمات من عبارة إلى عبارة\".[223][226] ويستخدم مترجم جوجل اللغة الإنجليزية كلغة وسيطة بين معظم أزواج اللغات.[226] تفشل نسبة كبيرة من المركبات الدوائية المرشحة في الحصول على الموافقة التنظيمية. تُعزى هذه الإخفاقات إلى قصور في الفعالية الدوائية (أي التأثير على الهدف المقصود)، أو ظهور تفاعلات جانبية غير مرغوب فيها (أي تأثيرات خارج الهدف)، أو حدوث سمية غير متوقعة.[227][228] وقد سعى الباحثون إلى استكشاف إمكانية استخدام تقنيات التعلم العميق للتنبؤ بالأهداف الجزيئية الحيوية، [229][230] والأهداف غير المقصودة، والتأثيرات السامة للمركبات الكيميائية الموجودة في الأغذية والمنتجات المنزلية والأدوية.[231][232][233] يعد نظام \"أتوم نت\" مثالًا بارزًا على أنظمة التعلم العميق المصممة لتطوير الأدوية بشكل عقلاني بناءً على بنية الجزيء.[234] وقد تم توظيف \"أتوم نت\" للتنبؤ بجزيئات حيوية جديدة مرشحة لعلاج أمراض مثل فيروس إيبولا، [235] والتصلب المتعدد.[235][236] شهد عام 2017 استخدام الشبكات العصبية البيانية لأول مرة في مجال التنبؤ بخصائص جزيئية متنوعة ضمن قاعدة بيانات واسعة في علم السموم.[237] وفي عام 2019، تم اللجوء إلى الشبكات العصبية التوليدية لإنتاج جزيئات خضعت للتحقق التجريبي حتى مرحلة التجارب على الفئران.[238][239] تم توظيف تقنيات التعلم العميق بالتعزيز لتقدير قيمة الإجراءات التسويقية المباشرة المحتملة، وذلك بالاستناد إلى معطيات عميقة حول المتغيرات الخاصة بتردد الشراء، القيمة الإجمالية للطلبات، ومدة آخر عملية شراء (RFM). وقد أظهرت النتائج أن الدالة المستخدمة في تقدير القيمة تحمل تفسيرًا بديهيًا يتمثل في قيمة عمر العميل.[240] استخدمت أنظمة التوصية التعلم العميق لاستخراج ميزات ذات مغزىً لنموذج عامل كامن للتوصيات الموسيقية والبحثية القائمة على المحتوى.[241][242] تم تطبيق التعلم العميق متعدد المناظر لتعلم تفضيلات المستخدم من نطاقات متعددة.[243] يستخدم النموذج نهجًا هجينًا قائمًا على التعاون والمحتوى ويعزز التوصيات في مهام متعددة. استُخدم مشفر تلقائي للشبكات العصبية الاصطناعية في حقل المعلوماتية الحيوية، بهدف التنبؤ بالشروح الوراثية والعلاقات الوظيفية بين الجينات.[244] وفي سياق المعلوماتية الطبية، استُغل التعلم العميق للتنبؤ بجودة النوم بالاعتماد على البيانات المستمدة من الأجهزة القابلة للارتداء، [245] وكذلك للتنبؤ بالمضاعفات الصحية استنادًا إلى البيانات المسجلة في السجلات الصحية الإلكترونية.[246] وقد أظهرت الشبكات العصبية العميقة تفوقًا ملحوظًا في التنبؤ ببنية البروتين، وذلك انطلاقًا من تسلسل الأحماض الأمينية المكونة له. وفي عام 2020، حقق نظام ألفافولد القائم على التعلم العميق، مستوى دقة تفوق بكثير جميع الطرق الحسابية السابقة.[247][248] يمكن الاستعانة بالشبكات العصبية العميقة في تقدير إنتروبيا عملية عشوائية، حيث يُطلق على هذا التقدير اسم \"مقدر الإنتروبيا العصبي المشترك\" (NJEE).[249] يزودنا هذا التقدير بنظرة ثاقبة حول الأثر الذي تحدثه المتغيرات العشوائية المدخلة على متغير عشوائي مستقل. من الناحية العملية، تُدرب الشبكة العصبية العميقة بحيث تعمل كتصنيف يقوم بتعيين متجه أو مصفوفة من المدخلات (X) إلى توزيع احتمالي للمخرجات على الفئات المحتملة للمتغير العشوائي (Y)، وذلك بالنظر إلى المدخلات (X). على سبيل المثال، في مهام تصنيف الصور، يقوم \"مقدر الإنتروبيا العصبي المشترك\" بتعيين متجه من قيم ألوان وحدات البكسل إلى احتمالات للفئات التصويرية المحتملة. عمليًا، يتم الحصول على توزيع احتمالي لـ (Y) بواسطة طبقة \"سوفت ماكس\" حيث يكون عدد العقد فيها مساويًا لحجم أبجدية (Y). يستخدم \"مقدر الإنتروبيا العصبي المشترك\" دوال تنشيط قابلة للاشتقاق باستمرار، مما يجعل شروط نظرية التقريب الشاملة قابلة للتطبيق. وقد تبين أن هذه الطريقة توفر مقدرًا متسقًا بقوة ويتفوق على الطرق الأخرى في حال كانت أحجام الأبجدية كبيرة.[249] أظهرت الدراسات أن تقنيات التعلم العميق تحقق نتائج متفوقة في مختلف التطبيقات الطبية. ومن أبرز هذه التطبيقات: تصنيف الخلايا السرطانية، واكتشاف الآفات المرضية، وتجزئة الأعضاء، وتحسين جودة الصور الطبية.[250][251] وتشير الأبحاث الحديثة إلى أن أدوات التعلم العميق تتميز بدقة عالية في تشخيص الأمراض المختلفة، مما يجعلها أداة قيمة للمختصين في مجال الرعاية الصحية، ويساهم في رفع كفاءة عمليات التشخيص.[252][253] يُشكّل العثور على جمهور محمول مناسب لإعلانات الأجهزة المحمولة تحديًا دائمًا، إذ يتطلب الأمر مراعاة وتحليل العديد من نقاط البيانات قبل تكوين شريحة مستهدفة واستخدامها في عرض الإعلانات بواسطة أي خادم إعلاني.[254] وقد استُخدم التعلم العميق في تفسير مجموعات البيانات الإعلانية الكبيرة متعددة الأبعاد. تجمع العديد من نقاط البيانات أثناء دورة طلب/خدمة/نقرة إعلان الإنترنت، ويمكن أن تشكل هذه المعلومات أساسًا للتعلم الآلي لتحسين اختيار الإعلانات. طُبّق التعلم العميق بنجاح على المسائل العكسية كإزالة التشويش، والدقة الفائقة، والرسم، وتلوين الأفلام.[255] وتشمل هذه التطبيقات أساليب تعلّم مثل \"مجالات الانكماش لاستعادة الصورة بشكل فعال\" [256] التي تُدرّب على مجموعة بيانات للصور، و\"ديب إيمج برير\" التي تُدرّب على الصورة التي تحتاج إلى استعادة. يُطبق التعلم العميق بنجاح على الكشف عن الاحتيال المالي، والكشف عن التهرب الضريبي، [257] ومكافحة غسل الأموال.[258] في نوفمبر 2023 أعلن باحثون في شركة ديب مايند ومختبر لورنس بيركلي الوطني عن تطويرهم لنظام ذكاء اصطناعي أسموه \"ج نوم\" (بالإنجليزية: GNoME)، وقد ساهم هذا النظام بشكل كبير في ميدان علم المواد من خلال اكتشافه لأكثر من مليوني مادة جديدة في فترة زمنية وجيزة نسبيًا. يستند نظام \"ج نوم\" في عمله على تقنيات التعلم العميق التي تمكنه من استكشاف بنى المواد المحتملة بشكل فعال، مما أدى إلى زيادة كبيرة في تحديد البنى البلورية غير العضوية المستقرة. وقد تم التحقق من دقة تنبؤات هذا النظام من خلال تجارب روبوتية آلية حققت نسبة نجاح ملحوظة بلغت 71%. أُتيحت البيانات المتعلقة بالمواد المكتشفة حديثًا للجميع من خلال قاعدة بيانات مشروع المواد، مما يتيح للباحثين فرصة اختيار المواد التي تمتلك الخصائص المطلوبة لتطبيقها في مجالات مختلفة. يمثل هذا التطور نقلة نوعية في مجال الاكتشاف العلمي، ويؤكد على أهمية دمج الذكاء الاصطناعي في أبحاث علوم المواد، مما قد يساهم في تسريع عملية ابتكار المواد وتقليل التكاليف اللازمة لتطوير المنتجات. يشير استخدام الذكاء الاصطناعي والتعلم العميق إلى إمكانية تقليل أو حتى إلغاء التجارب المخبرية اليدوية، مما يتيح للعلماء التركيز بشكل أكبر على تصميم المركبات الفريدة وتحليلها.[259][260][261] طبقت وزارة الدفاع الأمريكية التعلم العميق لتدريب الروبوتات على مهام جديدة من خلال المشاهدة.[262] استُخدمت الشبكات العصبية المستوحاة من الفيزياء في حل المعادلات التفاضلية الجزئية، سواء كانت مباشرة أو عكسية، وذلك بطريقة تعتمد على البيانات.[263] ومن الأمثلة البارزة على ذلك إعادة بناء تدفق السوائل الذي يخضع لقوانين نافييه-ستوكس. ولا يتطلب الاستعانة بهذه الشبكات إنشاء شبكة تفاضلية، والتي تتطلب عادةً تكاليف باهظة كما هو الحال في الطرق التقليدية لديناميكا الموائع الحسابية.[264][265] تُعد طريقة المعادلة التفاضلية العشوائية العميقة ذات التوجه الخلفي أسلوبًا حاسوبيًا مبتكرًا يجمع بين قوة التعلم العميق ومرونة المعادلات التفاضلية العشوائية ذات التوجه الخلفي (BSDE). تُعد هذه الطريقة مثالية لحل المسائل المعقدة ذات الأبعاد العالية التي تظهر بشكل متكرر في مجال المالية الرياضية. بفضل قدرة الشبكات العصبية العميقة على تقريب الدوال بدقة عالية، تستطيع هذه الطريقة تجاوز التحديات الحسابية التي تواجهها الطرق التقليدية في التعامل مع الأبعاد الكبيرة. فطرق التقريب التقليدية، مثل طريقة الفروق المحدودة أو محاكاة مونت كارلو، تعاني من ما يُعرف بـ \"لعنة الأبعاد\"، حيث تزداد تكلفة الحساب بشكل كبير مع زيادة عدد الأبعاد. على النقيض من ذلك، تستغل طريقة المعادلة التفاضلية العشوائية العميقة ذات التوجه الخلفي الشبكات العصبية العميقة لتقريب حلول المعادلات التفاضلية الجزئية عالية الأبعاد، مما يقلل بشكل ملحوظ من العبء الحسابي.[266] وعلاوة على ذلك، فإن دمج الشبكات العصبية المسترشدة بالفيزياء (PINNs) في إطار عمل المعادلات التفاضلية العشوائية العكسية العميق يعزز من قدرتها بدمج القوانين الفيزيائية الأساسية بشكل مباشر في بنية الشبكة العصبية. وهذا يكفل ألا تقتصر الحلول على ملاءمة البيانات، بل تتوافق أيضًا مع المعادلات التفاضلية العشوائية الحاكمة للمشكلة. تستفيد الشبكات العصبية المسترشدة بالفيزياء من قوة التعلم العميق مع مراعاة القيود التي تفرضها النماذج الفيزيائية، مما يؤدي إلى حلول أكثر دقة وموثوقية للمسائل المتعلقة بالرياضيات المالية. تُعَد عملية إعادة بناء الصور بمثابة إعادة تركيب الصورة الأصلية من القياسات المرتبطة بها. وقد أظهرت العديد من الدراسات تفوق أساليب التعلم العميق بشكل ملحوظ على الطرق التحليلية في مختلف التطبيقات، من بينها التصوير الطيفي، [267] والتصوير بالموجات فوق الصوتية.[268] تعتمد أنظمة التنبؤ بالأحوال الجوية التقليدية على حل مجموعة معقدة من المعادلات التفاضلية الجزئية. أما نموذج جرافكاست القائم على التعلم العميق، فيعتمد على تدريب واسع على بيانات تاريخية للأحوال الجوية للتنبؤ بتطور الأنماط الجوية بمرور الوقت. ويتيح هذا النموذج القدرة على التنبؤ بالأحوال الجوية على مستوى العالم، بدقة تفصيلية عالية، وعلى مدى عشرة أيام، وذلك في زمن قياسي يقل عن دقيقة، بحيث تتساوى دقته مع أحدث الأنظمة المتخصصة.[269][270] الساعة فوق الجينية هي اختبار كيميائي حيوي يُستخدم لتقدير العمر البيولوجي. وقد استخدم غالكين وزملاؤه الشبكات العصبية العميقة لتدريب ساعة شيخوخة فوق جينية بدقة غير مسبوقة، وذلك باستخدام أكثر من 6000 عينة دم.[271] وتعتمد هذه الساعة على معلومات مستقاة من 1000 موقع ثنائي النوكليوتيد من نوع CpG، وتتمكن من التنبؤ بوجود حالات مرضية معينة لدى الأفراد، مثل داء الأمعاء الالتهابي، والخرف الجبهي الصدغي، وسرطان المبيض، والسمنة، وذلك بمعدل أعلى مقارنة بالأفراد الأصحاء. ومن المقرر إطلاق هذه الساعة للاستخدام العام في عام 2021 بواسطة شركة انزلك ميديكن الفرعية ديب لنجفتي. يرتبط التعلم العميق ارتباطًا وثيقًا بمجموعة من النظريات التطورية للدماغ، وتحديدًا تطور القشرة المخية الحديثة،[272][273] والتي طرحها علماء الأعصاب الإدراكيون في بدايات التسعينيات الميلادية.[274][275] وقد تجسّدت هذه النظريات التنموية في نماذج حسابية، مما جعلها سلفًا لأنظمة التعلم العميق. وتتشارك هذه النماذج التنموية في خاصية أن الديناميات التعليمية المختلفة المقترحة في الدماغ، مثل موجة عامل نمو الأعصاب، تدعم تنظيمًا ذاتيًا شبيهًا إلى حد كبير بتنظيم الشبكات العصبية المستخدمة في نماذج التعلم العميق. كالقشرة المخية الحديثة تستخدم الشبكات العصبية تسلسلًا هرميًا من المرشحات متعددة الطبقات، حيث تأخذ كل طبقة المعلومات من طبقة سابقة أو من بيئة التشغيل، ثم تمرر ناتجها، وربما الإدخال الأصلي، إلى طبقات أخرى. وتنتج هذه العملية مجموعة ذاتية التنظيم من المحولات، مضبوطة بدقة لبيئة تشغيلها. وقد ذكر وصف عام 1995م أن \"... يبدو أن دماغ الرضيع ينظم نفسه تحت تأثير موجات مما يسمى عوامل التغذية... تتصل مناطق مختلفة من الدماغ بالتسلسل، مع نضج طبقة واحدة من الأنسجة قبل الأخرى وهكذا حتى ينضج الدماغ بأكمله\".[276] استُخدمت طائفة واسعة من الأساليب لدراسة مدى تقارب نماذج التعلم العميق مع النماذج العصبية البيولوجية. من ناحية، اقترح باحثون تعديلات عديدة على خوارزمية الانتشار العكسي لجعلها أكثر واقعية بيولوجيًا.[277][278] ودفع آخرون بأن أشكال التعلم العميق غير الموجه، مثل تلك القائمة على النماذج التوليدية الهرمية وشبكة الاعتقاد العميق، قد تكون أقرب إلى الواقع البيولوجي.[279][280] وفي هذا الصدد، رُبطت نماذج الشبكات العصبية التوليدية ببيانات عصبية بيولوجية حول المعالجة القائمة على التعيين في القشرة الدماغية.[281] رغم غياب مقارنة منهجية بين تنظيم الدماغ البشري والتشفير العصبي في الشبكات العميقة حتى الآن، إلا أن العديد من أوجه التشابه قد لوحظت. فعلى سبيل المثال، يمكن أن تكون الحسابات التي تقوم بها وحدات التعلم العميق شبيهة بحسابات الخلايا العصبية الفردية، [282] والمجموعات العصبية.[283] وبالمثل، فإن التمثيلات التي تولدها نماذج التعلم العميق تشبه تلك التي قيست في نظام الرؤية عند الرئيسيات على مستوى الخلية العصبية الفردية، [284] وعلى مستوى المجموعات العصبية.[285] يشارك مختبر الذكاء الاصطناعي في فيسبوك بمهام شتى، من بينها وضع علامات آلية على الصور المُحمَّلة بأسماء الأشخاص الظاهرين فيها.[286] وقد طورت شركة ديب مايند التابعة لشركة غوغل نظامًا قادرًا على تعلم كيفية لعب ألعاب الفيديو من نوع أتاري مستخدمًا وحدات البكسل فقط كمدخلات للبيانات. وفي عام 2015، عرضت الشركة نظام ألفا غو الذي تعلم لعبة غو إلى حد الكفاءة الذي مكنه من هزيمة لاعب محترف في هذه اللعبة.[287][288][289] أما مترجم غوغل فيستخدم شبكة عصبية للترجمة بين أكثر من مئة لغة. وفي عام 2017 أطلقت شركة كوفاريانت دوت أي التي ركزت على دمج التعلم العميق في المصانع.[290] في 2008 طور باحثون بجامعة تكساس في أوستن إطار عمل للتعلم الآلي أطلقوا عليه اسم \"تامر\"، [291] وهو اختصار للتدريب اليدوي عبر التعزيز التقييمي. وقد طرح هذا الإطار أساليب مبتكرة لتمكين الروبوتات وبرامج الحاسوب من تعلم كيفية أداء المهام المختلفة من خلال التفاعل المباشر مع معلم بشري.[262] وفي تطور لاحق لهذا الإطار، تم تقديم خوارزمية جديدة أُطلق عليها اسم \"ديب تامر\" في عام 2018م، وذلك في إطار تعاون مشترك بين مختبر أبحاث الجيش الأمريكي وعلماء من جامعة تكساس. وقد استعان \"ديب تامر\" بالتعلم العميق لتمكين الروبوتات من اكتساب مهارات جديدة من خلال الملاحظة البصرية.[262] بفضل \"ديب تامر\"، بات بإمكان الروبوت تعلم مهمة ما من خلال التفاعل المباشر مع مدرب بشري، أو من خلال مشاهدة مقاطع فيديو توثق أداء الإنسان للمهمة ذاتها. وبعد ذلك، يتدرب الروبوت على أداء المهمة بمساعدة المدرب الذي يقدم له تعليقات إيجابية وسلبية مثل \"عمل جيد\" و\"عمل سيئ\".[292] جذب التعلم العميق الانتقادات والتعليقات، في بعض الحالات من خارج مجال علوم الكمبيوتر. من أبرز الانتقادات الموجهة إلى هذه الطرق نقص النظرية التي تقوم عليها.[293] فبينما يُنفذ التعلم في أغلب الشبكات العميقة باستعمال الانحدار التدريجي الذي يُفهم فهمًا جيدًا، فإن النظرية المتعلقة بخوارزميات أخرى، كخوارزمية التباعد المتناقض، أقل وضوحًا.[بحاجة لمصدر] (مثلاً، هل تتلاقى هذه الخوارزمية؟ وإذا كان الأمر كذلك، فما سرعة تلاقها؟ وما الذي تتلاقى إليه؟) وغالبًا ما تُعتبر أساليب التعلم العميق بمثابة صندوق أسود، حيث يركز معظم الباحثين على التجريب بدلًا من التحليل النظري.[294] يرى آخرون أنه ينبغي النظر إلى التعلم العميق على أنه خطوة نحو تحقيق الذكاء الاصطناعي العام، وليس كحل شامل لكل المشكلات. فبالرغم من قوة أساليب التعلم العميق، إلا أنها لا تزال تفتقر إلى العديد من القدرات اللازمة لتحقيق هذا الهدف بالكامل. وقد أشار عالم النفس الباحث غاري ماركوس: إن التعلم العميق، بحقائق الأمور، جزءٌ يسير من التحدي الجسيم المتمثل في بناء آلات ذكية. إذ تفتقر هذه التقنيات إلى سبل تمثل العلاقات السببية (...)، ولا تملك طرقًا واضحة لإجراء استنتاجات منطقية، كما أنها ما زالت بعيدة كل البعد عن دمج المعرفة المجردة، كالمعلومات المتعلقة بماهية الأشياء وغاياتها وكيفية استعمالها عادةً. وتستخدم أقوى أنظمة الذكاء الاصطناعي، كواتسون (...)، تقنيات كهذه كعنصر واحد فحسب في مجموعة بالغة التعقيد من التقنيات، تتراوح بين التقنيات الإحصائية للاستدلال البايزي إلى التفكير الاستنتاجي.[295] في إشارة أخرى إلى فكرة أن الحساسية الفنية قد تكون متأصلة في مستويات منخفضة نسبيًا من التسلسل الهرمي الإدراكي، تُظهر سلسلة منشورة من التمثيلات الرسومية للحالات الداخلية للشبكات العصبية العميقة (عشرين إلى ثلاثين طبقة) التي تحاول التمييز بين البيانات العشوائية والصور التي تم تدريبها عليها جاذبيةً بصرية لافتة للنظر.[296] فقد حظي البحث الأصلي بأكثر من ألف تعليق، وكان موضوع المقالة الأكثر زيارةً على موقع الغارديان لفترة من الزمن.[297] تُبدي بعض نماذج التعلم العميق سلوكيات شاذة، [298] مثل تصنيف صور غير قابلة للتفكيك بثقة عالية ضمن فئات صور عادية معروفة مسبقاً، [299][300] وخطأ في تصنيف صور طرأ عليها تعديل طفيف بعد تصنيفها بشكل صحيح (2013).[298] اقترح جويرتزل أن هذه السلوكيات تعود إلى قيود في التمثيلات الداخلية لهذه النماذج، وأن هذه القيود ستعيق دمجها في أنظمة ذكاء اصطناعي عام (AGI) متعددة المكونات وغير متجانسة.[301] قد يتم التغلب على هذه المشكلات من خلال نماذج تعلم عميق تبني داخليًا حالات مماثلة للتحليلات النحوية للصور للكيانات والأحداث المرئية.[298] إن تعلم قواعد نحوية (بصرية أو لغوية) من بيانات التدريب يعادل فرض قيد على النظام بالتفكير المنطقي الذي يستند إلى مفاهيم قابلة للتعبير عنها بقواعد إنتاج نحوية، وهو هدف أساسي لكل من اكتساب اللغة البشرية [302] والذكاء الاصطناعي.[303] مع انتقال تقنيات التعلم العميق من بيئة التجارب المعملية إلى عالم التطبيقات العملية، كشفت الأبحاث والدراسات المتتالية عن مدى قابلية الشبكات العصبية الاصطناعية للاختراق والتضليل.[304] فبفضل قدرتها على تحديد الأنماط المعقدة التي تعتمد عليها هذه الأنظمة في عملها، يستطيع المهاجمون التلاعب بمدخلاتها بطرق خفية، بحيث تتوهم الشبكة العصبية وجود تطابق معين لا يستطيع الإنسان العادي إدراكه. على سبيل المثال، يمكن للمهاجم إجراء تعديلات طفيفة على صورة بحيث تظن الشبكة العصبية أنها تطابق الصورة المستهدفة، على الرغم من أن الإنسان لا يلاحظ أي تغيير يذكر في الصورة. وتُعرف هذه الهجمات باسم هجمات التعلم الآلي العدائية.[305] في عام 2016، نجح باحثون في استخدام شبكة عصبية اصطناعية واحدة لتعديل الصور بطريقة تجريبية، حيث كانت الشبكة تقوم بتحديد نقاط القوة والضعف في الشبكات العصبية الأخرى، ومن ثمّ تقوم بإنشاء صور مضللة لها. واللافت للنظر أن هذه الصور المعدلة بدت طبيعية تمامًا للعين البشرية. كما أثبتت مجموعة بحثية أخرى أن طباعة هذه الصور المعدلة ثم تصويرها مرة أخرى كانت كافية لخداع أنظمة تصنيف الصور المتقدمة.[306] ومن بين الحلول المقترحة لمواجهة هذه التحديات، نجد تقنية البحث العكسي عن الصور، والتي تعتمد على إرسال الصورة المشتبه فيها إلى محركات بحث متخصصة مثل \"تن آي\"، والتي بدورها تبحث عن صور مشابهة لها في قواعد بياناتها الضخمة. كما يمكن تطوير هذه التقنية لتشمل البحث عن أجزاء من الصورة، مما يزيد من دقة النتائج.[307] أظهرت دراسة أخرى أن بعض أنواع النظارات المزودة بتقنيات التمويه يمكن أن تضلل أنظمة التعرف على الوجوه، مما يجعلها تخلط بين الأفراد العاديين والشخصيات الشهيرة. وبذلك يتسنى لشخص ما أن يتقمص هوية شخص آخر. وفي عام 2017، نجح باحثون في تضليل الشبكات العصبية الاصطناعية من خلال إضافة ملصقات إلى علامات التوقف، مما أدى إلى خطأ في تصنيفها.[306] ومع ذلك، يمكن تدريب الشبكات العصبية الاصطناعية على اكتشاف محاولات الخداع هذه، مما قد يؤدي إلى نشوء صراع مستمر بين المهاجمين والمدافعين، يشبه إلى حد كبير السباق التسلحي في مجال مكافحة البرامج الضارة. وقد تم بالفعل تدريب شبكات عصبية اصطناعية على التغلب على برامج مكافحة البرامج الضارة القائمة على الشبكات العصبية الاصطناعية من خلال هجوم متكرر ومتطور على هذه البرامج. يتم ذلك عن طريق تعديل البرامج الضارة بشكل مستمر باستخدام خوارزميات وراثية، حتى تنجح في خداع برنامج مكافحة البرامج الضارة مع الحفاظ على قدرتها على إلحاق الضرر بالهدف.[306] في عام 2016، بيّنت مجموعة بحثية أن أصواتًا معينة قادرة على توجيه نظام جوجل ناو الصوتي لفتح عنوان ويب محدد. وخلص الباحثون إلى أن هذه الثغرة يمكن استغلالها كمنطلق لشن هجمات أكثر تعقيدًا، مثل فتح صفحات ويب تضم برمجيات خبيثة.[306] يتمثل جوهر الهجمات العدائية في مجال تعلم الآلة في التلاعب المتعمد ببيانات التدريب المقدمة لأنظمة التعلم الآلي، بهدف إعاقتها عن تحقيق الكفاءة المرجوة.[306] تعتمد أغلب أنظمة التعلم العميق على بيانات التدريب والتحقق التي ينشئها البشر أو يشرحونها.[308] وقد جادلت فلسفة الإعلام بأن العمل اليدوي ذو الأجر المتدني لا يُنشر بانتظام فقط لهذا الغرض (كما في منصة أمازون ميكانيكال تورك)، بل إن الأشكال الضمنية للعمل البشري الجزئي التي غالبًا ما تُتجاهل بهذا الصدد تُنشر أيضًا.[309] ويميز الفيلسوف راينر مولهاف بين خمسة أنواع من \"الالتقاط الآلي\" للعمل البشري الجزئي بهدف توليد بيانات التدريب وهي: يُجادل مولهاف بأن أغلب التطبيقات التجارية للتعلم العميق التي يتفاعل معها المستخدم مباشرة، مثل نظام التعرف على الوجوه في فيسبوك، لا يكفي فيها تدريب الشبكة العصبية الاصطناعية مرة واحدة. بل إن هناك حاجة مستمرة إلى بيانات تحقق يوفرها الإنسان لمعايرة الشبكة العصبية وتحديثها بشكل متواصل. ولتحقيق هذا الغرض، قدم فيسبوك ميزة تتيح للمستخدم، حالما يتعرف النظام عليه في صورة ما، أن يتلقى إشعارًا. ويمكن للمستخدم حينها أن يختار ما إذا كان يرغب في وضع علامة عليه علنًا على الصورة أم لا، أو أن يبلغ فيسبوك بأن الصورة ليست له.[310] وتعد هذه الواجهة التفاعلية آلية لتوليد \"تدفق مستمر من بيانات التحقق\"، [309] التي تساهم في مواصلة تدريب الشبكة في الوقت الفعلي. ويرى مولهاف أن مشاركة المستخدمين من البشر في توفير بيانات التدريب والتحقق هي سمة أساسية في معظم تطبيقات التعلم العميق التجارية التي يتفاعل معها المستخدم مباشرة، لدرجة أنه يمكن وصف هذه الأنظمة بأنها \"ذكاء اصطناعي بمساعدة الإنسان\".[309]"
  },
  {
    "url": "https://az.wikipedia.org/wiki/D%C9%99rin_%C3%B6yr%C9%99nm%C9%99",
    "title": "Dərin öyrənmə — Vikipediya",
    "content": "Dərin öyrənmə — təmsil öyrənmə yolu ilə süni neyron şəbəkələrə əsaslanan maşın öyrənmə metodlarının alt dəsti. \"Dərin\" sözü şəbəkədə çoxlu təbəqələrin istifadəsinə aiddir. İstifadə olunan üsullar nəzarətli, yarı nəzarətli və ya nəzarətsiz ola bilər.[2] Dərin neyron şəbəkələr, dərin etimad şəbəkələri, təkrarlanan neyron şəbəkələr, konvolyusiya neyron şəbəkələr və transformatorlar kimi dərin öyrənmə arxitekturaları kompüter görüşü, nitqin tanınması, təbii dilin emalı, maşın tərcüməsi, bioinformatika, dərman hazırlanması, tibbi şəkil təhlili, klimatologiya, material analizi və stolüstü oyun proqramları kimi sahələrə tətbiq edilmişdir. Burada onlar insan ekspert performansı ilə müqayisə edilə bilən və bəzi hallarda onları üstələyən nəticələr əldə etmişlər.[3][4][5] Süni neyron şəbəkələri (SNŞ) bioloji sistemlərdə informasiyanın emalı və paylanmış kommunikasiya qovşaqlarından ilhamlanaraq yaradılmışdır. SNŞ-lərin bioloji beyinlərdən müxtəlif fərqləri mövcuddur. Xüsusilə, süni neyron şəbəkələri statik və simvolik olur, əksər canlı orqanizmlərin bioloji beyni dinamik (plastik) və analoqdur.[6][7] SNŞ ümumiyyətlə beyin funksiyası üçün aşağı keyfiyyətli modellər kimi qəbul edilir.[8] Dərin öyrənmə, xam daxiletmə məlumatlarından daha yüksək səviyyəli xüsusiyyətləri mərhələli olaraq çıxarmaq üçün çoxlu qatlardan istifadə edən maşın öyrənmə alqoritmləri sinfidir.[9]:199–200 Məsələn, şəklin emalı zamanı aşağı təbəqələr kənarları, yüksək təbəqələr isə rəqəmlər, hərflər və ya üzlər kimi insana aid olan anlayışları müəyyən edə bilər. Dərin öyrənməyə başqa nöqteyi-nəzərdən baxsaq, dərin öyrənmə mənbədən (məsələn, itin təsviri) öyrənilmiş obyektə (itlər) qədər insanın öyrənmə proseslərinin \"kompüterləşdirilməsi\" və ya \"avtomatlaşdırılması\" deməkdir. Buna görə də, \"daha dərin\" öyrənmə və ya \"ən dərin\" öyrənmə[10] kimi adlanan anlayışlar daha məntiqlidir. Ən dərin öyrənmə mənbədən öyrənilən son obyektə qədər tam avtomatik öyrənmə deməkdir. Beləliklə, daha dərin öyrənmə qarışıq öyrənmə prosesinə aiddir: mənbədən öyrənilmiş yarımobyektə insan öyrənmə prosesi, ardınca insanın öyrəndiyi yarımobyektdən son öyrənilmiş obyektə qədər kompüter öyrənmə prosesi."
  },
  {
    "url": "https://bn.wikipedia.org/wiki/%E0%A6%97%E0%A6%AD%E0%A7%80%E0%A6%B0_%E0%A6%B6%E0%A6%BF%E0%A6%96%E0%A6%A8",
    "title": "গভীর শিখন - উইকিপিডিয়া",
    "content": "গভীর জ্ঞানার্জন বা গভীর শিখন (গভীর কাঠামোগত শিক্ষণ বা ক্রমাধিকারতান্ত্রিক শিক্ষণ বা ডিপ লার্নিং নামেও পরিচিত) হল কৃত্রিম স্নায়ুতন্ত্রের উপর ভিত্তি করে যন্ত্রীয় শিখন পদ্ধতিগুলির একটি বৃহত্তর পরিবারের অংশ। শিক্ষা তত্ত্বাবধানকৃত, আধাতত্ত্বাবধানকৃত বা অতত্ত্বাবধানকৃত হতে পারে। [১][২][৩] সহজ কথায় মেশিন লার্নিং-এর যে শাখা ইনপুটের বাহ্যিক বৈশিষ্টের বাইরেও ভিন্ন ভিন্ন দৃষ্টিভংগী থেকে নতুন বৈশিষ্ট্য আবিষ্কার ও তাদের মধ্যে যৌক্তিক সম্পর্ক নিরূপণপূর্বক ইনপুটের সাংখ্যিক রূপ নির্ণয়ে কাজ করে তাকে ডিপ লার্নিং বলে। গভীর নিউরাল নেটওয়ার্ক, গভীর বিশ্বাস নেটওয়ার্ক, পুনরাবৃত্তিমূলক নিউরাল নেটওয়ার্ক এবং কনভলুশনাল নিউরাল নেটওয়ার্ক- এর মতো গভীর শিক্ষণ আর্কিটেকচার  কম্পিউটার ভিশন , বাচন শনাক্তকরণ , প্রাকৃতিক ভাষা প্রক্রিয়াজাতকরণ, অডিও শনাক্তকরণ, সামাজিক নেটওয়ার্ক ফিল্টারিং, মেশিন অনুবাদ, জৈব তথ্যবিজ্ঞান, ড্রাগ ডিজাইন , মেডিকেল ইমেজ বিশ্লেষণ, উপাদান পরিদর্শন এবং বোর্ড গেম প্রোগ্রাম -সহ বিভিন্ন ক্ষেত্রে প্রয়োগ করা হয়েছে, যেখানে তারা তুলনামূলক এবং কিছু ক্ষেত্রে মানব বিশেষজ্ঞদের থেকে উচ্চতর ফলাফল দিয়েছে । [৪][৫][৬] কৃত্রিম নিউরাল নেটওয়ার্ক (এএনএন) জৈবিক ব্যবস্থায় তথ্য প্রক্রিয়াকরণ  এবং বিস্তৃত যোগাযোগ নোড দ্বারা অনুপ্রাণিত। কৃত্রিম নিউরাল নেটওয়ার্ক ও  জৈবিক মস্তিষ্কের বিভিন্ন পার্থক্য আছে। বিশেষত, নিউরাল নেটওয়ার্ক স্থিত এবং প্রতীকী;অন্যদিকে অধিকাংশ জীবের জৈবিক মস্তিষ্ক গতিশীল (নমনীয়) এবং এনালগ। [৭][৮][৯]"
  },
  {
    "url": "https://zh-min-nan.wikipedia.org/wiki/Chhim-t%C5%8D%CD%98_ha%CC%8Dk-si%CC%8Dp",
    "title": "Chhim-tō͘ ha̍k-si̍p – Wikipedia",
    "content": "Chhim-tō͘ ha̍k-si̍p (Eng-gí: deep learning) sī ki-hāi ha̍k-si̍p ê chi̍t ê hun-iá. Che sī chi̍t chióng chhì-tô͘ beh sú-iōng pau-hâm ho̍k-cha̍p kiat-kò͘ ia̍h-sī iû chē têng hui-sòaⁿ-sèng piàn-ōaⁿ kò͘-sêng ê chē ê chhú-lí-chân tùi chu-liāu chìn-hêng ko-chân thiu-siōng ê ián-sǹg-hoat. Ha̍k-si̍p ê kòe-têng ē-tàng sī kàm-tok, pòaⁿ-kàm-tok ia̍h-sī bô-kàm-tok ha̍k-si̍p. Chhim-tō͘ ha̍k-si̍p sī ki-hāi ha̍k-si̍p lāi chi̍t chióng tùi chu-liāu chò te̍k-teng ha̍k-si̍p ê ián-sǹg-hoat."
  },
  {
    "url": "https://bg.wikipedia.org/wiki/%D0%94%D1%8A%D0%BB%D0%B1%D0%BE%D0%BA%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5",
    "title": "Дълбоко обучение – Уикипедия",
    "content": "Дълбокото обучение (известно още като дълбоко структурирано обучение) е част от по-широко семейство методи за машинно самообучение, основано на изкуствени невронни мрежи с учебно представяне. В превод от немски: mehrschichtiges Lernen, наричано многослойно обучение, е метод за машинно самообучение на машините, което използва изкуствени невронни мрежи с многобройни междинни слоеве между слоя за въвеждане и слоя за извеждане и с това образува широкообхватна вътрешна структура. Ученето може да бъде контролирано, полуконтролирано или неконтролирано.[1][2][3] Дълбоко обучаващи се архитектури като дълбоки невронни мрежи, дълбоки мрежи за сигурност, графични невронни мрежи, повтарящи се невронни мрежи и конволюционни невронни мрежи намират приложение в компютърното зрение, разпознаването на реч, обработката на естествен език, машинния превод, биоинформатиката, проектирането на лекарства, медицинския анализ на изображения и други.[4][5][6][7] Концепцията за изкуствените невронни мрежи произлиза от обработката на информация и разпределените комуникационни възли в биологичните системи. Те обаче са по-скоро статични и символични, докато биологичният мозък на повечето живи организми е динамичен (пластичен) и аналогов.[8][9][10] Дълбокото обучение е клас алгоритми за машинно обучение,[12]\nкойто използва множество слоеве за прогресивно извличане на функции от по-високо ниво от входа. Например при обработката на изображения по-ниските слоеве могат да идентифицират ръбове, докато по-високите слоеве могат да идентифицират понятията, свързани с човека, като цифри, букви или лица. Повечето съвременни модели на дълбоко обучение се основават на изкуствени невронни мрежи, по-специално на конволюционни невронни мрежи.[13] При задълбочено обучение всяко ниво се научава да трансформира своите входни данни в малко по-абстрактно и съставно представяне. В приложение за разпознаване на изображения входът може да бъде матрица от пиксели; първият представителен слой може да абстрахира пикселите и да кодира ръбове; вторият слой може да композира и кодира аранжименти на ръбове; третият слой може да кодира носа и очите; четвъртият слой може да разпознае, че изображението съдържа лице. Важното е, че при дълбокото обучение подходящите функции се разпределят автоматично между слоевете. Все пак това не премахва напълно необходимостта от ръчна настройка; например различен брой слоеве и размери на слоевете могат да осигурят различна степен на абстракция.[14][15] Думата „дълбоко“ в „дълбоко обучение“ се отнася до броя на слоевете, през които данните се трансформират. ВАЖНО: Този шаблон се отнася единствено до авторските права върху съдържанието на статията. Добавянето му не отменя изискването да се посочват конкретни източници на твърденията, които да бъдат благонадеждни.​"
  },
  {
    "url": "https://bs.wikipedia.org/wiki/Duboko_u%C4%8Denje",
    "title": "Duboko učenje - Wikipedia",
    "content": "Dubinsko učenje je dio šire porodice metoda mašinskog učenja, koje se zasnivaju na umjetnim neuronskim mrežama s reprezentacijskim učenjem. Učenje može biti nadgledano, polunadzirano ili bez nadzora.[2] Arhitekture dubokog učenja kao što su duboke neuronske mreže, mreže dubokih vjerovanja, učenje s dubokim pojačanjem, rekurentne neuronske mreže, konvolucione neuronske mreže i transformatori, primijenjene su na razna polja uključujući računarski vid, prepoznavanje govora, obradu prirodnog jezika, mašinsko prevođenje, bioinformatiku, osmišljavanje lijekova, analize medicinskih snimaka, nauke o klimi, inspekcije materijala i programi društvenih igara, gdje su dali rezultate koji su usporedivi i u nekim slučajevima prevazilaze performanse stručnjaka.[3][4][5] Vještačke neuronske mreže (ANN) inspirisane su obradom informacija i distribuiranim komunikacionim čvorovima u biološkim sistemima. ANN imaju višestruke razlike od biološkog mozga. Naime, umjetne neuronske mreže imaju tendenciju da budu statične i simbolične, dok je biološki mozak većine živih organizama dinamičan (plastičan) i analogan.[6][7]"
  },
  {
    "url": "https://ca.wikipedia.org/wiki/Aprenentatge_profund",
    "title": "Aprenentatge profund - Viquipèdia, l'enciclopèdia lliure",
    "content": "L'aprenentatge profund[1] (en anglès, deep learning) és una tècnica d’extracció i transformació de noves característiques del processament de la informació, les quals poden ser de forma supervisada o no. Són algoritmes que funcionen en un sistema per capes, simulant el funcionament bàsic del cervell que s'utilitza amb les neurones. És a dir, el conjunt de capes que forma el deep learning representen les neurones del cervell. Aquest mètode va ser promogut als anys 80 per l'investigador japonès Kunihiko Fukushima, el qual va proposar un model neuronal entre cinc i sis capes nomenat neocognitró. Actualment la definició d’aprenentatge profund és un sinònim modern de les aplicacions de les xarxes neuronals. Hi ha molts sistemes actuals de reconeixement de veu, visió artificial i reconeixements d’imatges que utilitzen aquesta tecnologia. Hi ha diverses definicions que ens permeten obtenir una idea més detallada del que engloba el concepte del Deep Learning. La majoria dels models moderns d'aprenentatge profund es basen en xarxes neuronals artificials, específicament xarxa neuronal convolucional (CNN), encara que també poden incloure fórmules proposicionals o variables latents organitzades en models generatius profunds, com els nodes en xarxes de creences profundes i màquines Boltzmann. En l'aprenentatge profund, cada nivell aprèn a transformar les seves dades d'entrada en una representació lleugerament més abstracta i composta. En una aplicació de reconeixement d'imatges, l'entrada en brut pot ser una matriu de píxels; la primera capa representativa pot absorbir els píxels i codificar les vores; la segona capa pot compondre i codificar els arranjaments de les vores; la tercera capa pot codificar un nas i ulls; i la quarta capa pot reconèixer que la imatge conté una cara. És important destacar que un procés d'aprenentatge profund pot aprendre quines característiques té per a situar-se de manera òptima en quin nivell. Això no elimina la necessitat d'un afinament manual; per exemple, els diferents nombres de capes i mides de les capes poden proporcionar diferents graus d'abstracció. La paraula profund en aprenentatge profund es refereix al nombre de capes a través de les quals es transformen les dades. Més precisament, els sistemes d'aprenentatge profund tenen una profunditat substancial de la ruta de l'assignació de crèdit (PAC). La PAC és la cadena de transformacions de l'entrada a la sortida. Els PAC descriuen les connexions potencialment causals entre entrada i sortida. Per a una xarxa neuronal, la profunditat de les capes cap endavant és la de la xarxa i és el nombre de capes ocultes més una capa extra (ja que la capa de sortida també està parametritzada). Per a xarxes neuronals recurrents, en les quals un senyal pot propagar-se a través d'una capa més d'una vegada, la profunditat de la CAP és potencialment il·limitada. No hi ha un llindar de profunditat acordat, universalment divideix l'aprenentatge superficial de l'aprenentatge profund, però la majoria dels investigadors estan d'acord que l'aprenentatge profund implica una profunditat de la CAP més alta que 2. S'ha demostrat que la capa de profunditat 2 és un aproximador universal en el sentit que pot emular qualsevol funció. Més enllà d'això, més capes no afegeixen a la capacitat de l'aproximador de funcions de la xarxa. Els models profunds (CAP . 2) són capaços d'extreure millors característiques que els models superficials i, per tant, les capes addicionals ajuden a aprendre les característiques de manera efectiva. Les arquitectures d'aprenentatge profund es poden construir amb un mètode de capa a capa cobdiciós. L'aprenentatge profund ajuda a desentranyar aquestes abstraccions i triar quines característiques milloren el rendiment. Per a les tasques d'aprenentatge supervisat, els mètodes d'aprenentatge profund eliminen l'enginyeria de funcions, traduint les dades en representacions intermèdies compactes similars als components principals, i deriven estructures en capes que eliminen la redundància en la representació. Els algoritmes d'aprenentatge profund es poden aplicar a tasques d'aprenentatge no supervisades. Es tracta d'un benefici important perquè les dades no etiquetades són més abundants que les dades etiquetades. Exemples d'estructures profundes que es poden formar de manera no supervisada són xarxes de creences profundes. Les xarxes neuronals profundes s'interpreten generalment en termes del teorema d'aproximació universal o inferència probabilística. El teorema d'aproximació universal clàssic es refereix a la capacitat de xarxes neuronals amb una sola capa oculta de mida finita per aproximar funcions contínues. El 1989, la primera demostració va ser publicada per George Cybenko per a funcions d'activació sigmoide i es va generalitzar per a arquitectures multicapa sigmoide el 1991 per Kurt Hornik. Els treballs recents també van mostrar que l'aproximació universal també es compleix per a funcions d'activació no fitades com la unitat lineal rectificada. El teorema d'aproximació universal de les xarxes neuronals profundes es refereix a la capacitat de les xarxes amb amplada limitada, però la profunditat pot créixer. Lu et al. va demostrar que si l'amplada d'una xarxa neuronal profunda amb activació de ReLU és estrictament més gran que la dimensió d'entrada, doncs la xarxa pot aproximar qualsevol funció integral de Lebesgue; en canvi si l'amplada és més petita o igual a la dimensió d'entrada, doncs una xarxa neuronal profunda no és un aproximador universal. La interpretació probabilística deriva del camp de l'aprenentatge automàtic. Té inferència, així com els conceptes d'optimització de la formació i les proves, relacionats amb l'ajust i la generalització, respectivament. Més específicament, la interpretació probabilística considera la no linealitat d'activació com una funció de distribució acumulada. La interpretació probabilística va portar a la introducció de la sortida com a regularitzador en les xarxes neuronals. La interpretació probabilística va ser introduïda per investigadors com Hopfield, Widrow i Narendra i popularitzada en enquestes com la de Bishop. Algunes fonts assenyalen que Frank Rosenblatt va desenvolupar i explorar tots els ingredients bàsics dels sistemes d'aprenentatge profund d'avui. El va descriure en el seu llibre Principis de Neurodinàmica: Perceptrons i la Teoria dels Mecanismes Brain, publicat per Cornell Aeronautical Laboratory, Inc., Universitat Cornell el 1962. El primer algorisme general d'aprenentatge de treball per a perceptrons supervisats, profunds i amb múltiples capes va ser publicat per Alexey Ivakhnenko i Lapa el 1967 va descriure una xarxa profunda amb vuit capes entrenades pel mètode de grup de manipulació de dades. Altres arquitectures de treball d'aprenentatge profund, específicament les construïdes per a visió informàtica, van començar amb el Neocognitron introduït per Kunihiko Fukushima el 1980. El terme Aprenentatge Profund va ser introduït a la comunitat d'aprenentatge de màquines per Rina Dechter el 1986, i a les xarxes neuronals artificials per Igor Aizenberg i col·laboradors el 2000, en el context de les neurones del llindar booleà. El 1989, Yann LeCun et al. va aplicar l'algorisme estàndard de retropropagació, que havia estat al voltant com el mode invers de la diferenciació automàtica des de 1970, a una xarxa neuronal profunda amb el propòsit de reconèixer codis postals escrits a mà en el correu. Mentre que l'algorisme funcionava, l'entrenament requeria 3 dies. El 1994, André de Carvalho, juntament amb Mike Fairhurst i David Bisset, van publicar els resultats experimentals d'una xarxa neuronal booleana multicapa, també coneguda com una xarxa neuronal sense pes, composta per un mòdul de xarxa neuronal d'extracció de característiques de 3 capes (SOFT) seguit per un mòdul de xarxa neuronal de classificació multicapa (GSN), que van ser entrenats de forma independent. Cada capa en el mòdul d'extracció de característiques extreu característiques amb una complexitat creixent respecte a la capa anterior. El 1995, Brendan Frey va demostrar que era possible entrenar (més de dos dies) una xarxa que contenia sis capes totalment connectades i diversos centenars d'unitats ocultes utilitzant l'algorisme d'activació-dormit, codesenvolupat amb Peter Dayan i Hinton. Molts factors contribueixen a la velocitat lenta, inclòs el problema del gradient de fuga analitzat el 1991 per Sepp Hochreiter. Des de 1997, Sven Behnke va estendre l'enfocament convolucional jeràrquic de l'abstracció neuronal Pyramid per connexions laterals i endarrerides amb la finalitat d'incorporar de manera flexible el context a les decisions i resoldre iterativament les ambigüitats locals. Els models més senzills que utilitzen característiques artesanals específiques de la tasca com filtres de Gabor i màquines vectorials de suport (SVM) van ser una elecció popular en els anys 90 i 2000, a causa del cost computacional de la xarxa neuronal artificial (ANN) i la manca de comprensió de com el cervell filtra les seves xarxes biològiques. S'han explorat tant l'aprenentatge superficial com l'aprenentatge profund (per exemple, les xarxes recurrents) d'ANN durant molts anys. Aquests mètodes mai van superar la tecnologia de mescla gaussiana no uniforme (per exemple, el model de barreja gaussiana gaussiana / el model de barreja de Gauss / el Model ocult de Màrkov (GMM-HMM) basat en models generatius de parla amb discriminació. S'han analitzat les dificultats clau gradient, incloent la disminució del gradient i una feble estructura de correlació temporal en els models predictius neuronals. Les dificultats addicionals van ser la manca de dades d'entrenament i la capacitat de computació limitada. La majoria dels investigadors de reconeixement de la parla es van allunyar de les xarxes neuronals per seguir el modelatge generatiu. Una excepció va ser a SRI International a finals de la dècada de 1990. Fundat per la NSA i DARPA del govern dels Estats Units, SRI va estudiar xarxes neuronals profundes en el reconeixement de la parla i l'altaveu. L'equip de reconeixement d'altaveus liderat per Larry Heck va reportar un èxit significatiu amb xarxes neuronals profundes en el processament de la parla a l'Institut Nacional d'Estàndards i Tecnologia d'Avaluació d'Oportunitats. La xarxa neuronal profunda de SRI es va desplegar llavors en el Verificador de Nuance, representant la primera aplicació industrial important d'aprenentatge profund. El principi d'elevar les característiques de la cara sobre l'optimització artesanal va ser explorat per primera vegada amb èxit en l'arquitectura d'un codificador profund en l'espectrograma de la llista o característiques lineals de la banca de filtres a finals de la dècada de 1990, mostrant la seva superioritat sobre les característiques Mel-Cepstral que contenen etapes de transformació fixa a partir d'espectrogrames. Les característiques bàsiques de la parla, les formes d'ona, més tard van produir excel·lents resultats a gran escala. Molts aspectes del reconeixement de la parla van ser assumits per un mètode d'aprenentatge profund anomenat memòria a curt termini (LSTM), una xarxa neuronal recurrent publicada per Hochreiter i Schmidhuber en 1997 L50 LSTM RNNs eviten el problema del gradient decreixent i poden aprendre Very Deep Learning tasques que requereixen records d'esdeveniments que van passar milers de passos de temps discrets abans, que és important per a la parla. El 2003, el LSTM va començar a ser competitiu amb els reconeixedors de parla tradicionals en certes tasques. Més tard es va combinar amb la classificació temporal connectiva (CTC) en les piles de RNN LSTM.ST53. El 2015, el reconeixement de veu de Google va experimentar un salt de rendiment dramàtic del 49% a través del LSTM entrenat per CTC, que van posar a disposició a través de Google Voice Search. En 2006, les publicacions de Geoff Hinton, Ruslan Salakhutdinov, Osindero i Teh van mostrar com una xarxa neuronal de moltes capes podia ser efectivament pre-entrenada una capa alhora, tractant cada capa al seu torn com una màquina de Boltzmann restringida sense supervisió, i després ajustant-la mitjançant retropropagació supervisada. Els articles es referien a aprendre per a xarxes de creences profundes. L'aprenentatge profund forma part dels sistemes d'art en diverses disciplines, en particular la visió informàtica i el reconeixement automàtic de la parla (ASR). Els resultats en conjunts d'avaluació comunament utilitzats com ara TIMIT (ASR) i MNIST (classificació d'imatge), així com una sèrie de tasques de reconeixement de veu de gran vocabulari han millorat de manera constant. Les xarxes neuronals convolucionals (CNNs) van ser reemplaçades per ASR, per CTC5252, per LSTM però tenen més èxit en la visió de l'ordinador. L'impacte de l'aprenentatge profund en la indústria va començar a principis de la dècada del 2000, quan les CNN ja processaven d'un 10% a 20% de tots els controls escrits als Estats Units, segons Yann LeCun. Aplicacions industrials d'aprenentatge profund al reconeixement de parla a gran escala, van començar al voltant del 2010. El 2009, el NIPS Workshop on Deep Learning for Speech Recognition va ser motivat per les limitacions dels models generatius profunds de parla, i la possibilitat que, donats els conjunts de dades més capaços i a gran escala, les xarxes neuronals profundes (DNN) poguessin arribar a ser pràctiques. Es creia que les DNN pre-training utilitzant models generatius de xarxes de creences profundes (DBN) superarien les principals dificultats de les xarxes neuronals. No obstant això, es va descobrir que la substitució del pre-training per grans quantitats de dades d'entrenament per retropropagació simple quan s'utilitzaven DNNs amb grans capes de producció dependents del context, produïa taxes d'error dramàticament més baixes que el model de mescla gaussiana (GMM)/Hidden Markov Model (HMM) i també que els errors de reconeixement produïts pels dos tipus de sistemes eren característicament diferents, oferint coneixements tècnics sobre com integrar l'aprenentatge profund en el sistema de descodificació de la parla altament eficient existent i en temps d'execució desplegat per tots els sistemes de reconeixement de la parla principals. Anàlisi al voltant de 2009–2010, contrastant el GMM (i altres models generatius de parla) contra els models DNN, estimulà la inversió industrial primerenca en l'aprenentatge profund per al reconeixement de la parla, eventualment conduint a un ús generalitzat i dominant en aquesta indústria. Aquesta anàlisi es va fer amb un rendiment comparable (menys de l'1,5% en la taxa d'error) entre les DNN discriminatives i els models generatius. El 2010, els investigadors van estendre l'aprenentatge profund del TIMIT al reconeixement de parla de vocabulari gran, adoptant grans capes de sortida de la DNN basades en estats HMM dependents del context construïts per arbres de decisió. Els avanços en el maquinari han impulsat l'interès renovat per l'aprenentatge profund. El 2009, Nvidia va participar en el que es va anomenar el big-bang de l'aprenentatge profund, com lesxarxes neuronals d'aprenentatge profund van ser entrenades amb les unitats de processament gràfic de Nvidia (GPUs). Aquell any, Andrew Ng va determinar que les GPUs podrien augmentar la velocitat dels sistemes d'aprenentatge profund en unes 100 vegades. En particular, les GPUs estan ben adaptades per als càlculs de matriu/vectors implicats en l'aprenentatge automàtic. GPUs acceleren els algoritmes d'entrenament per ordres de magnitud, reduint els temps d'execució de setmanes a dies. Més, el maquinari especialitzat i les optimitzacions d'algorismes d'aprenentatge profund es poden utilitzar per al processament eficient dels models d'aprenentatge profund. L'aprenentatge profund es pot definir com la suma dels següents factors: D\nL\n=\nN\nN\n+\nI\nA\n+\nG\nM\n+\nP\nP\n+\nS\nP\n+\nO\np\nt\ni\nm\ni\nt\na\nt\ni\no\nn\n\n\n{\\displaystyle DL=NN+IA+GM+PP+SP+Optimitation} Sent DL: Aprenentatge profund (Deep Learning), NN: xarxa neuronal (Neuronal Network), GM: Modelat gràfic (Graph Modeling), PP: Processament de patrons, SP: Processament de senyals (Signal processing). Són tècniques que reben major quantitat de dades, més capes que es desenvolupen aquestes jerarquies entre la informació que permetran obtenir característiques de forma supervisada i realitzar classificacions. - Aprendre la jerarquia - Tot el camí des dels píxels fins a arribar als classificadors. - Una capa extreu característiques de la sortida de la capa anterior. Capacitar totes les capes de manera conjunta - Aprenentatge d'extrem a extrem de les arquitectures de profunditat amb retropropagació. - Funciona bé quan les quantitats d'etiquetes és gran. - Estructura del model és important (per exemple, estructura convolucional) - Aprendre estructura estadística o dependències de les dades a partir de dades sense etiqueta - Formació per capes - És útil quan la quantitat d'etiquetes no és gran. Regressió logística Perceptró (Perceptron) Màquines Restringides Boltzmann Codificació Sparse xarxa neuronal convolucional xarxa neuronal recurrent Les xarxes de creences profundes Màquines Profund Boltzmann Codificació Jeràrquica Sparse L'aprenentatge profund està format per xarxes neuronals artificials (RNA) formades per un conjunt de neurones artificials interconnectades i diverses capes interrelacionades entre elles, la sortida d'una capa és l'entrada a la següent, amb la qual es poden enviar informació. El nombre de capes intermèdies i el nombre de neurones de cada capa dependrà del tipus d'aplicació que s'utilitzi. En la capa inicial hi ha una neurona per cada variable amb la qual volem predir la classe. Aquesta neurona rep les dades corresponents d'aquesta variable i ho envia a la superior. A la capa final hi ha una neurona per cada classe, aquesta agafa senyals de la capa inferior i aplica la probabilitat que pertany a la dada de la classe. La capa inferior rep les dades i les envia a la segona capa, aquesta converteix les dades en senyals i els envia a la segona capa i així successivament. Les xarxes neuronals presenten diversos problemes:\n- La quantitat de variables lliure que s'han d'entrenar és molt alta.\n- La funció objectiu no és convexa en els seus paràmetres, això causa que l'algoritme d'optimització s'estanqui en els seus òptims locals. - El resultat depèn de la inicialització dels paràmetres. Els problemes anteriors es poden solucionar o millorar de la següent manera: - Utilitzant les Xarxes de convolució Neuronal, utilitzades per al reconeixement de caràcters. - Múltiples inicialitzacions de la xarxa. - Utilització de Màquines de Boltzmann i Deep Belief Networks. L'aprenentatge és essencial per a totes les xarxes neuronals, per tant l'elecció de l'algoritme d'aprenentatge és un punt molt important pel desenvolupament d'una xarxa. Hi ha dos tipus d'aprenentatges, el supervisat i el no supervisat, el primer es proveeix d'una resposta correcta durant el seu entrenament i el segon es caracteritza per no tenir present l'objectiu que ha d'assolir. L'aprenentatge supervisat es caracteritza per un procés d'aprenentatge realitzat amb un entrenament controlat per un agent extern (supervisor) que determina la resposta que hauria de generar la xarxa a partir d'una entrada concreta. La funció del supervisor és controlar la sortida la xarxa i en el cas que no coincideixi amb la sortida desitjada es procedeix a modificar el pes de les connexions per aconseguir una sortida aproximada a l'esperada. El procediment del mètode amb xarxes de convolució supervisat és el següent: Entrada: Pixels / Característiques Capa 1: Filtració + No linealitat Capa 2: Pooling (posada en comú) Capa 3: Normalització Sortida: Característiques Aprenentatge per correcció d'error: Consisteix a ajustar el pes de les connexions de la xarxa en funció de l'error comès a la sortida. Aprenentatge per reforç: Es caracteritza per no indicar donat l'entrenament exactament la sortida que es desitja que proporcioni la xarxa en una determinada entrada. Aprenentatge estocàstic: Consisteix a fer canvis aleatoris al pes de les diferents connexions i avaluar el seu efecte a partir d'un objectiu Regla del Perceptró Aprenentatge Supervisat Hebbià Aquest tipus de xarxes representa un grau de similitud entre la informació que hi ha a l'entrada i les informacions que s'han anat mostrant durant el procediment. Està format per un conjunt de regles que donen a la xarxa una habilitat per aprendre associacions entre els diversos patrons que es formen en un conjunt. Un cop els patrons es coneixen, es permet que les xarxes facin tasques útils de reconeixement de patrons i habilitat per tenir memòria. Aprenentatge Hebbià Aprenentatge competitiu i comparatiu Des de la dècada de 2010, els avenços tant en algorismes d'aprenentatge automàtic com en maquinari d'ordinador han portat a mètodes més eficients per a la formació de xarxes neuronals profundes que contenen moltes capes d'unitats ocultes no lineals i una gran capa de sortida. Pel 2019, les unitats de processament gràfic (GPUs), sovint amb millores específiques de la IA, havien desplaçat les CPU com el mètode dominant d'entrenament a núvols comercials a gran escala. OpenAI va estimar el càlcul del maquinari utilitzat en els projectes d'aprenentatge profund més grans d'AlexNet (2012) a AlphaZero (2017), i va trobar un augment de 300.000 vegades en la quantitat de computació requerida, amb una línia de tendència de 3.4 mesos. Els circuits electrònics especials anomenats processadors d'aprenentatge profund van ser dissenyats per accelerar els algoritmes d'aprenentatge profund. Els processadors d'aprenentatge profund inclouen unitats de processament neuronal (NPU) en mòbils Huawei i servidors de computació al núvol com unitats de processament tensorial (TPU) en la Google Cloud Platform. Els semiconductors prims atòmicament es consideren prometedors per a un maquinari d'aprenentatge profund eficient des del punt de vista energètic, on s'utilitza la mateixa estructura de dispositius bàsics per a operacions lògiques i emmagatzematge de dades. El 2020, Marega et al. va publicar experiments amb un material de canal actiu de gran àrea per desenvolupar dispositius lògics en memòria i circuits basats en transistors d'efecte de camp de porta flotant (FGFET, de les seves sigles en anglès). En 2021, J. Feldmann et al. van proposar un accelerador de maquinari fotònic integrat per al processament convolucional paral·lel. Els autors identifiquen dos avantatges clau de la integració fotònica sobre els seus homòlegs electrònics: (1) transferència massiva de dades a través de divisió de longituds d'ona multiplexat en conjunció amb combois de freqüència, i (2) velocitats de modulació de dades extremadament altes. El seu sistema pot executar bilions d'operacions multi-acumulades per segon, indicant el potencial de la integració fotònica en aplicacions de IA que requereixen un gran volum de dades. El reconeixement automàtic per veu a gran escala és el primer i més convincent cas d'aprenentatge profund. LSTM RNNs pot aprendre Very Deep Learning tasques que impliquen intervals de diversos segons que contenen esdeveniments de parla separats per milers de passos de temps discrets, on un pas de temps correspon a uns 10 ms. El LSTM amb portes oblidades és competitiu amb els reconeixedors de veu tradicionals en certes tasques. L'èxit inicial en el reconeixement de la parla es basava en tasques de reconeixement a petita escala basades en TIMIT. El conjunt de dades conté 630 parlants de vuit dialectes principals de l'anglès americà, on cada parlant llegeix 10 frases. La seva mida petita permet provar moltes configuracions. El més important, la tasca TIMIT es refereix al reconeixement de la seqüència de telèfons, que, a diferència del reconeixement de la seqüència de paraules, permet models de llenguatge de bigrames de telèfon febles. Això permet analitzar més fàcilment la força dels aspectes de modelatge acústic del reconeixement de la parla. Les taxes d'error llistades a continuació, incloent aquests primers resultats i mesurades com a percentatge d'errors de telèfon (PER), s'han resumit des de 1991. El debut de DNNs per al reconeixement de l'altaveu a finals de la dècada de 1990 i el reconeixement de la parla al voltant de 2009-2011 i del LSTM al voltant de 2003-2007, va accelerar el progrés en vuit àrees principals: Tots els principals sistemes comercials de reconeixement de veu (per exemple, Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu i iFlyTek, i una sèrie de productes de parla de Nuance, etc.) es basen en l'aprenentatge profund. Un conjunt d'avaluació comú per a la classificació d'imatges és el conjunt de dades de la base de dades MNIST. MNIST està compost de dígits escrits a mà i inclou 60.000 exemples d'entrenament i 10.000 exemples de prova. Com amb TIMIT, la seva petita mida permet als usuaris provar diverses configuracions. Hi ha disponible una llista exhaustiva de resultats en aquest conjunt. El reconeixement d'imatge basat en l'aprenentatge profund s'ha convertit en superhumà, produint resultats més precisos que els concursants humans. Això va tenir lloc per primera vegada el 2011 en reconeixement de senyals de trànsit, i el 2014, amb reconeixement de cares humanes. Un altre exemple és la Dismorfologia Facial Anàlítica Nova (FDNA) utilitzada per analitzar casos de malformació humana connectats a una gran base de dades de síndromes genètics. Les xarxes neuronals s'han utilitzat per implementar models lingüístics des de principis dels anys 2000. LSTM ha ajudat a millorar la traducció automàtica i el modelatge de llenguatge. Altres tècniques clau en aquest camp són mostreig negatiu i incrustació de paraules. La incrustació de paraules, com el word2vec, es pot considerar com una capa representativa en una arquitectura d'aprenentatge profund que transforma una paraula atòmica en una representació posicional de la paraula relativa a altres paraules en el conjunt de dades; la posició es representa com un punt en un espai vectorial. L'ús de la incrustació de paraules com una capa d'entrada RNN permet a la xarxa analitzar frases i frases utilitzant una gramàtica vectorial de composició efectiva. Una gramàtica vectorial de composició es pot considerar com a gramàtica lliure de context probabilístic (PCFG) implementada per una RNN. Arquitectures automàtiques recursives construïdes a dalt de les paraules poden avaluar la similitud de frases i detectar la parafrasejament. Arquitectures neuronals profundes proporcionen els millors resultats per a l'anàlisi de sentiments, informació recuperació, enteniment, comprensió de la llengua parlada, traducció automàtica, enllaç d'entitat contextual, reconeixement d'estil d'escriptura, classificació de text i altres. Els desenvolupaments recents generalitzen la incrustació de paraules a la incrustació de frases. Google Translate (GT) utilitza una gran xarxa de memòria a curt termini (LSTM) d'extrem a extrem. Google Neural Machine Translation (GNMT) utilitza un mètode de traducció a màquina basat en exemples en el qual el sistema s'escapa de milions d'exemples. Es tradueix s'han de complir les frases alhora, en lloc de les peces. Google Translate admet més de cent idiomes.1145. La xarxa codifica la semàntica de la frase en lloc de simplement memoritzar les traduccions frase a frase. GT utilitza l'anglès com un intermedi entre la majoria de parelles lingüístiques. Un gran percentatge de drogues candidates no aconsegueixen l'aprovació de la normativa. Aquests errors són causats per una eficàcia insuficient (efecte en l'objectiu), interaccions no desitjades (efectes fora de l'objectiu) o efectes tòxics inesperats. Investigació ha explorat l'ús d'aprenentatge profund per predir els objectius biomoleculars, fora de l'objectiu, i efectes tòxics de productes químics ambientals en nutrients, productes domèstics i drogues. L'AtomNet és un sistema d'aprenentatge profund per al disseny de fàrmacs racionals basats en estructures. L'AtomNet es va utilitzar per predir biomolècules candidates per a malalties com el virus de l'Ebola i l'esclerosi múltiple. El 2017, les xarxes neuronals gràfiques es van utilitzar per primera vegada per predir diverses propietats de les molècules en un gran conjunt de dades toxicològiques. El 2019, les xarxes neuronals generatives es van utilitzar per produir molècules que van ser validades experimentalment fins a ratolins. L'aprenentatge profund reforçat s'ha utilitzat per aproximar el valor de possibles accions de màrqueting directe, definides en termes de variables RFM. Es va demostrar que la funció de valor estimat tenia una interpretació natural com a valor de vida del client. Els sistemes de recomanació han utilitzat aprenentatge profund per extreure característiques significatives per a un model de factor latent per a la música basada en contingut i les recomanacions de diari. S'ha aplicat aprenentatge multi-vista a les preferències d'usuari d'aprenentatge des de múltiples dominis. El model utilitza un enfocament híbrid col·laboratiu i basat en contingut i millora les recomanacions en múltiples tasques. Una ANN autoencoder es va utilitzar en bioinformàtica, per predir anotacions d'ontologia gènica i relacions de funció gènica.16162.\nEn la bininformàtica, l'aprenentatge profund es va utilitzar per predir la qualitat del son basant-se en les dades dels portadors i les prediccions sobre les dades dels registres de salut electrònics. S'ha demostrat que l'aprenentatge profund produeix resultats competitius en l'aplicació mèdica com la classificació de les cèl·lules canceroses, la detecció de lesions, la segmentació d'òrgans i la millora de la imatge. Les eines modernes d'aprenentatge profund demostren l'alta precisió de la detecció de diverses malalties i la utilitat del seu ús pels especialistes per millorar l'eficiència del diagnòstic. Trobar l'audiència mòbil apropiada per a la publicitat mòbil sempre és un repte, ja que molts punts de dades s'han de considerar i analitzar abans que un segment d’aquest objectiu es pugui crear i utilitzar en anuncis que serveixin qualsevol tipus de servidor d'anuncis. L'aprenentatge profund s'ha utilitzat per interpretar grans conjunts de dades publicitàries de molts dimensions. Molts punts de dades es recullen durant el cicle de publicitat d'Internet de sol·licitud/servei/clic. Aquesta informació pot formar la base de l'aprenentatge automàtic per millorar la selecció d'anuncis. L'aprenentatge profund s'ha aplicat amb èxit a problemes inversos com ara la desnobilització, la superrevolució, la pintura i la coloració de la pel·lícula. Aquestes aplicacions inclouen mètodes d'aprenentatge com Camps de beguda per a la Restauració efectiva d'imatges que s'entrena en un conjunt de dades d'imatge, i Deep Image Prior, que s'entrena en la imatge que necessita restauració. La reconstrucció d'imatges és la reconstrucció de les imatges subjacents a partir de les mesures relacionades amb la imatge. Diverses obres van mostrar el millor i superior rendiment dels mètodes d'aprenentatge profund en comparació amb els mètodes analítics per a diverses aplicacions, per exemple, les imatges espectrals i les imatges ultrasòliques. L'aprenentatge profund s'aplica amb èxit a la detecció de fraus financers, la detecció d'evasió fiscal i el blanqueig de capitals. El Departament de Defensa dels Estats Units va aplicar l'aprenentatge profund per entrenar robots en noves tasques a través de l'observació. Les xarxes neuronals informades en física s'han utilitzat per resoldre equacions diferencials tant en problemes “d’anada” i “ tornada” d'una manera impulsada per les dades. Un exemple és el flux de fluids reconstruïts governat per les equacions de Navier-Stokes. L'ús de xarxes neuronals informades en física no requereix la generació d’una malla, sovint amb un cost elevat, i en la qual es basen els mètodes CFD convencionals. Deep Belief Network (DBN): models de probabilitat generatius que estan compostos per múltiples capes i variables ocultes. Boltzmann Machine (BM): xarxa connectada de forma simètrica, implementada amb neurones que tenen com a funció decidir si estan connectades o no. Restricted Boltzmann Machine (RBM): Tipus especial de BM on no es permet la interrelació entre neurones del mateix tipus. Deep Neural Network (DNN): perceptró de múltiples capes , diverses d'elles ocultes. Deep Autoencoders: model discriminatiu DNN que utilitza com a sortida els mateixos valors de l'entrada, es tracta d'un model no supervisat. Quan l'objectiu és eliminar soroll es comporta com un model generatiu. L'aprenentatge profund està estretament relacionat amb una classe de teories del desenvolupament del cervell (específicament, desenvolupament neocòrtic) proposada per neurocientífics cognitius a principis dels anys 90. Aquestes teories de desenvolupament van ser instanciades en models computacionals, convertint-les en predecessors de sistemes d'aprenentatge profund. Aquests models de desenvolupament comparteixen la propietat que diverses dinàmiques d'aprenentatge proposades en el cervell (per exemple, una ona de factor de creixement dels nervis) donen suport a l'autoorganització una mica anàloga a les xarxes neuronals utilitzades en els models d'aprenentatge profund. Igual que el neocòrtex, les xarxes neuronals utilitzen una jerarquia de filtres en capes en què cada capa considera informació d'una capa anterior (o l'entorn operatiu), i després passa la seva sortida (i possiblement l'entrada original), a altres capes. Aquest procés dona una pila auto organitzadora de transductors, ben ajustada al seu entorn operatiu. Una descripció de 1995 va declarar: «... el cervell de l'infant sembla organitzar-se sota la influència d'ones de anomenats factors tròfics... les diferents regions del cervell es connecten seqüencialment, amb una capa de teixit madurant abans d'una altra i així fins que tot el cervell sigui madur». S'han utilitzat diversos enfocaments per investigar la plausibleització dels models d'aprenentatge profund des d'una perspectiva neurobiològica. D'una banda, s'han proposat diverses variants de l'algorisme de retropropagació per tal d'augmentar el seu realisme de processament. Altres investigadors han argumentat que formes no supervisades d'aprenentatge profund, com les basades en models generatius jeràrquics i xarxes de creences profundes, poden estar més a prop de la realitat biològica. En aquest sentit, els models generatius de xarxes neuronals han estat relacionats amb l'evidència neurobiològica sobre el processament basat en el mostreig en l'escorça cerebral. Encara que és una comparació sistemàtica entre l'organització del cervell humà i la neuronal\nEncara no s'ha establert la codificació en xarxes profundes, s'han informat de diverses analogies. Per exemple, els càlculs de les unitats d'aprenentatge profund podrien ser similars als de les neurones reals i les poblacions neuronals. De la mateixa manera, les representacions desenvolupades per models d'aprenentatge profund són similars a les que es mesuren en el sistema visual de primats, tant en la unitat única com en els nivells de població. El laboratori IA del Facebook realitza tasques com ara etiquetar automàticament les imatges pujades amb els noms de les persones que hi ha. DeepMind Technologies de Google va desenvolupar un sistema capaç d'aprendre a jugar a videojocs Atari utilitzant només píxels com a entrada de dades. El 2015 van demostrar el seu sistema AlphaGo, que va aprendre el joc de Go prou bé per vèncer un jugador professional de Go. Google Translate utilitza una xarxa neuronal per traduir entre més de 100 llengües. El 2017, Covariant.ai va ser llançat, que se centra en la integració de l'aprenentatge profund a les fàbriques. A partir de 2008, investigadors de la Universitat de Texas a Austin (UT) van desenvolupar un marc d'aprenentatge automàtic anomenat Formació d'un agent manualment a través de Reforç Avaluatiu, o TAMER, que va proposar nous mètodes per a robots o programes d'ordinador per aprendre a fer tasques interaccionant amb un instructor humà. Es va desenvolupar per primera vegada com TAMER, un nou algorisme anomenat Deep TAMER es va introduir més tard el 2018 durant una col·laboració entre els investigadors de l'Exèrcit dels Estats Units (ARL) i UT. Deep TAMER va utilitzar l'aprenentatge profund per proporcionar a un robot la capacitat d'aprendre noves tasques mitjançant l'observació. Amb Deep TAMER, un robot va aprendre una tasca amb un entrenador humà, veient fluxos de vídeo o observant un ésser humà realitzant una tasca en persona. El robot més tard va exercir la tasca amb l'ajuda d'alguns entrenadors de l'entrenador, que van proporcionar comentaris com “bon treball” i “dolent treball”. En els últims cinc anys, companyies com Google, Apple i IBM, han comprat de forma agressiva startups i investigadors experts en aquests mètodes. Per als consumidors diaris, això es tradueix en un millor software, capaç d'ordenar fotografies, entendre la nostra veu i traduir textos de llengües estrangeres. Tots aquests mètodes no es basen en la sintaxi, sinó amb la semàntica, una frase pot estar sintàcticament correcta però semànticament no dir res. Per altra banda altres companyies utilitzen estadístiques per aconseguir resultats similars. L'aprenentatge profund ha atret tant crítiques com comentaris, en alguns casos de fora del camp de la informàtica. Vegeu també: IA explicable Una crítica principal es refereix a la manca de teoria al voltant d'alguns mètodes. L'aprenentatge en les arquitectures profundes més comunes s'implementa utilitzant un descens de gradient ben entès. No obstant això, la teoria que envolta altres algorismes, com la divergència contrastiva, és menys clara. (per exemple, convergeix?) En cas afirmatiu, amb quina rapidesa? Què aproxima?) Els mètodes d'aprenentatge profund es veuen sovint com una caixa negra, amb la majoria de les confirmacions fetes empíricament, en lloc de teòricament. Uns altres assenyalen que l'aprenentatge profund hauria de considerar-se un pas cap a la consecució d'una IA forta, no com una solució que abasti a tots. Malgrat el poder dels mètodes d'aprenentatge profund, encara manquen de gran part de la funcionalitat necessària per a fer realitat aquest objectiu. El psicòleg d'investigació Gary Marcus va assenyalar: Realistament, l'aprenentatge profund és només part del gran desafiament de construir màquines intel·ligents. Aquestes tècniques no tenen maneres de representar les relacions causals (...) no tenen formes òbvies de realitzar inferències lògiques, i també estan molt lluny d'integrar el coneixement abstracte, com ara informació sobre quins objectes són, per a què són i com s'utilitzen normalment. Els sistemes A.I. més poderosos, com Watson (...) utilitzen tècniques com l'aprenentatge profund com un sol element en un conjunt de tècniques molt complicat, que van des de la tècnica estadística de la inferència bayesiana fins al raonament deductiu. En referència a la idea que la sensibilitat artística podria ser inherent en nivells relativament baixos de la jerarquia cognitiva, una sèrie publicada de representacions gràfiques dels estats interns de les xarxes neuronals profundes (20-30 capes) intentant discernir dins de dades essencialment aleatòries les imatges en les quals van ser entrenades demostren una apel·lació visual: l'avís de recerca original va rebre més de 1.000 comentaris, i va ser el tema del que va ser durant un temps l'article més freqüentment accedit al lloc web de The Guardian. Algunes arquitectures d'aprenentatge profund mostren comportaments problemàtics, com ara classificar amb confiança les imatges irrecognoscibles com a pertanyents a una categoria familiar d'imatges normals (2014) i classificar erròniament les pertorbacions minúscules de les imatges classificades correctament (2013). Goertzel va plantejar la hipòtesi que aquests comportaments són deguts a limitacions en les seves representacions internes i que aquestes limitacions inhibeixen la integració en les arquitectures d'intel·ligència general artificial multicomponent (AGI) heterogènies. L'aprenentatge d'una gramàtica (visual o lingüística) de les dades d'entrenament seria equivalent a restringir el sistema a raonaments de sentit comú que funciona en termes de regles de producció gramaticals i és un objectiu bàsic de l'adquisició de llenguatge humà i intel·ligència artificial (AI). A mesura que l'aprenentatge profund es mou des del laboratori al món, la investigació i l'experiència demostren que les xarxes neuronals artificials són vulnerables als hacks i a l'engany. En identificar patrons que aquests sistemes utilitzen per funcionar, els atacants poden modificar les entrades a les xarxes neuronals artificials (ANN) de tal manera que l'ANN troba un combat que els observadors humans no reconeixerien. Per exemple, un atacant pot fer canvis subtils en una imatge de tal manera que l'ANN troba una coincidència, tot i que la imatge no sembla un objectiu de cerca. Aquesta manipulació es denomina un “atac adversari”. El 2016, els investigadors van utilitzar una ANN per doctorar imatges en forma de prova i error, identificar els punts focals d'una altra i així generar imatges que la van enganyar. Les imatges modificades no eren diferents dels ulls humans. Un altre grup va mostrar que les impressions d'imatges doctorades després fotografiades trucaven amb èxit un sistema de classificació d'imatges. Una defensa és la cerca d'imatges invertida, en la qual es presenta una possible imatge falsa a un lloc com TinEye que pot trobar altres instàncies. Un refinament consisteix a cercar utilitzant només parts de la imatge, per identificar imatges de les quals aquesta peça pot haver estat presa. Un altre grup va demostrar que certs espectacles psicodèlics podien enganyar un sistema de reconeixement facial en pensar que la gent normal era celebritat, permetent potencialment que una persona suplantés a una altra. El 2017, els investigadors van afegir adhesius als senyals d'aturada i van fer que una ANN els classifiqués malament. No obstant això, les ANNs poden ser entrenades per detectar intents d'engany, atacants i defensors potencialment líders en una carrera armamentista similar a la que ja defineix la indústria de defensa malware. Les ANNs han estat entrenades per derrotar el programari anti-malware basat en ANN atacant repetidament una defensa amb malware que va ser contínuament alterat per un algorisme genètic fins que va enganyar l'anti-malware mentre conservava la seva capacitat de danyar l'objectiu. El 2016, un altre grup va demostrar que certs sons podrien fer que el sistema de comandament de veu de Google Now obri una adreça web en particular, i va fer la hipòtesi que això podria servir-se com un esglaó per a més atacs (per exemple, obrir una pàgina web que alberga malware). En enverinament de dades, les dades falses es troben contínuament de contraban en un sistema d'aprenentatge automàtic establert per evitar que arribi a dominar-se. Deep Learning (Article original), respecte a l'article original s'ha intentat fer un article entenedor per a tot el públic a diferència de l'original que és molt tècnic, també s'ha donat més importància als dos tipus d'aprenentatge supervisat i no supervisat i s'ha fet una comparació entre ells buscant els avantatges i els defectes de cadascun d'ells. L'article també comenta com està afectant actualment aquesta tecnologia a les grans empreses i perquè la utilitzen."
  },
  {
    "url": "https://cs.wikipedia.org/wiki/Hlubok%C3%A9_u%C4%8Den%C3%AD",
    "title": "Hluboké učení – Wikipedie",
    "content": "Hluboké učení[2] (anglicky deep learning) je disciplína spadající do kategorie strojového učení. Zabývá se vývojem a aplikací algoritmů pro učení umělých neuronových sítí s hierarchickou reprezentací vlastností (angl. features). Učení může probíhat tzv. s učitelem (angl. supervised), bez učitele (angl. unsupervised), nebo může jít o kombinaci obojího (angl. semi-supervised).[3] Hluboké učení umožňuje extrakci komplexních vlastností (angl. features) z neupravených dat.[4]:s.199–200 Například při zpracování obrazu nižší vrstvy modelu identifikují hrany v obrazových datech sousedících pixelů, zatímco vyšší vrstvy pracují s obecnějšími koncepty bližšími lidskému chápání, jako jsou číslice, písmena nebo tváře. Mělké učení může dosáhnout podobných výsledků s méně prostředky, takže existuje i ekologičtější přístup.[5] Různé varianty hlubokého učení jsou využívány v oborech počítačového vidění, rozpoznávání řeči, zpracování přirozeného jazyka, strojového překladu, bioinformatiky, návrhu léčiv, analýzy lékařských snímků, výzkumu klimatu a hraní deskových her (kde tyto metody dosáhly srovnatelných nebo lepších výsledků než profesionální hráči).[6][7][8][9][10] Pojem „hluboké“ se vztahuje k počtu vrstev neuronů, ve kterých probíhá transformace dat. Přesněji řečeno jde o počet transformací dat cestou ze vstupu (vstupní vrstva) na výstup (výstupní vrstva). Pro dopředné neuronové sítě je hloubka sítě dána jako počet mezilehlých skrytých vrstev. Neexistuje proto obecně platná hranice, která by oddělovala hluboké učení od ne-hlubokého učení. Hloubka sítě musí být minimálně 1, neboť tehdy je již síť schopna aproximovat libovolnou vektorovou funkci.[11] Přidávání dalších vrstev nezlepšuje aproximační schopnosti sítě, ale umožňuje extrahovat z dat komplexnější vlastnosti. Větší počet vrstev tak zefektivňuje proces učení. U rekurentních neuronových sítí může signál putovat každou vrstvou několikrát.[12] Pro odhad parametrů sítě (trénování) se obvykle používá algoritmus zpětného šíření chyby. Trénování probíhá ve dvou fázích, tj. nejprve předučení sítě dopředným směrem např. pomocí autoenkodérů (učení bez učitele) a poté doučení sítě zpětným směrem (učení s učitelem). Eliminuje se tak tlumení zpětného šíření chyby. Hlavní dopad hlubokého učení nastal po roce 2000. Podle Yanna LeCuna[13] již tehdy zpracovávaly dopředné konvoluční sítě 10% až 20% objemu všech šeků vypsaných ve Spojených státech. Využití hlubokého učení pro zpracování řeči začalo ve větším měřítku kolem roku 2010. Dalšímu zájmu o hluboké učení napomohly pokroky v oblasti hardwaru. Společnost Nvidia napomohla v roce 2009 tzv. \"velkému třesku\" hlubokého učení, které probíhalo tréninkem neuronových sítích na GPU.[14] Andrew Ng toho roku odhadl, že díky GPU se může rychlost systémů hlubokého učení zvýšit až 100krát.[15] To zejména díky optimalizaci GPU pro práci s vektory a maticemi, jichž se ve strojovém učení užívá.[16][17][18] V roce 2012 tým vedený Georgem E. Dahlem využil víceúlohovou hlubokou neuronovou síť k předpovědi biologického cíle konkrétního léku.[19][20] V roce 2014 Hochreiterova skupina využila hluboké učení k detekci nežádoucích účinků chemikálií obsažených ve výživě, výrobcích pro domácnost a lécích na životní prostředí.[21][22][23] Další významné pokroky v oblasti rozpoznávání obrazu proběhly v letech 2011 a 2012. Rychlé konvoluční neuronové sítě využívající pro svůj běh GPU přinesly důležitý pokrok v oblasti počítačového vidění.[16][18][24][25][12] V roce 2011 tento přístup poprvé vedl k překonání lidských schopností v rozpoznávání obrazových vzorů. Vedl rovněž k vítězství v soutěži rozpoznávání rukou psaného čínského písma.[26] V říjnu roku 2012 podobný systém s převahou zvítězil v soutěži projektu ImageNet.[8] V listopadu téhož roku další z těchto systémů zvítězil v ICPR soutěži v analýze lékařských snímků pro detekci rakoviny.[27] V letech 2013 a 2014 se chybovost systémů v úloze ImageNet dále snižovala, podobný trend zaznamenalo rovněž odvětví rozpoznávání řeči. Dle některých výzkumníků bylo vítězství v ImageNet základem tzv. „revoluce hlubokého učení“ (angl. deep learning revolution), která transformovala celé odvětví umělé inteligence.[28] V březnu 2019 byli Yoshua Bengio, Geoffrey Hinton a Yann LeCun vyznamenání Turingovou cenou za konceptuální a inženýrský průlom, který ustavil hluboké neuronové sítě jako stěžejní pilíř výpočetních věd. Během dekády 2010-2019 umožnily pokroky v učících algoritmech a počítačovém hardwaru značné zefektivnění práce s hlubokými neuronovými sítěmi charakteristickými vysokým počtem skrytých vrstev a širokou výstupní vrstvou.[29] V roce 2019 už GPU s modifikacemi pro AI výpočty nahradily CPU v roli dominantního prostředku pro trénink komerčně využívané AI v cloudovém prostředí.[30] Společnost OpenAI odhadla odstup ve výkonu hardwaru využitého pro projekt hlubokého učení AlexNet z roku 2012 a pro projekt AlphaZero z roku 2017 a zjistila 300tisícinásobné navýšení výpočetního výkonu, tedy zdvojnásobení výkonu každého 3,4 měsíce[31][32] (pro srovnání Moorův zákon přisuzuje pokroku v hardwaru zdvojnásobování výpočetního výkonu každé 2 roky). Pro akceleraci výpočtů souvisejících s hlubokým učením byly vyvinuty specializované výpočetní obvody - NPU (angl. neural processing unit) v mobilních zařízeních[33] a TPU (angl. tensor processing unit) pro výpočetní servery v rámci Google Cloud Platform.[34] Společnost Cerebras Systems vyvinula dedikovaný systém pro práci s modely hlubokého učení, nazvaný CS-2, založený na největším procesoru vůbec, druhé generaci WSE-2 (Wafer Scale Engine).[35][36] V roce 2021 J. Feldmann a kolektiv navrhli pro účely konvolučního zpracování integrovaný fotonický hardwarový akcelerátor.[37] Autoři uvádějí dvě klíčové výhody fotonického řešení ve srovnání s klasickým elektronickým: (1) masivně paralelní zpracování dat díky multiplexu v rámci vlnových délek, (2) extrémně rychlou datovou modulaci.[37] Jejich systém dokáže provádět bilióny operací za sekundu, čímž vykazuje značný potenciál fotoniky pro budoucí využití v systémech spoléhajících na AI.[37] Optimální hardwarová implementace provádění výpočtů umožňuje značně urychlit nalezení výsledku. Používají se různé způsoby: Rozpoznávání obrazu založené na hlubokém učení již překonalo schopnosti člověka. Poprvé se tak stalo v roce 2011 při rozpoznávání dopravního značení, poté v roce 2014 v rozpoznávání lidských tváří.[39][40] Vozidla využívající hluboké učení dnes analyzují 360° kamerové záběry v reálném čase.[41] Pro rozpoznávání obrazu se dnes užívají Konvoluční neuronové sítě (CNN), které jsou specializovaným typem umělých neuronových sítí, které alespoň v jedné ze svých vrstev používají matematickou operaci zvanou konvoluce namísto obecného násobení matic.[42] Jsou speciálně navrženy pro zpracování pixelových dat a používají se při rozpoznávání a zpracování obrazu. V porovnání s jinými algoritmy klasifikace obrazu používají CNN relativně málo předzpracování. To znamená, že síť se učí optimalizovat filtry (nebo jádra) prostřednictvím automatického učení, zatímco v tradičních algoritmech jsou tyto filtry vytvářeny ručně. Tato nezávislost na předchozích znalostech a lidských zásazích do extrakce příznaků je velkou výhodou. Pro klasifikaci ručně psaných číslic se běžně využívá datový soubor MNIST. Jde o vzorník ručně psaných číslic obsahující 60 tisíc trénovacích položek a 10 tisíc testovacích položek. Relativně malý datový objem umožňuje testovat různé konfigurace systému pro rozpoznávání.[43] Automatické rozpoznávání řeči je nejočividnější aplikací hlubokého učení s širokým využitím. Rekurentní neuronové sítě jsou schopné zpracovávat i „velmi hluboké úlohy“,[12] které obsahují intervaly několika sekund trvající mluvené řeči oddělené několika tisíci diskrétních časových kroků, kde jeden krok odpovídá zhruba 10 ms. Počátečního úspěchu v rozpoznávání řeči bylo dosaženo na úlohách malého rozsahu založených na TIMIT (vzorník mluvené americké angličtiny od různých mluvčích doplněný transkripcí). Tato data obsahují po deseti větách od 630 mluvčích hovořících v osmi hlavních dialektech americké angličtiny.[44] Všechny dnes komerčně široce využívané systémy pro rozpoznávání řeči (Microsoft Cortana, Amazon Alexa, Google Now, Apple Siri a další) jsou založeny na hlubokém učení.[4][45][46] Klíčovou technikou v tomto oboru je vnoření slov (angl. word embedding). Tento přístup reprezentuje každé jednotlivé slovo jako pozici ve vícerozměrném prostoru relativně vůči dalším slovům obsaženým v datovém souboru. Využití vnoření slov na vstupní vrstvě umožňuje rekurentním neuronovým sítím provádět rozbor vět a frází efektivní vektorovou gramatikou. Rekurzivní autoenkodéry umožňují posoudit příbuznost vět a detekovat parafráze.[47] Nedávné pokroky zobecnily vnoření slov na vnoření celých vět (angl. sentence embedding). Překladač Google a Google Neural Machine Translation využívají techniky hlubokého učení. Překladač Google díky tomu překládá významy vět namísto překladu jednotlivých frází. Podporuje přes sto jazyků[48] a využívá angličtinu jako prostředníka pro překlad mezi libovolnými dvěma jazyky.[49] Hluboké učení prokázalo konkurenceschopnost v medicínských aplikacích, jako je klasifikace rakovinných buněk, detekce lézí, vylepšení obrazu a dalších.[50][51] Moderní nástroje využívající hluboké učení prokazují vysokou přesnost v detekci různých chorob a jejich využití dovoluje odborníkům zvyšovat efektivitu diagnóz.[52][53] Jde o rekonstrukci obrazu na základě podkladových dat, která se k obrazu vážou, jako je například spektroskopické zobrazování nebo ultrazvukové zobrazování. Několik prací na toto téma demonstrovalo lepší výkon systémů využívajících hluboké učení ve srovnání s uvedenými metodami.[54][55] Hluboké učení se úspěšně uplatňuje v detekci finančních podvodů,[56] detekci daňových úniků a praní špinavých peněz.[57] Úzce spjaté s obecným rozpoznáváním obrazu je využití hlubokého učení pro různé typy zadání týkající se uměleckých děl. Hluboké neuronové sítě se prokázaly schopnými např.: Hluboké učení na sebe strhlo vlnu kritiky, a to nejen z okruhu počítačových věd. Hlavní kritika hlubokého učení se týká nedostatku teoretického pozadí jeho některých metod.[60] V nejběžnějších případech je učení implementováno pomocí dobře prozkoumané gradientní metody. Jiné metody však takto dobře vysvětleny nejsou. Často je na ně nahlíženo jako na černou skříňku, kde je většina závěrů o fungování modelu utvářena na základě empirických výsledků, ne teorie.[61] Některé systémy hlubokého učení vykazují problematické chování,[62] jako je například zařazování objektivně nerozpoznatelných obrázků do známých kategorií[63] nebo naopak nesprávné zařazování obrázků na základě pouze drobných změn.[64] Goertzel se domnívá, že toto chování může být způsobeno limitací vnitřní reprezentace.[62] S tím, jak se hluboké učení postupně dostává do běžného užívání, výzkum a zkušenosti ukazují, že jsou umělé neuronové sítě zranitelné podvratným jednáním.[65] Útočník může například pomocí analýzy vnitřního fungování systému upravit hledaný obrázek tak, že síť nalezne shodu, přestože z lidského hlediska jde o zcela nesouvisející obrázky.[66] V roce 2016 výzkumníci využili jednu neuronovou síť k takovéto záměrné úpravě obrázků. Identifikovali vzory chování druhé sítě a pak byli schopni vygenerovat obrázky, které dokázaly tuto síť zmást. Upravené obrázky vypadaly pro lidské oko nerozlišitelně. Jiná skupina ukázala, že i vytištěné upravené obrázky jsou následně chybně zařazovány.[67] Jedna z forem obrany proti takovému postupu by mohla souviset s využitím zpětného vyhledávání obrázků (angl. reverse image search, kde je podezřelý obrázek nejprve uploadován do služby jako např. TinEye, kde lze vyhledat ostatní jeho výskyty. Vylepšením může být vyhledávání pouze částí obrázku.[68] Umělé neuronové sítě mohou být nicméně trénovány k rozpoznávání případů takového napadení. To může ve výsledku vést k závodům ve zbrojení podobným malwarové scéně. Existuje rovněž technika „otravy dat“ (angl. data poisoning), kdy jsou podvržená data propašována do tréninkového datového souboru, což učícímu se systému znemožňuje dosahovat přesných výsledků.[67] V tomto článku byl použit překlad textu z článku Deep learning na anglické Wikipedii."
  },
  {
    "url": "https://da.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Wikipedia, den frie encyklopædi",
    "content": "Deep learning (også: deep structured learning eller hierarchical learning) er en del af området maskinlæring via kunstige neurale netværk. Deep learning er baseret på en konfiguration af algoritmer, som forsøger at modellere abstraktioner i data på højt niveau ved at anvende mange proceslag med komplekse strukturer, bestående af mange lineare og ikke-linear afbildninger.[1][2]\nDeep learning kan være overvåget, halv-overvåget eller uovervåget og har fået stor gennemslagskraft indenfor blandt andet billedklassificering, computervision, sprogbehandling, biostatistik og lydgenkendelse.[2] [3] Geoffrey Hinton, Yann LeCun och Yoshua Bengio er pionerer indenfor deep learning.[4]"
  },
  {
    "url": "https://ary.wikipedia.org/wiki/%D8%AA%D8%B9%D9%84%D8%A7%D9%85_%D8%BA%D8%A7%D8%B1%D9%82",
    "title": "تعلام غارق - ويكيپيديا",
    "content": "التعلام لغارق (ب النڭليزية deep learning) هوّ مجال فرعي ديال التعلام لماكيني لي كيركّز على تخدام ريزوات د النورونات neural networks ب بزاف د الطّبقات، على ود باش يدير عطاشات بحال لكلاصاج classification، الرجوع regression، و تعلام لخصايل feature learning. هاد المجال واخد لإلهام من النيورولوجيا (ب لعربية علم الأعصاب)، و كيهتم ب تكويم النورونات الصطيناعية ف \"طبقات\" و \"التدريب\" ديالها باش يتريطيو الداطا. لوصف \"غارق\" كيشير ل تخدام بزاف د الطبقات (مابين 3 تال لمئات ولّا لألاف) ف الريزو. الطرقان ديال التدريب يقدرو يكونو ب لحضيان supervised، ب نص حضيان semi-supervised ولّا بلا حضيان unsupervised.[2] الشكال اللولين ديال الريزوات د النورونات تستلهمو من لپروصيصاج د لمعلومات و التواصل لموزّع ف لعڭدات ديال لأنظمة لبيولوجية، سيرتو الدماغ د بنادم. ولايني النورونات الصطيناعية دابا ماكيهدفوش باش يموضيليو لخدمة ديال الدماغ، و بشكل عام كيتّعتابرو موضيلات ب جودة عيانة ل هاد لغاراض.[3]"
  },
  {
    "url": "https://de.wikipedia.org/wiki/Deep_Learning",
    "title": "Deep Learning – Wikipedia",
    "content": "Deep Learning (deutsch mehrschichtiges Lernen, tiefes Lernen oder tiefgehendes Lernen) bezeichnet eine Methode des maschinellen Lernens, die künstliche neuronale Netze (KNN) mit zahlreichen Zwischenschichten (englisch hidden layers) zwischen Ein- und Ausgabeschicht einsetzt und dadurch eine umfangreiche innere Struktur herausbildet. Deep Learning erlaubt die Verarbeitung und Analyse komplexer Datenmuster; dazu verwendet Deep Learning tiefe hierarchische neuronale Netze, die automatisch abstrakte Merkmale aus den Daten extrahieren. Dies ermöglicht eine effiziente Verarbeitung komplexer Informationen, was wiederum zu präzisen Vorhersagen und Entscheidungen in verschiedenen Anwendungen führt. Die in der Anfangszeit der künstlichen Intelligenz gelösten Probleme waren für den Menschen intellektuell schwierig, aber für Computer einfach zu verarbeiten. Diese Probleme ließen sich durch formale mathematische Regeln beschreiben. Die wahre Herausforderung an die künstliche Intelligenz bestand jedoch in der Lösung von Aufgaben, die für die Menschen leicht durchzuführen sind, deren Lösung sich aber nur schwer durch mathematische Regeln formulieren lassen. Dies sind Aufgaben, die der Mensch intuitiv löst, wie Sprach- oder Gesichtserkennung.[2][3] Eine computerbasierte Lösung für diese Art von Aufgaben beinhaltet die Fähigkeit von Computern, aus der Erfahrung zu lernen und die Welt in Bezug auf eine Hierarchie von Konzepten zu verstehen. Hierbei ist jedes Konzept durch seine Beziehung zu einfacheren Konzepten definiert. Durch das Sammeln von Wissen aus der Erfahrung vermeidet dieser Ansatz die Notwendigkeit für die menschlichen Bediener, all das Wissen, das der Computer für seine Arbeit benötigt, formal spezifizieren zu müssen. Die Hierarchie der Konzepte erlaubt es dem Computer, komplizierte Konzepte zu erlernen, indem er sie aus einfacheren zusammensetzt. Wenn man ein Diagramm zeichnet, das zeigt, wie diese Konzepte übereinander aufgebaut werden, dann ist das Diagramm tief, mit vielen Schichten. Aus diesem Grund wird dieser Ansatz in der künstlichen Intelligenz „Deep Learning“ genannt.[2][4] Es ist schwierig für einen Computer, die Bedeutung von rohen sensorischen Eingangsdaten zu verstehen, wie in der Handschrifterkennung, wo ein Text zunächst nur als eine Sammlung von Bildpunkten existiert. Die Überführung einer Menge von Bildpunkten in eine Kette von Ziffern und Buchstaben ist sehr kompliziert. Komplexe Muster müssen aus Rohdaten extrahiert werden. Das Lernen oder Auswerten dieser Zuordnung scheint unüberwindbar schwierig, wenn sie manuell programmiert würde.[2] Eine der häufigsten Techniken in der künstlichen Intelligenz ist maschinelles Lernen. Maschinelles Lernen ist ein selbstadaptiver Algorithmus. Deep Learning, eine Teilmenge des maschinellen Lernens, nutzt eine Reihe hierarchischer Schichten bzw. eine Hierarchie von Konzepten, um den Prozess des maschinellen Lernens durchzuführen. Die hierbei benutzten künstlichen neuronalen Netze wurden inspiriert von Gehirnen, in denen biologische Neuronen miteinander verbunden sind. Die erste Schicht des neuronalen Netzes, die sichtbare Eingangsschicht, verarbeitet eine Rohdateneingabe, wie beispielsweise die einzelnen Pixel eines Bildes. Die Dateneingabe enthält Variablen, die der Beobachtung zugänglich sind, daher „sichtbare Schicht“.[2][5][6][7] Die erste Schicht leitet ihre Ausgaben an die nächste Schicht weiter. Diese zweite Schicht verarbeitet die Informationen der vorherigen Schicht und gibt das Ergebnis ebenfalls weiter. Die nächste Schicht nimmt die Informationen der zweiten Schicht entgegen und verarbeitet sie weiter. Dies geht über alle Ebenen des künstlichen neuronalen Netzes so weiter. Diese Schichten werden als versteckte Ebenen (englisch hidden layers) bezeichnet. Die in ihnen enthaltenen Merkmale werden zunehmend abstrakt. Ihre Werte sind nicht in den Ursprungsdaten angegeben. Stattdessen muss das Modell bestimmen, welche Konzepte für die Erklärung der Beziehungen in den beobachteten Daten nützlich sind. Das Ergebnis wird in der letzten, wieder sichtbaren Schicht ausgegeben. Hierdurch wird die gewünschte komplizierte Datenverarbeitung in eine Reihe von verschachtelten einfachen Zuordnungen unterteilt, die jeweils durch eine andere Schicht des Modells beschrieben werden.[8][2][6][4][9] Die Group method of data handling-KNNs (GMDH-ANN) der 1960er-Jahre von Oleksij Iwachnenko waren die ersten Deep-Learning-Systeme des Feedforward-Multilayer-Perzeptron-Typs.[10][11][12] Karl Steinbuchs Lernmatrix[13] war eines der ersten künstlichen neuronalen Netze, das aus mehreren Schichten von Lerneinheiten oder lernenden „Neuronen“ bestand. Damit war er einer der Wegbereiter des Deep Learning, bei dem es um tiefe neuronale Netze geht, die viele Aufgaben erlernen können, bei denen frühere einschichtige Perzeptronen scheitern. Weitere Deep-Learning-Ansätze, vor allem aus dem Bereich des maschinellen Sehens, begannen mit dem Neocognitron, einer Convolutional Neural Network (CNN)-Architektur, die von Kunihiko Fukushima 1980 entwickelt wurde.[14] Alex Waibels CNN namens TDNN (1987) wurde durch backpropagation trainiert und erzielte Bewegungsinvarianz.[15] Im Jahr 1989 verwendeten Yann LeCun und Kollegen den Backpropagation-Algorithmus für das Training mehrschichtiger KNNs (siehe Multi-Layer-Perzeptron), mit dem Ziel, handgeschriebene Postleitzahlen zu erkennen.[16] Sven Behnke hat seit 1997 in der Neuronalen Abstraktionspyramide[17] den vorwärtsgerichteten hierarchisch-konvolutionalen Ansatz durch seitliche und rückwärtsgerichtete Verbindungen erweitert, um so flexibel Kontext in Entscheidungen einzubeziehen und iterativ lokale Mehrdeutigkeiten aufzulösen. Der Begriff „Deep Learning“ wurde im Kontext des maschinellen Lernens erstmals 1986 von Rina Dechter verwendet, wobei sie hiermit ein Verfahren bezeichnet, bei dem alle verwendeten Lösungen eines betrachteten Suchraums aufgezeichnet werden, die zu keiner gewünschten Lösung geführt haben. Die Analyse dieser aufgezeichneten Lösungen soll es ermöglichen, anschließende Versuche besser zu steuern und somit mögliche Sackgassen in der Lösungsfindung frühzeitig zu verhindern.[18] Heute wird der Begriff jedoch vorwiegend im Zusammenhang mit künstlichen neuronalen Netzen verwendet und tauchte in diesem Kontext erstmals im Jahr 2000 auf, in der Veröffentlichung Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications von Igor Aizenberg und Kollegen.[19][20][21] Zwischen 2009 und 2012 gewannen die rekurrenten bzw. tiefen vorwärtsgerichteten neuronalen Netze der Forschungsgruppe von Jürgen Schmidhuber am Schweizer KI-Labor IDSIA eine Serie von acht internationalen Wettbewerben in den Bereichen Mustererkennung und maschinelles Lernen.[22] Insbesondere gewannen ihre rekurrenten LSTM-Netze[23][24] drei Wettbewerbe zur verbundenen Handschrifterkennung bei der 2009 Intl. Conf. on Document Analysis and Recognition (ICDAR) ohne eingebautes A-priori-Wissen über die drei verschiedenen zu lernenden Sprachen. Die LSTM-Netze erlernten gleichzeitige Segmentierung und Erkennung. Dies waren die ersten internationalen Wettbewerbe, die durch Deep Learning[25] oder rekurrente Netze gewonnen wurden. Die jüngsten Erfolge von Deep Learning-Methoden, wie der Gewinn eines Go-Turniers durch das Programm AlphaGo gegen die weltbesten menschlichen Spieler, gründen sich neben der gestiegenen Verarbeitungsgeschwindigkeit der Hardware auf den Einsatz von Deep Learning zum Training des in AlphaGo verwendeten neuronalen Netzes.[26] Gleiches gilt für die seit 2020 gelungene Vorhersage von Proteinfaltungen.[27] Diese Netze nutzen künstlich erzeugte Neuronen (Perzeptronen), um Muster zu erkennen. Für Beiträge zu neuronalen Netzwerken und Deep Learning erhielten Yann LeCun, Yoshua Bengio und Geoffrey Hinton 2018 den Turing Award[28] und Hinton zusammen mit John Hopfield 2024 den Physik-Nobelpreis.[29] Tiefe neuronale Netze können eine Komplexität von bis zu hundert Millionen einzelnen Parametern und zehn Milliarden Rechenoperationen pro Eingangsdatum aufweisen. Die Interpretierbarkeit der Parameter und Erklärbarkeit des Zustandekommens der Ergebnisse ist dabei nur noch eingeschränkt möglich und erfordert den Einsatz spezieller Techniken, die unter Explainable Artificial Intelligence zusammengefasst werden. Ein weiterer Ansatz ist die Verwendung von Methoden aus der Physik von Vielteilchensysteme (Statistische Physik).[30] Eine weitere Begleiterscheinung des Deep Learning ist die Anfälligkeit für Falschberechnungen, die durch geringfügig veränderte Eingabesignale ausgelöst werden können. Ein Angreifer könnte z. B. Bilddaten absichtlich so manipulieren, dass eine Bilderkennung für Bilder, die für einen Menschen normal aussehen, falsche Ergebnisse liefert. Dieses Phänomen wird unter Adversarial Examples zusammengefasst.[31] Zu Grenzen und Erklärbarkeit von KI gibt es zwei unterschiedliche Konzepte. Bei beiden lassen sich die Logik, die Vorhersagen und die Entscheidungen einer KI nicht einfach ausdrücken. Ein bekanntes Beispiel für die Risiken opaker KI ist ein Experiment, das Microsoft im Jahr 2016 mit der Veröffentlichung eines Twitter-Chatbots namens Tay durchgeführt hat. In weniger als 24 Stunden musste Microsoft den Bot abschalten, weil er damit begann, anzügliche und beleidigende Tweets zu verfassen. Neben der meist in Schulungsbeispielen zum Verständnis der internen Struktur vorgestellten Möglichkeit, ein neuronales Netz komplett eigenhändig zu programmieren, gibt es eine Reihe von Softwarebibliotheken,[32] häufig Open Source, lauffähig auf meist mehreren Betriebssystemen, die in gängigen Programmiersprachen wie C, C++, Java oder Python geschrieben sind. Einige dieser Programmbibliotheken unterstützen GPUs oder TPUs zur Rechenbeschleunigung oder stellen Tutorials zur Benutzung dieser Bibliotheken bereit. Mit ONNX können Modelle zwischen einigen dieser Tools ausgetauscht werden. Der Text ist unter der Lizenz „Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“ verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit den Nutzungsbedingungen und der Datenschutzrichtlinie einverstanden."
  },
  {
    "url": "https://et.wikipedia.org/wiki/S%C3%BCgav%C3%B5pe",
    "title": "Sügavõpe – Vikipeedia",
    "content": "Sügavõpe ehk süvaõpe moodustab osa laiemast masinõppe meetoditest, mis põhinevad tehisnärvivõrkudel koos omadusõppega. Sügavõpe võib olla juhendatud, pool-juhendatud või juhendamata.[1][2][3] Sügavõppes on mitmed lihtsate „protsessorite kihid“ ühendatud võrguks niiviisi, et süsteemi sisend läbib järjekorras kõiki kihte. Sellise arhitektuuri inspiratsiooniks oli silmade võrkkesta poolt tajutud valguse info töötlemine inimajus. Antud lähenemine võimaldab tajuda keerukamaid struktuure ilma vajaduseta suurte andmehulkade järele. Süvaõppe idee kohaselt korraldatakse õppimist kahel tasemel, sellest ka nimetus sügavaõpe. Suure hulga näidete põhjal leitakse ülemisel tasemel nende näidete seas küllalt sageli esinevad mustrid. Neid mustreid kasutades võib öelda, et nende mustrite ehk sisuliselt mõistete keeles, kirjeldatakse näited uuesti ära. Nii saadakse hoopis sisukamad ja lühemad näidete kirjeldused. Nüüd saab alumisel tasemel uuesti korraldada õppimist, kuid juba näidete palju paremaid kirjeldusi kasutades. Näiteks näotuvastuse puhul võivad ülemisel tasemel tekkida mustritena sellised mõisted nagu nina, silmad, suu, kõrvad jne. Neid uusi mõisteid kasutades ja neid iseloomustades saab alumisel tasemel nägusid kirjeldada. Näo kohta võib öelda näiteks, et see on väikese suu, kõvera ninaga ja suurte kõrvadega vms. Tegelikult pole põhjust arvata, et arvuti süvaõppe meetodil just sellised inimesele omased mõisted leiab (nina, suu jne.) Süvaõppe idee koos näidetega esitas 1970. aastal Mihail Bongard raamatus,[4] mis ilmus nii vene kui inglise keeles. Süvaõppe head rakendused said võimalikuks alles käesoleval sajandil, mil arvutite võimsus on saanud piisavaks."
  },
  {
    "url": "https://el.wikipedia.org/wiki/%CE%92%CE%B1%CE%B8%CE%B9%CE%AC_%CE%BC%CE%AC%CE%B8%CE%B7%CF%83%CE%B7",
    "title": "Βαθιά μάθηση - Βικιπαίδεια",
    "content": "Η βαθιά μάθηση είναι ένα υποσύνολο μεθόδων μηχανικής μάθησης που βασίζονται σε νευρωνικά δίκτυα με αναπαραστατική μάθηση. Το πεδίο εμπνέεται από τη βιολογική νευροεπιστήμη και επικεντρώνεται στην κατάρτιση τεχνητών νευρώνων σε στρώματα και στην \"εκπαίδευση\" τους για την επεξεργασία δεδομένων. Το επίθετο \"βαθιά\" αναφέρεται στην χρήση πολλαπλών στρωμάτων (από τρεις έως αρκετές εκατοντάδες ή χιλιάδες) στο δίκτυο. Οι μεθόδοι που χρησιμοποιούνται μπορούν να είναι είτε εποπτευόμενες, ή ημι-εποπτευόμενες ή μη εποπτευμένες.[2] Μερικές κοινές αρχιτεκτονικές δικτύων βαθιάς μάθησης περιλαμβάνουν πλήρως συνδεδεμένα δίκτυα, δίκτυα βαθιάς πίστης (DBN), επαναλαμβανόμενα νευρωνικά δίκτυα, συνδυαστικά νευρωνικά δίκτυα, μετασχηματιστές βαθιάς μάθησης και πεδία νευρωνικής ακτινοβολίας. Αυτές οι αρχιτεκτονικές έχουν εφαρμοστεί σε πεδία όπως οπτική υπολογιστή, αναγνώριση της ομιλίας, επεξεργασία της φυσικής γλώσσας, μεταφραστική μηχανή, βιοπληροφορική, σχεδιασμός φαρμάκων, ανάλυση ιατρικής εικόνας, κλιματική επιστήμη, επιθεώρηση υλικών και προγράμματα επιτραπέζου παιχνιδιού, όπου έχουν παράγει αποτελέσματα συγκρίσιμα με και σε ορισμένες περιπτώσεις ξεπερνώντας την απόδοση των ανθρωπίνων εμπειρογνωμόνων.[3][4][5]"
  },
  {
    "url": "https://es.wikipedia.org/wiki/Aprendizaje_profundo",
    "title": "Aprendizaje profundo - Wikipedia, la enciclopedia libre",
    "content": "El aprendizaje profundo (en inglés, deep learning) es una rama del aprendizaje automático que se basa en el uso de algoritmos para modelar y comprender datos complejos. Estos algoritmos se inspiran en la estructura y función del cerebro humano, utilizando múltiples capas de procesamiento para extraer y transformar información de manera iterativa. El objetivo es crear modelos que puedan aprender representaciones abstractas de los datos, lo que les permite realizar tareas como el reconocimiento de patrones, la clasificación y la predicción con alta precisión.[2]​ En esencia, el aprendizaje profundo busca identificar las mejores maneras de representar los datos para facilitar el aprendizaje de tareas específicas. Por ejemplo, una imagen puede representarse como un conjunto de pixeles, pero algunas representaciones, como las que resaltan las características importantes de la imagen, pueden hacer que sea más fácil para un modelo determinar si la imagen contiene un rostro humano. El aprendizaje profundo ha demostrado ser especialmente eficaz en áreas como la visión artificial, el reconocimiento del habla y el procesamiento de audio y música. Arquitecturas como las redes neuronales convolucionales y los transformadores han logrado resultados sobresalientes en estas áreas, superando a menudo a otros enfoques de aprendizaje automático. Aunque no existe una definición única y universalmente aceptada de aprendizaje profundo, el concepto central implica el uso de múltiples capas de procesamiento para extraer características y patrones de los datos. Las diferentes definiciones suelen destacar aspectos como: En resumen, el aprendizaje profundo se caracteriza por el uso de múltiples capas de procesamiento no lineal para aprender representaciones jerárquicas de los datos, ya sea de forma supervisada o no supervisada. La diferencia clave entre los algoritmos de aprendizaje profundo y los de aprendizaje \"poco profundo\" radica en el número de transformaciones que se aplican a los datos. Mientras que los algoritmos \"poco profundos\" pueden aplicar una o dos transformaciones, los algoritmos de aprendizaje profundo suelen utilizar muchas más capas, lo que les permite aprender representaciones más complejas.[3]​: 6  Aunque no hay un número exacto de capas que defina cuándo un algoritmo se considera \"profundo\", la mayoría de los investigadores coinciden en que implica más de dos transformaciones intermedias.[3]​: 7 La ejecución de algoritmos de aprendizaje profundo requiere una gran cantidad de potencia de cálculo, especialmente durante el proceso de entrenamiento. Las GPU (unidades de procesamiento gráfico) se han convertido en una herramienta esencial para acelerar este proceso, gracias a su capacidad para realizar cálculos en paralelo de manera eficiente. Esta técnica se conoce como computación de propósito general en GPU (GPGPU, del inglés general-purpose computing on graphics processing units). El uso de GPU permite reducir significativamente el tiempo necesario para entrenar modelos de aprendizaje profundo, lo que ha impulsado el desarrollo de aplicaciones en diversos campos, como la biología. Por ejemplo, se han utilizado redes neuronales convolucionales (CNN) para segmentar glándulas en imágenes de histología, lo que puede ayudar en el diagnóstico de enfermedades.[4]​[5]​ Los principales proveedores de servicios en la nube, como Amazon, Azure e IBM, ofrecen servicios de infraestructura que incluyen acceso a GPU. Esto permite a los usuarios ejecutar algoritmos de aprendizaje profundo sin necesidad de invertir en hardware costoso.[6]​ Google ofrece una plataforma de aprendizaje automático (PaaS) que incluye servicios para crear y desplegar modelos de aprendizaje profundo. Esta plataforma, que se basa en la biblioteca de código abierto TensorFlow, proporciona modelos pre-entrenados y herramientas para personalizar modelos según las necesidades específicas del usuario. El auge del aprendizaje profundo en la década de 2010 se ha debido tanto a los avances en los algoritmos como a las mejoras en el hardware de computación. Las GPU, a menudo optimizadas específicamente para tareas de inteligencia artificial, han superado a las CPU como la opción preferida para el entrenamiento de modelos de aprendizaje profundo a gran escala.[7]​ Se ha estimado que la cantidad de potencia de cálculo necesaria para entrenar modelos de aprendizaje profundo ha aumentado exponencialmente en los últimos años. Un análisis de OpenAI reveló que la cantidad de cálculo utilizada en los proyectos de aprendizaje profundo más grandes creció 300.000 veces entre 2012 y 2017, con un tiempo de duplicación de tan solo 3,4 meses.[8]​"
  },
  {
    "url": "https://eo.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Vikipedio",
    "content": "Malneta versio Deepl learning (angla por profunda lernado) estas unu el la multaj metodoj de maŝinlernado bazita sur artefaritaj neŭralaj retoj. Lernado povas esti kontrolita, duonkontrolita aŭ nekontrolita[1]. Profunda lernado povas esti aplikita en lokoj kiel ekzemple bildprilaborado, parolrekono, komputa lingvistiko, maŝintradukado, biokomputiko, determinado de kuraciloj, medicina bildrekono, kaj programoj por tabulludoj. Ili foje povas atingi rezultojn, kiuj estas egalaj aŭ foje pli bonaj ol tiuj de specialistoj[2]. Profunda lernado estas klaso de maŝinlernadaj algoritmoj, kiuj uzas plurajn tavolojn por iom post iom ĉerpi pli altnivelajn funkciojn el kruda enigo. Ekzemple, en bildprilaborado, pli malaltaj tavoloj povas identigi randojn, dum pli altaj tavoloj identigas la konceptojn gravaj al ekzemple nombroj, literoj aŭ vizaĝoj[3]."
  },
  {
    "url": "https://eu.wikipedia.org/wiki/Ikaskuntza_sakon",
    "title": "Ikaskuntza sakon - Wikipedia, entziklopedia askea.",
    "content": "Ikaskuntza sakona (ingelesez: deep learning), egitura sakoneko ikaskuntza edo ikaskuntza hierarkikoa ikasketa automatikoko metodo bat da. Ez da ataza espezifiko bat ebazteko algoritmoa, modu automatikoan eta datuetatik abiatuz ikasteko sortutako metodoa baizik. Ikaskuntza sakona nerbio-sistema biologikoetan informazioaren prozesamendua nola gertatzen den eta komunikazio-eredua zein izan litekeen interpretatuz sortutako ikaskuntza-metodo konputazionala da. Neurozientzien alorrean egindako ikerketa-lanek diotenez, burmuinetan informazioa neurona izeneko zeluletan eta haien artean osatutako neurona-sareetan prozesatzen eta gordetzen da, eta oso garrantzitsuak dira neuronen arteko loturak eta haien arteko estimulu eta neurona-erantzunak (sinapsia). Izaki bizidunek ikasteko dugun gaitasun hori metodo konputazional baten bidez simulatzeko asmoz, neurona-sare biologikoak inspirazio iturri hartu eta neurona-sare artifizialak diseinatu ziren (ingelesez Artificial neural networks (ANNs)). Hasierako eredu konputazional haren garapenetik sortu dira ikaskuntza sakoneko hainbat arkitektura: neurona-sare sakona (deep neural network (DNN)), uste-sare sakona (deep belief network (DBN)) eta neurona-sare errepikaria (recurrent neural network (RNN)). Modu automatikoan ikasteko eredu horiek hainbat aplikazio-eremutan erabiliak izan dira eta lortutako emaitzak giza-adituek lortutakoekin alderagarriak edota hobeak izan dira. Aplikazio-eremu horietako batzuk honakoak dira: ikusmen artifiziala, hizketa-ezagutze automatikoa, hizkuntzaren prozesamendua, audio-ezagutze automatikoa, sare sozialen iragazketa, itzulpengintza automatikoa, bioinformatika eta sendagaien diseinua. Ikaskuntza gainbegiratua, partzialki gainbegiratua edo gainbegiratu gabea izan daiteke. Neurona-sare artifizialak edo konexio-sistemak (ingelesez Artificial neural networks (ANNs), connectionist systems), animalien burmuinetako neurona-sare biologikoetan oinarritutako ikaskuntza-sistema konputazionalak dira. Horrelako sistemek adibideen bidez ikasten dute, ataza edo zeregin partikular baterako bereziki programatuak izan barik. Ikusmen artifizialean, esaterako, neurona-sare artifizialak katuak identifikatzen ikas dezake “katu” edo “ez katu” moduan eskuz etiketatu diren irudiak aztertuz. Ikaskuntza-prozesutik lortutako eredua irudi berrietan katuak identifikatzeko erabiliko da. Erregeletan oinarritutako sistemen bidez adierazten zailak diren problemetan oso erabilgarriak dira sistema hauek. Itzulpen automatikoa aplikazio argia da, erregeletan oinarritutako itzulpen automatikorako sistemak erabili ziren hasiera batean baina gaur egun gero eta gehiago ari dira garatzen adibideetan oinarritutako itzulpengintzarako sistemak. Neurona-sare artifiziala elkarri konektaturiko hainbat neurona artifizialez osatuta dago (burmuineko axoien parekoak). Neuronen arteko konexio (sinapsi) bakoitzak seinalea transmiti diezaioke beste neurona bati. Neurona hartzaileak (sinapsi-ondorenekoak) seinalea prozesa ditzake eta ondoren hari konektaturiko beste neuronei seinalea bidali. Neuronek eta haien arteko konexioek pisuak izan ditzakete esleituta, ikaskuntza prozesuak aurrera egin ahala aldatzen joango direnak. Pisuak handitzen edo txikitzen direnaren arabera handituko edo txikituko da bidalitako seinaleen indarra. Oro har, neuronak geruzatan antolatuta daude. Geruza desberdinetan kokatutako neuronek hainbat bilakaera ezar ditzakete haien sarrera-seinaleetan. Seinaleak lehen geruzatik (sarrera) azken geruzaraino (irteera) joaten dira; geruzak behin baino gehiagotan zeharkatzea posiblea izan daiteke. Giza-burmuinak problemak ebazteko darabilen antzeko prozedura konputazionala lortu nahi izan zen neurona-sare artifizialak sortzearekin. Gerora, arreta handiagoa jarri izan da gaitasun mental espezifiko batzuk lortzeko helburuan; horrek eredu konputazionala biologikotik gehiago aldentzea ekarri du. Atzera-elikatzearen prozesua (backpropagation) edo informazioa (seinalea) geruzetan atzetik aurrera pasatzea eta sarea egokitzea informazio hori islatzeko eredu konputazionalaren eta biologikoaren arteko aldentzea bideratu duten adibide garbiak dira. Neurona-sare artifizialak hainbat atazatan erabili izan dira: ikusmen artifiziala, hizketa-ezagutze automatikoa, itzulpengintza automatikoa, sare sozialen iragazketa, mahai-jokoak, bideo-jokoak eta mediku-diagnostikoak. Gaur egungo neurona-sare artifizialek milaka neurona (milioi gutxi batzuk gehienez) eta milioika konexio izan ohi dituzte. Duten konputazio-ahalmena giza-burmuinak duena baino askoz ere txikiagoa izan arren, hainbat atazatan oso emaitza onak lortzeko gai direla erakutsi izan dute. Neurona-sare sakona (ingelesez deep neural network (DNN)) sarrera-geruzaren eta irteera-geruzaren artean hainbat ezkutuko neurona-geruza dituen neurona-sare artifiziala da. Azaleko neurona-sare artifizialak bezala, sakonek ere erlazio ez-lineal konplexuetarako ereduak sortzeko gaitasuna dute. Neurona-sare sakonek eredu konposizionalak sortzeko gaitasuna dute: objektuak osagaiz osatuta daude eta osagai horien konposizio mailakatuaren bidez haiek adieraztea lortzen da. Behe-geruzetan eraikitako ereduen konposizioa neurona-sare sakonek dituzten geruza gehigarriei esker gertatzen da. Ereduen konposaketa mailakatu horri esker konplexua izan daitekeen informazioaren eredua eraikitzea lortzen da neurona-sare artifizial arruntek (azalekoek) beharko luketena baina neurona gutxiagorekin. Neurona-sare sakonen aldaera desberdinak daude. Arkitektura-mota desberdinak arlo espezifiko desberdinetarako aplikazioetan lortu dute arrakasta. Neurona-sareen arkitekturen arteko konparaketa egitea ez da beti posiblea izaten, horretarako datu-multzo bera erabiliz ebaluatu behar dira eta. Neurona-sare sakonetan normalean informazioa sarrera-geruzetatik irteera-geruzetarantz doa (feedforward), atzera-elikatzerik gabe, baina ez beti. Neurona-sare errepikarietan (ingelesez recurrent neural networks (RNNs)) datuak edozein noranzkotan doaz neurona-sarean barrena; hizkuntza-ereduak sortzeko erabili ohi dira. Zirkunboluzio neurona-sareak (ingelesez convolutional neural networks (CNNs)) ere neurona-sare sakonak dira, ikusmen artifizialean eta hizketa-ezagutze automatikoan oso erabiliak direnak. Hizketa-ezagutze automatikoa hizketa testu bihurtzeko teknologia da. Hizketaren soinua konputagailuak uhin analogiko moduan jaso, aztertu eta soinuaren unitateak edo hitzak eratzen dituzten fonemak identifikatu behar dira. Ekoizpen ona eta kalitatezkoa sortzeak lan handia eskatzen du zeren eredu ezberdin ugari batu behar dira eta batze honek jarraitua izan behar da. Hizketa-ezagutze merkataritza-sistema gehienak (Cortana, Siri, etc.) ikaskuntza sakonean oinarrituak daude. Ikusmen artifiziala mundu errealeko irudiak bereganatu, prozesatu, analizatu eta ulertzeko metodoak erabiltzen dituen zientziaren jakintzagaia da, irudi horiek konputagailu bidez zenbaki edo zeinu informazioan ekoizteko asmoz. Ikaskuntza sakonean oinarritutako ikusmen artifizialak, gaur egun, gizakiak baino emaitza hobeak lortzen dituzte. Neurona sare sakonak gai dira: Hizkuntzaren prozesamendua informatika, adimen artifizial eta hizkuntzalaritzaren alorra da, hizkuntzalaritza konputazionaleko ingeniaritza lantzen duena. Hizkuntzaren bidez pertsona eta makinen arteko komunikazioa, baita pertsonen artekoa ere, errazteko tresna konputazionalak ikertzeaz arduratzen da. Neurona sareak hizkuntzaren prozesamenduan 2000 urtetik aurrera erabiltzen hasi ziren eta arrakasta handia izan dute. Ikaskuntza sakona aplikatzen hasi dira ikertzaileek sendagai berriak aurkitzeko. Adibidez, AtomNet ikaskuntza sakonean oinarritutako sistema bat da, botiken diseinu arrazionalen ereduak egiten dituena. Ikaskuntza sakon indartua erabili da marketin zuzeneko ekintza posibleen balioaren hurbilketak egiteko. Gomendio-sistemek ikaskuntza sakona erabiltzen dute beren gomendioak ahalik eta gehien hobetzeko. Neurona-sare artifizialak erabiltzen dira geneen arteko erlazio funtzioak aurreikusteko. Gomendio sistemen antzera, mugikorretako publizitateak ere erakusten diren iragarkiak bezeroarentzat interesgarriak izateko erabiltzen da ikaskuntza sakona."
  },
  {
    "url": "https://fa.wikipedia.org/wiki/%DB%8C%D8%A7%D8%AF%DA%AF%DB%8C%D8%B1%DB%8C_%D8%B9%D9%85%DB%8C%D9%82",
    "title": "یادگیری عمیق - ویکی‌پدیا، دانشنامهٔ آزاد",
    "content": "یادگیری عمیق، یادگیری ژرف یا ژرف‌آموزی (به انگلیسی: Deep learning) (به بیانی دیگر: یادگیری ژرف ماشین، یادگیری ساختار ژرف یا یادگیری سلسله مراتبی) یک زیر شاخه از یادگیری ماشین و بر مبنای مجموعه‌ای از الگوریتم‌ها است که در تلاشند تا مفاهیم انتزاعی سطح بالا در دادگان را مدل کنند که این فرایند را با استفاده از یک گراف عمیق که دارای چندین لایه پردازشی متشکل از چندین لایه تبدیلات خطی و غیرخطی اند، مدل می‌کنند. به بیان دیگر پایهٔ آن بر یادگیری نمایش دانش و ویژگی‌ها در لایه‌های مدل است.[۱] الگوریتم‌های DL از مجموعه‌های بزرگی از داده‌های آموزشی برای شناسایی روابط بین عناصری مانند اَشکال، کلمات و رنگ‌ها استفاده می‌کنند. این روابط به الگوریتم‌های DL برای پیش‌بینی کمک می‌کنند. الگوریتم‌های DL می‌توانند بسیاری از روابط (از جمله روابطی که شاید توسط انسان تشخیص داده نشوند) را شناسایی کرده و داده‌های بسیار پیچیده را تفسیر یا پیش‌بینی کنند.[۱] به قولی دیگر «یادگیری عمیق» یک نوع «یادگیری ماشین» به همراه شبکه‌های عصبی چندلایه است که با دقتی فزاینده الگوهای موجود در داده‌ها کشف کرده و به همین‌خاطر می‌تواند علائق کاربر را بشناسد، اشیا را شناسایی کرده و زبان‌ها را بفهمد.[۲] یک نمونه آموزشی (برای نمونه: تصویر یک گربه) می‌تواند به صورت‌های گوناگون بسان یک بردار ریاضی پر شده از مقدار به ازای هر پیکسل و در دید کلی‌تر به شکل یک مجموعه از زیرشکل‌های کوچک‌تر (نظیر اعضای صورت گربه) مدل‌سازی شود. برخی از این روش‌های مدل‌سازی سبب ساده شدن فرایند یادگیری ماشین (برای نمونه: تشخیص تصویر گربه) می‌شوند.\nدر یادگیری عمیق امید به جایگزینی استخراج این ویژگی‌های تصویر به دست بشر (مانند اعضای گربه) با روش‌های کامل‌خودکار بی‌نظارت و نیمه‌نظارتی وجود دارد.[۳] انگیزهٔ نخستین در به وجود آمدن این ساختار یادگیری از راه بررسی ساختار عصبی در مغز انسان الهام گرفته شده‌است که در آن یاخته‌های عصبی با فرستادن پیام به یکدیگر درک را امکان‌پذیر می‌کنند.[۴]\nبسته به فرض‌های گوناگون در مورد نحوهٔ اتصال این یاخته‌های عصبی، مدل‌ها و ساختارهای مختلفی در این حوزه پیشنهاد و بررسی شده‌اند، هرچند که این مدل‌ها به صورت طبیعی در مغز انسان وجود ندارد و مغز انسان پیچیدگی‌های بیشتری دارد. این مدل‌ها نظیر شبکه عصبی عمیق، شبکه عصبی هم‌گشتی، شبکه باور عمیق و چندین نمونه دیگر؛ پیشرفت‌های خوبی را در حوزه‌های پردازش زبان‌های طبیعی و پردازش تصویر ایجاد کرده‌اند. در حقیقت عبارت یادگیری عمیق، بررسی روش‌های تازه برای شبکه عصبی مصنوعی است.[۵][۶] یادگیری عمیق، رده‌ای از الگوریتم‌های یادگیری ماشین است[۷]: ۱۹۹–۲۰۰  که از چندین لایه برای استخراج ویژگی‌های سطح بالا از ورودی خام استفاده می‌کنند. به بیانی دیگر، رده‌ای از تکنیک‌های یادگیری ماشین که از چندین لایهٔ پردازش اطلاعات و به‌ویژه اطلاعات غیرخطی بهره می‌برد تا عملیات تبدیل یا استخراج ویژگی نظارت‌شده یا نظارت‌نشده را عموماً با هدف تحلیل یا بازشناخت الگو، کلاس‌بندی، خوشه‌بندی انجام دهد.[۸] برای نمونه، در پردازش تصویر، لایه‌های پست‌تر می‌توانند لبه‌ها را تشخیص دهند، در حالی که لایه‌های عالی‌تر ممکن است ویژگی‌های پرمعناتر برای انسان، همچون حروف یا چهره‌ها، را تشخیص دهند. یادگیری عمیق زیرشاخه‌ای از یادگیری ماشین است که از لایه‌های متعدد تبدیلات خطی به منظور پردازش سیگنال‌های حسی مانند صدا و تصویر استفاده می‌کند. ماشین در این روش هر مفهوم پیچیده را به مفاهیم ساده‌تری تقسیم می‌کند، و با ادامهٔ این روند به مفاهیم پایه‌ای می‌رسد که قادر به تصمیم‌گیری برای آن‌ها است و بدین ترتیب نیازی به نظارت کامل انسان برای مشخص کردن اطلاعات لازم ماشین در هر لحظه نیست. موضوعی که در یادگیری عمیق اهمیت زیادی دارد، نحوهٔ ارائهٔ اطلاعات است. ارائه دادن اطلاعات به ماشین باید به شیوه‌ای باشد که ماشین در کمترین زمان اطلاعات کلیدی را که می‌تواند با استناد به آن‌ها تصمیم بگیرد را دریافت کند. هنگام طراحی الگوریتم‌های یادگیری عمیق می‌بایست به عوامل دگرگونی (به انگلیسی: factors of variation) که اطلاعات مشاهده شده را توضیح می‌دهند توجه کنیم، این عوامل معمولاً عوامل قابل‌مشاهده‌ای نیستند بلکه عواملی هستند که بر روی دستهٔ قابل‌مشاهده تأثیرگذار بوده یا زادهٔ ساختارهای ذهنی انسان برای ساده‌تر کردن مسائل هستند. برای مثال در هنگام پردازش گفتار عوامل دگرگونی می‌توانند لهجهٔ گوینده، سن یا جنسیت او باشند. در هنگام پردازش تصویر یک ماشین، میزان درخشش خورشید یک عامل دگرگونی است. یکی از مشکلات هوش مصنوعی تأثیر زیاد عوامل دگرگونی بر روی اطلاعات دریافتی است. برای مثال بسیاری از پیکسل‌های دریافتی از یک ماشین قرمز در شب ممکن است سیاه دیده بشوند. برای حل این مشکلات بعضاً به درک بالای اطلاعات (در حدود انسان) نیازمندیم و در واقع گاهی یافتن نحوهٔ مناسب نمایش اطلاعات به اندازهٔ خود مسئله سخت و زمان‌بر است. اگرچه شبکه های عصبی (و شبکه های عصبی عمیق) در ابتدا به عنوان یک رویکرد عملگرایانه (و کمتر مبتنی بر طرح ریزی نظریاتی) مورد استفاده و پذیرش قرار گرفتند، ولی هم در قرن بیستم (در کارهای امثال کلمگرف[۱۰][۱۱]و دیگران) و هم در دهه سوم قرن بیست و یکم[۱۲]، مبانی نظری برای توصیف عمیقِ مفاهیمِ ساختار، عملکرد و رفتار این شبکه ها توسط محققین فراهم آمد. نخستین الگوریتم عملی یادگیرنده برای پرسپترون‌های چندلایهٔ نظارت‌شده، عمیق و پیش‌خور، در دههٔ ۱۹۶۰ توسط الکسی ایواخننکو - معروف به «پدر یادگیری عمیق»[۱۳] - و والنتن لاپا منتشر شد.[۱۴] در سال ۱۹۷۱، مقاله‌ای یک شبکهٔ عمیق با هشت لایه را توصیف کرد که عملیات یادگیری را با متد گروهی مدیریت داده (GMDH) انجام داده بود.[۱۵] سایر معماری‌های یادگیری عمیق و به ویژه آن‌هایی که برای بینایی رایانه ساخته شده بودند، در ۱۹۸۰ و با Neocognitron معرفی‌شده توسط کونیهیکو فوکوشیما آغاز گشتند.[۱۶] لفظ یادگیری عمیق، نخستین‌بار در ۱۹۸۶ و توسط رینا دِختِر در زمینهٔ یادگیری ماشین به کار رفت؛ وی در مقاله‌ای تحت عنوان یادگیری به هنگام جستجو در مسائل ارضای محدودیت (Learning While Searching in Constraint-Satisfaction-Problems) از این لفظ برای پروسه‌ای استفاده کرد که در آن تمامی راه‌حل‌ها در یک فضای جستجو که به پاسخ مناسب نمی‌رسیدند نیز ذخیره می‌شدند. تحلیل این راه‌حل‌های ذخیره‌شده امکان کنترل بهتر در تلاش‌های بعدی را ممکن می‌ساخت، و به دنبال آن در همان مراحل نخستین از گیر کردن در بن‌بست‌های احتمالی نیز جلوگیری می‌کرد.[۱۷][۱۸] با این وجود امروزه لفظ یادگیری عمیق عموماً در حوزهٔ شبکه‌های عصبی مصنوعی به کار می‌رود که نخستین‌بار در سال ۲۰۰۰ و توسط ایگور آیزنبرگ و همکاران در حوزهٔ یادشده استفاده شد؛ به‌طور دقیق‌تر، در کتاب نورون‌های دودویی چندمقداری و جهانی: نظریه، یادگیری و کاربردها (Multi-Valued and Universal Binary Neurons: Theory, Learning and Applications) و در زمینهٔ نورون‌های حدآستانهٔ بولی.[۱۹][۲۰] در سال ۱۹۸۹، یان لی‌کان و همکاران الگوریتم استاندارد پس‌انتشار را برای یک شبکهٔ عصبی عمیق با هدف تشخیص متن‌های دست‌نویس (به‌طور خاص با هدف بازشناسی کدهای پستی دست‌نویس روی نامه‌های پستی) به کار بست. درحالی که الگوریتم کار می‌کرد، عملیات یادگیری آن به سه روز زمان نیاز داشت. مدل مورد استفاده از یک لایهٔ ورودی با ۲۵۶ واحد (پیکسل‌های یک تصویر مربعی ۱۶×۱۶)، یک لایهٔ خروجی با ۱۰ واحد (که مشخص می‌کرد تصویر رقمی که به ورودی داده‌شده‌است، کدام یک از ارقام ۰ تا ۹ است) و سه لایهٔ پنهان در میان این دو تشکیل شده بود. با ارزیابی مدل بر روی مجموعه‌دادهٔ تست، ۸/۱٪ رده‌بندی اشتباه و ۱۹/۴٪ بازپس‌زنی برای ۱٪ نرخ خطا در میان الگوهای تست باقی‌مانده به دست آمد که نشان می‌داد که این مدل نسبت به مدل‌های ارائه‌شدهٔ پیشین از دقت بالاتری برخوردار است و استفاده از الگوریتم پس‌انتشار، روشی مناسب در فرایند تعلیم شبکه‌های عصبی عمیق است.[۲۱] لازم است ذکر شود که خود الگوریتم پس‌انتشار، از پیش و از ۱۹۷۰ نیز به عنوان حالت معکوس مشتق خودکار وجود داشت.[۲۲][۲۳] تا سال ۱۹۹۱، چنین سامانه‌هایی عموماً برای تشخیص ارقام دوبُعدی دست‌نویس ایزوله‌شده (به این معنا که ارقام به صورت تنها و بدون جزئیات و ویژگی‌های اضافهٔ دیگری در پس‌زمینه - مثلاً متن و حروف اضافه - نوشته‌شده‌اند) به کار می‌رفتند؛ درحالی که بازشناسی اجسام سه‌بُعدی همچنان یک چالش بود. در سال ۱۹۹۲، پژوهشی از جان ونگ و همکاران با توضیح معایب و محدودیت‌های شبکه‌های عصبی سه‌لایه‌ای که برای این منظور به کار می‌رفتند، از مفهوم شبکهٔ سلسله‌مراتبی (به انگلیسی: hierarchical network) استفاده کرده و مدلی موسوم به Cresceptron را ارائه دادند که قادر بود اجسام سه‌بعدی در محیط‌های شلوغ را نیز تشخیص دهد.[۲۴][۲۵][۲۶] مفهوم تجمیع حداکثری (به انگلیسی: max pooling) نیز نخستین‌بار در همین پژوهش پیاده‌سازی شد.[نیازمند منبع] از آن‌جایی که این مدل می‌توانست مستقیماً تصاویر طبیعی (سوژه‌های سه‌بعدی، با حضور عناصر دیگر در پس‌زمینه) را به عنوان ورودی دریافت کند، تبدیل به بنیانی برای یادگیری بصری همه‌منظوره شد. در سال ۱۹۹۴، آندره د کاروالیو به همراه مایک فیرهورست و دیوید بیسیت، معماری‌ای مبتنی بر یک شبکهٔ عصبی بولی چندلایه را ارائه داد که تحت عنوان شبکهٔ عصبی بی‌وزن نیز شناخته می‌شد. این معماری از یک ماژول شبکهٔ عصبی سه‌لایه‌ای خودسامان‌دهندهٔ استخراج ویژگی (به انگلیسی: self-organising feature extraction یا SOFT) به همراه یک ماژول شبکهٔ عصبی چندلایهٔ رده‌بندی (به‌طور خاص یک شبکه با معماری GSN یا goal-seeking network) تشکیل شده بود که به صورت مستقل عملیات یادگیری را انجام می‌دادند. در ماژول استخراج ویژگی، تصویر ورودی به چند زیرمجموعه تقسیم شده و هر قسمت به یک بلاک از نورون‌ها (که دارای سلسله‌مراتب بوده و در چند لایه تقسیم شده‌بودند) داده می‌شد و هر بلاک به صورت موازی با دیگر بلاک‌ها و مستقل از آن‌ها آموزش می‌دید.[۲۷][۲۸] در سال ۱۹۹۵، یوزف زِپ هُخ‌رایتر که پیش‌تر و در ۱۹۹۱ در پایان‌نامهٔ خود - تحت عنوان: بررسی‌هایی در شبکه‌های عصبی پویا (به آلمانی: Untersuchungen zu dynamischen neuronalen Netzen) - به بررسی مسئلهٔ گرادیان کاهشی پرداخته بود[۲۹] (که خود پیش‌تر و در ۱۸۴۷مطرح شده[۳۰] و در ۱۹۴۴ نیز برای مسائل بهینه‌سازی غیرخطی مورد مطالعه قرار گرفته بود[۳۱]) به همراه یورگن اشمیدهوبر، معماری ال‌اس‌تی‌ام را ارائه داد[۳۲] و در مقالهٔ دیگری در سال ۱۹۹۷، آن را بهبود بخشید[۳۳] که زمینهٔ بزرگی را برای پیشرفت شبکه‌های عصبی بازگشتی فراهم ساخت[نیازمند منبع]. در همان سال ۱۹۹۵، برندن فرِی به همراه جفری هینتون و پیتر دایان نشان دادند که می‌توان با استفاده از الگوریتم بیدار-خواب، شبکه‌ای تشکیل‌شده از شش لایهٔ کاملاً هم‌بند و با چندصد واحد پردازشی مخفی را آموزش داد.[۳۴] تا قبل از پیدایش یادگیری عمیق، روش‌های یادگیری ماشین سنتی، بیش‌از حد به بازنمایی‌هایی (انتخاب ویژگی‌ها) که از داده‌ها به‌دست می‌آورند، وابسته بودند. این روش‌ها، نیاز به یک متخصص در دامنه موضوع داشت تا استخراج ویژگی‌ها را به‌صورت دستی انجام دهد. حال آن‌که، این استخراج ویژگی‌ها به صورت دستی فرآیندی چالش‌انگیز و زمان‌بر است. پیدایش یادگیری عمیق توانست به‌سرعت جایگزین این روش‌های سنتی شود. چرا که می‌توانست استخراج ویژگی‌ها را به‌صورت خودکار متناسب با هر مسئله به‌دست آورد.[۳۵] در حالی‌که مدل‌های یادگیری عمیق در دهه گذشته، در برخورد با ورودی‌هایی به شکل تصاویر، گفتار یا ویدیو که اساس ساختار آن‌ها اقلیدسی است، موفقیت‌آمیز عمل کرده، اخیراً، علاقه محققان در تلاش برای استفاده از یادگیری بر روی داده‌های غیراقلیدسی افزایش یافته‌است. یادگیری عمیق هندسی، زمینه نوظهور تحقیقاتی است که سعی در تعمیم معماری یادگیری عمیق برای کار با داده‌های غیراقلیدسی دارد، تا این شکاف را پر کند.[۳۵] شبکه‌های عصبی گراف، دسته‌ای از روش‌های یادگیری عمیق هستند که به‌طور خاص، برای استنباط بر داده‌های توصیف‌شده توسط گراف‌ها طراحی شده‌اند. ایجاد مدل‌هایی که مستقیماً بروی گراف‌ها کار می‌کنند، مطلوب‌تر است. چراکه، می‌توانیم اطلاعات بیشتری در مورد ساختار و خصوصیات آن‌ها را به‌دست آوریم. شبکه‌های عصبی گراف، به‌طور مستقیم برروی گراف‌ها اعمال می‌شوند و روشی آسان برای انجام وظایفی همانند، پیش‌بینی سطح گره، یال و گراف ارائه می‌کنند. تا پیش از، توسعه شبکه‌های عصبی گراف، روش‌های یادگیری عمیق توانایی اعمال برروی یال‌ها در جهت استخراج دانش و پیش‌بینی را نداشتند. در عوض، تنها بر اساس ویژگی‌های گره عمل می‌کردند.[۳۵] یکی از نخستین زمینه‌های بسیار موفق برای یادگیری عمیق که پتانسیل بالقوهٔ این روش در حل مسائل را نشان داد، در حوزهٔ بازشناسی تصویر رخ داد. از سال ۲۰۱۰ و در پروژه‌ای موسوم به ایمیج‌نت مسابقه‌ای سالانه برگزار می‌شود که شرکت‌کنندگان با ارائهٔ الگوریتم‌های کامپیوتری گوناگون، تلاش به بازشناسی تصاویر دیجیتالی در مقیاس کلان کرده و بر سر دست‌یابی به دقّت‌های بالاتر با یک‌دیگر رقابت می‌کنند. حال در سال ۲۰۱۲، یک شبکهٔ عصبی هم‌گشتی به نام الکس‌نت در این رقابت به کار رفت و با کسب نتایجی بسیار چشم‌گیر، توجه‌های گسترده‌ای را به سوی روش یادگیری عمیق جلب کرد؛ به شکلی که به باور برخی، در این سال «انقلاب یادگیری عمیق» رخ داد. لازم است ذکر شود که دقت الکس‌نت در تشخیص تصاویر پایگاه داده‌ی ایمیج‌نت از دقت انسان نیز فراتر بود (هرچند البته حتی پیش از ارائهٔ الکس‌نت نیز الگوریتم‌های دیگری به عملکرد فراانسانی دست پیدا کرده بودند).[۳۶][۳۷] امروزه نیز شبکه‌های عصبی در بینایی رایانه دارای نقشی کلیدی بوده و برای اهداف گوناگونی چون بازشناسی تصویر، تشخیص چهره، رهگیری اجسام، حذف نویز، رنگی‌کردن تصاویر سیاه و سفید، ترمیم تصاویر آسیب‌دیده، رده‌بندی تصاویر پزشکی و… به کار می‌رود.[۳۸] از دیگر زمینه‌های موفق برای یادگیری عمیق، تشخیص و بازشناسی خودکار گفتار در مقیاس گسترده است که معمولاً توسط مدل‌های مبتنی بر شبکه عصبی بازگشتی (به ویژه از نوع ال‌اس‌تی‌ام) و شبکه عصبی هم‌گشتی انجام می‌گیرد.[۳۹] یادگیری عمیق اثر بزرگی در پیشرفت شاخهٔ پردازش زبان‌های طبیعی ایجاد کرده و با ایجاد یک چهارچوب مدل‌سازی قدرتمند، به نتایج چشم‌گیری دست یافته‌است.[۴۰] به عنوان نمونه، مدل زبانی جی‌پی‌تی-۳ از اوپن ای‌آی با بهره‌گیری روش‌های یادگیری عمیق قادر به تولید متونی مشابه متون نوشته‌شده توسط انسان است.[۴۱] سیگنال‌های الکترومیوگرافی می‌توانند به عنوان رابطی میان انسان و ماشین عمل کرده و با تحلیل آن‌ها از مقصود کاربر جهت کنترل تجهیزات گوناگون بهره برد. به عنوان نمونه، افراد دچار نقص عضو می‌توانند اعضایی مصنوعی را جایگزین عضو قطع‌شدهٔ خود کرده و آن‌ها را به شیوهٔ مؤثری کنترل کنند. یا با همین روش می‌توان اعضایی کمکی و تقویت‌کننده همچون اسکلت خارجی را کنترل کرد. برای تحلیل این سیگنال‌های خام و ارائهٔ خروجی مناسب برای کنترل دستگاه، بهره‌گیری از روش یادگیری عمیق می‌تواند بسیار کاربردی باشد.[۴۲] سامانه‌های پیشنهادگر از یادگیری عمیق جهت استخراج ویژگی‌های معنادار برای یک مدل فاکتورهای پنهان به منظور پیشنهادهای محتوا-محور موسیقی و مجله بهره برده‌اند.[۴۳][۴۴] یادگیری عمیق چنددیدگاهی (به انگلیسی: multi-view deep learning) جهت یادگیری ترجیح‌های کاربر از چندین دامنه به کار می‌رود.[۴۵] در طرح‌های سرمایه‌گذاری، از یادگیری عمیق برای افزایش میزان بازده استفاده می‌شود.[۴۶] گوگل و تسلا امروزه ثابت کرده‌اند که خودروهای بدون سرنشین یا خودران امکان‌پذیر هستند. اما به هرحال باید گفت این خودروها هنوز نیاز به یادگیری بیشتر و تمرین و آزمایش‌های متفاوتی هستند و فعالیت‌ها و متغیرهای بسیاری باید مورد بررسی قرار بگیرد. تشخیص صدا جزئی جدانشدنی از فرایند زبان است. تحلیل صدای ورودی برای یک سامانه هوش مصنوعی بسیار سخت است؛ زیرا که عوامل متعددی در تشخیص درست صدا نقش دارند. به‌طور مثال وجود نویز در پیش‌زمینه، لهجه‌ها، خلاقیت‌ها و بازی‌های زبانی افراد و همچنین ناتوانی‌های خاص گفتاری و عوامل دیگر تشخیص دقیق واژه‌های به کار رفته در صدای به چیزی که رایانه بتواند آن را تحلیل کند، سخت می‌کند. استفاده از شبکه‌های «یادگیری عمیق» می‌تواند در نرم‌افزارهای متعددی با تشخیص الگوی به‌کار رفته توسط کاربر، کیفیت ارائهٔ خدمات را بالا ببرد. هوش مصنوعی‌ای که به خوبی با کاربر خود تطابق یافته باشد، می‌تواند اطلاعات غیر مرتبط را نادیده گرفته و برای کاربر اطلاعات مرتبط ارائه کند. تشخیص الگو همچنین می‌تواند تحلیل کلان داده را موثرتر سازد. هوش مصنوعی ضعیف، موفقیت‌هایی در حوزه تولید متن با معنا و پیشرفت در حوزه کدنویسی داشته‌است. به‌طور مثال GPT-۳ از شرکت OpenAI یک نرم‌افزار اوپن سورس زبان تولیدکننده است که می‌تواند با کمترین دستورالعمل از سوی کاربر، کدنویسی کرده یا برنامه‌های ساده کامپیوتری ایجاد کند. در آینده احتمالاً شاهد حضور نرم‌افزارهای بیشتری که از این نوع فناوری‌ها بهره‌مندند خواهیم بود و کار با نرم‌افزارهای تخصصی رنگ و بوی دیگری به خود خواهد گرفت. هوش مصنوعی نقش مهم‌تری در معیارهای امنیت سایبری سازمان‌ها، افراد و دولت‌ها به عهده خواهد گرفت که شامل نظارت، شناسایی تهدیدهای اطلاعاتی، اطلاع از نفوذ امنیتی، پاسخ به اتفاقات و تحلیل ریسک خواهد بود. هوش مصنوعی همچنان می‌تواند ابزارهای نظارتی قدرتمندی در اختیار دولت‌ها قرار دهد تا به‌طور راحت‌تری مخالفان خود را شناسایی کنند. برنامه‌های رایانه‌ای روزبه‌روز در تولید محتوا بهتر می‌شوند و هوش مصنوعی هم‌اکنون نیز در شناسایی موارد تخلف قانون حق نشر، ساخت بازی‌های ویدیویی و فیلم‌های سینمایی نقش مهمی دارند و در آینده این نقش برجسته‌تر خواهد شد. الگوریتم‌های یادگیری عمیق تجزیه و تحلیل داده‌های پیچیده را ساده می‌کنند و دقت تشخیص ناهنجاری را افزایش می‌دهند. شبکه‌های عصبی کانولوشنال (CNN) بینش‌هایی را ارائه می‌کنند که به متخصصان پزشکی در شناسایی به موقع و دقیق مسائل بهداشتی کمک می‌کند. در یک مطالعه در سال ۲۰۱۸، CNNها بیش از ۱۰ درصد دقت بالاتری را در شناسایی ملانوم در مقایسه با متخصصان نشان دادند. شبکه‌های عصبی عمیق (DNNs) مدل‌های یادگیری هستند که دارای محاسبات خیلی زیادی‌اند. این مدل‌ها کاربردهای روزافزونی در بخش‌های مختلف دارند. از آنجا که FPGAها یک زیرساخت قابل‌برنامه‌نویسی برای شتاب‌دادن به محاسبات می‌دهند و همچنین امروزه در بیشتر دنیا به سادگی قابل دسترس‌اند، FPGAها یک انتخاب خیلی خوب برای پیاده‌سازی مدل‌های DNN هستند. اما استفاده از FPGAها به دلیل اینکه به‌دست آوردن کارایی و مصرف انرژی کم آنها به‌طور همزمان کار خیلی سختی است و همچنین DNNها به حافظه زیادی نیاز دارند (در FPGA معمولاً حافظه روی برد کم است)، پیاده‌سازی مدل‌ها با FPGA بسیار دشوار است. نساج یا بافنده شبکه‌های عصبی (DNNWEAVER) یک چارچوب است که به صورت خودکار یک کد قابل‌سنتز برای یک زوج مرتب (DNN, FPGA) تولید می‌کند. مدل DNN را به صورتی که در Caffe معرفی می‌کنند به عنوان ورودی می‌دهند. روند کلی کار این برنامه به این صورت است که ابتدا مدل ورودی را به یک زبان میانی تبدیل می‌کند که یک گراف از جریان داده کلی را نشان می‌دهد. در این مرحله با الگوریتمهای مختلف مدل داده شده را گروه‌بندی می‌کند تا با توجه به حافظه FPGA و دیگر منابع موجود روی FPGA به حداکثر استفاده مجدد از داده‌ها و بالاترین سطح کارایی برسد. بعد از آن نتیجه نهایی یک کد سنتزپذیر است که با بالاترین کارایی روی FPGA موردنظر تمامی نیازهای مدل ورودی را برآورده می‌سازد. این کار کمک می‌کند تا چرخه طولانی لازم برای استفاده از FPGA در شبکه‌های عصبی عمیق کوتاه‌تر شود. هدف از این کار تولید یک چارچوب خودکار است که اولاً برنامه‌نویس را از جزئیات مربوط به طراحی و بهینه‌سازی سخت‌افزار جدا می‌کند. دوماً خود چارچوب به منابع محدود موجود روی FPGA دست و پنجه نرم می‌کند؛ و در نهایت چارچوب پایدار برای FPGA فراهم می‌کند که کارایی بالایی را برای پیاده‌سازی مدل‌های مختلف شبکه‌های عصبی عمیق روی FPGAهای گوناگون ارائه می‌کند. برای رسیدن به چنین هدف‌هایی DNNWEAVER طراحی شده‌است. در مرحله اول DNNWEAVER نیاز به یک مدل شبکه عصبی عمیق دارد که با استفاده از یک رابط سطح بالا مشخص شده‌است. ورودی برنامه DNNWEAVER یک توصیف‌گر سطح بالا از شبکه‌های عصبی عمیق است که در دانشگاه برکلی با نام قالب Caffe[۴۸] درست شده‌است. Caffe یکی از پراستفاده‌ترین چارچوب‌های یادگیری عمیق است که توصیف شبکه را به عنوان ورودی می‌گیرد و به مدلی قابل اجرا روی CPU یا GPU تبدیل می‌کند. DNNWEAVER به صورت خودکار مدل ورودی را به کد سنتزپذیر زبان وریلاگ تبدیل می‌کند. DNNWEAVER از چهار بخش نرم‌افزاری تشکیل شده‌است: ترجمه‌گر شبکه عمیق توصیف‌شده را به معماری مجموعه دستورها خاصی که مسیر داده را مشخص می‌کند تبدیل می‌کند. هر دستور در این مجموعه نشان دهنده یک گره در گراف مسیر داده شبکه است. توجه کنید که FPGA مستقیماً این دستورها را اجرا نمی‌کند. کامپایلر DNNWEAVER این دستورها را به سیگنال‌های کنترلی FPGA می‌نگارد و یک تقویم اجرا را درست می‌کند. این روش یک واسط نرم‌افزار-سخت‌افزار یکپارچه را فراهم می‌کند؛ بنابراین به کمک این معماری مجموعه دستورها انواع گوناگونی از پیاده‌سازی شتاب‌دهنده‌ها را که با محدودیت‌های FPGA مقصد سازگار است، امکان‌پذیر می‌کند. نقشه‌کش طرح دستورها تولید شده در مرحله قبل را به عنوان ورودی می‌گیرد و با کمک الگوریتم بهینه‌سازی منابع الگو قالب‌های سخت‌افزاری را برای FPGA مقصد بهینه می‌کند. نقشه‌کش طرح محاسبات هر لایه را به گروه‌هایی از عملگرها بخش‌بندی می‌کند که داده‌ها را به اشتراک می‌گذارند یا استفاده مجدد می‌کنند. به هرکدام این بخش‌ها تکه می‌گوییم. هر تکه بعد از محاسبه در حافظه ریخته می‌شود و FPGA به سراغ محاسبه تکه بعدی می‌رود. شتاب‌دهنده شبکه‌های عصبی مشکل اساسی با مقدار زیاد حافظه موردنیاز برخلاف حافظه کم FPGA دارد که DNNWEAVER این مشکل را با استفاده از الگوریتم بهینه‌سازی منابع الگو بهبود می‌دهد و به کمک آن یک تعادل بین عملگرهای موازی و استفاده مجدد از داده‌ها با توجه به محدودیت‌های FPGA مقصد برقرار می‌کند. نسّاجی طرح مرحله یکی‌مانده به آخر از DNNWEAVER است که ورودی آن مقدار منابع موردنیاز و تقویم اجرای تولید شده در مرحله قبل است و به کمک آن هسته شتاب‌دهنده را تولید می‌کند. DNNWEAVER از تعدادی قالب بهینه‌سازی دستی استفاده می‌کند که با توجه به گرفتن منابع و سازمان‌دهی سخت‌افزار که در مرحله قبل تولید شده از آن قالب‌ها استفاده می‌کند. آخرین قسمت DNNWEAVER مجتمع‌کننده است. در این مرحله کدهای رابط حافظه به کدهای شتاب‌دهنده اضافه می‌شود. از آنجایی که FPGAهای مختلف از رابط‌های مختلفی برای ارتباط با DRAM خارجی استفاده می‌کنند، این بخش شامل کتابخانه‌هایی از رابط‌های DRAM است که در هر بار کد لازم را اضافه می‌کند. در این حوزه کاری چندین سخت‌افزار با اهداف مختلف ایجاد شده‌است. هر چند که DNNWEAVER یک شتاب‌دهنده نیست، بلکه تولیدکننده کد یک شتاب‌دهنده است. Tabla[۴۹] یک شتاب‌دهنده FPGA برای فاز آموزش الگوریتم‌های یادگیری ماشین ارائه می‌کند در حالی که DNNWEAVER رو رابط شبکه‌های عصبی تمرکز دارد. Tabla از زبان ریاضیاتی مختص به خودش استفاده می‌کند در حالی که DNNWEAVER از زبان Caffe برای مشخص کردن مدل استفاده می‌کند. کار انجام شده توسط چن و همکارانش[۵۰] روی طرحی تحلیلی بر اساس مدل خط بیشترین کار می‌کند تا با استفاده از آن سریع‌ترین طرح را برای شبکه عصبی عمیق مربوط روی FPGA بیابند. اما این کار برخی لایه‌ها مثل ائتلافی (Pooling) را دربر نمی‌گیرد."
  },
  {
    "url": "https://fr.wikipedia.org/wiki/Apprentissage_profond",
    "title": "Apprentissage profond — Wikipédia",
    "content": "modifier - modifier le code - modifier Wikidata L'apprentissage profond[1],[2] ou apprentissage en profondeur[1] (en anglais : deep learning) est un sous-domaine de l’intelligence artificielle qui utilise des réseaux neuronaux artificiels formant de nombreuses couches pour résoudre des tâches complexes. L'apprentissage profond permet des progrès importants et rapides dans les domaines de l'analyse du signal sonore ou visuel, notamment de la reconnaissance faciale, de la reconnaissance vocale, de la vision par ordinateur, du traitement automatisé du langage. Les développements de l'apprentissage profond sont rendus possibles par des investissements privés et publics importants, notamment de la part des GAFAM (Google, Apple, Facebook, Amazon, Microsoft)[3], durant les années 2000. Pour créer un modèle informatique prédictif de manière classique, on modélise les données par extraction de caractéristiques, cette dernière étant souvent effectuée au moyen d'un algorithme. Selon la méthode de l'apprentissage profond, l'extraction de caractéristiques résulte elle-même d'un processus d'apprentissage : on parle donc d'apprentissage de représentations. En pratique, la machine apprend des représentations hiérarchisées, souvent dans les couches cachées de réseaux de neurones artificiels, chacune étant définie à partir de représentations plus simples[DLB2016 1]. Ces représentations étant apprises directement à partir des données, cela évite que les humains aient à expliciter la manière de les construire au moyen d'un algorithme. Si l'on représente la manière dont ces représentations sont construites les unes à partir des autres au moyen d'un graphe, celui-ci contiendra de multiples couches, justifiant ainsi la qualification de « profond ». L'apprentissage profond est considéré comme « la troisième vague » de développement, après le « cybernétique » des années 1940-1960 puis le « connexionniste »  des années 1980, chacun ayant été suivi par un hiver de l'intelligence artificielle[DLB2016 2].\nLe concept d'apprentissage profond prend forme dans les années 2010, avec la convergence de trois facteurs[DLB2016 3] : En 2012, le modèle AlexNet, conçu par Alex Krizhevsky, Ilya Sutskever et leur directeur de thèse Geoffrey Hinton[4], obtient les meilleures performances à la campagne d'évaluation internationale ImageNet de reconnaissance d'images. Le réseau surpasse largement le deuxième[5] et popularise ainsi les approches par apprentissage profond en vision par ordinateur. En 2015, le programme AlphaGo, un modèle neuronal profond qui a « appris » à jouer au jeu de go grâce à l'apprentissage par renforcement, bat le champion européen Fan Hui[6] par cinq parties à zéro. En mars 2016, le même programme bat le champion du monde Lee Sedol par 4 parties à 1[7]. Ces matches ont eu un fort retentissement dans le grand public, en particulier en Asie. En 2017, à la conférence NIPS, des chercheurs travaillant pour la plupart dans des équipes de recherche de Google proposent l'architecture transformeur[8], qui servira peu de temps après de base aux grands modèles de langage. L'année suivante, l'entreprise propose le modèle BERT, basée sur la partie « encodeur » du transformeur. Ce modèle de langage permettra une amélioration significative des performances en traitement automatique des langues. La même année, OpenAI propose le modèle GPT, qui est pour sa part fondé sur la partie « décodeur » des transformeurs. En 2018, Yann Le Cun, Yoshua Bengio et Geoffrey Hinton sont récipiendaires du prix Turing « Pour les percées conceptuelles et techniques qui ont fait des réseaux neuronaux profonds une composante essentielle de l'informatique[9] ». En 2019, OpenAI publie GPT-2, un modèle de fondation capable de générer du texte. Tout en exprimant leurs inquiétudes sur les détournements possibles de ce type de technologie, les chercheurs de l'association renoncent à partager la version complète[10]. L'apprentissage profond s'applique à divers secteurs des NTIC, notamment : Dans le système de santé, l'apprentissage profond peut aussi[3] : En physique, l'apprentissage profond est utilisé pour la recherche sur les particules exotiques[41]. Sont pointés de possibles usages malveillants de l'apprentissage profond. Il est devenu possible avec les hypertrucages d'incruster le visage d'une personne sur une autre, à son insu, et de lui faire faire ou dire des choses qu'elle n'a pas faites (comme dans le film Running Man de 1986), l'apprentissage profond recréant les mouvements du visage en rendant l'incrustation ressemblante. Ainsi, plusieurs actrices comme Gal Gadot, Emma Watson, Cara Delevingne, Emma Stone, Natalie Portman ou Scarlett Johansson se sont retrouvées avec leur visage incrusté sur celui d'une actrice pornographique, soulevant des craintes quant à la généralisation d'un tel usage, permettant à n'importe qui de nuire à la réputation d'une autre personne[42]. Face à ce danger, plusieurs plates-formes telles que Pornhub, Twitter et Reddit ont réagi en interdisant la publication de telles vidéos, et l'utilisateur « deepfakes », créateur du logiciel éponyme permettant à tout usager de créer des fausses vidéos à caractère pornographique, a été banni de Reddit et son fil dédié supprimé[43]."
  },
  {
    "url": "https://ga.wikipedia.org/wiki/Domhainfhoghlaim",
    "title": "Domhainfhoghlaim - Vicipéid",
    "content": "Tá domhainfhoghlaim (ar a dtugtar foghlaim dhomhain struchtúrtha freisin) mar chuid de theaghlach níos leithne de mhodhanna meaisínfhoghlama, bunaithe ar líonraí néaracha saorga le foghlaim ionadaíochta. Is féidir an fhoghlaim a mhaoirsiú, a leath-mhaoirsiú nó a bheith gan mhaoirsiú ar bith.[2]"
  },
  {
    "url": "https://gl.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Wikipedia, a enciclopedia libre",
    "content": "A aprendizaxe profunda tamén chamada aprendizaxe estruturada profunda ou aprendizaxe xerárquica (en inglés deep learning, deep structured learning ou hierarchical learning) é un tipo de aprendizaxe automática, que se usa principalmente con certos tipos de redes neurais artificiais.[2] Como ocorre con outros tipos de aprendizaxe automática, as sesións de aprendizaxe poden ser sen supervisión, semisupervisadas ou supervisadas. En moitos casos, as estruturas organízanse de xeito que haxa polo menos unha capa intermedia (ou capa oculta), entre a capa de entrada e a capa de saída. Algunhas tarefas, como recoñecer e comprender a fala, as imaxes ou a caligrafía, son fáciles de facer para os humanos. Porén, para un ordenador estas tarefas son moi difíciles de facer. Nunha rede neural multicapa (que ten máis de dúas capas), a información procesada farase máis abstracta con cada capa engadida. Os modelos de aprendizaxe profunda están inspirados nos patróns de procesamento da información e comunicación nos sistemas nerviosos biolóxicos; son diferentes das propiedades estruturais e funcionais dos cerebros biolóxicos (especialmente do cerebro humano) en moitos aspectos, o que os fai incompatibles coas evidencias da neurociencia.[3] [4]"
  },
  {
    "url": "https://ko.wikipedia.org/wiki/%EB%94%A5_%EB%9F%AC%EB%8B%9D",
    "title": "딥 러닝 - 위키백과, 우리 모두의 백과사전",
    "content": "심층 학습(深層學習) 또는 딥 러닝(영어: deep structured learning, deep learning 또는 hierarchical learning)은 여러 '비선형 변환기법'의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계 학습 알고리즘의 집합[1]으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다. 어떠한 데이터가 있을 때 이를 컴퓨터가 알아 들을 수 있는 형태(예를 들어 이미지의 경우는 픽셀정보를 열벡터로 표현하는 등)로 표현(representation)하고 이를 학습에 적용하기 위해 많은 연구(어떻게 하면 더 좋은 표현기법을 만들고 또 어떻게 이것들을 학습할 모델을 만들지에 대한)가 진행되고 있으며, 이러한 노력의 결과로 deep neural networks, convolutional deep neural networks, deep belief networks와 같은 다양한 딥 러닝 기법들이 컴퓨터 비전, 음성인식, 자연어 처리, 음성/신호처리 등 최첨단 분야에 적용되고 있다. 2012년 스탠포드대학의 앤드류 응과 구글이 함께한 딥 러닝 프로젝트에서는 16,000개의 컴퓨터 프로세서와 10억 개 이상의 neural networks 그리고 DNN(deep neural networks)을 이용하여 유튜브에 업로드 되어 있는 천만 개 넘는 비디오 중 고양이 인식에 성공하였다.[2] 이 소프트웨어 프레임워크를 논문에서는 DistBelief로 언급하고 있다[3]. 이뿐만 아니라 마이크로소프트, 페이스북[4] 등도 연구팀을 인수하거나 자체 개발팀을 운영하면서 인상적인 업적들을 만들어 내고 있다. MIT가 2013년을 빛낼 10대 혁신기술 중 하나로 선정[5] 하고 가트너(Gartner, Inc.)가 2014 세계 IT 시장 10대 주요 예측[6]에 포함시키는 등 최근들어 딥 러닝에 대한 관심이 높아지고 있지만 사실 딥 러닝 구조는 인공신경망(ANN, artificial neural networks)에 기반하여 설계된 개념으로 역사를 따지자면 최소 1980년 Kunihiko Fukushima에 의해 소개된 Neocognitron[7]까지 거슬러 올라가야 한다. 맥컬럭-월터 피츠는 1943년에 ANN의 가능성을 처음으로 주장했다. 1957년에 미국의 신경 생물학자인 프랑크 로젠블랫은 이 개념을 실증하는데 성공했고 퍼셉트론이라 불렀다. 그러나 당시 성능이 너무 후져서 묻혔다가, 힌튼이 역전파 알고리즘을 이용하여 1995년에 다시 ANN이 부활하게 되었다. 1989년에 얀 르쿤과 그의 동료들은 오류역전파 알고리즘(backpropagation algorithm)[8]에 기반하여 우편물에 손으로 쓰여진 우편번호를 인식하는 deep neural networks를 소개[9] 했다. 알고리즘이 성공적으로 동작했음에도 불구하고, 신경망 학습에 소요되는 시간(10 개의 숫자를 인식하기 위해 학습하는 시간)이 거의 3일이 걸렸고 이것은 다른 분야에 일반적으로 적용되기에는 비현실적인 것으로 여겨졌다. 많은 요소들이 느린 속도에 원인을 제공했는데, 그 중 하나는 1991년 Jürgen Schmidhuber의 제자였던 Sepp Hochreiter에 의해 분석된 vanishing gradient problem(지역최솟값에 머무르게 되는 원인)이었다[10][11]. 또한 불연속 시뮬레이션에서 초기 상태를 어떻게 선택하느냐에 따라 수렴이 안되고 진동 또는 발산하는 문제, 트레이닝셋에 너무 가깝게 맞추어 학습되는 과적합 (Overfitting) 문제, 원론적으로 생물학적 신경망과는 다르다는 이슈들이 끊임 없이 제기되면서 인공신경망은 관심에서 멀어졌고 90년대와 2000년대에는 서포트 벡터 머신 같은 기법들이 각광받게 된다. 본격적으로 딥 러닝이란 용어를 사용한 것은 2000년대 딥 러닝의 중흥기를 이끌어간다고 평가할 수 있는 제프리 힌튼[12]과 Ruslan Salakhutdinov에 의해서이며, 기존 신경망의 과적합 문제를 해결하기 위해 이들은 unsupervised RBM(restricted Boltzmann machine)을 통해 학습시킬 앞먹임 신경망(Feedforward Neural Network)의 각 층을 효과적으로 사전훈련(pre-trainning)하여 과적합을 방지할 수 있는 수준의 initialize point를 잡았고, 이를 다시 supervised backpropagation를 사용[13]하는 형태로 학습을 진행한다. 또한 2013년에는 신호처리학회인 ICASSP에서 RBM을 대체하여 과적합을 방지할 수 있는 Drop-out[14]이라는 개념이 소개되면서 사전훈련보다 훨씬 더 간단하고 강력한 형태로 과적합을 방지할 수 있게 되었다. 딥 러닝이 부활하게 된 이유는 크게 세 가지로 꼽힌다.\n첫 번째는 앞서 딥 러닝의 역사에서 언급한 바 있는 기존 인공신경망 모델의 단점이 극복되었다는 점이다. 그러나 과적합 문제만 해결되었다고 해서 느린 학습시간이 줄어드는 것은 아니다.\n두 번째 이유로, 여기에는 하드웨어의 발전이라는 또다른 요인이 존재 한다. 특히 강력한 GPU는 딥 러닝에서 복잡한 행렬 연산에 소요되는 시간을 크게 단축시켰다.\n마지막으로 언급하지만 가장 중요한 세 번째 이유로 빅 데이터를 들 수 있다. 대량으로 쏟아져 나오는 데이터들, 그리고 그것들을 수집하기 위한 노력 특히 SNS 사용자들에 의해 생산되는 다량의 자료와 태그정보들 모두가 종합되고 분석 되어 학습에 이용될 수 있다.\n- 인공신경망의 학습에 사용되는 트레이닝벡터는 이름이 붙어 있는(labeled) 데이터여야 하는데(supervised learning의 경우) 대량의 트레이닝셋 모두에 label을 달아주는 일은 불가능한 일이다. 이런 이유로 초기 학습에 사용되는 일부 데이터에 대해서만 지도학습(supervised learning)을 수행하고 나머지 트레이닝셋에 대해서는 비지도학습(unsupervised learning)을 진행하며, 학습된 결과는 기존 학습의 결과와 앞서 분석된 메타태그 정보들을 종합하여 인식기가 완성 된다. 딥 러닝의 부활 이후 다양한 분야, 특히 자동 음성 인식(ASR, automatic speech recognition)과 컴퓨터비전 분야에서 최고수준의 성능을 보여주고 있으며 이들은 보통 딥 러닝의 새로운 응용들의 지속적인 성능 향상을 위해 만들어진 TIMIT(Texas Instruments와 MIT가 제작한 음성 Database), MNIST(이미지 클러스터링을 위한 hand-written 숫자 이미지 데이터베이스로 National Institute of Standards and Technology가 제작) 등의 데이터베이스를 사용했다. 최근에는 Convolution Neural Networks 기반의 딥 러닝 알고리즘이 뛰어난 성능을 발휘하고 있으며, 컴퓨터비전과 음성인식등의 분야에서 특히 탁월한 성능을 보이고 있다. 다양한 종류의 심층 신경망 구조가 존재하지만, 대부분의 경우 대표적인 몇 가지 구조들에서 파생된 것이다. 그렇지만 여러 종류의 구조들의 성능을 동시에 비교하는 것이 항상 가능한 것은 아닌데, 그 이유는 특정 구조들의 경우 주어진 데이터 집합에 적합하도록 구현되지 않은 경우도 있기 때문이다. 심층 신경망(Deep Neural Network, DNN)은 입력층(input layer)과 출력층(output layer) 사이에 여러 개의 은닉층(hidden layer)들로 이뤄진 인공신경망(Artificial Neural Network, ANN)이다.[15][16]\n심층 신경망은 일반적인 인공신경망과 마찬가지로 복잡한 비선형 관계(non-linear relationship)들을 모델링할 수 있다.\n예를 들어, 물체 식별 모델을 위한 심층 신경망 구조에서는 각 물체가 영상의 기본적 요소들의 계층적 구성으로 표현될 수 있다.[17]\n이때, 추가 계층들은 점진적으로 모여진 하위 계층들의 특징들을 규합시킬 수 있다.\n심층 신경망의 이러한 특징은, 비슷하게 수행된 인공신경망에 비해 더 적은 수의 유닛(unit, node)들 만으로도 복잡한 데이터를 모델링할 수 있게 해준다.[15] 이전의 심층 신경망들은 보통 앞먹임 신경망으로 설계되어 왔지만, 최근의 연구들은 심층 학습 구조들을 순환 신경망(Recurrent Neural Network, RNN)에 성공적으로 적용했다. 일례로 언어 모델링(language modeling) 분야에 심층 신경망 구조를 적용한 사례 등이 있다[18]\n합성곱 신경망(Convolutional Neural Network, CNN)의 경우에는 컴퓨터 비전(computer vision) 분야에서 잘 적용되었을 뿐만 아니라, 각각의 성공적인 적용 사례에 대한 문서화 또한 잘 되어 있다.[19]\n더욱 최근에는 합성곱 신경망이 자동음성인식(Automatic Speech Recognition, ASR)을 위한 음향 모델링(acoustic modeling) 분야에 적용되었으며, 기존의 모델들보다 더욱 성공적으로 적용되었다는 평가를 받고 있다.[20] 심층 신경망은 표준 오류역전파 알고리즘으로 학습될 수 있다.\n이때, 가중치(weight)들은 아래의 등식을 이용한 확률적 경사 하강법(stochastic gradient descent)을 통하여 갱신될 수 있다. 여기서, \n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n는 학습률(learning rate)을 의미하며, \n\n\n\nC\n\n\n{\\displaystyle C}\n\n는 비용함수(cost function)를 의미한다.\n비용함수의 선택은 학습의 형태(지도 학습, 자율 학습 (기계 학습), 강화 학습 등)와 활성화함수(activation function)같은 요인들에 의해서 결정된다.\n예를 들면, 다종 분류 문제(multiclass classification problem)에 지도 학습을 수행할 때, 일반적으로 활성화함수와 비용함수는 각각 softmax 함수와 교차 엔트로피 함수(cross entropy function)로 결정된다.\nsoftmax 함수는 \n\n\n\n\np\n\nj\n\n\n=\n\n\n\nexp\n⁡\n(\n\nx\n\nj\n\n\n)\n\n\n\n∑\n\nk\n\n\nexp\n⁡\n(\n\nx\n\nk\n\n\n)\n\n\n\n\n\n{\\displaystyle p_{j}={\\frac {\\exp(x_{j})}{\\sum _{k}\\exp(x_{k})}}}\n\n 로 정의된다, 이때, \n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n는 클래스 확률(class probability)을 나타내며, \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n 와 \n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n는 각각 유닛 \n\n\n\nj\n\n\n{\\displaystyle j}\n\n 로의 전체 입력(total input)과 유닛 \n\n\n\nk\n\n\n{\\displaystyle k}\n\n 로의 전체 입력을 나타낸다.\n교차 엔트로피는 \n\n\n\nC\n=\n−\n\n∑\n\nj\n\n\n\nd\n\nj\n\n\nlog\n⁡\n(\n\np\n\nj\n\n\n)\n\n\n{\\displaystyle C=-\\sum _{j}d_{j}\\log(p_{j})}\n\n 로 정의된다, 이때, \n\n\n\n\nd\n\nj\n\n\n\n\n{\\displaystyle d_{j}}\n\n는 출력 유닛 \n\n\n\nj\n\n\n{\\displaystyle j}\n\n 에 대한 목표 확률(target probability)을 나타내며, \n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n는 해당 활성화함수를 적용한 이후의 \n\n\n\nj\n\n\n{\\displaystyle j}\n\n 에 대한 확률 출력(probability output)이다.[21] 기존의 인공신경망과 같이, 심층 신경망 또한 나이브(naive)한 방식으로 학습될 경우 많은 문제들이 발생할 수 있다. 그 중 과적합과 높은 시간 복잡도가 흔히 발생하는 문제들이다. 심층 신경망이 과적합에 취약한 이유는 추가된 계층들이 학습 데이터의 rare dependency의 모형화가 가능하도록 해주기 때문이다. 과적합을 극복하기 위해서 weight decay (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n–regularization) 또는 sparsity (\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n–regularization) 와 같은 정칙화(regularization) 방법들이 사용될 수 있다.[22] 그리고 최근에 들어서는 심층 신경망에 적용되고 있는 정칙화 방법 중 하나로 dropout 정칙화가 등장했다. dropout 정칙화에서는 학습 도중 은닉 계층들의 몇몇 유닛들이 임의로 생략된다. 이러한 방법은 학습 데이터(training data)에서 발생할 수 있는 rare dependency를 해결하는데 도움을 준다.[23] 오차역전파법과 경사 하강법은 구현의 용이함과 국지적 최적화(local optima)에 잘 도달한다는 특성으로 인해 다른 방법들에 비해 선호되어온 방법들이다. 그러나 이 방법들은 심층 신경망을 학습 시킬 때 시간 복잡도가 매우 높다. 심층 신경망을 학습시킬 때에는 크기(계층의 수 와 계층 당 유닛 수), 학습률, 초기 가중치 등 많은 매개변수(parameter)들이 고려되어야 한다. 하지만 최적의 매개변수들을 찾기 위해 매개변수 공간 전부를 확인하는 것은 계산에 필요한 시간과 자원의 제약으로 인해 불가능하다. 시간 복잡도를 해결하기 위해, 미니 배치(mini batch, 여러 학습 예제들의 경사를 동시에 계산), 드롭 아웃(drop out)과 같은 다양한 '묘책’들이 등장하였다.[24] 또한, 행렬 및 벡터 계산에 특화된 GPU는 많은 처리량을 바탕으로 두드러지는 학습 속도 향상을 보여주었다.[16] 합성곱 신경망(Convolutional Neural Network, CNN)은 최소한의 전처리(preprocess)를 사용하도록 설계된 다계층 퍼셉트론(multilayer perceptrons)의 한 종류이다. CNN은 하나 또는 여러개의 합성곱 계층과 그 위에 올려진 일반적인 인공 신경망 계층들로 이루어져 있으며, 가중치와 통합 계층(pooling layer)들을 추가로 활용한다. 이러한 구조 덕분에 CNN은 2차원 구조의 입력 데이터를 충분히 활용할 수 있다. 다른 딥 러닝 구조들과 비교해서, CNN은 영상, 음성 분야 모두에서 좋은 성능을 보여준다. CNN은 또한 표준 역전달을 통해 훈련될 수 있다. CNN은 다른 피드포워드 인공신경망 기법들보다 쉽게 훈련되는 편이고 적은 수의 매개변수를 사용한다는 이점이 있다. 최근 딥 러닝에서는 합성곱 심층 신뢰 신경망 (Convolutional Deep Belief Network, CDBN) 가 개발되었는데, 기존 CNN과 구조적으로 매우 비슷해서, 그림의 2차원 구조를 잘 이용할 수 있으며 그와 동시에 심층 신뢰 신경망 (Deep Belief Network, DBN)에서의 선훈련에 의한 장점도 취할 수 있다. CDBN은 다양한 영상과 신호 처리 기법에 사용될 수 있는 일반적인 구조를 제공하며 CIFAR[25]와 같은 표준 이미지 데이터에 대한 여러 벤치마크 결과에 사용되고 있다. 순환 신경망은 인공신경망을 구성하는 유닛 사이의 연결이 Directed cycle을 구성하는 신경망을 말한다. 순환 신경망은 앞먹임 신경망과 달리, 임의의 입력을 처리하기 위해 신경망 내부의 메모리를 활용할 수 있다. 이러한 특성에 의해 순환 신경망은 필기체 인식(Handwriting recognition)과 같은 분야에 활용되고 있고, 높은 인식률을 나타낸다.[26] 순환 신경망을 구성할 수 있는 구조에는 여러 가지 방식이 사용되고 있다. 완전 순환망(Fully Recurrent Network), Hopfield Network, Elman Network, Echo state network(ESN), Long short term memory network(LSTM), Bi-directional RNN, Continuous-time RNN(CTRNN), Hierarchical RNN, Second Order RNN 등이 대표적인 예이다. 순환 신경망을 훈련(Training)시키기 위해 대표적으로 경사 하강법, Hessian Free Optimization, Global Optimization Methods 방식이 쓰이고 있다.\n하지만 순환 신경망은 많은 수의 뉴런 유닛이나 많은 수의 입력 유닛이 있는 경우에 훈련이 쉽지 않은 스케일링 이슈를 가지고있다. 볼츠만 머신에서, 층간 연결을 없앤 형태의 모델이다. 층간 연결을 없애면, 머신은 가시 유닛(Visible Unit)과 은닉 유닛(Hidden Unit)으로 이루어진 무방향 이분 그래프 형태의 모양이 된다. 결론적으로 모델의 층간 연결을 없앰으로써, 얻는 이점으로 뉴럴 네트워크는 깊어질 수 있었다. 가장 큰 이점은 가시 유닛이 관찰되고 고정(Clamped)되었을 때 은닉 유닛을 추론(Inference) 하는 MCMC 과정이 단 한 번에 끝난다는 것이다. RBM은 확률모델의 계산이 불가능하기 때문에, 학습시 근사법인 MCMC 나 또는 제프리 힌튼 교수가 발견한 CD(Contrastive Divergence)를 사용하는데, RBM의 모델의 간단함에서 오는 추론에 대한 이점은 샘플링 근간의 학습법이 실용적인 문제에 적용되는데 기여했다. 자세한 학습 방법은 아래에 설명되어 있다. RBM은 DBN의 기본 뼈대가 된다, RBM을 쌓아올리면서(Stacking), Greedy하게 학습함으로써, DBN을 완성한다. DBN을 아는 사람은 기본적으로 RBM은 무방향이기에, RBM이 방향성 있는 모델인 DBN이 되는지 의아할 것이다. 이는 제프리 힌튼의 제자인 Teh, Y. W의 우연 발견적인 연구 성과인데, 원래 힌튼 교수는 Graphical Model 이 방향성을 가질 때 생기는 Explaining Away 효과 때문에 학습이 매우 어려워 연구를 무방향성 모델로 선회했다가, RBM을 쌓으면, 다층 RBM이 되는 것이 아니라, DBN과 비슷해진다는 사실을 발견하게 된다. 이 발견은 DBN학습을 단순히 여러 개의 RBM 학습으로 환원시킴으로써(앞서서 얘기했듯이 RBM은 학습이 어렵지 않다) 어려운 DBN 학습의 복잡도를 층의 개수에 비례하는 복잡도로 낮추었다. 이는 선행학습(Pre-training) 과 미세조정(fine-tuning)의 새로운 학습 패러다임으로 발전하게 된다. RBM 훈련 과정에서의 가중치 갱신은, 다음의 식을 기반으로 경사 하강법을 통해 이루어진다. \n\n\n\nΔ\n\nw\n\ni\nj\n\n\n(\nt\n+\n1\n)\n=\n\nw\n\ni\nj\n\n\n(\nt\n)\n+\nη\n\n\n\n∂\nlog\n⁡\n(\np\n(\nv\n)\n)\n\n\n∂\n\nw\n\ni\nj\n\n\n\n\n\n\n\n{\\displaystyle \\Delta w_{ij}(t+1)=w_{ij}(t)+\\eta {\\frac {\\partial \\log(p(v))}{\\partial w_{ij}}}}\n\n\n여기서 \n\n\n\np\n(\nv\n)\n\n\n{\\displaystyle p(v)}\n\n는 다음의 식으로 주어지는 가시 벡터의 확률을 나타내고, \n\n\n\np\n(\nv\n)\n=\n\n\n1\nZ\n\n\n\n∑\n\nh\n\n\n\ne\n\n−\nE\n(\nv\n,\nh\n)\n\n\n\n\n{\\displaystyle p(v)={\\frac {1}{Z}}\\sum _{h}e^{-E(v,h)}}\n\n \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n는 정규화(normalization)를 위해 사용되는 분배함수(partition function)이며, \n\n\n\nE\n(\nv\n,\nh\n)\n\n\n{\\displaystyle E(v,h)}\n\n는 신경망의 상태(state)에 부여되는 에너지 함수이다. 에너지가 낮을수록 해당 신경망이 더 적합한 상태임을 의미한다. 기울기 \n\n\n\n\n\n\n∂\nlog\n⁡\n(\np\n(\nv\n)\n)\n\n\n∂\n\nw\n\ni\nj\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial \\log(p(v))}{\\partial w_{ij}}}}\n\n는 좀 더 간단하게 \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\ndata\n\n\n−\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{data}}-\\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n 로 표현이 되는데 이 때 \n\n\n\n⟨\n⋯\n\n⟩\n\np\n\n\n\n\n{\\displaystyle \\langle \\cdots \\rangle _{p}}\n\n는 분포 \n\n\n\np\n\n\n{\\displaystyle p}\n\n에 관한 평균을 의미한다. \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n을 샘플링 하려면 alternating Gibbs sampling을 매우 많이 반복해야 하기 때문에 CD에서는 alternating Gibbs sampling을 \n\n\n\nn\n\n\n{\\displaystyle n}\n\n 회만 반복한다.(실험을 통해 \n\n\n\nn\n=\n1\n\n\n{\\displaystyle n=1}\n\n 일 경우에도 충분한 성능이 나온다고 알려져 있다). \n\n\n\nn\n\n\n{\\displaystyle n}\n\n회 반복 후 샘플링 된 데이터가 \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n를 대신한다. CD 수행 과정은 다음과 같다: 심층 신뢰 신경망(Deep Belief Network, DBN)이란 기계학습에서 사용되는 그래프 생성 모형(generative graphical model)으로, 딥 러닝에서는 잠재변수(latent variable)의 다중계층으로 이루어진 심층 신경망을 의미한다. 계층 간에는 연결이 있지만 계층 내의 유닛 간에는 연결이 없다는 특징이 있다. DBN은 생성 모형이라는 특성상 선행학습에 사용될 수 있고, 선행학습을 통해 초기 가중치를 학습한 후 역전파 혹은 다른 판별 알고리즘을 통해 가중치의 미조정을 할 수 있다. 이러한 특성은 훈련용 데이터가 적을 때 굉장히 유용한데, 이는 훈련용 데이터가 적을수록 가중치의 초기값이 결과적인 모델에 끼치는 영향이 세지기 때문이다. 선행학습된 가중치 초기값은 임의로 설정된 가중치 초기값에 비해 최적의 가중치에 가깝게 되고 이는 미조정 단계의 성능과 속도향상을 가능케 한다. DBN은 비지도 방식으로 계층마다 학습을 진행하는데 이때 각각의 계층은 보통 RBM의 형태를 띄고 있다. RBM들을 쌓아 올리면서 DBN을 훈련시키는 방법에 대한 설명은 아래에 제공 되어 있다. RBM은 에너지 기반의 생성 모형으로 가시 유닛과 은닉 유닛으로 이루어진 무방향 이분 그래프 형태이다. 가시 유닛들과 은닉 유닛들 사이에만 연결이 존재한다. RBM이 훈련되고 나면 다른 RBM이 그 위에 쌓아 올려짐으로써 다중 계층 모형을 형성한다. RBM이 쌓아 올려질 때마다, 이미 훈련된 RBM의 최상위 계층이 새로 쌓이는 RBM의 입력으로 쓰인다. 이 입력을 이용하여 새 RBM이 훈련되고, 원하는 만큼의 계층이 쌓일 때까지 해당 과정이 반복된다. 실험결과에 따르면, CD의 최대가능도 근사가 굉장히 투박함에도 불구하고, 심층 신경망 구조를 학습하기에는 충분히 효과적인 방식이라고 한다. 2015년 2월, 강화 학습을 위한 가장 최신 딥 러닝 모델이 네이처에 소개되었다.[27] 아래 표는 TIMIT 데이터에 대한 자동 음성 인식 결과를 보여준다. 이 데이터셋은 딥 러닝의 초창기 평가를 위한 일반적인 데이터로서, 미국의 8가지 방언을 사용하는 총 630명의 사람이 읽은 10가지 문장으로 이루어져 있다.[28] 데이터의 크기가 작기 때문에 다양한 설정을 효과적으로 적용할 수 있다. 더 중요한 점은 TIMIT에서 음소 순서 인식(phone-sequence recognition)을 고려한다는 점이다. 따라서, 단어 순서 인식(word-sequence recognition)과는 달리 아주 약한 \"언어모델\"을 허용하고 음성 인식에서의 음향 모델 측면을 더 쉽게 분석할 수 있다. 2009 ~ 2010년 무렵, 크고 작은 범위의 음성인식에 대한 딥 러닝 기술 활용을 위해서 많은 투자가 있었는데, Li Deng과 그의 동료들은 TIMIT에서의 GMM 과 DNN 모델을 비교하는 실험을 수행하였다.[29][30] 결국 그들은 음성인식에서의 딥 러닝 활용에 있어서 가장 앞서나가게 되었다. 이 분석은 먼저 식별적 DNN과 발생적 모델 사이의 성능 비교(1.5% 이하의 오차율)로 수행되었다. 아래 표에 나타난 오차율은 앞에서 말한 초창기 실험을 포함하여 과거 20 여 년 간 수행된 실험들의 음소 오차율(Phone error rate)을 요약한 것이다. TIMIT로부터 대량 어휘 음성인식(large vocabulary speech recognition)으로의 딥 러닝의 확장은 2010년 산업계 연구자들에 의해 성공적으로 수행되었다.[33][34] 자동 음성 인식 분야의 2014년 10월까지의 최신 동향은 마이크로소프트 리서치의 책 에 잘 정리되어있다.[35] 또한 자동 음성인식과 관련된 배경 지식과 다양한 기계학습 패러다임의 영향을 잘 정리한 글을 참고할 수 있다.[36]\n대용량 자동 음성인식은 최근 딥 러닝의 역사에서 산업계와 학계를 모두 아우르는 처음이자 가장 성공적인 케이스라고 할 수 있다. 2010년부터 2014년까지, 신호처리와 음성인식에 대한 주요 학술회의인 IEEE-ICASSP 와 Interspeech는 음성인식을 위한 딥 러닝 분야의 합격 논문 개수에 있어서 거의 기하급수적인 성장을 보여주었다. 더 중요한 것은, 현재 모든 주요 상업 음성인식 시스템(MS 코타나, 스카이프 번역기, 구글 나우, 애플 시리 등등)이 딥 러닝 기법에 기반하고있다는 점이다.[37][38][39] 뉘앙스 커뮤니케이션즈의 CTO의 최근 인터뷰도 참고할만하다.[40] 일반적으로 이미지 분류를 위한 평가 데이터로서 MNIST 데이터베이스 데이터가 이용된다. MNIST는 손으로 쓴 숫자들로 구성되어 있으며, 60000개의 학습 예제들과 10000개의 테스트 예제들을 포함한다. TIMIT와 유사하게, 적은 용량의 MNIST 데이터는 복수의 테스트 환경설정이 가능하게 해준다. MNIST 데이터에 대한 종합적인 결과들을 [123]에서 확인할 수 있다. 현재까지 MNIST 데이터에 대한 가장 우수한 결과는 Ciresan 등이 작성한[41]에서 달성되었으며, 오차율 0.23%를 기록했다. 제프리 힌튼과 그의 제자들은 2012년 가을에 대규모 ImageNet 대회에서 당시 최신 기계 학습 방법들의 성능을 훌쩍 뛰어넘는 결과를 보여주며 우승하였다. 이로 인해 컴퓨터 비전의 주요 분야인 영상 인식 및 사물 인식 분야에서의 딥 러닝의 중요성이 대두되었다. 그 당시, 대규모에 딥 러닝이 상당히 잘 작동한다는 것을 알고 있었던 그들은, 20년 전에 고안된 심층 합성곱 신경망 구조를 대규모 작업에 맞도록 대규모로 사용하였다. 2013년부터 2014년에 이르기까지, 딥 러닝을 이용한 ImageNet 과제 결과의 오차율은 대규모 음성인식 분야와 추세를 같이하며 빠르게 줄어나갔다. 자동 음성인식 분야의 자동 음성 번역 및 이해 분야로의 확장과 마찬가지로, 이미지 분류 분야는 자동 영상 captioning이라는 더욱 도전적인 분야로 확장되었다. 자동 영상 캡셔닝은 딥 러닝을 핵심 기반 기술로 사용하는 분야이다.[42]\n[43]\n[44]\n[45] 적용 사례로는 360°카메라 화면을 이해할 수 있도록 딥 러닝을 통해 학습된 자동차 탑재용 컴퓨터 등이 있다.[46] 2000년대 초부터 인공신경망은 언어 모형을 구현하기 위해 사용되어 왔다. 이 분야에서의 핵심 기법은 negative sampling과 단어 표현(word expression)이다. word2vec과 같은 단어 표현은 데이터집합으로 주어진 단어들 사이의 관계를 학습하는 인공신경망을 이용하여 단어를 벡터 공간 상에 나타내는 것이라고 할 수 있다. 단어표현을 재귀 신경망(recursive neural network)의 입력 계층으로 이용하면 해당 신경망이 compositional vector grammar를 통해 문장과 구(phrase)를 분석하도록 학습시킬 수 있다. 이 compositional vector grammar는 재귀 신경망으로 구현된 PCFG 라고 할 수 있다. 단어표현을 기반으로 구성된 Recursive autoencoder는 문장 간의 유사도 판단과 의역 탐지를 하도록 훈련이 가능하다. 이러한 심층 인공신경망 구조들은 자동으로 감정 분석(sentiment analysis), 정보 검색(information retrieval)을 비롯한 다양한 자연언어처리 관련 연구에서 최첨단 기술로서 쓰이고 있다. 제약 산업에서 많은 수의 약들은 시장에 출시되지 못한다. 이렇게 실패하는 이유중 하나는 생각한 만큼의 효험을 보이지 못하거나, 예상치 못한 다른 작용을 일으키기 때문이다. 이를 해결하기 위해 2012년 George Dahl의 팀은 다중 DNN을 약의 효험을 예측하는 데에 활용하여 \"Merck Molecular Activity Challenge\"에 우승하였고[47][48], 2014년 Sepp Hochreiter의 팀은 예상치 못한 다른 작용을 사전에 탐지하기 위해 딥 러닝을 활용하여 \"Tox21 Data Challenge\"에서 우승하였다.[49][50] 이러한 성공은 딥 러닝이 약학에 있어 가상 실험 방법(Virtual Screening Method)에 적합함을 보여준다.[51][52] 현재 많은 수의 연구자들이 여러 가지 데이터를 조합하여 약물을 발견하는 데에 딥 러닝을 활용하고 있다.[53] 최근 직접 마케팅 기획, 고객 관계 관리 자동화를 위한 수단 적합성 산출 등에 직접적으로 심층 강화 학습을 활용하여 성공하는 사례들이 알려지고 있다. 인공 신경망은 RFM으로 정의된 고객들에 대한 활용 가능한 마케팅 활동의 값을 예측할 때 사용되었다. 예측된 함수는 고객 생명 주기 값(customer lifetime value, CLV)으로 변환되어 사용되었다.[54] 딥 러닝은 인지신경과학자(Cognitive neuroscientist)들이 1990년대 초에 제안한 뇌 발달(Brain development)과 밀접한 관련이 있다.[55][56][57] 이러한 발달 관점의 이론들이 도입되면서, 순수한 전산 기반의 딥 러닝 모델들을 위한 기술적인 기반도 마련되었다. 발달 이론에서는 신경발달요인(Nerve growth factor)의 물결과 같은 뇌에서의 다양한 학습 역학이 결국은 서로 연관된 신경망들의 자기조직화를 도와준다는 특징이 있다. 이는 나중에 전산 기반의 딥 러닝 모델에서도 활용되어서, 인공신경망의 계층적인 필터 구조(각 동작 환경에서 필요한 정보만 걸러내는 다중 계층 구조)가 실제 뇌의 피질과 유사해보이게 되었다. 이러한 과정을 통해 자기조직적인 변환기의 계층구조가 만들어지고 각 환경에 맞도록 조율된다. 1995년에 뉴욕 타임스에서는 다음과 같이 말하였다. \"...유아의 두뇌는 영양적인 요인의 영향으로 스스로 조직화되는 것 같다... 뇌에서 한 층의 조직이 먼저 성숙되고 다른 부분과 순차적으로 연결되는 방식으로 전체 뇌가 성숙될 때까지 반복된다.\"[58]\n인간의 인식 발달 및 진화와 관련하여 딥 러닝의 중요성은 많은 과학자들의 관심을 끌고 있다. 가까운 영장류 동물들과 인간이 차별화되는 부분 중 하나는 바로 발달 시기이다.[59] 다른 영장류 동물들의 뇌가 출산 전에 거의 완성되는 반면에, 인간의 뇌는 비교적 출산 후에도 계속 발달하는 편이다. 그래서 인간의 경우 뇌가 발달되는 중요한 시기 동안 세상 밖의 훨씬 더 복잡한 경험에 노출될 수 있다. 이러한 현상은 인간이 다른 동물들에 비하여 빠르게 변화하는 환경에 더 잘 적응할 수 있도록 만든다. 이러한 변화의 정도는 대뇌 피질 발달에 반영되기도 하고, 또한 두뇌의 자기조직화 시기에 자극적인 환경으로부터의 정보 추출에 변화를 준다. 물론, 이러한 유연성은 인간이 다른 동물들에 비해 긴 미성숙기(보호자에게 도움을 받고 훈련을 받아야 하는 의존적인 시기)를 가지게 된 원인이기도 하다. 딥 러닝의 이런 이론들은 결국 인간 진화의 기본적인 조건으로서 문화와 인식의 공진화를 보여준다. 딥 러닝은 종종 강인공지능의 현실화를 향한 발걸음으로 묘사되기도 하기 때문에 많은 기관에서 딥 러닝의 활용에 관심을 보이고 있다. 2013년 3월, 제프리 힌튼과 두 명의 제자(Alex Krizhevsky and Ilya Sutskever)가 구글에 채용되었다. 이들은 구글의 기존 기계 학습 제품들을 향상시키고 구글이 가지고있는 데이터를 더 잘 활용할 수 있도록 돕는 일을 할 것이다. 구글은 또한 힌튼의 회사 DNNresearch를 인수하였다. 2013년 12월에는 페이스북이 캘리포니아, 런던, 뉴욕에서 운영될 인공지능 연구실의 수장으로 얀 르쿤을 채용했다고 발표하였다. 이 인공지능 연구실에서는 딥 러닝 기법들을 연구 하여 페이스북의 사진에 자동으로 사람의 이름을 연결시키는 등의 기능을 구현할 예정이다.[60] 2014년, 구글은 영국의 스타트업기업인 딥마인드 테크놀로지스를 인수하였다. 딥마인드에서는 화면의 정보만을 가지고 비디오 게임을 어떻게 플레이하는지를 스스로 학습하는 시스템을 개발하였다. 바이두는 앤드류 응을 그들의 새로운 실리콘밸리 딥 러닝 연구실의 수장으로 채용하였다. 딥 러닝에 대한 부정적 시각중 하나로 딥 러닝에 사용되는 방법들의 이론적인 뒷받침이 빈약하다는 것이 있다. 대부분의 딥 러닝 알고리즘은 경사 하강법에 기초를 두고 있다. 그런데 경사 하강법 자체는 이론적으로 이해가 잘 되었지만, 이와 함께 사용하는 다른 알고리즘은 이론적인 검증이 빈약하다. 그러한 알고리즘 중 하나인 contrastive divergence는 이 알고리즘이 실제로 정말 수렴하는지, 수렴한다면 얼마나 빠르게 수렴하는지에 대한 부분이 현재 분명하게 증명되지 않았다. 이와 같이, 딥 러닝에 사용되는 방법들은 이론적이기보다는 경험적으로 검증된 방법들을 사용하기 때문에 종종 블랙박스로 이해되기도 한다. 또다른 사람들은 보통 사람들이 딥 러닝을 모든 것을 해결해주는 솔루션으로 바라보는 것에 대해 딥 러닝은 유일한 솔루션이 아니고, 강한 인공 지능을 실현하는 데에 한 걸음 더 다가갈 수 있게 해주는 것으로 봐야한다고 주장한다. 딥 러닝을 모든 것을 해결해주는 솔루션으로 보기에는 기능적으로 여전히 부족한 점이 많기 때문이다. 심리학자인 게리 마커스는 다음과 같이 말했다. \"현실적으로, 딥 러닝은 지능형 기계를 구축하는 것의 일부분에 불과합니다. 딥 러닝에 쓰이는 테크닉들은 인과 관계를 나타내는 능력이 부족하고 논리적 추론을 하는 것에 확실한 방법을 가지고있지 않습니다. 그리고 이 방법들은 어떤 물체가 무엇인지, 용도가 무엇인지 등과 같은 추상적인 정보를 인식하는 데에 오랜 시간이 걸립니다. 왓슨과 같이 현재 가장 강력한 인공지능 시스템은 딥 러닝을 복잡한 여러 개의 테크닉 중 단지 한 부분을 위해 사용하고 있습니다.\"[61]"
  },
  {
    "url": "https://hy.wikipedia.org/wiki/%D4%BD%D5%B8%D6%80_%D5%B8%D6%82%D5%BD%D5%B8%D6%82%D6%81%D5%B8%D6%82%D5%B4",
    "title": "Խոր ուսուցում - Վիքիպեդիա",
    "content": "Խոր ուսուցում (հայտնի է որպես խոր կառուցվածքային ուսուցում կամ հիերարխիկ ուսուցում), մեքենայական ուսուցման մաս, որը հիմնված է տվյալների ներկայացման ուսուցման վրա, ի տարբերություն խնդրային-հատուկ (task-specific) ալգորիթմների։ Ուսուցումը կարող է լինել վերահսկվող (supervised), կիսավերահսկվող (semi-supervised)  և առանց վերահսկման (unsupervised)։ Խոր ուսուցման  շատ տարբերակներ հայտնի են եղել դեռ 1980-ական թվականներից, բայց արդյունքները չեն եղել տպավորիչ, մինչ 2000 թվականը` երբ արհեստական նեյրոնային ցանցերի և հաշվողական հզորություններում գրանցվեց առաջընթաց, որը թույլատրեց նեյրոնային ցանցում ստեղծել բարդ տեխնիկական կառուցվածք, որը ուներ բավականին արտադրողականություն, ինչը թույլ էր տալիս լուծել բավականին բարդ խնդիրներ։ Որոշ խնդիրների ժամանակ լուծման որակը համեմատելի է, իսկ որոշակի խնդիրների ժամանակ գործում է  «սպիտակուց» փորձագետների արդյունավետությունը[2]։ Խոր ուսուցումը համարվում է մեքենայական ուսուցման ալգորիթմական ենթաբաժին, որը ՝ Ոչ գծային շերտերի կազմը կախված է լուծելի խնդիրներից։ Օգտագործվում են և՛ նեյրոնային ցանցի թաքնված շերտերը, և՛ բարդ տրամաբանական փոփոխությունների շերտերը։ Համակարգը կարող է ներառել թաքնված փոփոխականներ, ինչպիսիք են խոր ցանցի հանգույցները[3]։ Չնայած նրան, որ «Խոր ուսուցում» տերմինը մեքենայական ուսուցման մեջ առաջացել է 1986 թվականին Ռինի Դիխտեռի աշխատանքից հետո, առաջին ընդհանուր աշխատանքային ալգորիթմը խոր բազմաշերտ ընկալումներով հրատարակվել է դեռ սովետական գիտնականների՝ Ալեքսեյ Իվախնենկոյի և Վալենտինա Լապի «Կիբերնետիկ կանխագուշկաող սարքեր» (ռուս.՝ «Кибернетические предсказывающие устройства») գրքում։ Խոր ուսուցումը հայտնի է դարձել 2000-ական  թվականներից, երբ ամեն ինչ զարգացել էր՝ համակարգիչները դարձել էին բավականին հզոր, որպեսզի ուսումնասիրեն մեծ նեյրոնային ցանցերը, տվյալների հավաքածուները դարձել էին բավականին ընդարձակ և մեծ ցանցեր ուսումնասիրելը արդեն իմաստ ուներ, իսկ արհեստական նեյրոնային ցանցերի թեորեմում գրանցվել էր հերթական առաջընթացը։ Այս ժամանակ որոշ գիտնականներ ցույց տվեցին, որ կարող են արդյունավետ ուսումնասիրել բազմաշերտ նեյրոնային ցանցը։ Այն հնարավոր էր եթե սկզբում ուսումնասիրեն ամեն շերտը առանձին-առանձին  Բոլցմանի մեքենայի օգնությամբ, իսկ հետո ուսումնասիրեն սխալի հակադարձ տարածման մեթոդի միջոցով[3]։"
  },
  {
    "url": "https://hi.wikipedia.org/wiki/%E0%A4%A1%E0%A5%80%E0%A4%AA_%E0%A4%B2%E0%A4%B0%E0%A5%8D%E0%A4%A8%E0%A4%BF%E0%A4%82%E0%A4%97",
    "title": "डीप लर्निंग - विकिपीडिया",
    "content": "डीप लर्निंग  सीखने के साथ तंत्रिका नेटवर्क पर आधारित मशीन लर्निंग विधियों का उपसमुच्चय है। विशेषण \"डीप\" नेटवर्क में कई परतों के उपयोग को संदर्भित करता है।[1] उपयोग की जाने वाली विधियाँ या तो पर्यवेक्षित, अर्ध-पर्यवेक्षित या अपर्यवेक्षित हो सकती हैं। डीप तंत्रिका नेटवर्क, डीप बिलीफ नेटवर्क, रिकरेंट न्यूरल नेटवर्क, कन्वोल्यूशनल न्यूरल नेटवर्क और ट्रांसफॉर्मर जैसे डीप-लर्निंग आर्किटेक्चर को कंप्यूटर विज़न, स्पीच रिकग्निशन,कन्नड़ भाषाविज्ञान, मशीन अनुवाद, बायोइन्फॉर्मेटिक्स, औषधि डिजाइन, मेडिकल इमेज एनालिसिस, क्लाइमेट साइंस, मैटेरियल इंस्पेक्शन और बोर्ड गेम प्रोग्राम जैसे क्षेत्रों में लागू किया गया है, जहाँ उन्होंने मानव विशेषज्ञ प्रदर्शन के बराबर और भी मामलों में बेहतर परिणाम दिये हैं \nतंत्रिका नेटवर्क के शुरुआती रूप जैविक प्रणालियों, विशेष रूप से मानव मस्तिष्क में सूचना प्रसंस्करण और वितरित संचार नोड्स से प्रेरित थे। हालाँकि, वर्तमान तंत्रिका नेटवर्क जीवों के मस्तिष्क के कार्य[2] को मॉडल करने का इरादा नहीं रखते हैं, और आम तौर पर उस उद्देश्य के लिए कम गुणवत्ता वाले मॉडल के रूप में देखे जाते हैं।\nअवलोकन\nअधिकांश आधुनिक डीप लर्निंग मॉडल बहु-स्तरित तंत्रिका नेटवर्क जैसे कि कन्वोल्यूशनल न्यूरल नेटवर्क और ट्रांसफॉर्मर पर आधारित होते हैं, हालांकि वे डीप जेनेरेटिव मॉडल जैसे कि डीप बिलीफ नेटवर्क और डीप बोल्ट्जमैन मशीनों में नोड्स में परत-वार व्यवस्थित प्रस्ताव सूत्र या अव्यक्त चर भी शामिल कर सकते हैं। मूल रूप से, डीप लर्निंग मशीन लर्निंग एल्गोरिदम के एक वर्ग को संदर्भित करता है जिसमें परतों के पदानुक्रम का उपयोग इनपुट डेटा को थोड़ा अधिक अमूर्त और समग्र प्रतिनिधित्व में बदलने के लिए किया जाता है। उदाहरण के लिए, एक छवि पहचान मॉडल में, कच्चा इनपुट एक छवि हो सकती है (पिक्सल के टेंसर के रूप में दर्शाया गया है)। पहली प्रतिनिधित्व परत रेखाओं और वृत्तों जैसी बुनियादी आकृतियों की पहचान करने का प्रयास कर सकती है, दूसरी परत किनारों की व्यवस्था की रचना और एन्कोड कर सकती है, तीसरी परत नाक और आंखों को एन्कोड कर सकती है, और चौथी परत पहचान सकती है कि छवि में एक चेहरा है।\nमहत्वपूर्ण रूप से, एक गहन शिक्षण प्रक्रिया यह सीख सकती है कि किस स्तर पर कौन सी सुविधाएँ इष्टतम रूप से रखी जानी चाहिए। गहन शिक्षण से पहले, मशीन लर्निंग तकनीकों में अक्सर डेटा को एक वर्गीकरण एल्गोरिथ्म के संचालन के लिए अधिक उपयुक्त प्रतिनिधित्व में बदलने के लिए हाथ से तैयार की गई सुविधा इंजीनियरिंग शामिल होती थी। गहन शिक्षण दृष्टिकोण में, सुविधाएँ हाथ से तैयार नहीं की जाती हैं और मॉडल स्वचालित रूप से डेटा से उपयोगी सुविधा अभ्यावेदन खोजता है। यह हाथ से ट्यूनिंग की आवश्यकता को समाप्त नहीं करता है; उदाहरण के लिए, परतों और परत के आकार की अलग-अलग संख्याएँ अमूर्तता की विभिन्न डिग्री प्रदान कर सकती हैं।\n\"डीप लर्निंग\" में \"डीप\" शब्द परतों की संख्या को संदर्भित करता है जिसके माध्यम से डेटा को रूपांतरित किया जाता है। अधिक सटीक रूप से, डीप लर्निंग सिस्टम में पर्याप्त क्रेडिट असाइनमेंट पथ (CAP) गहराई होती है। CAP इनपुट से आउटपुट तक परिवर्तनों की श्रृंखला है। CAP इनपुट और आउटपुट के बीच संभावित कारण कनेक्शन का वर्णन करते हैं। फीडफॉरवर्ड न्यूरल नेटवर्क के लिए, CAP की गहराई नेटवर्क की गहराई होती है और छिपी हुई परतों की संख्या प्लस एक होती है (क्योंकि आउटपुट परत भी पैरामीटराइज़ की जाती है)। आवर्तक तंत्रिका नेटवर्क के लिए, जिसमें एक संकेत एक परत के माध्यम से एक से अधिक बार प्रसारित हो सकता है, CAP गहराई संभावित रूप से असीमित है। गहराई की कोई सार्वभौमिक रूप से सहमत सीमा उथले सीखने को डीप लर्निंग से अलग नहीं करती है, लेकिन अधिकांश शोधकर्ता इस बात से सहमत हैं कि डीप लर्निंग में 2 से अधिक CAP गहराई शामिल होती है। गहराई 2 के CAP को इस अर्थ में एक सार्वभौमिक सन्निकटनकर्ता के रूप में दिखाया गया है कि यह किसी भी फ़ंक्शन का अनुकरण कर सकता है। इससे आगे, अधिक परतें नेटवर्क की फ़ंक्शन सन्निकटन क्षमता में नहीं जुड़ती हैं।  गहरे मॉडल उथले मॉडल की तुलना में बेहतर विशेषताएं निकालने में सक्षम हैं और इसलिए, अतिरिक्त परतें विशेषताओं को प्रभावी ढंग से सीखने में मदद करती हैं।\nडीप लर्निंग आर्किटेक्चर का निर्माण लालची परत-दर-परत विधि से किया जा सकता है।डीप लर्निंग इन अमूर्तताओं को अलग करने और यह चुनने में मदद करता है कि कौन सी विशेषताएँ प्रदर्शन को बेहतर बनाती हैं। डीप लर्निंग एल्गोरिदम को अनसुपरवाइज्ड लर्निंग कार्यों पर लागू किया जा सकता है। यह एक महत्वपूर्ण लाभ है क्योंकि लेबल रहित डेटा लेबल वाले डेटा की तुलना में अधिक प्रचुर मात्रा में होता है। डीप संरचनाओं के उदाहरण जिन्हें अनसुपरवाइज्ड तरीके से प्रशिक्षित किया जा सकता है, वे डीप बिलीफ नेटवर्क हैं।\nव्याख्याएँ\nगहरे तंत्रिका नेटवर्क की व्याख्या आम तौर पर सार्वभौमिक सन्निकटन प्रमेय के संदर्भ में की जाती है इतिहास\nकृत्रिम तंत्रिका नेटवर्क (एएनएन) के दो प्रकार थे: फीडफॉरवर्ड न्यूरल नेटवर्क (एफएनएन) और आवर्तक तंत्रिका नेटवर्क (आरएनएन)। आरएनएन की कनेक्टिविटी संरचना में चक्र होते हैं, एफएनएन में नहीं। 1920 के दशक में, विल्हेम लेनज़ और अर्न्स्ट इसिंग ने इसिंग मॉडल का निर्माण और विश्लेषण किया जो अनिवार्य रूप से न्यूरॉन जैसे थ्रेशोल्ड तत्वों से युक्त एक गैर-शिक्षण आरएनएन वास्तुकला है। 1972 में, शुनिची अमारी ने इस वास्तुकला को अनुकूली बनाया। उनके शिक्षण आरएनएन को 1982 में जॉन हॉपफील्ड द्वारा लोकप्रिय बनाया गया था।"
  },
  {
    "url": "https://id.wikipedia.org/wiki/Pemelajaran_dalam",
    "title": "Pemelajaran dalam - Wikipedia bahasa Indonesia, ensiklopedia bebas",
    "content": "Pemelajaran dalam (bahasa Inggris: deep learning) atau sering dikenal dengan istilah pemelajaran struktural mendalam (bahasa Inggris: deep structured learning) atau pemelajaran hierarki (bahasa Inggris: hierarchical learning) adalah salah satu cabang dari ilmu pemelajaran mesin (bahasa Inggris: machine learning) yang terdiri algoritme pemodelan abstraksi tingkat tinggi pada data menggunakan sekumpulan fungsi transformasi non-linear yang ditata berlapis-lapis dan mendalam.[1] Teknik dan algoritme dalam pemelajaran dalam dapat digunakan baik untuk kebutuhan pemelajaran terarah (supervised learning), pemelajaran tak terarah (unsupervised learning) dan semi-terarah (semi-supervised learning) dalam berbagai aplikasi seperti pengenalan citra, pengenalan suara, klasifikasi teks, dan sebagainya. Model pada pembelajaran dalam pada dasarnya dibangun berdasarkan jaringan saraf tiruan, yang risetnya sudah berlangsung sejak era 80-an namun baru-baru ini kembali bangkit dengan adanya komputer yang semakin cepat apalagi ditambah dengan kemampuan kartu grafis modern yang mampu melakukan kalkulasi berbasis matriks secara simultan. Berdasarkan riset yang baru-baru ini dilakukan, pemelajaran dalam mampu melakukan pengenalan grafis, pola tulis tangan dan beberapa pola lainnya lebih akurat dibandingkan dengan algoritme pemelajaran mesin lainnya.[2] Deep Feedforward Network atau dikenal dengan Multilayer Perceptron (MLP) merupakan pengembangan dari jaringan saraf tiruan yang menekankan pada penggunakan satu atau lebih lapis tersembunyi (hidden layer) pada jaringannya dan penggunaan fungsi transformasi non-linear sebagai fungsi transformasi. Jaringan ini disebut Feedforward oleh karena sifatnya yang membawa informasi dari lapis masukan (input layer) untuk dibawa dan ditransformasi ke depan hingga lapis luaran (output layer). Recurrent Neural Network (RNN) merupakan pengembangan dari Deep Feedforward Network yang mana informasi dari suatu neuron dapat berputar kembali ke neuron yang sama (Deep Feddforward Network hanya membawa informasi ke lapis A ke lapis B secara progresif tanpa kembali ke lapis sebelumnya). Convolutional Neural Network (CNN) merupakan modifikasi dari Deep Feedforward Network yang mana setiap lapisnya dibuat dalam bentuk topologi grid mendalam."
  },
  {
    "url": "https://it.wikipedia.org/wiki/Apprendimento_profondo",
    "title": "Apprendimento profondo - Wikipedia",
    "content": "L'apprendimento profondo (in inglese deep learning) è quel campo di ricerca dell'apprendimento automatico (in inglese machine learning) e dell'intelligenza artificiale che si basa su diversi livelli di rappresentazione, corrispondenti a gerarchie di caratteristiche di fattori o concetti, dove i concetti di alto livello sono definiti sulla base di quelli di basso. In altre parole, per apprendimento profondo si intende un insieme di tecniche basate su reti neurali artificiali organizzate in diversi strati, dove ogni strato calcola i valori per quello successivo affinché l'informazione venga elaborata in maniera sempre più completa[1]. Tra le architetture di apprendimento profondo si annoverano le reti neurali profonde, la convoluzione di reti neurali profonde, le deep belief network, e reti neurali ricorsive, che sono state applicate nella visione artificiale, nel riconoscimento automatico del discorso, nell'elaborazione del linguaggio naturale, nel riconoscimento audio e nella bioinformatica. \"Apprendimento profondo\" è un'espressione oggi famosa che ridà lustro al concetto di rete neurale. L'apprendimento profondo è definito come una classe di algoritmi di apprendimento automatico che[2]: Ciò che queste definizioni hanno in comune sono i livelli multipli di unità non lineari e l'apprendimento (supervisionato o non supervisionato) in ogni livello della rappresentazione di caratteristiche, in cui i livelli formano una gerarchia delle caratteristiche stesse[2]. La composizione di ciascun livello di unità non lineari usata in un algoritmo di apprendimento profondo dipende dal problema che deve essere risolto. Nell'apprendimento profondo possono venire usati livelli nascosti di una rete neurale artificiale e insiemi di formule proposizionali[4]. I primi studi sulle reti neurali multistrato sono stati prodotti dallo scienziato giapponese Kunihiko Fukushima che, con il modello del cognitrone nel 1975, e del neo-cognitrone[5] poi, ha introdotto l'idea di area di connessione per i neuroni che si è sviluppata nelle reti neurali convoluzionali. Lo studio delle reti neurali artificiali multistrato si è sviluppato già negli anni ottanta, ma solo nell'ultimo decennio si sta dimostrando la loro utilità in un'ampia gamma di settori e applicazioni. Più nel dettaglio, il recente successo dell'apprendimento profondo è dovuto al superamento di alcuni ostacoli che in passato hanno impedito il raggiungimento dei risultati attesi, come la mancanza di dati o di un'adeguata capacità computazionale. Infatti, oggi sono incrementati i dati a disposizione, sono stati sviluppati sistemi di calcolo parallelo basati su GPU e, soprattutto, sono stati ottimizzati i metodi di addestramento delle reti neurali, che oggi possono trovare soluzioni a problemi che in passato hanno impedito ai ricercatori di ottenere risultati soddisfacenti. Oggi i sistemi di apprendimento profondo, fra altre utilità, permettono di identificare oggetti nelle immagini e nei video, trascrivere il parlato in testo, e individuare e interpretare gli interessi degli utenti online, mostrando i risultati più pertinenti per la loro ricerca. Grazie a queste e altre soluzioni, l'apprendimento profondo sta vivendo anni di rapido progresso, arrivando anche, in molti casi, a superare le prestazioni degli esseri umani. Basta pensare all'applicazione degli algoritmi di apprendimento profondo nell’ambito dei problemi decisionali sequenziali, all’interno dell'apprendimento per rinforzo: un caso esemplare è stato lo sviluppo di AlphaGo, un software che nel 2016 ha battuto il campione mondiale di Go, diversi anni in anticipo rispetto alle previsioni degli esperti.[1] La rete neurale convoluzionale (Convolution Neural Network, CNN) è un metodo di scelta per elaborare dati visuali e dati di tipo 2D. Una CNN è composta da uno più strati convoluzionali con strati completamente connessi verso l'alto. Usa anche pesi e strati comuni (pooling layer). In particolare il  \"max-pooling\" è spesso usato nell'architettura convoluzionale di Fukushima. Quest'architettura permette alle CNN di avere dei vantaggi dalle strutture 2D di ingresso. Sono particolarmente efficaci nell'area delle immagini e di riconoscimento del discorso. Possono essere allenate anche con la backpropagation standard. Sono inoltre facili da allenare rispetto ad altre reti neurali profonde o feed-forward ed hanno molti meno parametri da stimare. Un programma di CNN è il DeepDream di Google[6]. Le reti neurali ricorsive (Recurrent Neural Networks, RNN) nascono con il tentativo di rendere le reti neurali Turing complete aggiungendo una componente di memoria. Le reti neurali feed-forward rispondono in modo costante agli stessi input, senza però poter fare collegamenti tra input diversi, come potrebbe essere utile nell'ambito della semantica. Le reti neurali ricorsive invece pesano ogni input in base allo stato precedente e lo stesso input potrebbe dare output diversi a seconda del contesto in cui è inserito. Nonostante questo le RNN, a meno che non vengano inseriti elementi casuali, restano reti deterministiche in quanto la stessa sequenza di input porterà sempre alla stessa sequenza di output. Lo stato di una RNN è implementato aggiungendo a un layer dei neuroni ricorsivi che avranno come valore ad ogni istante di tempo il valore all'istante precedente sommato a una variazione data dagli input dei neuroni feed-forward. Le reti neurali di questo tipo diventano impossibili da allenare per retropropagazione dell'errore, quindi viene utilizzato come stratagemma quello di svolgere le ricorsività considerando il loro funzionamento per ogni singolo istante di tempo[poco chiaro]. Mentre nel caso di percorsi a ritroso molto brevi questo funziona, per i casi in cui bisogna percorrere diversi step a ritroso si incorre nel problema del Vanishing Gradient o dell'Exploding Gradient nel caso la funzione di costo abbia derivata sempre minore di uno (ad esempio sigmoide e tanh) o sempre maggiore di uno. Questo infatti porta a una convergenza a zero esponenziale nel primo caso e a un'esplosione esponenziale del gradiente nel secondo. In nessun caso il gradiente è più computabile anche solo dopo pochi passi. Per risolvere questo problema si fa in modo di utilizzare una funzione rettificatore. Questa è una classe di modelli d'apprendimento profondi usando il Q-learning, una forma di apprendimento per rinforzo, del Google DeepMind. Risultati preliminari sono stati presentati nel 2014 con un articolo pubblicato su Nature nel febbraio 2015. L'applicazione a cui si fa riferimento è un gioco dell'Atari 2600. L'hashing semantico tratta la strutturazione di valori crittografati sulla base di un imprevisto del sistema neurale iniziale e trattiene invece informazioni fondamentali sulla base semantica del vocabolario. Nel 2018, l'azienda britannica DeepMind, il Moorfields Eye Hospital e l'University College di Londra hanno lanciato un software che promette di riconoscere l'anatomia dell'occhio, la patologia e suggerire ai medici un trattamento sanitario adeguato.[7]L'occhio del paziente viene esaminato mediante una tomografia ottica a coerenza di fase che acquisisce immagini a colori in alta definizione, a loro volta elaborate da un algoritmo che le confronta con la base di conoscenza clinica, in modo tale da identificare la diagnosi e la soluzione terapeutica ottimale per il caso specifico. Altri progetti"
  },
  {
    "url": "https://he.wikipedia.org/wiki/%D7%9C%D7%9E%D7%99%D7%93%D7%94_%D7%A2%D7%9E%D7%95%D7%A7%D7%94",
    "title": "למידה עמוקה – ויקיפדיה",
    "content": "למידה עמוקה (באנגלית: Deep Learning ולפעמים Deep Structured Learning) היא אוסף שיטות למידת מכונה מבוססות על רשתות עצביות מלאכותיות. שם התואר \"עמוקה\" בשם מתייחס ללמידה במבנה של שכבות, המאפשר לרשת ללמוד תכונות בהדרגה, תהליך המתבצע על ידי חילוץ דפוסים או תכונות מנתונים גולמיים כדי לשפר את ביצועי המודל. הלמידה יכולה להיות מונחית, מונחית למחצה או בלתי מונחית.[1][2] ארכיטקטורות למידה עמוקה כגון רשתות עצביות עמוקות (deep neural networks), למידת חיזוק עמוקה (deep reinforcement learning), רשתות עצביות חוזרות (recurrent neural networks), ורשתות קונבולוציה יושמו בתחומים מגוונים ביניהם: ראייה ממוחשבת, זיהוי דיבור, עיבוד שפה טבעית, תרגום מכונה, ביואינפורמטיקה, תכנון תרופות, ניתוח תמונות רפואיות, בדיקות חומרים ומשחקי לוח, בהן הניבו תוצאות דומות לאלו שהניבו מומחים אנושיים ובמקרים מסוימים אף עלו עליהן.[3][4] מודל הרשת העצבית המלאכותית נוצר בהשראת עיבוד מידע וצמתי תקשורת מבוזרים המצויים במערכות ביולוגיות. רשתות אלו שונות ממוח ביולוגי, היות שהמבנה של רשתות עצביות מלאכותיות נוטה להיות סטטי, בעוד שהמוח הביולוגי של רוב האורגניזמים החיים הוא דינמי. הבדל נוסף הוא, שמחשבים פועלים בעיקר בעיבוד טורי, או עם כמות קטנה של עיבוד מקבילי[דרוש מקור], ואילו מוחות של יצורים פועלים בעיבוד מקבילי[דרוש מקור]. חוקרי הבינה המלאכותית, מרווין מינסקי וסימור פפרט, הראו שפרספטרון (אלגוריתם הלומד תוך כדי ריצה) יחיד אינו יכול ליצור את השער הלוגי XOR,[5] אולם משפט הקירוב האוניברסלי הראה שרשת בעלת שכבה נסתרת אחת ברוחב לא מוגבל עם פונקציית הפעלה לא ליניארית יכולה להיות מסַוֶג אוניברסלי.[6] למידה עמוקה בדרך כלל עוסקת במספר גדול של שכבות בגודל מוגבל. למידה עמוקה היא מחלקה של אלגוריתמים בתחום למידת המכונה[8] המשתמש במספר שכבות כדי לחלץ בהדרגה תכונות ברמה גבוהה יותר מהקלט הגולמי. לדוגמה, בעיבוד תמונה דיגיטלי, שכבות נמוכות עשויות לזהות קצוות, בעוד שכבות גבוהות יותר עשויות לזהות את המושגים הרלוונטיים לאדם כגון ספַרוֹת, אותיות או פרצופים. רוב המודלים המודרניים של למידה עמוקה מבוססים על רשתות עצביות מלאכותיות, במיוחד רשתות עצביות קונבולוציוניות (CNN), אם כי הם יכולים לכלול גם נוסחאות הצעה (פרופוזיציוניות) או משתנים סמויים המאורגנים בשכבה במודלים מחוּללים עמוקים (generative models) כמו הצמתים ברשת אמונה עמוקה ומכונת בולצמן עמוקה.[9] בלמידה עמוקה, כל שכבה לומדת להפוך את נתוני הקלט שלה לייצוג מופשט ומרוכב יותר. ביישום של זיהוי תמונה, הקלט הגולמי עשוי להיות מטריצה של פיקסלים; שכבת הייצוג הראשונה עשויה להפשיט את הפיקסלים ולקודד את הקצוות, השכבה השנייה עשויה להרכיב ולקודד סידורים של קצוות; השכבה השלישית עשויה לקודד אף ועיניים; והשכבה הרביעית עשויה לזהות שהתמונה מכילה פנים. נקודה חשובה היא שתהליך למידה עמוק יכול ללמוד אילו תכונות למקם בצורה אופטימלית באיזו רמה בעצמו. אמנם, זה לא בהכרח מבטל את הצורך בהנחיה ידנית. לדוגמה, מספר משתנה של שכבות וגודלי שכבות יכולים לספק רמות שונות של הפשטה.[10][11] המילה \"עמוקה\" ב\"למידה עמוקה\" מתייחסת למספר השכבות שבאמצעותן הנתונים עוברים עיבוד או שינוי. ליתר דיוק, למערכות למידה עמוקה יש עומק CAP (או credit assignment path) משמעותי. ה-CAP הוא שרשרת הטרנספורמציות מקלט לפלט. CAPs מתארים קשרים סיבתיים פוטנציאליים בין קלט ופלט. עבור רשת זרימה קדימה, העומק של ה-CAPs הוא זה של הרשת והוא מספר השכבות הנסתרות פלוס אחת (כיוון שגם שכבת הפלט מותאמת לפרמטרים). עבור רשתות עצביות חוזרות, שבהן אות עשוי להתפשט בשכבה יותר מפעם אחת, עומק ה-CAP עשוי להיות בלתי מוגבל. אין סף מוסכם אוניברסלי של עומק שמבדיל בין למידה \"רדודה\" ללמידה עמוקה, אבל רוב החוקרים מסכימים שלמידה עמוקה כוללת עומק CAP גבוה מ-2. CAP של עומק 2 הוכח כקירוב אוניברסלי במובן זה שהוא יכול לחקות כל פונקציה.[12] יתרה מכך, שכבות נוספות אינן מוסיפות ליכולת קירוב הפונקציות של הרשת. מודלים עמוקים (CAP > 2) מסוגלים לחלץ תכונות טובות יותר ממודלים רדודים ומכאן, שכבות נוספות עוזרות ללמוד את התכונות בצורה יעילה. ניתן לבנות ארכיטקטורות למידה עמוקה בשיטה חמדנית שכבה אחר שכבה. למידה עמוקה עוזרת להפריד את ההפשטות הללו ולבחור אילו תכונות משפרות את הביצועים.[10][11] עבור משימות למידה מפוקחות, שיטות למידה עמוקה מבטלות את הנדסת המאפיינים, על ידי תרגום הנתונים לייצוגי ביניים קומפקטיים הדומים לגורמים הראשיים, ומפיקות מבנים שכבתיים המסירים יתירות בייצוג. ניתן ליישם אלגוריתמים של למידה עמוקה על משימות למידה-ללא-פיקוח. זהו יתרון חשוב מכיוון שהנתונים שאינם מתויגים נמצאים בשפע רב יותר מהנתונים המסומנים. דוגמאות למבנים עמוקים שניתן לאמן באופן לא מפוקח הם מדחסי היסטוריה עצבית[13] ורשתות אמונה עמוקות.[10][11] רשתות עצביות עמוקות מתפרשות בדרך כלל במונחים של משפט הקירוב האוניברסלי[19] או היסק הסתברותי או בייסיאני. משפט הקירוב האוניברסלי הקלאסי נוגע ליכולתן של רשתות זרימה קדימה עם שכבה נסתרת יחידה בגודל סופי לקירוב פונקציות רציפות. בשנת 1989 פורסמה ההוכחה הראשונה על ידי ג'ורג' סיבנקו עבור פונקציות הפעלה סיגמואידיות והוכללה לארכיטקטורות זרימה קדימה רב-שכבתיות.[20] משפט הקירוב האוניברסלי עבור רשתות עצביות עמוקות נוגע לקיבולת של רשתות עם רוחב מוגבל אך עם עומק שעשוי לגדול. לוּ הוכיח שאם הרוחב של רשת עצבית עמוקה עם הפעלת ReLU גדול בהחלט מממד הקלט, אזי הרשת יכולה להעריך כל פונקציה הניתנת לשילוב של אינטגרל לבג, אם הרוחב קטן או שווה לממד הקלט, אז רשת עצבים עמוקה אינה קירוב אוניברסלי. הפרשנות ההסתברותית נובעת מתחום למידת המכונה. הוא כולל מסקנות, וכן את מושגי האופטימיזצה של אימון ובדיקה, הקשורים להתאוה והכללה, בהתאמה. ליתר דיוק, הפרשנות ההסתברותית מחשיבה את אי-ליניאריות ההפעלה כפונקציית התפלגות מצטר. הפרשנות ההסתברותית הובילה להכנסת הנשירה (dropout) כמסדרת (regulazier) ברשתות עצביות. הפרשנות ההסתברותית הוצגה על ידי חוקרים כולל הופפילד.[21] בשנת 2012, צוות בראשות ג'ורג' א'דאהל זכה ב\"אתגר הפעילות המולקולרית של מרק\" באמצעות רשתות עצביות עמוקות רב-משימתיות כדי לחזות את היעד הביו-מולקולרי של תרופה אחת.[22][23] בשנת 2014, הקבוצה של הוכריטר השתמשה בלמידה עמוקה כדי לזהות השפעות מחוץ למטרה ורעילויות של כימיקלים סביבתיים ברכיבים תזונתיים, במוצרים ביתיים ובתרופות וזכתה ב\"אתגר הנתונים של Tox21\" של המכונים NIH, FDA ו-NCATS.[24][25][26] השפעות נוספות משמעותיות בזיהוי תמונה או אובייקט הורגשו מ-2011 עד 2012. אף על פי שרשתות CNN שאומנו על-ידי back propagation היו בנמצא כבר עשרות שנים, והטמעות GPU של רשתות נוירונים היו בנמצא במשך שנים, כולל CNNs, היה צורך בהטמעות מהירות משמעותית של CNNs על GPUs כדי להתקדם בראייה ממוחשבת.[27][28][29][30] בשנת 2011, גישה זו השיגה לראשונה ביצועים על אנושיים בתחרות זיהוי דפוסים חזותיים. כמו כן, ב-2011 היא זכתה בתחרות כתב היד הסיני של ICDAR, ובמאי 2012 היא זכתה בתחרות פילוח התמונות של ISBI.[31] עד 2011, רשתות CNN לא מילאו תפקיד מרכזי בכנסים של ראייה ממוחשבת, אבל ביוני 2012, מאמר של סיקסאן בכנס המוביל CVPR[32] הראה כיצד איגום מקסימלי של CNN ב-GPU יכול לשפר באופן דרמטי רשומות בנצ'מרק רבים של ראייה. באוקטובר 2012, מערכת דומה מאת קריזבסקי[33] זכתה בתחרות ImageNet בקנה מידה גדול בהפרש משמעותי על פני שיטות למידת מכונה רדודות. בנובמבר 2012, המערכת של Ciresan זכתה גם בתחרות ה-ICPR בנושא ניתוח תמונות רפואיות גדולות לגילוי סרטן, ובשנה שלאחר מכן גם ב-MICCAI Grand Challenge באותו נושא.[34] בשנים 2013 ו-2014, שיעור השגיאות במשימת ImageNet באמצעות למידה עמוקה הופחת עוד יותר, בעקבות מגמה דומה בזיהוי דיבור בקנה מידה גדול. לאחר מכן הורחב סיווג התמונות למשימה המאתגרת יותר של יצירת תיאורים (כתוביות) לתמונות, לעיתים קרובות כשילוב של CNNs ו-LSTMs.[35] כמה חוקרים קובעים שהניצחון ב-ImageNet באוקטובר 2012 עיגן את תחילתה של \"מהפכת הלמידה העמוקה\" ששינתה את תעשיית הבינה המלאכותית.[36] במרץ 2019, יהושע בנג'יו, ג'פרי הינטון ויאן לקון זכו בפרס טיורינג על פריצות דרך מושגיות והנדסיות שהפכו רשתות עצביות עמוקות למרכיב קריטי במחשוב. רשתות עצביות מלאכותיות (ANNs) או מערכות מקושרות הן מערכות מחשוב בהשראת הרשתות העצביות הביולוגיות המהוות מוח חי. מערכות כאלה לומדות (משפרות בהדרגה את יכולתן) לבצע משימות על ידי בחינת דוגמאות, בדרך כלל ללא תכנות ספציפי למשימה. לדוגמה, בזיהוי תמונות, הם עשויים ללמוד לזהות תמונות המכילות חתולים על ידי ניתוח תמונות לדוגמה שסומנו באופן ידני כ\"חתול\" או \"ללא חתול\" ושימוש בתוצאות האנליטיות כדי לזהות חתולים בתמונות אחרות. הם מצאו את רוב השימוש ביישומים שקשה לבטא עם אלגוריתם מחשב מסורתי באמצעות תכנות לוגי \"מסורתי\". ANN מבוסס על אוסף של יחידות מחוברות הנקראות נוירונים מלאכותיים, (בדומה לנוירונים ביולוגיים במוח). כל חיבור (סינפסה) בין נוירונים יכול להעביר אות לנוירון אחר. הנוירון הקולט (הפוסט-סינפטי) יכול לעבד את האות ואז לאותת לנוירונים במורד הזרם המחוברים אליו. לנוירונים יכול להיות מצב, המיוצג בדרך כלל על ידי מספרים ממשיים, בדרך כלל בין 0 ל-1. לנוירונים ולסינפסות עשוי להיות גם משקל שמשתנה ככל שהלמידה מתקדמת, מה שיכול להגביר או להקטין את עוצמת האות שהוא שולח במורד הזרם. בדרך כלל, נוירונים מאורגנים בשכבות. שכבות שונות עשויות לבצע סוגים שונים של טרנספורמציות על הקלט שלהן. האותות עוברים מהשכבה הראשונה (הקלט עצמו), לשכבה האחרונה (הפלט), לפעמים לאחר חציית השכבות מספר פעמים. המטרה המקורית של גישת הרשת העצבית הייתה לפתור בעיות באותו אופן שבו יעשה מוח אנושי. עם הזמן, תשומת הלב התמקדה בהתאמת יכולות מנטליות ספציפיות, מה שהוביל לסטיות מביולוגיה כגון התפשטות לאחור, או העברת מידע בכיוון ההפוך והתאמת הרשת לשקף מידע זה. רשתות עצביות שימשו במגוון משימות, כולל ראייה ממוחשבת, זיהוי דיבור, תרגום מכונה, סינון רשתות חברתיות, משחקי לוח ווידאו ואבחון רפואי. נכון לשנת 2017, לרשתות עצביות יש בדרך כלל כמה אלפים עד כמה מיליוני יחידות ומיליוני חיבורים. אף על פי שמספר זה הוא בכמה סדרי גודל פחות ממספר הנוירונים במוח אנושי, רשתות אלו יכולות לבצע משימות רבות ברמה גבוהה מעבר לזו של בני אדם (למשל, זיהוי פנים, משחק \"Go\"[37]). רשת עצבית עמוקה (DNN) היא רשת עצבית מלאכותית (ANN) עם שכבות מרובות בין שכבות הקלט והפלט.[9] ישנם סוגים שונים של רשתות עצביות אך הן תמיד מורכבות מאותם מרכיבים: נוירונים, סינפסות, משקלים, הטיות ותפקודים.[38] רכיבים אלו פועלים באופן יחסית דומה למוח האנושי וניתן לאמן אותם כמו כל אלגוריתם ML אחר. לדוגמה, DNN שמאומן לזהות גזעי כלבים יעבור על התמונה הנתונה ויחשב את ההסתברות שהכלב בתמונה הוא גזע מסוים. המשתמש יכול לעיין בתוצאות ולבחור אילו הסתברויות הרשת צריכה להציג (מעל סף מסוים וכו') ולהחזיר את התווית המוצעת. כל מניפולציה מתמטית כשלעצמה נחשבת לשכבה, ול-DNN מורכב יש רבדים רבים, ומכאן השם רשתות \"עמוקות\". DNNs יכולים למדל קשרים לא ליניאריים מורכבים. ארכיטקטורות DNN מייצרות מודלים מורכבים שבהם האובייקט מתבטא כקומפוזיציה מרובדת של פרימיטיבים.[39] השכבות הנוספות מאפשרות הרכבה של תכונות משכבות נמוכות יותר, ועשויות ליצור מודלים של נתונים מורכבים עם פחות יחידות מאשר רשת רדודה בעלת ביצועים דומים.[9] לדוגמה, הוכח כי קל יותר באופן אקספוננציאלי, לבצע קירוב לפולינומים דלילים רבי-משתנים עם DNNs מאשר עם רשתות רדודות. ארכיטקטורות עמוקות כוללות גרסאות רבות של כמה גישות בסיסיות. כל ארכיטקטורה מצאה הצלחה בתחומים ספציפיים. לא תמיד ניתן להשוות את הביצועים של ארכיטקטורות מרובות, אלא אם כן הם הוערכו על אותם מערכי נתונים. DNNs הם בדרך כלל רשתות הזנה קדימה שבהן נתונים זורמים משכבת הקלט לשכבת הפלט מבלי לחזור אחורה. בתחילה, ה-DNN יוצר מפה של נוירונים וירטואליים ומקצה ערכים מספריים אקראיים, או \"משקלות\", לקשרים ביניהם. המשקולות והכניסות מוכפלות ומחזירות פלט בין 0 ל-1. אם הרשת לא זיהתה במדויק דפוס מסוים, אלגוריתם יתאים את המשקולות.[40] כך האלגוריתם יכול להפוך פרמטרים מסוימים למשפיעים יותר, עד שהוא יקבע את המניפולציה המתמטית הנכונה לעיבוד מלא של הנתונים. רשתות עצביות חוזרות (RNNs), שבהן נתונים יכולים לזרום לכל כיוון, משמשות ליישומים כמו מודלים של שפות.[41][42][43] זיכרון גדול לטווח-קצר יעיל במיוחד לשימוש זה.[44][45] רשתות קונבולוציה (CNN) משמשות בראייה ממוחשבת.[46] CNNs יושמו גם למידול אקוסטי לזיהוי דיבור אוטומטי (ASR).[47] כמו ב-ANN, בעיות רבות יכולות להתעורר עם DNNs עם הכשרה \"נאיבית\". שתי בעיות נפוצות הן התאמת יתר וזמן חישוב. DNNs נוטים להתאים יתר על המידה משום שכבות ההפשטה הנוספות, המאפשרות להם למדל תלויות נדירות בנתוני האימון. הסדרת (רגולריזציה) שיטות כגון \"הגיזום היחיד\" של איבננקו[48] או דעיכת משקל (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n רגוליזציה) או דלילות ( \n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n - רגוליזציה) ניתן ליישם במהלך האימון כדי לטפל בחלק משמעותי מהתאמת היתר.[49] לחלופין, הסדרת נשירה משמיטה באופן אקראי יחידות מהשכבות הנסתרות במהלך האימון. זה עוזר לשלול תלות נדירה.[50] לבסוף, ניתן להגדיל את הנתונים באמצעות שיטות כגון חיתוך וסיבוב, כך שניתן להגדיל מערכי אימונים קטנים יותר כדי להקטין את הסיכוי להתאמת יתר.[51] DNNs חייבים לקחת בחשבון פרמטרים רבים של אימון, כגון הגודל (מספר השכבות ומספר היחידות לשכבה), קצב הלמידה והמשקלים הראשוניים. סריקה של מרחב הפרמטרים בחיפוש עבור פרמטרים אופטימליים ייתכן שלא יהיה אפשרי בשל עלות הזמן ומשאבי החישוב. טריקים שונים, כגון batching (חישוב גרדיאנט על מספר דוגמאות אימון בו-זמנית ולא דוגמאות בודדות)[52] מאיצים את החישוב. יכולות עיבוד גדולות של ארכיטקטורות ליבות רבות (כגון GPUs או Intel Xeon Phi) הביאו להאצות משמעותיות באימון, בגלל ההתאמה של ארכיטקטורות עיבוד כאלה למטריצה ולחישובי וקטור.[53][54] לחלופין, מהנדסים עשויים לחפש סוגים אחרים של רשתות עצביות עם אלגוריתמי אימון פשוטים ומתכנסים יותר. CMAC (בקר דגם articulation cerebellar) הוא סוג כזה של רשת עצבית. זה לא דורש שיעורי למידה או משקלים ראשוניים אקראיים עבור CMAC. ניתן להבטיח שתהליך האימון יתכנס בשלב אחד עם אצווה חדשה של נתונים, והמורכבות החישובית של אלגוריתם האימון היא ליניארית ביחס למספר הנוירונים המעורבים.[55] מאז שנות ה-2010, התקדמות הן באלגוריתמי למידת מכונה והן בחומרת המחשבים הובילה לשיטות יעילות יותר לאימון רשתות עצביות עמוקות, המכילות שכבות רבות של יחידות נסתרות לא ליניאריות ושכבת פלט גדולה מאוד באופן יחסי.[56] עד שנת 2019, יחידות עיבוד גרפי (GPUs), לעיתים קרובות עם שיפורים ספציפיים ל-AI, החליפו את המעבדים כשיטה הדומיננטית לאימון AI ענן מסחרי בקנה מידה גדול.[57] בשנת 2015 החלה גוגל להשתמש ביחידות (TPU) שפיתחה להאצת למידת מכונה של רשתות עצביות עמוקות, באמצעות תוכנת TensorFlow.[58] OpenAI העריכה את חישוב החומרה המשמש בפרויקטים הגדולים ביותר של למידה עמוקה מ-AlexNet (2012) ועד AlphaZero (2017), ומצאה עלייה של פי 300,000 בכמות החישוב הנדרשת, עם קו מגמה של זמן הכפל של 3.4 חודשים.[59][60]"
  },
  {
    "url": "https://hu.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning – Wikipédia",
    "content": "Ez a lap egy ellenőrzött változata Pontosságellenőrzött A Deep learning, magyarul: mélytanulás, a gépi tanulás egy olyan részhalmaza, amely a neurális hálózatok felhasználására összpontosít olyan feladatok elvégzésére, mint az osztályozás, a regresszió és a reprezentációs tanulás. A terület a biológiai idegtudományból merít ihletet, és középpontjában a mesterséges neuronok rétegekbe való halmozása és „betanítása” áll, hogy adatokat dolgozzanak fel. A „mély” jelző arra utal, hogy a hálózatban több (háromtól több százig vagy több ezerig terjedő) réteget használnak. Az alkalmazott módszerek lehetnek felügyelt, félig felügyelt vagy nem felügyelt módszerek.[1] Néhány gyakori mély tanulási hálózati architektúra a teljesen összekapcsolt hálózatok, a mély hit-hálózatok, a rekurrens neurális hálózatok, a konvolúciós neurális hálózatok, a generatív adverzális hálózatok, a transzformátorok és a neurális sugárzási mezők. Ezeket az architektúrákat olyan területeken alkalmazták, mint a számítógépes látás, beszédfelismerés, természetes nyelvfeldolgozás, gépi fordítás, bioinformatika, gyógyszertervezés, orvosi képelemzés, klímatudomány, anyagvizsgálat és társasjátékprogramok, ahol az emberi szakértői teljesítményhez hasonló, sőt egyes esetekben azt felülmúló eredményeket produkáltak.[2][3][4] A neurális hálózatok korai formáit a biológiai rendszerek, különösen az emberi agy információfeldolgozó és elosztott kommunikációs csomópontjai ihlették. A jelenlegi neurális hálózatoknak azonban nem célja az élőlények agyműködésének modellezése, és e célból általában alacsony minőségű modelleknek tekintik őket.[5] A legtöbb modern mélytanulási modell többrétegű neurális hálózatokon, például konvolúciós neurális hálózatokon és transzformátorokon alapul, bár tartalmazhatnak kijelentő formulákat vagy rétegenként szervezett látens változókat is a mély generatív modellekben, mint például a mély hiedelemhálózatok és a mély Boltzmann-gépek csomópontjai.[6] Alapvetően a mélytanulás a gépi tanulási algoritmusok olyan osztályára utal, amelyben a bemeneti adatok fokozatosan absztraktabb és összetettebb reprezentációvá történő átalakítására réteghierarchiát használnak. Például egy képfelismerő modellben a nyers bemenet lehet egy kép (pixelek tenzoraként reprezentálva). Az első reprezentációs réteg megpróbálhatja azonosítani az alapvető alakzatokat, például vonalakat és köröket, a második réteg összeállíthatja és kódolhatja az élek elrendezését, a harmadik réteg kódolhatja az orrot és a szemeket, a negyedik réteg pedig felismerheti, hogy a kép egy arcot tartalmaz. Fontos, hogy egy mély tanulási folyamat képes önállóan megtanulni, hogy mely jellemzőket melyik szinten kell optimálisan elhelyezni. A mélytanulást megelőzően a gépi tanulási technikák gyakran kézzel készített jellemzőmérnökséget igényeltek, hogy az adatokat az osztályozó algoritmus számára megfelelőbb reprezentációvá alakítsák át. A mélytanulási megközelítésben a jellemzőket nem kézzel alakítják ki, és a modell automatikusan felfedezi a hasznos jellemzőreprezentációkat az adatokból. Ez nem szünteti meg a kézi hangolás szükségességét; például a rétegek változó száma és a rétegméretek különböző absztrakciós fokokat biztosíthatnak.[1][7] A „mély” szó a „deep learning”-ben a rétegek számára utal, amelyeken keresztül az adatokat átalakítják. Pontosabban a mélytanuló rendszerek jelentős kredit hozzárendelési útvonal (CAP) mélységgel rendelkeznek. A CAP a bemenetről a kimenetre történő transzformációk láncolata. A CAP-ok a bemenet és a kimenet közötti potenciálisan oksági kapcsolatokat írják le. Egy előrecsatolt neurális hálózat esetében a CAP-ok mélysége a hálózat mélysége, és a rejtett rétegek száma plusz egy (mivel a kimeneti réteg is paraméterezett). A rekurrens neurális hálózatok esetében, amelyekben egy jel többször is áthaladhat egy rétegen, a CAP mélysége potenciálisan korlátlan.[8] Nincs általánosan elfogadott mélységi küszöbérték, amely elválasztja a sekély tanulást a mély tanulástól, de a legtöbb kutató egyetért abban, hogy a mély tanulás kettőnél nagyobb CAP-mélységgel jár. A kettes mélységű CAP bizonyítottan univerzális approximátor abban az értelemben, hogy bármilyen függvényt képes utánozni.[9] Ezen túlmenően a több réteg nem növeli a hálózat függvényközelítő képességét. A mély modellek (CAP > kettő) jobb jellemzők kinyerésére képesek, mint a sekély modellek, ezért az extra rétegek segítenek a jellemzők hatékony megtanulásában. A mélytanulási architektúrák mohó rétegenkénti módszerrel építhetők fel.[10] A mélytanulás segít ezen absztrakciók szétválasztásában és annak kiválasztásában, hogy mely jellemzők javítják a teljesítményt.[7] A mélytanulási algoritmusok alkalmazhatók felügyelet nélküli tanulási feladatokra. Ez azért fontos előny, mert a címkézetlen adatok nagyobb mennyiségben állnak rendelkezésre, mint a címkézett adatok. A felügyelet nélküli módon képezhető mély struktúrák példái a mély hiedelemhálózatok.[7][11] A mélytanulás kifejezést Rina Dechter vezette be a gépi tanulás közösségébe 1986-ban,[12] a mesterséges neurális hálózatokba pedig Igor Aizenberg és munkatársai 2000-ben, a Boolean-küszöbneuronokkal összefüggésben,[13][14] bár megjelenésének története ennél nyilvánvalóan bonyolultabb.[15] A mély neurális hálózatokat általában az univerzális közelítési tétel[16][17][18][19][20] vagy a valószínűségi következtetés szempontjából értelmezik.[7][8][21][22][23] A klasszikus univerzális approximációs tétel arra vonatkozik, hogy az egyetlen véges méretű rejtett réteggel rendelkező előrecsatolt neurális hálózatok képesek-e folytonos függvények közelítésére.[16][17][18][19] Az első bizonyítást 1989-ben George Cybenko publikálta szigmoid aktiválási függvényekre,[16] majd 1991-ben Kurt Hornik általánosította többrétegű feed-forward architektúrákra.[17] A legújabb munkák azt is kimutatták, hogy az univerzális approximáció nem korlátos aktiválási függvényekre is érvényes, mint például Kunihiko Fukushima egyenesített lineáris egységére.[24][25] A mély neurális hálózatokra vonatkozó univerzális közelítési tétel a korlátos szélességű, de a mélység növekedését megengedő hálózatok kapacitására vonatkozik. Lu és társai bebizonyították, hogy ha egy ReLU aktiválású mély neurális hálózat szélessége szigorúan nagyobb, mint a bemeneti dimenzió, akkor a hálózat bármely Lebesgue-féle integrálható függvényt képes közelíteni; ha a szélesség kisebb vagy egyenlő a bemeneti dimenzióval, akkor a mély neurális hálózat nem univerzális közelítő. A valószínűségi értelmezés[23] a gépi tanulás területéről származik. Jellemzői a következtetés,[6][7][8][11][22][23] valamint a képzés és a tesztelés optimalizálási fogalmai, amelyek az illesztéshez, illetve az általánosításhoz kapcsolódnak. Pontosabban, a valószínűségi értelmezés az aktiválási nemlinearitást kumulatív eloszlásfüggvénynek tekinti.[23] A valószínűségi értelmezés vezetett a kiesés mint regularizátor bevezetéséhez a neurális hálózatokban. A valószínűségi értelmezést többek között Hopfield, Widrow és Narendra kutatók vezették be, és olyan felmérésekben népszerűsítették, mint például Bishop.[26]"
  },
  {
    "url": "https://ml.wikipedia.org/wiki/%E0%B4%A1%E0%B5%80%E0%B4%AA%E0%B5%8D_%E0%B4%B2%E0%B5%87%E0%B4%A3%E0%B4%BF%E0%B4%82%E0%B4%97%E0%B5%8D",
    "title": "ഡീപ് ലേണിംഗ് - വിക്കിപീഡിയ",
    "content": "കൃത്രിമ ന്യൂറൽ നെറ്റ്‌വർക്കുകളെ അടിസ്ഥാനമാക്കിയുള്ള യന്ത്ര പഠന രീതികളുടെ വിശാലമായ കുടുംബത്തിന്റെ ഭാഗമാണ് ഡീപ് ലേണിംഗ്(Deep learning) (ഡീപ് സ്ട്രക്ചേർഡ് ലേണിംഗ് അല്ലെങ്കിൽ ഹൈറാർക്കിക്കൽ ലേണിംഗ് എന്നും അറിയപ്പെടുന്നു). പഠനത്തിന് മേൽനോട്ടം വഹിക്കാം, അർദ്ധ മേൽനോട്ടം അല്ലെങ്കിൽ മേൽനോട്ടമില്ലാതെ ഇരിക്കുകയോ ചെയ്യാം.[1] കമ്പ്യൂട്ടർ ദർശനം, സംഭാഷണങ്ങളെ തിരിച്ചറിയുക, സ്വാഭാവിക ഭാഷയുടെ വിവിധ ഘട്ടങ്ങൾ, ശബ്ദങ്ങൾ തിരിച്ചറിയൽ, സമൂഹ മാധ്യമ ഫിൽട്ടറിംഗ്, യന്ത്ര വിവർത്തനം, ബയോ ഇൻഫോർമാറ്റിക്‌സ്, മരുന്ന് രൂപകൽപ്പന എന്നിവയുൾപ്പെടെയുള്ള മേഖലകളിൽ ആഴത്തിലുള്ള ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ, ആഴത്തിലുള്ള ബിലീഫ് ശൃംഖലകൾ, ആവർത്തിച്ചുള്ള ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ, കൺവൻഷണൽ ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ എന്നിവയിലെല്ലാം ഡീപ് ലേണിംഗ് ആർക്കിടെക്ചർ പ്രയോഗിച്ചു. മെഡിക്കൽ ഇമേജ് വിശകലനം, മെറ്റീരിയൽ പരിശോധന, ബോർഡ് ഗെയിം പ്രോഗ്രാമുകൾ എന്നിവയുൾപ്പെടെ അവ മനുഷ്യ വിദഗ്ദ്ധരെ അപേക്ഷിച്ച് താരതമ്യപ്പെടുത്താവുന്നതും ചില സന്ദർഭങ്ങളിൽ മികച്ചതുമായ ഫലങ്ങൾ നൽകി.[2][3][4] കൃത്രിമ ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ (ANNs) വിവര സംസ്കരണത്തിലൂടെ പ്രചോദനം ഉൾക്കൊണ്ട് ബയോളജിക്കൽ സിസ്റ്റങ്ങളിൽ ആശയവിനിമയ നോഡുകൾ വിതരണം ചെയ്തു. ബയോളജിക്കൽ തലച്ചോറുകളിൽ നിന്ന് ANNs ന് വ്യത്യാസങ്ങളുണ്ട്. പ്രത്യേകിച്ചും, ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ സ്ഥിരവും പ്രതീകാത്മകവുമാണ്, അതേസമയം മിക്ക ജീവജാലങ്ങളുടെയും ജൈവ മസ്തിഷ്കം ചലനാത്മകവും (പ്ലാസ്റ്റിക്) അനലോഗുമാണ്.[5][6][7] റോ ഇൻ‌പുട്ടിൽ‌ നിന്നും ഉയർന്ന ലെവൽ‌ സവിശേഷതകൾ‌ ക്രമേണ എക്‌സ്‌ട്രാക്റ്റു  ചെയ്യുന്നതിന് ഒന്നിലധികം ലെയറുകൾ‌ ഉപയോഗിക്കുന്ന മെഷീൻ‌ ലേണിംഗ് അൽ‌ഗോരിതംസിന്റെ ഒരു ക്ലാസാണ് ഡീപ് ലേണിംഗ്.[8] ഉദാഹരണത്തിന്, ഇമേജ് പ്രോസസ്സിംഗിൽ, താഴത്തെ പാളികൾ അരികുകൾ തിരിച്ചറിഞ്ഞേക്കാം, അതേസമയം ഉയർന്ന പാളി അക്കങ്ങൾ / അക്ഷരങ്ങൾ അല്ലെങ്കിൽ മുഖങ്ങൾ പോലുള്ള അർത്ഥവത്തായ ഇനങ്ങൾ തിരിച്ചറിയുന്നു. മിക്ക ആധുനിക ഡീപ് ലേണിംഗ് മോഡലുകളും കൃത്രിമ ന്യൂറൽ നെറ്റ്‌വർക്കുകളെ അടിസ്ഥാനമാക്കിയുള്ളതാണ്, പ്രത്യേകിച്ചും, കൺവൊല്യൂഷണൽ ന്യൂറൽ നെറ്റ്‌വർക്കുകൾ (സിഎൻഎൻ), എന്നിരുന്നാലും അവയ്ക്ക് പ്രൊപ്പോസിഷണൽ ഫോർമുലകളോ ആഴത്തിലുള്ള ജനറേറ്റീവ് മോഡലുകളിൽ ലെയർ തിരിച്ചുള്ള ക്രമീകരിച്ച ലേറ്റന്റ് വേരിയബിളുകളോ ഉൾപ്പെടുത്താം. ഡീപ് ബിലീഫ് നെറ്റ്‌വർക്കുകളിലെ നോഡുകൾ, ഡീപ് ബോൾട്ട്സ്‌മാൻ മെഷീനുകൾ എന്നിവ പോലുള്ളവ.[9]"
  },
  {
    "url": "https://ms.wikipedia.org/wiki/Pembelajaran_dalam",
    "title": "Pembelajaran dalam - Wikipedia Bahasa Melayu, ensiklopedia bebas",
    "content": "Pembelajaran dalam (bahasa Inggeris: Deep Learning) atau sering dikenal dengan istilah Pemelajaran Struktural Mendalam (bahasa Inggeris: Deep Structured Learning) atau Pemelajaran Hierarki (bahasa Inggeris: Hierarchical learning) adalah salah satu cabang dari ilmu pemelajaran mesin (bahasa Inggeris: Machine Learning) yang terdiri algoritme pemodelan abstraksi tingkat tinggi pada data menggunakan sekumpulan fungsi transformasi non-linear yang ditata berlapis-lapis dan mendalam.[1] Teknik dan algoritme dalam pemelajaran dalam dapat digunakan baik untuk kebutuhan pemelajaran terarah (supervised learning), pembelajaran tidak diarah (unsupervised learning) dan semi-terarah (semi-supervised learning) dalam berbagai aplikasi seperti pengenalan citra, pengenalan suara, klasifikasi teks, dan sebagainya. Model pada pembelajaran dalam pada dasarnya dibangun berdasarkan Jaringan saraf tiruan, yang risetnya sudah berlangsung sejak era 80-an namun baru-baru ini kembali bangkit dengan adanya komputer yang semakin cepat apalagi ditambah dengan kemampuan KAD grafiK moden yang mampu melakukan perkiraan berasaskan matriks secara serentak. Berdasarkan kajian yang telah dilakukan baru-baru ini, pemelajaran Dalam mampu melakukan pengenalan grafis, pola tulis tangan dan beberapa pola lainnya lebih akurat dibandingkan dengan algoritme pemelajaran mesin lainnya.[2] Pembelajaran sebegini dilaksanakan melalui beberapa pendekatan tertentu menggunaan binaan atau struktur penyaluran maklumat yang tersendiri Deep Feedforward Network atau dikenal dengan Multilayer Perceptron (MLP) merupakan pengembangan dari Jaringan saraf tiruan yang menekankan pada penggunakan satu atau lebih lapis tersembunyi (hidden layer) pada jaringannya dan penggunaan fungsi transformasi non-linear sebagai fungsi transformasi. Jaringan ini disebut Feedforward oleh karena sifatnya yang membawa informasi dari lapis masukan (input layer) untuk dibawa dan ditransformasi ke depan hingga lapis luaran (output layer). Recurrent Neural Network merupakan pengembangan dari Deep Feedforward Network yang mana informasi dari suatu neuron dapat berputar kembali ke neuron yang sama (Deep Feddforward Network hanya membawa informasi ke lapis A ke lapis B secara progresif tanpa kembali ke lapis sebelumnya). Convolutional Neural Network merupakan modifikasi dari Deep Feedforward Network yang mana setiap lapisnya dibuat dalam bentuk topologi grid mendalam."
  },
  {
    "url": "https://mn.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning — Википедиа нэвтэрхий толь",
    "content": "Гүнзгий сургалт (гүний сургалт; Англи: Deep learning) — Машин сургалтын (удирдлагатай, хагас удирдлагатай, удирдлагагүй, бэхэлгээт) аргуудын нэг бүлэг бөгөөд өгсөн датаны онцлог(Англи: feature/representation learning) дээр суурилсан сургалт хийдэг. Ямар нэг тодорхой проблемд зориулсан алгоритм ашигладаггүйгээрээ онцлог. Гүний сургалтын ихэнхи арга нь 1980-аад оны үед бий болсон ба тухайн үед гарсан үр дүн тийм ч хангалттай сайн байгаагүй юм. 2000-аад оны дундуур компьютерийн хүчин чадал тэр дундаа график картын чадал сайжирснаар их хэмжээний нейрон сүлжээг загварчлах боловсруулах боломжтой болсноор машин хөрвүүлэлт, зураг дүрс, дуу таних мөн автоматаар тоглоом тоглох технологид үсрэнгүй өөрчлөлт авчирсан. 1986 онд шинжлэх ухааны сэтгүүлүүдэд Рина Дехтерийн ажил хэвлэгдэж гүний сургалт гэх нэр томъёо гарч ирсэн хэдий ч өмнө нь 1965 онд ЗХУын эрдэмтэд болох Алексей Григорьевич Ивахненко болон Валентин Григорьевич Лапа нарын бүтээл болох \"Кибернетикийн таамагладаг төхөөрөмжүүд\"(орос. Кибернетические предсказывающие устройства) нэртэй номонд уг арга дэлгэрэнгүйгээр мөн алгоримтайгаа хэвлэгдэж олны хүртээл болсон. 1989 онд Ян Лекун backpropagation аргаар гараар бичсэн ZIP-код таних програмд амжилттай ашигласан боловч уг аргын сургалтын хурд удаан, сургалтанд ашиглагдах параметрүүд болон градиенттэй холбоотой асуудлууд байгааг 1991 онд Иорген Шмидхубер болон Сепп Хохрейтер нар илрүүлсэн байна. Үүнээс үүдээд 1990ээд оны дараагаар байр сууриа SVM(туслах/дэмжлэг векторын арга) аргад алджээ. 2000 оны дунд үеээс график процессорууд ихэд дэлгэрч хурд чадал нь сайжирснаар дээрх проблемуудаас ангижрах зам нээгдсэн. Хинтон, Салахутдинов, Осиндеро, Тэ нарын хэвлүүлсэн материалд их хэмжээний датаг гүний сургалтанд амжилттай ашигласнаар нейрон сүлжээг ашиглан хиймэл оюун ухаан хийх арга нэг алхамаар урагшилсан гэж хэлж болно. 2012 оны үед хэсэг орос болон канад эрдэмтдийн бүтээсэн гүний сургалтын архитектур(60 сая параметр, 500000 нейрон, 5 convolutional layer) нь ImageNet уралдаанд бусдаас хол илүү байж бусад судлаачдын анхаарал татсанаар сүүлийн жилүүдэд судлаачид ихээр гүний аргад судалгаа хийх болсон билээ. Гүнзгий сургалт нь машин сургалтын нэг бүлэг аргууд бөгөөд: Дараах алгоритмуудыг ашиглагддаг: Эдгээр аргуудыг нийлүүлээд хиймэл оюун ухааны төрөл бүрийн проблемийг шийдэхэд ашигладаг. Гүний сургалт бол зүгээр нейрон сүлжээний мода болгож эсвэл ANN-г ребрендинг хийж буй нэршил хэмээн зарим судлаачид үздэг[1][2]. Гүнзгий сургалтыг одоогийн байдлаар доорх ажиллагаанд амжилттай ашиглаад байна: Програмчлалын хэлнүүд болох С++, Python, Java дээр нээлттэй эхийн бүлгэм болон байгууллагуудын гаргасан доорх үнэгүй сангууд бий:"
  },
  {
    "url": "https://nl.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Wikipedia",
    "content": "Deep learning of diep leren is een van de vele methodes van machinaal leren, gebaseerd op kunstmatige neurale netwerken. Het leren kan gesuperviseerd gebeuren, semi-gesuperviseerd, of niet gesuperviseerd.[1] Deep learning kan in gebieden worden toegepast zoals beeldverwerking, spraakherkenning, computationele taalkunde, computervertaling, bio-informatica, bepalen van de samenstelling van geneesmiddel, medische beeldherkenning en programma's voor bordspelen. Ze kunnen soms resultaten behalen die gelijk zijn aan of soms beter dan die van specialisten.[2][3][4] Deep learning is een klasse algoritmen voor machinaal leren die meerdere lagen gebruikt om geleidelijk kenmerken van een hoger niveau te extraheren uit de ruwe invoer. Bij beeldverwerking kunnen lagere lagen bijvoorbeeld randen identificeren, terwijl hogere lagen de concepten identificeren die relevant zijn voor een mens, zoals cijfers, letters of gezichten. Vanuit een andere invalshoek bekeken verwijst deep learning naar het \"computersimuleren\" of \"automatiseren\" van menselijke leerprocessen van een bron (bijvoorbeeld een afbeelding van honden) naar een geleerd object (honden). Daarom is een begrip als \"deeper\" learning of \"deepest\" learning zinvol. Deepest learning verwijst naar het volledig automatisch leren van een bron naar een uiteindelijk geleerd object. Deeper learning verwijst dus naar een gemengd leerproces: een menselijk leerproces van een bron naar een geleerd semi-object, gevolgd door een computerleerproces van het menselijk geleerd semi-object naar een uiteindelijk geleerd object."
  },
  {
    "url": "https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0",
    "title": "ディープラーニング - Wikipedia",
    "content": "Category:機械学習 ディープラーニング（英: deep learning）または深層学習（しんそうがくしゅう）とは、対象の全体像から細部までの各々の粒度の概念を階層構造として関連させて学習する手法のことである[2][注釈 1]。深層学習は複数の独立した機械学習手法の総称であり、2006年以降に急速に進歩した。その中でも最も普及した手法は、（狭義には4層以上[3][注釈 2]の）多層の人工ニューラルネットワーク（ディープニューラルネットワーク、英: deep neural network; DNN）による機械学習手法である[4]。 要素技術としてはバックプロパゲーションなど、20世紀のうちに開発されていたものの、4層以上の深層ニューラルネットについて、局所最適解や勾配消失などの技術的な問題によって十分学習させられず、性能も芳しくなかった。しかし、ジェフリー・ヒントンの研究チームが2006年に多層ニューラルネットワークを用いたオートエンコーダを発表したことをきっかけに、多層ニューラルネットワークの学習の研究が進展し、同時に学習に必要な計算機の能力向上とインターネットの発展による学習データの流通が相まって、十分に学習させられるようになった。その結果、音声・画像・自然言語を対象とする諸問題に対して他の手法を圧倒する高い性能を示し[5]、2000年代末から2010年代にかけて急速に普及した[3][6][7]。 深層学習は機械学習の殆ど全てを独占する程に大きな影響を与えており、2015年に発表された拡散モデルに代表される生成モデルの多くに組み込まれたほか、2017年に発表されたTransformerをはじめとする大規模言語モデルなどの基盤にもなっている[8]。しかし、深層学習によって行われたパターン認識がどのような根拠に基づいているかを解析することは難しく、ブラックボックス問題を引き起こしている[9]。また、開発競争がきわめて激しく、最新の手法が数ヶ月で古くなるような事もあり得る状況であるため、常に最先端の技術を追いかけ続けることは容易ではない[10][注釈 3]。 ディープラーニングは、学習に用いる具体的な数学的概念を問わず、対象の全体像から細部までの各々の粒度の概念を4層以上の階層構造として学習する手法を指す[2][注釈 1]。21世紀に入って、オートエンコーダを始めとするジェフリー・ヒントンらによる多層ニューラルネットワークによる学習の研究や、学習に必要な計算機の能力向上、および、インターネットの発展による学習データの流通により、多層ニューラルネットによる手法が最初に確立された。その結果、音声・画像・自然言語を対象とする諸問題に対し、他の手法を圧倒する高い性能を示し[5]、2010年代に普及した[3]。結果として（狭義には4層以上[3][注釈 4]の）多層の人工ニューラルネットワーク（ディープニューラルネットワーク、英: deep neural network; DNN）による機械学習手法[4]が広く知られるようになったが、ニューラルネットワーク以外でも深層学習は構成可能であり、現在はニューラルネットワークよりも抽象的な深層学習の数学的概念が模索されている最中にある[7]。ビジネスの現場では多層ニューラルネットワークの応用が盛んであり、「ディープラーニング=ニューラルネットワーク」などと解釈される事が多いが、学界ではニューラルネットワーク以外の手法も含めた抽象的な概念として説明される[7][注釈 5]。 ディープラーニングはニューラルネットワークの分野で最初に実現されたため、歴史はニューラルネットワークの発展から順次記載する。 ニューラルネットワークの構成要素となるパーセプトロンが考案されたのは1957年であるが、計算機の性能の大幅な不足や、2層からなる単純パーセプトロンでは排他的論理和の認識ができないなどの欠点があったため、研究が大きく続けられることはなかった[11]。その後、1980年代より、排他的論理和の問題を扱うことができる3層からなる多層パーセプトロンの学習を可能にするバックプロパゲーションが開発されたが、非効率的なメカニズムや、動詞の過去形など複雑な認識ができない（そもそも3層ニューラルネットで任意関数は全て近似可能であり、大脳新皮質がなぜ3層以上存在するのかが不明であった）などの要因により、1990年代後半には沈静化した[12][13]。 ディープラーニングのような多層ニューラルネットワークを志向する先駆的研究として、日本の福島邦彦（NHK放送技術研究所、その後大阪大学基礎工学部生物工学科）によって1979年に発表されたネオコグニトロン[14][15]が挙げられる[16][17]。ネオコグニトロンには自己組織化機能があり、自ら学習することによってパターン認識能力を獲得（概念形成）していく。応用例として、福島らは手書き文字データベース（複数個のデータ）から自己学習によって手書き文字認識能力（各文字の概念）が獲得されることを実証した。しかし、当時は単に「手書き文字認識方式の一つ」として扱われ、その汎用性についての認識が世間に広がらなかった[18]。この当時はネオコグニトロンを検証する上ではデジタルコンピュータが貧弱過ぎたため、ソフトウェアでの検証が不可能であり、回路素子を繋ぎ合わせてネオコグニトロンを実装して検証が行われた。学習方法に誤差逆伝播法ではなく add-if silent を使用している以外は畳み込みニューラルネットワーク（CNN）と同一であり、ディープラーニング的な手法としては約30年も先駆けている。 1998年には畳み込みニューラルネットワーク（CNN）の直系の元祖となるLeNet-5（末尾の数字は5層であることを表す）が提案された。論文の中で、ニューラルネットワークの層構造を板状の図形で図示する方法が初めて用いられた。LeNet-5はネオコグニトロンに誤差逆伝播法を適用し、人工ニューロンの活性化関数として勾配消失問題をシグモイド関数よりも緩和できるtanh関数を導入した手法である[19][20]。 初期のディープラーニングはジェフリー・ヒントンによる貢献が大きいため、ニューラルネットワークによる理論実証の過程を記載する[21]。 単層パーセプトロンの「線型分離不可能な問題」を解けない、という限界は、多層パーセプトロンの機械学習がバックプロパゲーションにより実現されたことで、ある程度は解決された。しかし、層数を増やした多層ニューラルネットの学習は、局所最適解や勾配消失などの技術的な問題によって、十分に学習させられず、性能も芳しくないとして、1990年代を中心とした時期には研究なども退潮気味にあった。また、これら理論の不備以前の問題として、発展的な機械学習を行うにはコンピュータの計算性能が大幅に不足しており、大量のデータの入手も難しかったため、研究の大きな障害になっていた。しかし、インターネットが広く普及し、コンピュータの性能が向上した2006年にニューラルネットワークの代表的な研究者であるジェフリー・ヒントンらの研究チームが、制限ボルツマンマシンによるオートエンコーダ（自己符号化器）の深層化に成功[注釈 6]し、再び注目を集めるようになった。この時発明された手法は積層自己符号化器（スタックトオートエンコーダ）と呼ばれた。この際、発表した論文から、これまでの多層ニューラルネットよりもさらに深いネットワーク構造を意味する、ディープネットワークの用語が定着した。元々はジェフリー・ヒントンらの開発したディープネットワークは層が直列された単純な構造をしていたが、現在のアルゴリズムは複数の分岐やループのある複雑なグラフ構造を持つ。そのため、基本技術をまとめて複雑なグラフ構造を簡単に実現できるようにしたライブラリも公開されている。2012年には物体の認識率を競うILSVRCにおいてジェフリー・ヒントン率いるトロント大学のチームがAlexNetによって従来の手法（エラー率26%）に比べてエラー率17%と実に10%もの劇的な進歩を遂げたことが機械学習の研究者らに衝撃を与えた。その後もILSVRCでは毎年上位はディープラーニングを使ったチームが占めるようになり、エラー率は2014年時点で5%程度にまで改善した[22]。 コンピュータのハード性能の急激な進歩、インターネット普及によるデータ収集の容易化、CPUよりも単純な演算の並列処理に優れたGPUの低価格化、また、それらの計算資源の拡張を礎として、画像処理におけるディープラーニングの有用性が競技会で世界的に認知された2012年頃からは急速に研究が活発となり、第三次人工知能ブームが到来したとされている[23]。これ以後は様々なアプリに人工知能が組み込まれ、ユーザーに最適な回答を返す事が出来るようになって行った。 2016年、グーグル翻訳はディープラーニングを利用して、従来の統計的翻訳から、ほぼ人間レベルの翻訳へと変貌を遂げた[24]。その結果、人間にとって機械翻訳は日常生活での常識となり、計算神経科学などを研究する学者が揺るがされる時代となった[25]。2022年には、Stable Diffusionなどにおけるディープラーニングの利用がPixivのような画像投稿サイトを変革し[26]、ディープラーニングを利用したChatGPTなどが世界に革命をもたらした[27]。 ディープラーニングは物体認識を中心にさまざまな分野で活用されている。また、Googleをはじめとした多くのIT企業が研究開発に力を入れている。国家の経済成長を大きく左右する技術であるため、国家間の研究開発競争は経済戦争を引き起こしている。 GoogleのAndroid 4.3[28]は、音声認識にディープラーニング技術を活用することで、精度を25から50パーセント向上させた[29]。2012年、スタンフォード大学との共同研究であるグーグル・ブレイン（英語版）は、1,000のサーバーの16,000のコアを使い、3日間で猫の画像に反応するニューラルネットワークを構築したと発表して話題となった[30][31]。この研究では、200ドット四方の1,000万枚の画像を解析させている。ただし、人間の脳には遠く及ばないと指摘されている[32]。GoogleLeNetと呼ばれるチームによるトロント大学との共同研究では、画像の説明文を自動で生成できる「Image to Text」と呼ばれるシステムを開発した。これは、コンピュータビジョンと自然言語処理を組み合わせ、ユーザーがアップロードした画像を認識し、説明文を表示するもの[33][34][35]である。2015年3月、Schroffらは800万人の2億枚の画像を99.6%の精度で判定した（22層）[36]。2016年1月、AlphaGoと呼ばれるシステムが中国系フランス人のヨーロッパ囲碁王者である樊麾と2015年10月に対局し、5戦全勝の成績を収めていたことが発表された。主に開発に携わったのは2013年にGoogleが買収したDeepMind。囲碁はチェスよりも盤面が広いために打てる手数の多さは比較にならないほどで人間のプロと互角に打てるようになるまでさらに10年はかかるという予測を覆した点と、囲碁に特化したエキスパートマシンではなく汎用的にも用いることができるシステムを使っている点に注目が集まった[37][38]。2016年から2017年にかけては、いずれも世界トップクラスの棋士である韓国の李世乭と中国の柯潔と対戦し、2016年の李世ドルとの5番勝負では4勝1敗、2017年の柯潔との3番勝負では3連勝を収めた[39][40]。 Facebookは、ユーザーがアップロードした画像を、ディープラーニングによって認識させ、何が写っているかの判別精度を向上させている[29]。また、人工知能研究ラボを2013年に立ち上げ[41]、その成果としてディープラーニング開発環境を2015年1月16日にオープンソースで公開した。これは、GPU環境において、従来のコードの23.5倍の速度を実現しており[42]、ディープラーニングの研究開発の促進が期待されている[43]。 ニューラルネットワークによるディープラーニングを使ったニューラル機械翻訳（NMT）が登場したことで、翻訳の品質が大幅に向上した[44]。 エンターテインメントにおいても、NPCのAIはメタ解析によるディープラーニングが主流となり[45]、ゲームデザインには人間とAIの協調が求められるようになった[46]。また、自動運転車の障害物センサー[47]や医療にも使われている[48]。 利点が多い一方で、倫理的な問題や犯罪も発生している。例えば、中国では天網に代表されるようにディープラーニングが国民に対する当局の監視強化を目的に急速に普及しており[49][50][51]、世界のディープラーニング用サーバーの4分の3を占めているとされる[52]。米国政府によれば2013年からディープラーニングに関する論文数では中国が米国を超えて世界一となっている[53]。ヒントンらと並んで「ディープラーニングの父」と呼ばれているヨシュア・ベンジオは中国が市民の監視や独裁政治の強化に人工知能を利用していることに警鐘を鳴らした[54][55]。また、ディープフェイクという、本物と区別の付かない偽画像生成技術が登場し、特定の有名人の顔や声を使って事実と異なる発言やポルノ（フェイクポルノと呼ばれる）を収めた動画が多数流通するようになってからは、重大な名誉毀損や人格権の侵害の可能性があることから、警察が作成者やサイト運営者の摘発に動いている[56]。さらに、偽の画像や音声を用いて様々な無人制御システムを撹乱する攻撃が想定されるため、被害を未然に防ぐ観点から対策が行われている[57]。 日常生活では、ディープフェイクなどの用途で有名であるが、学術分野では医学や生物学の分野に革命をもたらしている[58]。 ネットワークモデルは現在も盛んに研究されており、毎年新しいものが提案されている。 畳み込みニューラルネットワーク (Convolutional Neural Networks: CNN) とは、全結合していない順伝播型ニューラルネットワークの一種。特に2次元の畳込みニューラルネットワークは人間の視覚野のニューロンの結合と似たニューラルネットワークであり、人間の認知とよく似た学習が行われることが期待される。結合がスパース（疎）であるため、全結合しているニューラルネットワークに比べて学習が高速である。 1979年に福島邦彦が発表したネオコグニトロンから発展し、1988年にHomma Toshiteruらが音素の認識に[59]、1989年にYann LeCunらが文字画像の認識に使用し[60][61]、1998年にLeCunらが発表したLeNet-5へと続き、2012年にILSVRCでの物体カテゴリ認識で優勝したAlexNetも深層畳み込みニューラルネットワークである[62]。ネオコグニトロンの時から深層であったが、近年は深層であることを強調するため、深層が頭につき、深層畳み込みニューラルネットワークと呼ばれることもある。自然言語処理に対する応用もなされはじめた。 まず3層のオートエンコーダで学習を行い、学習が完了したら次の層（4層目）をオートエンコーダとして学習する。これを必要な分だけ繰り返していき、最後に全層の学習を行う。事前学習とも呼ばれる。類似技術にディープビリーフネットワーク、ディープボルツマンマシンなどがある。 入力データを出力に変える変換を学習するのではなく、残差を学習する。通常の多層ニューラルネットより勾配消失がおきにくく、はるかに多層化できる。実験的には1000層まで学習されたものもある。欠点としては、入力次元数と出力次元数を変えることができない。 2つのネットワークが相反した目的のもとに学習するネットワークモデル。Discriminatorが損失関数の役目を担う。二乗誤差最小化などでは、ピークが一つしか無いことを仮定しているが、discriminatorはニューラルネットであるのでピークを複数持つ確率分布を近似でき、より一般の確率分布を扱うことができる。 Self-Attention機構（自己注意機構）を利用したモデルである[63]。再帰型ニューラルネットワークの代替として考案された[63]。 従来のニューラルネットワークとは異なり、本来ディープラーニングには使われないはずの純粋な多層パーセプトロンのみで構成された画像認識モデルである[64]。画像を多数のパッチに分け、それらのパッチごとにパラメータ共有された層とパッチ間での変換を行う層を用意することで大幅な精度の向上がされている。欠点としては、固定されたサイズの画像しか入力が出来ない。 統計的な変動をもちいたホップフィールド・ネットワークの一種。 同一層間では接続を持たないボルツマンマシン。 回帰型ニューラルネットワーク（Recurrent Neural Network：RNN）とは、有向閉路を持つニューラルネットワークのこと。それ以前の入力によって変化する状態を保持する（オートマトン）。動画像、音声、言語など、入力データの順序によって出力が変わる場合に有効である。また、順伝播型ニューラルネットワークでは、近似できるピーク数が中間層の素子数に依存するのに対して、回帰型ニューラルネットワークでは無限の周期性を持つ関数を近似することが可能である。 1980年代から研究が始まり、1982年に発表されたホップフィールド・ネットワークが初期の研究。その後ElmanネットワークやJordanネットワークが発表され、1997年にS. HochreiterおよびJ. SchmidhuberらがLSTMネットワーク（長・短期記憶、Long short-term memory）を発表した。 確率的勾配法は誤差から勾配を計算して中間層の重みを修正するが、シグモイド関数などは見てすぐにわかる通り、勾配が0に近い領域が存在する。偶然その領域に進むと勾配が0に近くなり、重みがほぼ修正されなくなる。多層NNでは一か所でも勾配が0に近い層が存在すると、それより下の層の勾配も全て0に近くなるため、確率的には層数が増えるほど学習が難しくなる。詳しくはバックプロパゲーション、活性化関数も参照のこと。 トレーニングデータでは高識別率を達成しながら、テストデータでは識別率が低い現象。過剰適合も参照のこと。 学習が、大域的な最適解ではなく、局所的には適した解へと収束し、抜け出せなくなること。 深層学習以外でも広く使われているが、入力データが画像など、どのようなテストデータが来るかあらかじめある程度の想定（モデル化）ができる場合は、たとえば画像の回転や引き延ばしを行うことで入力データ数を増やすことも昔から行われている。 ニューラルネットワークにおいては古くからシグモイド関数や \n\n\n\nt\na\nn\nh\n\n\n{\\displaystyle tanh}\n\n 関数がよく使われていたが、多層のニューラルネットでは層数の増加に伴って、最適なパラメータを決めるため用いる勾配を逆伝播法で求める際に、勾配消失といわれる問題が生じ易くなる困難があった。そこで近年では勾配消失を避けるために、ReLUなどの他の種類の関数が活性化関数として用いられるようになった。詳しくは活性化関数を参照。 ReLU（rectified linear unit ランプ関数とも呼ばれる。ランプは坂道の意味である） 出力が0.0 - 1.0の範囲に制限されないので勾配消失の問題が起きにくく、またシグモイド関数に比べて計算が簡単であるため学習が速く進む等のメリットがある[65]。 複数の次元の最大値を出力する関数。入力値のどれか一つでも大きい値を持っていれば良いので勾配消失問題が生じる確率が極めて低くなる。CNNのプーリングと同じ計算である。高性能と言われるが、性質上、次元が減少する。特徴選択も兼ねていると言える。 ドロップアウトはランダムに任意のニューロン（次元）を何割か無視してしまう技術である。入力データを増やせずとも、次元を減らすことで解の有意性を上げることができる。ドロップアウトして得た学習結果は、テスト時には同時に使用し、結果は平均して用いる。これはRandom forestと同様、検出率の低い識別器でも並列化することで信頼度を上げることができるためである。 ラッソ回帰とも呼ばれる。辞書行列と係数行列の内積（線形結合）で入力データ(列ベクトル)を近似するとき、係数行列は疎行列（非零の要素が僅かしかない行列）になる。L1正則化のこと。 バッチ学習を行う際に、バッチ正則化層を設け、白色化 (入力データを平均 0、分散 1 に正則化) する。従来は、内部共変量シフト (internal covariance shift) を抑えることで、学習が効率的に進むとされていたが、現在では単に内部共変量シフトだけによるものではないと考えられている[66][67][68]。 深層学習における量子化（英: quantization）は活性化値の連続-離散変換である。 情報科学一般における量子化と同様に、連続値を離散値へ変換・近似する。深層学習では誤差逆伝播に勾配連続性が必要であるため、中間層で量子化を採用するには何らかの工夫が必要である。また、大きな入力セットのサンプル値データを小さな出力セットのサンプル値データに変換することでもある[69]。 以下は勾配生成アルゴリズムの一例である： 以下は量子化を深層学習モデルへ組み込む技法の一例である： 英: vector quantization"
  },
  {
    "url": "https://no.wikipedia.org/wiki/Dyp_l%C3%A6ring",
    "title": "Dyp læring – Wikipedia",
    "content": "Dyp læring (engelsk: deep learning) er en læreprosess som en bruker til å trene opp dataalgoritmer ved hjelp av såkalte kunstige nevrale nettverk (også kjent som «nevrale nettverk»). Dybden i dyp læring viser til at de kunstige nevrale nettverkene har mange lag av beregningsenheter, såkalte nevroner. Data må gjennomgå mer enn to lag med ikke-lineære transformasjoner for at læringa og det nevrale nettverket kan karakteriseres som dypt. Dyp læring er en sentral metode innen maskinlæring – hvor det er et prinsipp at datamaskiner skal tilegne seg kunnskap og lære om noe de ikke vet eller kan fra før fra de data de blir tilført.[1] Dyp læring er basert på et sett med algoritmer som forsøker å modellere abstraksjoner i data på høynivå ved å bruke mange beregningslag med komplekse strukturer, som består av affine og ikke-lineære transformasjoner.[2][3] Læreprosessen kan være styrt (også kalt veiledet, eller på engelsk: supervised), halv-styrt (delvis veiledet) eller ikke-styrt (ikke-veiledet). Dyp læring har hatt en stor innvirkning på områder som bildeklassifisering, datamaskinsyn (engelsk: computer vision), språkbehandling, biostatistikk og lydgjenkjenning.[3][4] Metodikken står bak en stor andel av nyvinningene det siste tiåret innen kunstig intelligens."
  },
  {
    "url": "https://nn.wikipedia.org/wiki/Djupl%C3%A6ring",
    "title": "Djuplæring – Wikipedia",
    "content": "Djuplæring er ein del av maskinlæring som fokuserer på å nytte fleirlags nevrale nettverk til å utføre oppgåver som klassifisering, regresjon og representasjonslæring. Feltet hentar inspirasjon frå biologisk nevrovitskap og dreier seg om å stable kunstige nevron lagvis og «trene» dei til å handsame data. Ordet «djup» viser til bruken av fleire lag (frå tre til fleire hundre eller tusen) i nettverket. Metodane som blir brukte, kan vere rettleia læring (supervised), delvis rettleia (semi-supervised) eller ikkje-rettleia (unsupervised).[1] Det finst ei lang rekkje spesialiseringer i arkitekturtypar innan djuplæring i bruk, \neit døme er fullt tilknytte nettverk.  Bruksområde inkluderer maskinsyn, taleattkjenning, naturleg språkprosessering, maskinomsetjing, bioinformatikk, legemiddelutvikling, analyse av medisinske bilete, klimatologi, materialinspeksjon og brettspelprogram, der dei har gjeve resultat som kan måle seg med, og i nokre tilfelle overgå, ekspertprestasjonar frå menneske.[2][3][4] Tidlege former for nevrale nettverk var inspirerte av informasjonsprosessering og distribuert kommunikasjon mellom noder i biologiske system, særleg menneskehjernen. Men dagens nevrale nettverk prøver ikkje å modellere hjernefunksjonane til organismar, dei vert generelt rekna som dårlege modellar for det føremålet.[5]"
  },
  {
    "url": "https://oc.wikipedia.org/wiki/Aprendissatge_prigond",
    "title": "Aprendissatge prigond — Wikipèdia",
    "content": "L'aprendissatge prigond[1] (en anglés deep learning, deep structured learning, hierarchical learning) es un ensemble de metòdes d'aprendissatge automatic volent modelizar amb un naut nivèl d’abstraccion de donadas mercé a d’arquitecturas articuladas de diferentas transformacions non lineàrias. Aquestas tecnicas permetèron de progrès importants e rapids dins los domènis de l'analisi del senhal sonor o visual e per exemple la reconeissença faciala, de la reconeissença vocala, de la vision per ordinator, del tractament automatizat del lengatge. Dins las annadas 2000, auquestes progrès provoquèron d’investiments privats, universitaris e publics importants, per exemple fachs pels gigants de l’internet (Google, Apple, Facebook, Amazon, Microsoft)[2]. Lo « deep learning » fa partit d’una familha de metòdes d'aprendissatges automatics fondats su l’aprendissatge de modèls de donadas. Una observacion (un imatge, p. ex.) se pòt representar de diferents biais per un vector de donadas, Per exemple segon: Unas representacions e una bona capacitat d'analisi automatic de las diferenciacions[3] fasent lo pretzfach d’aprendissatge mai eficaç. Una de las perspectivas de las tecnicas de l'aprendissatge prigond es lo remplaçament d’unas òbras, encara pro laboriosas, per de modèls algoritmics d’aprendissatge supervisat, non supervisat (es a dire demandan pas de coneissença especificas al subjècte del problèma estudiat) o alara per de tecnicas d’extraccion ierarquica de las caracteristicas. Las recercas dins aqueste domèni s’esfòrça de bastir de representacions melhoras de real e de crear de modèls capables d’aprene aquestas representacions a partir de donadas non labelizadas a granda escalas. Unas d’aquestas representacions s’inspiran de las darrièras avançadas en neurosciéncia. S'agís, grosso modo, d'interpretacions del tractament de l’informacion e dels modèls de comunicacion del sistèma nerviós, a l'imatge del biais que lo sistèma nerviós establís de ligamss segon los messatges recebuts, de la responsa neuronala e del pes dels ligams entre las neurònas del cervèl. Las diferentas arquitecturas de « deep learning » coma lo « deep neural networks », los « convolutional deep neural networks », e los « deep belief network » an diferents camps d’aplicacion: Dins aquestes dos darrièrrs domènis, per exemple, obtenguèron de resultats plan prometeires. Las tecnicas d'aprendissatge prigond constituisson una classa d’algoritmes d'aprendissatge automatic que: Aquestas arquitecturas permeta, uèi de conferir de « sens » a de donadas en lor donant la forma d’imatges, de sons o de tèxtes. L'aprendissatge prigond utiliza de sias amagadas de rets de neurònas artificialas, de « maquinas de Boltzmann restrenchas », e de serias de calculs propositionals complèxes. Los algoritmes d'aprendissatge prigond s’opausan als algoritmes d’aprendissatge pauc prigonds a causa del nombre de transformacions realizadas sus las donadas entre la sisa de dintrada e la sisa de sortida, ont una transformacion correspond a una unitat de tractament definida per de peses e de lindals. Lo concèpte d'aprendissatge prigond prend forma dins las annadas 2010, amb la convergéncia de quatre factor: En octobre de 2015, lo programa AlphaGo, que se li aprenguèt a jogar al jòc de go mercé al metòde de l'aprendissatge prigond, venç lo campion europèu Fan Hui[10] per 5 partidas a 0. En març de 2016, lo mèsme programa venç lo campion del mond Lee Sedol per 4 partidas a 1[11]. L'aprendisstage prigond s'aplica a diferents sectors de las NTIC, coma: Lo metòde del Deep Learning es uèi utilizat per realizar de motors de traduccion automatica. L'aprendissatge prigond pòt, per exemple, ajudar a: Una aplicacion del deep learning per la santat publica es lo projècte Horus de la societat Eyra[24]. S’agís d’un aparelh portable utilizant la plataforma NVidia Jetson, qu’ajuda los malvesents o los cecs a s’orientar e a reconeisser de personas o d’objèctes, tornant transcriure en audio un imatge captat per una camèra. En fisica, l'aprendissatge prigond es utilizat per la recerca sus la matèria exotica[25]. De personalitats, d’esperelas eissidas de la comunautat dels conceptors e fornisseires en tecnologias, manifestèron lor crenta de veire, de mai a mens long tèrme, l'intelligéncia artificiala passar las performanças de l'intelligéncia umana[26]. Se pòt far mencion de l'astrofisician britanic Stephen Hawking[27], un dels fondators de Microsoft, Bill Gates[28], lo PDG de Tesla, Elon Musk[29] e l’especialista en IA[30] e l’informatician Stuart Russell. D'autres, coma Raymond Kurzweil, se reclamant de la filosofia transumaniste, se’n gausisson al contrari, qualificant aquesta avançada de singularitat tecnologica. Aquestas crentas sont relativizadas per de cercaires en intelligéncia artificiala, coma Yoshua Bengio. Aqueste darrièr estima en efièch que se foguèt largament exagerat los progrès recents de l'IA e sosestimat la lentor del procediment de melhoracion[31]. Aqueste sector reconegut estima que l'IA es encora luènh de l’estadi ont se pòt dire que l'ordinator « compren » es a dire: soslinha que percebre e crear de concèptes, es pas veraiment comprende, e afirma que manca encara quicòm de fondamental per passar lo pas[31]. Enfin, la crenta que l'IA poiriá melhorar d'esprela per escapar possiblament al contraròtle dels umans li sembla infondada per tres rasons: primièra, las tecnicas actualas, fòrça especializadas, exigissent de fòrça longs temps d'aprendissatge [32]; segonda, repausan pas sus una forma d'autoprogramacion; e tresena, l'autoprogramacion es pas un axe de recerca[31]. Son tanben evocats de possibles usatges malvolent de deep learning. Ven alara possible d'incrustar la cara d'una persona sus una autra, e a l’amagat, e de li far far o dire de causa que faguèt pas o diguèt pas, le deep learning tornant crear los movements de cara fasent l'incrustacion realista. Atal, diferntas actriças coma Gal Gadot, Emma Watson, Cara Delevingne, Emma Stone, Natalie Portman o Scarlett Johansson trobèron lor cara incrustada sus aqueste d'una actriça pornografica amb un logicial accessible al grand public nomeant Deepfakes, creant de crentas al subjècte de la generalizacion d'un tal usatge, permetent qui que siá de noire a la reputacion d'una autra persona[33]. Fàcia al dangièr, diferentas plataformas coma PornHub, Twitter e Reddit interdiguèron la publicacion de talas vidèos. Nocions"
  },
  {
    "url": "https://ps.wikipedia.org/wiki/%DA%98%D9%88%D8%B1%D9%87_%D8%B2%D8%AF%D9%87_%DA%A9%DA%93%D9%87",
    "title": "ژوره زده کړه - ويکيپېډيا",
    "content": "ژوره زده کړه ، ژور لوستنه يا(انګلیسي: Deep learning)  د ژور جوړښت زده کړه یا هراړخیز زده کړه ) د ماشین زده کړې فرعي څانګه ده ده چې د الګوريتمونو په اساس هڅه کوي چې په ډیټا کې د لوړې کچې هغه خلاصې مفکورې ماډل کړي چې دا پروسه يې کاروي. یو ژور ګراف ماډل کړي چې د پروسس کولو څو پرتونه لري چې د خطي او غیر خطي بدلونونو څو پرتونو څخه جوړ دي. په بل عبارت، دا د زده کړې، د پوهې استازيتوب او د ماډل دننيو ځانګړتياوو پربنسټ کار کوي [۱] په بل عبارت، \"ژوره زده کړه\" د څو پرتونو عصبي شبکې سره د \"ماشین زده کړې\"يو ډول دی چې په ډيټا کې نمونې د زیاتېدونکي دقیقیت سره کشف کوي او له همدې امله کولی شي د کارونکي ګټې وپیژني، شیان پیژني او په ژبو پوه شي. [۲] د روزنې بېلګه (د مثال په توګه: د پیشو عکس) په بېلابېلو لارو نمونه کېدای شي، د رياضيکي ويکتور په توګه چې د هر پکسل لپاره د ارزښتونو څخه ډک وي او په عمومي ډول د کوچنیو فرعي شکلونو سیټ (لکه د پیشو هرې برخې مخ). د دې ماډل کولو ځينې تګلارې د ماشين زده کړې پروسه ساده کوي (د مثال په توګه: د پیشو انځور پېژندنه). په ژوره زده کړه کې، امید شتون لري چې د دې انځور ځانګړتیاوو (لکه د پیشو غړو) د انسان لاس استخراج په بشپړ ډول اتوماتیک غیر څارل شوي او نیمه څارل شوي میتودونو سره بدل کړي. [۳] د دې زده کړې جوړښت رامینځته کولو لپاره لومړی انګېزه د انسان دماغ کې د عصبي جوړښت معاينه کولو نه الهام اخيستی ، چیرې چې عصبي حجرې د پوهیدو وړ کولو لپاره یو بل ته پیغامونه لیږي. [۴] د دې عصبي حجرو د نښلولو لارې په اړه د مختلفو انګیرنو پر بنسټ، په دې برخه کې بېلابېل ماډلونه او جوړښتونه وړاندې شوي او څېړل شوي دي، که څه هم دا ماډلونه په طبيعي توګه د انسان په مغزو کې نشته او د انسان دماغ ډېرې پېچلتياوې لري. دا ماډلونه لکه ژور عصبي شبکه ، د عصبي عصبي شبکه ، د ژور باور شبکه او څو نور مثالونه؛ دوی د طبیعي ژبې پروسس کولو او د عکس پروسس کولو په برخو کې ښه پرمختګ کړی دی. په حقیقت کې، د ژورې زده کړې اصطلاح د مصنوعي عصبي شبکو لپاره د نويو مېتودونو مطالعه ده. [۵] [۶] ژوره زده کړه د ماشین زده کړې الګوریتمونو ټولګه ده [۷] :کوم چې د خام ننوت څخه د لوړې کچې ځانګړتیاوې استخراجوي. په بل عبارت، د ماشین زده کړې تخنیکونو یوه کټګورۍ چې د مالوماتو پروسس کولو څو پرتونه کاروي، په ځانګړې توګه غیر خطي معلومات، د څارنې یا غیر څارل شوي ځانګړتیاوو بدلولو یا استخراج لپاره، په عمومي توګه د نمونې تحلیل یا پیژندنې ، طبقه بندي، کلستر کولو هدف لپاره. [۸] د بېلګې په توګه، د انځور پروسس کولو کې، ټیټ پرتونونه ممکن څنډې کشف کړي، په داسې حال کې چې لوړ پرتونونه ممکن د انسانانو لپاره ډېرې مانا لرونکې ځانګړتیاوې ومومي، لکه لیکونه یا مخونه. ژوره زده کړه د ماشین زده کړې یوه فرعي ساحه ده چې د سینسري سیګنالونو لکه آډیو او ویډیو پروسس کولو لپاره د خطي بدلونونو ډېری پرتونونه کاروي، په دې توګه ماشين هر پېچلی مفهوم په ساده مفکورو وېشي او په دوام يې، دا هغه بنسټیزو مفکورو ته رسیږي چې د پرېکړې کولو توان لري، او پدې توګه د اړتیاوو د ټاکلو لپاره بشپړ بشري څارنې ته اړتیا نشته. په هر وخت کې د ماشین معلومات. یوه مسله چې په ژوره زده کړه کې خورا مهمه ده د معلوماتو وړاندې کولو څرنګوالی دی. ماشین ته د معلوماتو ورکول باید په داسې ډول وي چې ماشین په لنډ وخت کې اړين مالومات ترسره کړي، چې د دوی په اساس پرېکړه وکړي، کله چې د ژورې زده کړې الګوریتم ډیزاین کړئ، موږ باید د بدلون فکتورونو ته پام وکړو چې د لیدل شوي معلوماتو تشریح کوي، دا فکتورونه معمولا د لیدلو وړ فکتورونه ندي، بلکه هغه فکتورونه دي چې د لیدلو وړ کټګورۍ باندې اغیزه لري یا د ساده کولو لپاره د انسان ذهني جوړښت څخه زیږیدلی. مسلې شته. د مثال په توګه، د وینا پروسس کولو په جریان کې، د بدلون عوامل کیدای شي د وینا کونکي تلفظ، عمر یا جنسیت وي. کله چې د موټر عکس پروسس کول، د لمر اندازه د بدلون فکتور دی. د مصنوعي استخباراتو یوه ستونزه په ترلاسه شوي معلوماتو کې د بدلون فکتورونو لوی نفوذ دی. د مثال په توګه، د شپې لخوا د سور موټر څخه ترلاسه شوي ډیری پکسلونه کیدای شي تور ښکاري. د دې ستونزو د حل لپاره، ځینې وختونه موږ د معلوماتو لوړ پوهه (د انسانانو په حدودو کې) ته اړتیا لرو، او په حقیقت کې، ځینې وختونه د معلوماتو ښودلو لپاره د سمې لارې موندل یو ستونزمن او د وخت ضایع کول دي. د څارنې، ژورې، فیډفورډ ملټي لییر پرسیپټرونونو لپاره د عملي زده کړې لومړی الګوریتم په 1960 کې د الیکسي ایواخینکو لخوا خپور شو - چې د \"ژورې زده کړې پلار\" په نوم پیژندل کیږي  - او ویلنټین لاپا. [۱۰] په 1971 کې، یوې مقالې د اتو پرتونو سره یوه ژوره شبکه تشریح کړه چې د زده کړې عملیات یې د ډیټا مدیریت ګروپ میتود (GMDH) سره ترسره کړل. [۱۱] نورې ژورې زده کړې معمارۍ، په ځانګړې توګه هغه چې د کمپیوټر لید لپاره جوړ شوي، په 1980 کې د نیوکوګنیترون سره پیل شوي چې د Kunihiko Fukushima لخوا معرفي شوي. [۱۲] د ژورې زده کړې اصطلاح د لومړي ځل لپاره په 1986 کې د رینا ډیکټر لخوا د ماشین زده کړې په برخه کې وکارول شوه؛ په یوه مقاله کې چې د زده کړې په وخت کې د محدودیت - اطمینان - ستونزو کې لټون کول ، هغه دا کلمه د داسې پروسې لپاره کارولې چې ټول حلونه یې د لټون ځای کې زیرمه شوي و چې سم ځواب ته نه و رسیدلی. د دې خوندي شوي حلونو تحلیل به په راتلونکو هڅو کې د ښه کنټرول لپاره اجازه ورکړي، په دې توګه په لومړیو مرحلو کې د ممکنه خنډونو مخه نیسي.   په هرصورت، نن ورځ د ژورې زده کړې اصطلاح عموما د مصنوعي عصبي شبکو په ساحه کې کارول کیږي، کوم چې په لومړي ځل په 2000 کې د ایګور ایزنبرګ او همکارانو لخوا په پورته ذکر شوي ساحه کې کارول کیږي؛ په ډیر دقیق ډول، په کتاب کې د څو ارزښت لرونکي او یونیورسل بائنري نیورون: تیوري، زده کړه او غوښتنلیکونه او د بولین حد نیورونونو په ساحه کې. [۱۳] [۱۴] په 1989 کې، جان لیکان او ال د لاس لیکل شوي متنونو پیژندلو لپاره (په ځانګړې توګه، په پوستي لیکونو کې د لاسي لیکل شوي پوستي کوډونو پیژندلو هدف لپاره) په ژورې عصبي شبکې کې معیاري بیکپروپیګیشن الګوریتم پلي کړ. په داسې حال کې چې الګوريتم کار کاوه، د دې زده کړې عملیات درې ورځو ته اړتیا لري. کارول شوی ماډل د 256 واحدونو (د 16x16 مربع انځور پکسلز) سره د ان پټ پرت څخه جوړ دی، د 10 واحدونو سره د محصول پرت (کوم چې دا مالومه کړه چې د 0 څخه تر 9 پورې کوم عدد د ان پټ ته ورکړل شوی ډیجیټل عکس دی) او درې پټ پرتونه. دا د دواړو تر منځ جوړ شو. د ازموینې ډیټا سیټ کې د ماډل په ارزولو سره، د پاتې ازموینې نمونو په مینځ کې د 1.8٪ غلط طبقه بندي او 4.4٪ یادونه د 1٪ غلطۍ نرخ لپاره ترلاسه شوي، کوم چې دا ښودلې چې دا ماډل د مخکینیو وړاندې شوي ماډلونو په پرتله خورا درست دی او د بیک پروپیګیشن الګوریتم کارول دي. د ژورو عصبي شبکو د روزنې په بهیر کې مناسب میتود. [۱۵] دا بايد وويل شي چې د شاخپرېدو الګوریتم پخپله د 1970 څخه دمخه او وروسته د اتوماتیک مشتق د متضاد حالت په توګه شتون درلود. [۱۶] [۱۷] تر 1991 پورې، دا ډول سیسټمونه په عمومي ډول د دوه اړخیزو لاسي لیکلو عددونو په جلا کولو کې د پیژندلو لپاره کارول کیده (په دې معنی چې عددونه یوازې لیکل شوي او په شالید کې د اضافي توضیحاتو او ځانګړتیاو پرته، لکه متن او وړاندیزونه )؛ پداسې حال کې چې د درې اړخیزو شیانو پیژندل لاهم یوه ننګونه وه. په 1992 کې، د جان وینګ او همکارانو لخوا یوې څیړنې د درې پرتی عصبي شبکو نیمګړتیاوې او محدودیتونه تشریح کړل چې د دې هدف لپاره کارول شوي، د هییررکیکل شبکې مفهوم یې کارولی او د کریسیپټرون په نوم یې یو ماډل وړاندې کړ، چې د ګڼې ګوڼې په چاپیریال کې د 3D شیانو پیژندلو توان لري. . [۱۸] [۱۹] [۲۰] په دې څیړنه کې د لومړي ځل لپاره د اعظمي پولینګ مفهوم (په انګلیسي کې: max pooling) هم پلي شو. [ ] دا ماډل کولی شي مستقیم طبیعي انځورونه ترلاسه کړي (3D توکي، په شالید کې د نورو عناصرو سره) د ان پټ په توګه، دا د عمومي هدف بصری زده کړې بنسټ شو. په 1994 کې، انډري ډي کاروالو، د مایک فیر هورسټ او ډیویډ بسیټ سره، د څو اړخيزه بولين عصبي شبکې پر بنسټ د جوړښت وړاندیز وکړ، چې د وزن پرته عصبي شبکې په نوم هم ياديږي. دا جوړښت د درې پرتونو ځان تنظیم کولو ځانګړتیا استخراج عصبي شبکې ماډل (په انګلیسي کې: د ځان تنظیم کولو فیچر استخراج یا SOFT) سره د ډلبندۍ ملټي لییر عصبي شبکې ماډل (په ځانګړې توګه د GSN یا هدف لټون شبکې جوړښت سره) ، کوم چې دوی په خپلواکه توګه د زده کړې عمليات ترسره کړي. د فیچر استخراج ماډل کې، د انپټ عکس په څو فرعي برخو ویشل شوی و او هره برخه د نیورونونو بلاک ته ورکړل شوې وه (کوم چې یو ترتیب لري او په څو پرتونو وېشل شوي) او هر بلاک د نورو بلاکونو سره موازي او په خپلواکه توګه روزل شوی و.  [۲۱] [۲۲] په 1995 کې، جوزف زیپ هوچریټر ، چې مخکې او په 1991 کې یې په خپله مقاله کې چې عنوان یې و: په متحرک عصبي شبکو کې څیړنې (په آلمان کې : Untersuchungen zu dynamische neuronalen Netzen) - د تدریجي کمیدو ستونزه وڅیړله [۲۳] او دا پخپله وه. په 1847 کې وړاندیز شوی [۲۴] او په 1944 کې د غیر خطي اصلاح کولو ستونزو لپاره مطالعه شوی و  ) د جورجین شمیدوبر سره یوځای ، هغه د LSTM جوړښت وړاندې کړ [۲۵] او په 1997 کې یې په یوه بله مقاله کې اصلاح کړ [۲۶] کوم چې د 1997 لپاره لوی بنسټ چمتو کړ. د تکراري عصبي شبکو پراختیا [ حوالې ته اړتیا ده ] په ورته 1995 کې، برینډن فری د جیفري هینټون او پیټر دیان سره وښودله چې دا ممکنه ده چې د ویک-سلیپ الګوریتم په کارولو سره د څو سوو پټ پروسس کولو واحدونو سره د شپږ بشپړ تړل شوي پرتونو څخه جوړه شبکه وروزل شي. [۲۷] مخکې له دې چې د ژورې زده کړې پیل شي، د ماشین زده کړې دودیز میتودونه د ډیټا څخه ترلاسه شوي نمایشونو (د ځانګړتیاوو انتخاب) پورې اړه لري. دا میتودونه د ډومین متخصص ته اړتیا لري ترڅو د فیچر استخراج په لاسي ډول ترسره کړي. په هرصورت، د دې لاسي ځانګړتیا استخراج یوه ننګونه او د وخت اخیستلو پروسه ده. د ژورې زده کړې رامینځته کیدل کولی شي په چټکۍ سره دا دودیز میتودونه ځای په ځای کړي. ځکه چې دا کولی شي په اتوماتيک ډول د هرې ستونزې سره سم ځانګړتیاوې استخراج کړي. [۲۸] پداسې حال کې چې په تیرو لسیزو کې د ژورې زده کړې ماډلونه د عکسونو، وینا یا ویډیو په بڼه د ان پټ سره معامله کولو کې بریالي شوي چې اساس یې د یوکلیډین جوړښت دی، پدې وروستیو کې، د څیړونکو لیوالتیا په غیر اکلیډین ډیټا کې د زده کړې پلي کولو هڅه کې زیاتوالی موندلی. جیومیټریک ژوره زده کړه د څیړنې یوه راڅرګنده شوې ساحه ده چې د دې تشې ډکولو لپاره د غیر یوکلیډین ډیټا سره کار کولو لپاره د ژورې زده کړې معمارۍ عمومي کولو هڅه کوي. [۲۹] د ګراف عصبي شبکې د ژورې زده کړې میتودونو ټولګي دي چې په ځانګړي ډول د ګرافونو لخوا بیان شوي ډیټا په اړه انعکاس رامینځته کولو لپاره ډیزاین شوي. دا خورا مطلوب دی چې ماډلونه رامینځته کړئ چې مستقیم په ګرافونو کې کار کوي. ځکه، موږ کولی شو د دوی د جوړښت او ځانګړتیاوو په اړه نور معلومات ترلاسه کړو. د ګراف عصبي شبکې مستقیم په ګرافونو کې پلي کیږي او د دندو ترسره کولو لپاره اسانه لار چمتو کوي لکه نوډ ، څنډه ، او د ګراف کچې وړاندوینې. د ګراف عصبي شبکو د پراختیا دمخه، د ژورې زده کړې میتودونه د پوهې استخراج او وړاندوینې په لور د څنډو د پلي کولو وړتیا نه درلوده. پرځای یې، دوی یوازې د نوډ ملکیتونو باندې کار کاوه. [۳۰] له 2010 راهیسې، په یوه پروژه کې چې د Imagenet په نوم پیژندل کیږي، کلنۍ سیالي ترسره کیږي چې ګډونوال هڅه کوي د مختلف کمپیوټر الګوریتمونو په وړاندې کولو سره د لوی پیمانه ډیجیټل انځورونه وپیژني او د لوړ دقت ترلاسه کولو لپاره یو بل سره سیالي وکړي. اوس، په 2012 کې، په دې سیالۍ کې د الیکسنیټ په نوم یو عصبي شبکه کارول شوې وه، او د خورا اغیزمنو پایلو په ترلاسه کولو سره یې د ژورې زده کړې میتود ته پراخه پاملرنه راجلب کړه؛ په هغه طریقه چې ځینې باور لري، \"ژور زده کړې انقلاب\" سږکال ترسره شو. دا باید یادونه وشي چې د امیجنټ ډیټابیس د عکسونو په پیژندلو کې د الیکسنیټ دقت د انسانانو له دقت څخه ډیر و (که څه هم البته ، حتی د الیکسنیټ له وړاندې کولو دمخه ، نورو الګوریتمونو عالي انساني فعالیت ترلاسه کړی و). [۱] [۲] حتی نن ورځ، عصبي شبکې د کمپیوټر لید کې کلیدي رول لوبوي او د مختلفو موخو لپاره کارول کیږي لکه د عکس پیژندنه، د مخ پیژندنه، د څیز تعقیب، شور لرې کول، د تور او سپینو انځورونو رنګ کول، د خراب شوي انځورونو ترمیم، د طبي انځورونو طبقه بندي کول ، او داسې نور. [۳] ژورې زده کړې د طبیعي ژبې پروسس کولو ساحې په پراختیا کې خورا ښه اغیزه کړې او د ځواکمن ماډل کولو چوکاټ رامینځته کولو سره یې اغیزمنې پایلې ترلاسه کړې.  د مثال په توګه، د OpenAI د GPT-3 ژبې ماډل د دې وړتیا لري چې د ژورې زده کړې میتودونو په کارولو سره د انسانانو لخوا لیکل شوي متنونو ته ورته متنونه تولید کړي. [۲] د الکترومیوګرافي سیګنالونه کولی شي د انسان او ماشین ترمینځ د انٹرفیس په توګه عمل وکړي او د دوی تحلیل کولو سره د مختلف تجهیزاتو کنټرول لپاره د کارونکي ارادې څخه ګټه پورته کړي. د بېلګې په توګه، شړل شوي غړي کولی شي مصنوعي پښې وکاروي ترڅو د خپلو مات شوي غړو ځای په ځای کړي او په مؤثره توګه یې کنټرول کړي. یا په ورته طریقه سره، مرستندویه او تقویه کونکي غړي لکه بهرنۍ کنکال کنټرول کیدی شي. د دې خام سیګنالونو تحلیل او د وسیلې کنټرول لپاره مناسب محصول چمتو کولو لپاره ، د ژورې زده کړې میتود کارول خورا ګټور کیدی شي. [۳] وړاندیز کونکي سیسټمونه د موسیقۍ او مجلې د منځپانګې پر بنسټ سپارښتنو لپاره د پټ فکتور ماډل لپاره د معنی وړ ځانګړتیاوو استخراج لپاره ژورې زده کړې کارولي دي. [۴] [۵] ملټي لید ژور زده کړه (په انګلیسي کې: multi-view deep learning) د څو ډومینونو څخه د کاروونکو غوره توبونو زده کولو لپاره کارول کیږي. [۶] د پانګې اچونې په پالنونو کې، ژورې زده کړې د بیرته راستنیدو نرخ زیاتولو لپاره کارول کیږي. [۷] نن ورځ، ګوګل او ټیسلا ثابته کړې چې بې چلوونکي یا پخپله موټر چلول ممکن دي. په هرصورت، دا باید وویل شي چې دا موټرونه لاهم زده کړې او مختلف تمرینونو او ازموینو ته اړتیا لري، او ډیری فعالیتونه او تغیرات باید وڅیړل شي. غږ پیژندنه د ژبې د پروسې یوه نه بیلیدونکې برخه ده. د آخذې غږ تحلیل د مصنوعي استخباراتو سیسټم لپاره خورا ستونزمن دی؛ ځکه چې ډیری فکتورونه د سم غږ پیژندلو کې رول لوبوي. د مثال په توګه، د شاليد شور، تلفظ، خلاقيت او د خلکو د ژبې لوبې، او همدارنګه د ځانګړو ويناوو معلوليت او نور عوامل دا ستونزمن کوي چې په غږ کې کارول شوي ټکي په سمه توګه په هغه څه کې توپیر وکړي چې کمپیوټر یې تحلیل کولی شي. د \"ژورې زده کړې\" شبکې کارول کولی شي په ډیری سافټویر کې د کارونکي لخوا کارول شوي نمونې په پیژندلو سره د خدماتو وړاندې کولو کیفیت لوړ کړي. یو ښه مطابقت لرونکی AI کولی شي غیر اړونده معلومات له پامه غورځوي او کارونکي ته اړوند معلومات چمتو کړي. د نمونې پیژندنه کولی شي د لوی معلوماتو تحلیل ډیر اغیزمن کړي. کمزوري مصنوعي هوښیارتیا د معنی لرونکي متن تولید او د کوډ کولو په برخه کې پرمختګونو کې بریاوې لري. د مثال په توګه، د OpenAI څخه GPT-3 د خلاصې سرچینې سافټویر جوړونکي ژبه ده چې کولی شي د کارونکي څخه لږ تر لږه لارښوونې سره ساده کمپیوټر پروګرامونه کوډ یا جوړ کړي. په راتلونکي کې، موږ به شاید د نورو سافټویر شتون وګورو چې د دې ډول ټیکنالوژۍ څخه ګټه پورته کوي، او د ځانګړي سافټویر سره کار کول به یو بل رنګ واخلي. مصنوعي استخبارات به د سازمانونو، افرادو او حکومتونو د سایبر امنیت اقداماتو کې خورا مهم رول ولوبوي، چې پدې کې به څارنه، د معلوماتو ګواښونو پیژندنه، د امنیتي لاسوهنو خبرتیا، د پیښو غبرګون او د خطر تحلیل شامل وي. AI لاهم کولی شي حکومتونو ته د څارنې ځواکمن وسیلې چمتو کړي ترڅو په اسانۍ سره خپل مخالفین وپیژني. د کمپیوټر پروګرامونه د منځپانګې په جوړولو کې ورځ تر بلې ښه کیږي، او مصنوعي هوښیارتیا اوس د کاپي رایټ څخه د سرغړونې د پیښو په پیژندلو، ویډیو ګیمونو او فلمونو جوړولو کې هم مهم رول لوبوي، او دا رول به په راتلونکي کې نور هم لوړ شي. ژورې عصبي شبکې (DNNs) د کمپیوټري پلوه د زده کړې ماډلونه دي. دا ماډلونه په مختلفو سکټورونو کې زیاتیدونکي غوښتنلیکونه لري. ځکه چې FPGAs د محاسبې ګړندي کولو لپاره د برنامه وړ زیربنا چمتو کوي او نن ورځ د نړۍ په ډیری برخو کې په اسانۍ سره شتون لري ، FPGAs د DNN ماډلونو پلي کولو لپاره خورا ښه انتخاب دی. مګر د FPGAs کارول خورا ستونزمن دي چې په ورته وخت کې د دوی موثریت او ټیټ انرژي مصرف ترلاسه کړي ، او همدارنګه DNNs ډیری حافظې ته اړتیا لري (د بورډ حافظه معمولا په FPGAs کې کوچنۍ وي) ، د FPGAs سره ماډل پلي کول خورا ستونزمن دي. د عصبي شبکې ویور (DNNWEAVER) یو چوکاټ دی چې په اتوماتيک ډول د ترتیب شوي جوړه (DNN، FPGA) لپاره د ترکیب وړ کوډ رامینځته کوي. د DNN ماډل د ان پټ په توګه ورکول کیږي ځکه چې دا په کیف کې معرفي شوی. د دې برنامې عمومي کاري پروسه دا ده چې دا لومړی د ان پټ ماډل په منځمهاله ژبه بدلوي چې د ټولیز ډیټا جریان ګراف ښیې. پدې مرحله کې، دا ورکړل شوي ماډل د مختلف الګوریتمونو سره ګروپ کوي ترڅو د معلوماتو اعظمي بیا کارولو او د FPGA حافظې او نورو سرچینو سره سم د موثریت لوړې کچې ترلاسه کړي چې په FPGA کې شتون لري. له هغې وروسته، وروستۍ پایله د ترکیب وړ کوډ دی چې د ان پټ ماډل ټولې اړتیاوې په مطلوب FPGA کې د لوړې موثریت سره پوره کوي. دا په ژورو عصبي شبکو کې د FPGAs کارولو لپاره اړین اوږد دورې وخت لنډولو کې مرسته کوي. د دې کار هدف دا دی چې یو اتوماتیک چوکاټ تولید کړي چې لومړی پروګرامر د هارډویر ډیزاین او اصلاح کولو توضیحاتو څخه جلا کوي. دوهم، چوکاټ پخپله په FPGA کې موجود محدودو سرچینو سره مبارزه کوي؛ او په نهایت کې ، دا د FPGA لپاره یو باثباته چوکاټ چمتو کوي ، کوم چې په مختلف FPGAs کې د ژور عصبي شبکو مختلف ماډلونو پلي کولو لپاره لوړ فعالیت چمتو کوي. DNNWEAVER د ورته اهدافو ترلاسه کولو لپاره ډیزاین شوی. په لومړي ګام کې، DNNWEAVER د ژور عصبي شبکې ماډل ته اړتیا لري چې د لوړې کچې انٹرفیس په کارولو سره مشخص شوی. د DNNWEAVER برنامه ان پټ د ژور عصبي شبکو د لوړې کچې توضیح کوونکی دی چې د برکلي په پوهنتون کې د کیف فارمیټ لاندې رامینځته شوی [۱۴] . کیف یو له خورا پراخه کارول شوي ژورې زده کړې چوکاټونو څخه دی چې د شبکې توضیحات د ان پټ په توګه اخلي او په CPU یا GPU کې یې د اجرا وړ ماډل ته بدلوي. DNNWEAVER په اوتومات ډول د ان پټ ماډل د ویریلوګ ترکیب وړ کوډ ته بدلوي. DNNWEAVER د سافټویر څلور برخې لري: ژباړونکی تشریح شوی ژوره شبکه په یو ځانګړي لارښود سیټ جوړښت کې بدلوي چې د معلوماتو لاره تعریفوي. په دې سیټ کې هر کمانډ د شبکې ډیټا لار ګراف کې د نوډ استازیتوب کوي. په یاد ولرئ چې FPGA مستقیم دا لارښوونې نه پلي کوي. د DNNWEAVER کمپیلر دا قوماندې د FPGA کنټرول سیګنالونو ته نقشه کوي او د اجرا کولو مهالویش رامینځته کوي. دا میتود یو مدغم سافټویر - هارډویر انٹرفیس چمتو کوي؛ له همدې امله ، د دې جوړښت په مرسته ، د لارښوونې سیټ د سرعت پلي کولو مختلف ډولونه وړوي چې د هدف FPGA محدودیتونو سره مطابقت لري. د ډیزاین نقشه کونکي په تیرو مرحلو کې رامینځته شوي کمانډونه د ان پټ په توګه اخلي او د منزل FPGA لپاره هارډویر ټیمپلیټونه د نمونې سرچینې اصلاح کولو الګوریتم په مرسته اصلاح کوي. نقشه کوونکی د هرې طبقې د محاسبې پلان د آپریټرانو په ګروپونو ویشي چې ډاټا شریکوي یا بیا کاروي. موږ د دې هرې برخې ټوټې بولو. د محاسبې څخه وروسته هره ټوټه په حافظه کې ډوب کیږي او FPGA د راتلونکي ټوټه محاسبه کولو ته ځي. د عصبي شبکې سرعت کوونکی د FPGA د ټیټ حافظې په مقابل کې د اړتیا وړ حافظې لوی مقدار سره اساسي ستونزه لري ، کوم چې DNNWEAVER دا ستونزه د ټیمپلیټ سرچینې اصلاح کولو الګوریتم په کارولو سره ښه کوي ، او د دې په مرسته د موازي آپریټرانو او ډیټا ترمینځ توازن رامینځته کوي. بیا کارول د منزل FPGA محدودیتونو سره سم رامینځته کیږي. هغه کوي. ډیزاین اوبدل د DNNWEAVER وروستی پاتې مرحله ده، چې د اړتیا وړ سرچینو مقدار او د اجرا کولو کليز په تیر مرحله کې تولید شوی، او د هغې په مرسته، دا د سرعت کور تولیدوي. DNNWEAVER یو شمیر د لاسي اصلاح کولو ټیمپلیټونه کاروي چې په تیر مرحله کې رامینځته شوي د سرچینو نیول او د هارډویر تنظیم په پام کې نیسي. د DNNWEAVER وروستۍ برخه ادغام کوونکی دی. پدې مرحله کې ، د حافظې انٹرفیس کوډونه د سرعت کوډونو کې اضافه شوي. څنګه چې مختلف FPGAs د بهرني DRAM سره د خبرو اترو لپاره مختلف انٹرفیسونه کاروي ، پدې برخه کې د DRAM انٹرفیس کتابتونونه شامل دي چې هر ځل اړین کوډ اضافه کوي. د کار په دې ډګر کې، د بېلابېلو موخو سره هارډویرونه جوړ شوي. که څه هم DNNWEAVER سرعت کوونکی ندی، دا د سرعت کوډ جنریټر دی. تابلا [۱۵] د ماشین زده کړې الګوریتم روزنې مرحلې لپاره د FPGA سرعت چمتو کوي پداسې حال کې چې DNNWEAVER د عصبي شبکې مخبڼې باندې تمرکز کوي. تابلا خپله رياضيکي ژبه کاروي، پداسې حال کې چې DNNWEAVER د ماډل مشخص کولو لپاره کیف کاروي. د چین او ال [۱۶] له خوا د لاين ماډل په تحليلي سکيم کې ترسره شوی کار په FPGA کې د ورته ژور عصبي شبکې لپاره ترټولو ګړندۍ سکیم موندلو لپاره ترټولو کارول کیږي. مګر پدې کې ځینې پرتونه شامل ندي لکه حوض کول."
  },
  {
    "url": "https://pl.wikipedia.org/wiki/Uczenie_g%C5%82%C4%99bokie",
    "title": "Uczenie głębokie – Wikipedia, wolna encyklopedia",
    "content": "Uczenie głębokie (ang. deep learning) – podkategoria uczenia maszynowego (ang. machine learning), polegająca na tworzeniu głębokich sieci neuronowych (sieci z wieloma poziomami neuronów). Techniki głębokiego uczenia mają za zadanie udoskonalić m.in. automatyczne przetwarzanie mowy, rozpoznawanie obrazów i przetwarzania języka naturalnego. Jednymi z najbardziej popularnych architektur sieci neuronowych są sieci rekurencyjne, sieci konwolucyjne, sieci generatywne GAN czy transformer[1][2][3]. Wczesne formy sieci neuronowych były inspirowane metodami przetwarzania informacji i rozmieszczenia elementów komunikacji w układach narządów, szczególnie w mózgowiu człowieka, jednak aktualne sieci neuronowe nie skupiają się na modelowaniu funkcji mózgu i są postrzegane jako modele o niskim stopniu odwzorowania[4]. Sieci neuronowe są głębokie, ponieważ struktura tych sieci składa się z wielu warstw sztucznych neuronów. Proste sieci neuronowe można zaprojektować ręcznie tak, by konkretna warstwa wykrywała konkretne cechy, a uczenie się polega na ustaleniu odpowiednich wag. W dużych sieciach neuronowych proces głębokiego uczenia jest do pewnego stopnia samodzielny - to znaczy sieć nie jest projektowana pod wykrywanie konkretnych cech, lecz wykrywa je na podstawie przetwarzania odpowiednio oznaczonych zbiorów danych. Zarówno takie zbiory, jak i sam sposób działania sieci muszą być przygotowane przez specjalistów, ale cechy wykrywa już sam program. Dzięki temu możliwe jest przetworzenie wielkiej ilości danych, a sieć może automatycznie nauczyć się reprezentacji cech wyższego poziomu, co oznacza, że mogą one wykryć skomplikowane wzorce w danych wejściowych[5][6]."
  },
  {
    "url": "https://pt.wikipedia.org/wiki/Aprendizagem_profunda",
    "title": "Aprendizagem profunda – Wikipédia, a enciclopédia livre",
    "content": "No aprendizado de máquina, a aprendizagem profunda se concentra na utilização de redes neurais multicamadas para executar tarefas como classificação, regressão e aprendizagem de representação. O campo se inspira na neurociência biológica e é centrado em empilhar neurônios artificiais em camadas e \"treiná-los\" para processar dados. O adjetivo \"profunda\" se refere ao uso de múltiplas camadas (variando de três a várias centenas ou milhares) na rede. Os métodos usados podem ser supervisionados, semissupervisionados ou não serem supervisionados.[1] Algumas arquiteturas comuns de redes de aprendizagem profunds incluem redes totalmente conectadas, redes de crenças profundas, redes neurais recorrentes, redes neurais convolucionais, redes adversárias generativas, transformadoras e campos de radiância neural. Essas arquiteturas foram aplicadas a campos como visão computacional, reconhecimento de fala, processamento de linguagem natural, tradução automática, bioinformática, design de medicamentos, análise de imagens médicas, ciência do clima, inspeção de materiais e programas de jogos de tabuleiro, onde produziram resultados comparáveis e, em alguns casos, superando o desempenho de especialistas humanos.[2][3][4] As primeiras formas de redes neurais foram inspiradas pelo processamento de informações e nós de comunicação distribuídos em sistemas biológicos, particularmente o cérebro humano. No entanto, as redes neurais atuais não pretendem modelar a função cerebral dos organismos e são geralmente vistas como modelos de baixa qualidade para esse propósito.[5] A maioria dos modelos modernos de aprendizagem profunda são baseados em redes neurais multicamadas, como redes neurais convolucionais e transformadoras, embora também possam incluir fórmulas proposicionais ou variáveis latentes organizadas em camadas em modelos generativos profundos, como os nós em redes de crenças profundas e máquinas de Boltzmann profundas.[6] Fundamentalmente, a aprendizagem profunda se refere a uma classe de algoritmos de aprendizado de máquina em que uma hierarquia de camadas é usada para transformar dados de entrada em uma representação progressivamente mais abstrata e composta. Por exemplo, em um modelo de reconhecimento de imagem, a entrada bruta pode ser uma imagem (representada como um tensor de pixels). A primeira camada representacional pode tentar identificar formas básicas, como linhas e círculos, a segunda camada pode compor e codificar arranjos de bordas, a terceira camada pode codificar um nariz e olhos, e a quarta camada pode reconhecer que a imagem contém um rosto. É importante ressaltar que um processo de aprendizagem profunda pode aprender quais recursos posicionar de forma ideal em qual nível por conta própria. Antes da aprendizagem profunda, as técnicas de aprendizado de máquina frequentemente envolviam engenharia de recursos feita à mão para transformar os dados em uma representação mais adequada para um algoritmo de classificação operar. Na abordagem de aprendizagem profunda, os recursos não são feitos à mão e o modelo descobre representações úteis de recursos a partir dos dados automaticamente. Isso não elimina a necessidade de ajuste manual; por exemplo, números variados de camadas e tamanhos de camadas podem fornecer diferentes graus de abstração.[7][1] A palavra \"profunda\" em \"aprendizagem profunda\" se refere ao número de camadas através das quais os dados são transformados. Mais precisamente, os sistemas de aprendizagem profunda têm uma profundidade substancial de caminho de atribuição de crédito (CAP). O caminho de atribuição de crédito é a sequência de transformações de entrada para saída. Os caminhos de atribuição de crédito descrevem conexões potencialmente causais entre entrada e saída. Para uma rede neural feedforward, a profundidade dos caminhos de atribuição de crédito é a da rede e é o número de camadas ocultas mais uma (já que a camada de saída também é parametrizada). Para redes neurais recorrentes, nas quais um sinal pode se propagar através de uma camada mais de uma vez, a profundidade do caminho de atribuição de crédito é potencialmente ilimitada.[8] Nenhum limite universalmente acordado de profundidade divide a aprendizagem superficial da aprendizagem profunda, mas a maioria dos pesquisadores concorda que a aprendizagem profunda envolve profundidade de caminho de atribuição de crédito maior que dois. O caminho de atribuição de crédito de profundidade dois demonstrou ser um aproximador universal no sentido de que pode emular qualquer função.[9] Além disso, mais camadas não aumentam a capacidade de aproximação de função da rede. Modelos profundos (caminho de atribuição de crédito maior que dois) conseguem extrair melhores características do que modelos superficiais e, portanto, camadas extras ajudam a aprender as características de forma eficaz. Arquiteturas de aprendizagem profunda podem ser construídas com um método ganancioso camada por camada.[10] A aprendizagem profunda ajuda a desembaraçar essas abstrações e escolher quais recursos melhoram o desempenho.[7] Algoritmos de aprendizagem profunda podem ser aplicados a tarefas de aprendizado que não são supervisionadas. Este é um benefício importante porque dados que não são rotulados são mais abundantes do que os dados rotulados. Exemplos de estruturas profundas que podem ser treinadas de maneira que não é supervisionada são redes de crenças profundas.[7][11] O termo aprendizagem profunda foi introduzido na comunidade de aprendizado de máquina por Rina Dechter em 1986,[12] e nas redes neurais artificiais por Igor Aizenberg e colegas em 2000, no contexto de neurônios de limiar booleano.[13][14] Embora a história de seu surgimento seja aparentemente mais complicada.[15] Redes neurais profundas são geralmente interpretadas em termos do teorema da aproximação universal[16][17][18][19][20] ou da inferência probabilística.[21][22][7][8][23] O teorema clássico da aproximação universal diz respeito à capacidade das redes neurais feedforward com uma única camada oculta de tamanho finito para aproximar funções contínuas.[16][17][18][19] Em 1989, a primeira prova foi publicada por George Cybenko para funções de ativação sigmoides[16] e foi generalizada para arquiteturas multicamadas feed-forward em 1991 por Kurt Hornik.[17] Trabalhos recentes também mostraram que a aproximação universal também é válida para funções de ativação que não são limitadas, como a unidade linear retificada (ReLU) de Kunihiko Fukushima.[24][25] O teorema da aproximação universal para redes neurais profundas diz respeito à capacidade de redes com largura limitada, mas a profundidade pode crescer. Lu et al.[20] provaram que se a largura de uma rede neural profunda com ativação de unidade linear retificada (ReLU) for estritamente maior do que a dimensão de entrada, então a rede pode aproximar qualquer função integrável de Lebesgue; se a largura for menor ou igual à dimensão de entrada, então uma rede neural profunda não é um aproximador universal. A interpretação probabilística[23] deriva do campo do aprendizado de máquina. Ela apresenta inferência,[22][6][7][8][11][23] bem como os conceitos de otimização de treinamento e teste, relacionados ao ajuste e generalização, respectivamente. Mais especificamente, a interpretação probabilística considera a não linearidade de ativação como uma função de distribuição cumulativa.[23] A interpretação probabilística levou à introdução do  abandono (dropout) ou da diluição como regularizador em redes neurais. A interpretação probabilística foi introduzida por pesquisadores como Hopfield, Widrow, e Narendra e popularizada em pesquisas como a de Bishop.[26] Existem dois tipos de redes neurais artificiais (ANNs): redes neurais feedforward (FNNs) ou perceptrons multicamadas (MLPs) e redes neurais recorrentes (RNNs). As redes neurais recorrentes têm ciclos em suas estruturas de conectividade, as redes neurais feedforward não. Na década de 1920, Wilhelm Lenz e Ernst Ising criaram o modelo de Ising[27][28] que é essencialmente uma arquitetura de rede neural recorrente que não é de aprendizagem consistindo de elementos de limiar semelhantes a neurônios. Em 1972, Shun'ichi Amari tornou essa arquitetura adaptável.[29][30] Sua rede neural recorrente de aprendizagem foi republicada por John Hopfield em 1982.[31] Outras redes neurais recorrentes anteriores foram publicadas por Kaoru Nakano em 1971.[32][33] Já em 1948, Alan Turing produziu um trabalho sobre \"Maquinário Inteligente\" que não foi publicado em sua vida,[34] contendo \"ideias relacionadas a redes neurais recorrentes de aprendizagem e evolução artificiais\".[30] Frank Rosenblatt (1958)[35] propôs a perceptron, uma perceptron multicamadas com 3 camadas: uma camada de entrada, uma camada oculta com pesos aleatórios que não aprendiam e uma camada de saída. Mais tarde, ele publicou um livro em 1962 que também introduziu experimentos de computador e variantes, incluindo uma versão com perceptrons de quatro camadas \"com redes pré-terminais adaptativas\" onde as duas últimas camadas aprenderam pesos (aqui ele dá créditos a H. D. Block e B. W. Knight).[36]:secção 16 O livro cita uma rede anterior de R. D. Joseph (1960)[37] \"funcionalmente equivalente a uma variação\" deste sistema de quatro camadas (o livro menciona Joseph mais de 30 vezes). Joseph deve, portanto, ser considerado o criador das perceptrons multicamadas adaptativas com unidades ocultas de aprendizagem próprias? Infelizmente, o algoritmo de aprendizagem não era funcional e caiu no esquecimento. O primeiro algoritmo de aprendizagem profunda funcional foi o método de manipulação de dados de grupo, um método para treinar redes neurais profundas arbitrariamente, publicado por Alexey Ivakhnenko e Lapa em 1965. Eles o consideraram uma forma de regressão polinomial,[38] ou uma generalização da perceptron de Rosenblatt para lidar com relacionamentos mais complexos, que não são lineares, e hierárquicos.[39] Um artigo de 1971 descreveu uma rede profunda com oito camadas treinada por este método,[40] que é baseado no treinamento de camada por camada por meio de análise de regressão. Unidades ocultas supérfluas são podadas usando um conjunto de validação separado. Como as funções de ativação dos nós são polinômios de Kolmogorov e Gabor, essas também foram as primeiras redes profundas com \"portas\" ou unidades multiplicativas.[30] A primeira perceptron multicamadas de aprendizagem profunda treinada por descida de gradiente estocástica[41] foi publicada em 1967 por Shun'ichi Amari.[42] Em experimentos de computador conduzidos pelo aluno de Amari, Saito, uma perceptron multicamadas de cinco camadas com duas camadas modificáveis aprendeu representações internas para classificar classes de padrões que não são separáveis linearmente.[30] Desenvolvimentos subsequentes em ajustes de hiperparâmetros e hardware fizeram da descida de gradiente estocástica de ponta a ponta a técnica de treinamento dominante atualmente. Em 1969, Kunihiko Fukushima introduziu a função de ativação de ReLU (unidade linear retificada).[24][30] A retificadora se tornou a função de ativação mais popular para aprendizagem profunda.[43] As arquiteturas de aprendizagem profunda para redes neurais convolucionais (CNNs) com camadas convolucionais e camadas de redução de amostragem começaram com a Neocognitron introduzida por Kunihiko Fukushima em 1979, embora não ter sido treinada por retropropagação.[44][45] A retropropagação é uma aplicação eficiente da regra sequencial derivada por Gottfried Wilhelm Leibniz em 1673[46] para redes de nós diferenciáveis. A terminologia \"erros de retropropagação\" foi realmente introduzida em 1962 por Rosenblatt,[36] mas ele não sabia como implementar isso, embora Henry J. Kelley tivesse um precursor contínuo da retropropagação em 1960 no contexto da teoria de controle.[47] A forma moderna de retropropagação foi publicada pela primeira vez na tese de mestrado de Seppo Linnainmaa (1970).[48][49][30] G.M. Ostrovski et al. republicaram em 1971.[50][51] Paul Werbos aplicou a retropropagação às redes neurais em 1982[52] (sua tese de doutorado de 1974, reimpressa em um livro de 1994,[53] ainda não descreveu o algoritmo[51]). Em 1986, David E. Rumelhart et al. popularizaram a retropropagação, mas não citaram o trabalho original.[54][55] A rede neural de atraso de tempo (TDNN) foi introduzida em 1987 por Alex Waibel para aplicar rede neural convolucional ao reconhecimento de fonemas. Ela usava convoluções, compartilhamento de peso e retropropagação.[56][57] Em 1988, Wei Zhang aplicou uma rede neural convolucional treinada em retropropagação ao reconhecimento de alfabeto.[58] Em 1989, Yann LeCun et al. criaram uma rede neural convolucional chamada LeNet para reconhecer códigos postais manuscritos em correspondências. O treinamento levou 3 dias.[59] Em 1990, Wei Zhang implementou uma rede neural convolucional em hardware de computação óptica.[60] Em 1991, uma rede neural convolucional foi aplicada à segmentação de objetos de imagens médicas[61] e à detecção de câncer de mama em mamografias.[62] A LeNet-5 (1998), uma rede neural convolucional de 7 níveis de Yann LeCun et al., que classifica dígitos, foi aplicada por vários bancos para reconhecer números manuscritos em cheques digitalizados em imagens de 32x32 pixels.[63] As redes neurais recorrentes (RNNs)[27][29] foram desenvolvidas ainda mais na década de 1980. A recorrência é usada para processamento de sequências e, quando uma rede recorrente é desenrolada, ela se assemelha matematicamente a uma camada de feedforward profunda. Consequentemente, elas têm propriedades e problemas semelhantes, e seus desenvolvimentos tiveram influências mútuas. Em redes neurais recorrentes, dois primeiros trabalhos influentes foram a rede de Jordan (1986)[64] e a rede de Elman (1990),[65] que aplicaram redes neurais recorrentes para estudar problemas em psicologia cognitiva. Na década de 1980, a retropropagação não funcionou bem para aprendizagem profunda com longos caminhos de atribuição de crédito. Para superar esse problema, em 1991, Jürgen Schmidhuber propôs uma hierarquia de redes neurais recorrentes pré-treinadas um nível de cada vez por aprendizado autossupervisionado, onde cada rede neural recorrente tenta prever sua própria próxima entrada, que é a próxima entrada inesperada da rede neural recorrente abaixo.[66][67] Este \"compressor de histórico neural\" usa codificação preditiva para aprender representações internas em múltiplas escalas de tempo auto-organizadas. Isso pode facilitar substancialmente a aprendizagem profunda a jusante. A hierarquia da rede neural recorrente pode ser colapsada em uma única rede neural recorrente, destilando uma rede fragmentadora de nível superior em uma rede automatizadora de nível inferior.[66][67][30] Em 1993, um compressor de histórico neural resolveu uma tarefa de \"aprendizagem muito profunda\" que exigia mais de 1000 camadas subsequentes em uma rede neural recorrente desdobrada no tempo.[68] O \"P\" em ChatGPT se refere a esse pré-treinamento. A tese de diploma de Sepp Hochreiter (1991)[69] implementou o compressor de história neural,[66] e identificou e analisou o problema do gradiente de desaparecimento.[69][70] Hochreiter propôs conexões residuais recorrentes para resolver o problema do gradiente de desaparecimento. Isso levou à memória de curto prazo longa (LSTM), publicada em 1995.[71] A memória de curto prazo longa pode aprender tarefas de \"aprendizagem muito profunda\"[8] com longos caminhos de atribuição de crédito que exigem memórias de eventos que aconteceram milhares de passos de tempo discretos antes. Essa memória de curto prazo longa ainda não era a arquitetura moderna, que exigia uma \"portão de esquecimento\", introduzido em 1999,[72] que se tornou a arquitetura de rede neural recorrente padrão. Em 1991, Jürgen Schmidhuber também publicou redes neurais adversárias que competem entre si na forma de um jogo de soma zero, onde o ganho de uma rede é a perda da outra rede.[73][74] A primeira rede é um modelo generativo que modela uma distribuição de probabilidade sobre padrões de saída. A segunda rede aprende por descida do gradiente para prever as reações do ambiente a esses padrões. Isso foi chamado de \"curiosidade artificial\". Em 2014, esse princípio foi usado em redes adversárias generativas (GANs).[75] Durante 1985–1995, inspirados pela mecânica estatística, várias arquiteturas e métodos foram desenvolvidos por Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., incluindo a máquina de Boltzmann,[76] a máquina de Boltzmann restrita,[77] a máquina de Helmholtz,[78] e o algoritmo de vigília e sono.[79] Eles foram projetados para aprendizado que não é supervisionado de modelos generativos profundos. No entanto, esses eram mais caros computacionalmente em comparação com a retropropagação. O algoritmo de aprendizado de máquina de Boltzmann, publicado em 1985, foi brevemente popular antes de ser eclipsado pelo algoritmo de retropropagação em 1986. (p. 112 [80]). Uma rede de 1988 se tornou o estado da arte na predição das estruturas de proteínas, uma aplicação inicial da aprendizagem profunda à bioinformática.[81] Tanto a aprendizagem superficial quanto a profunda (por exemplo, redes recorrentes) de redes neurais artificiais para reconhecimento de fala têm sido explorados por muitos anos.[82][83][84] Esses métodos nunca superaram a tecnologia do modelo de mistura gaussiana/modelo de Markov oculto (GMM-HMM) manual interno que não é uniforme baseada em modelos generativos de fala treinados discriminativamente.[85] As principais dificuldades foram analisadas, incluindo a diminuição de gradiente[69] e a estrutura de correlação temporal fraca em modelos preditivos neurais.[86][87] Dificuldades adicionais foram a falta de dados de treinamento e poder de computação limitado. A maioria dos pesquisadores de reconhecimento de fala se afastou das redes neurais para buscar modelagem generativa. Uma exceção foi no SRI International no final da década de 1990. Financiado pela NSA e pela DARPA do governo dos EUA, o SRI pesquisou em reconhecimentos de fala e falante. A equipe de reconhecimento de falante liderada por Larry Heck relatou sucesso significativo com redes neurais profundas no processamento de fala no benchmark de reconhecimento de falante do NIST de 1998.[88][89] Foi implantado no Nuance Verifier, representando a primeira grande aplicação industrial da aprendizagem profunda.[90] O princípio de elevar características \"brutas\" sobre otimização manual foi explorado pela primeira vez com sucesso na arquitetura do autocodificador profundo nas características de bancos de filtros lineares ou espectrogramas \"brutos\" no final da década de 1990,[89] mostrando sua superioridade sobre as características Mel-Cepstrais que contêm estágios de transformações fixas a partir de espectrogramas. As características brutas da fala, as formas das ondas, mais tarde produziram excelentes resultados em larga escala.[91] As redes neurais entraram em um período de calmaria, e modelos mais simples que usam recursos artesanais específicos para tarefas, como filtros de Gabor e máquinas de vetores de suporte (SVMs), tornaram-se as escolhas preferidas nas décadas de 1990 e 2000, devido ao custo computacional das redes neurais artificiais e à falta de compreensão de como o cérebro conecta suas redes biológicas. Em 2003, a memória de curto prazo longa tornou-se competitiva com os reconhecedores de fala tradicionais em certas tarefas.[92] Em 2006, Alex Graves, Santiago Fernández, Faustino Gomez e Schmidhuber combinaram-na com a classificação temporal conexionista (CTC)[93] em pilhas de memórias de curto prazo longas.[94] Em 2009, tornou-se a primeira rede neural recorrente a vencer um concurso de reconhecimento de padrões, em reconhecimento de escrita manual conectada.[95][8] Em 2006, publicações de Geoff Hinton, Ruslan Salakhutdinov, Osindero e Teh[96][97] redes de crenças profundas foram desenvolvidas para modelagem generativa. Elas são treinadas treinando uma máquina de Boltzmann restrita, então congelando-a e treinando outra em cima da primeira, e assim por diante, então opcionalmente ajustadas usando retropropagação supervisionada.[98] Elas poderiam modelar distribuições de probabilidade de alta dimensão, como a distribuição de imagens do Banco de dados do Instituto Nacional de Padrões e Tecnologia  modificado (MNIST), mas a convergência era lenta.[99][100][101] O impacto da aprendizagem profunda na indústria começou no início dos anos 2000, quando as redes neurais convolucionais já processavam cerca de 10% a 20% de todos os cheques emitidos nos EUA, de acordo com Yann LeCun.[102] As aplicações industriais da aprendizagem profunda para reconhecimento de fala em larga escala começaram por volta de 2010. A Oficina sobre Sistemas de Processamento de Informações Neurais (NIPS) de 2009 sobre Aprendizagem Profunds para Reconhecimento de Fala foi motivada pelas limitações de modelos generativos profundos de fala e pela possibilidade de que, dado hardware mais capaz e conjuntos de dados em larga escala, as redes neurais profundas pudessem se tornar práticas. Acreditava-se que o pré-treinamento de redes neurais profundas usando modelos generativos de redes de crenças profundas superaria as principais dificuldades das redes neurais. No entanto, descobriu-se que a substituição do pré-treinamento por grandes quantidades de dados de treinamento para retropropagação direta ao usar redes neurais profundas com grandes camadas de saída dependentes do contexto produzia taxas de erros dramaticamente menores do que o modelo de mistura gaussiana/modelo de Markov oculto de então última geração e também do que sistemas baseados em modelos generativos mais avançados.[103] A natureza dos erros de reconhecimento produzidos pelos dois tipos de sistemas era caracteristicamente diferente,[104] oferecendo percepções técnicas sobre como integrar a aprendizagem profunda no sistema de decodificação de fala altamente eficiente e em tempo de execução existente, implantado por todos os principais sistemas de reconhecimento de fala.[22][105][106] A análise por volta de 2009 e 2010, contrastando o modelo de mistura gaussiana (e outros modelos de fala generativa) versus modelos de redes neurais profundas, estimulou o investimento industrial inicial em aprendizagem profunda para reconhecimento de fala.[104] Essa análise foi feita com desempenho comparável (menos de 1,5% na taxa de erros) entre modelos generativos e redes neurais profundas discriminativos.[103][104][107] Em 2010, os pesquisadores estenderam a aprendizagem profunda a partir do TIMIT para o reconhecimento de fala de vocabulário grande, adotando grandes camadas de saída da rede neural profunda com base em estados do modelo de Markov oculto dependentes do contexto construídos por árvores de decisões.[108][109][110][105] A revolução da aprendizagem profunda começou em torno da visão computacional baseada em redes neurais convolucionais e em GPUs. Embora as redes neurais convolucionais treinadas por retropropagação já existissem há décadas e implementações de redes neurais em GPUs há anos,[111] incluindo redes neurais convolucionais,[112] implementações mais rápidas de redes neurais convolucionais em GPUs eram necessárias para progredir na visão computacional. Mais tarde, conforme a aprendizagem profunda se tornou difundido, otimizações de algoritmo e hardware especializados foram desenvolvidos especificamente para aprendizagem profunda.[113] Um avanço fundamental para a revolução da aprendizagem profunda foram os avanços de hardware, especialmente GPUs. Alguns trabalhos iniciais datam de 2004.[111][112] Em 2009, Raina, Madhavan e Andrew Ng relataram uma rede de crenças profundas de 100M treinada em 30 GPUs GeForce GTX 280 da Nvidia, uma demonstração inicial de aprendizagem profunda baseada em GPUs. Eles relataram um treinamento até 70 vezes mais rápido.[114] Em 2011, uma rede neural convolucional chamada DanNet[115][116] por Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella e Jürgen Schmidhuber alcançou pela primeira vez um desempenho sobre-humano em um concurso de reconhecimento de padrões visuais, superando os métodos tradicionais por um fator de 3.[8] Em seguida, ganhou mais concursos.[117][118] Eles também mostraram como o pool máximo de redes neurais convolucionais em GPUs melhorou significativamente o desempenho.[2] Em 2012, Andrew Ng e Jeff Dean criaram uma rede neural feedforward que aprendeu a reconhecer conceitos de nível superior, como gatos, apenas assistindo a imagens que não são rotuladas tiradas de vídeos do YouTube.[119] Em outubro de 2012, AlexNet por Alex Krizhevsky, Ilya Sutskever e Geoffrey Hinton[3] venceu a competição ImageNet em larga escala por uma margem significativa sobre métodos de aprendizado de máquina rasos. Outras melhorias incrementais incluíram a rede VGG-16 por Karen Simonyan e Andrew Zisserman[120] e a Inceptionv3 do Google.[121] O sucesso na classificação de imagens foi então estendido para a tarefa mais desafiadora de gerar descrições (legendas) para imagens, geralmente como uma combinação de redes neurais convolucionais e memórias de curto prazo longas.[122][123][124] Em 2014, o estado da arte era treinar \"rede neural muito profunda\" com 20 a 30 camadas.[125] Empilhar muitas camadas levou a uma redução acentuada na precisão do treinamento,[126] conhecida como problema de \"degradação\".[127] Em 2015, duas técnicas foram desenvolvidas para treinar redes muito profundas: a Highway Network foi publicada em maio de 2015, e a rede neural residual (ResNet)[128] em dezembro de 2015. A ResNet se comporta como uma Highway Net de portas abertas. Na mesma época, a aprendizagem profunda começou a impactar o campo da arte. Os primeiros exemplos incluíram o DeepDream do Google (2015) e a transferência de estilo neural (2015),[129] ambos baseados em redes neurais de classificação de imagens pré-treinadas, como a VGG-19. A rede adversária generativa (GAN) (de Ian Goodfellow et al., 2014)[130] (com base no princípio da curiosidade artificial de Jürgen Schmidhuber[73][75]) tornou-se o estado da arte na modelagem generativa durante o período de 2014 a 2018. Excelente qualidade de imagem é alcançada pela StyleGAN da Nvidia (2018)[131] com base na rede adversária generativa progressiva de Tero Karras et al.[132] Aqui, o gerador da rede adversária generativa é desenvolvido de pequena para grande escala de forma piramidal. A geração de imagens por rede adversária generativa alcançou sucesso popular e provocou discussões sobre deepfakes.[133] Os modelos de difusão (2015)[134] eclipsaram as redes adversárias generativas na modelagem generativa desde então, com sistemas como DALL·E 2 (2022) e Stable Diffusion (2022). Em 2015, o reconhecimento de fala do Google melhorou em 49% por um modelo baseado em memória de curto prazo longa, que eles disponibilizaram por meio do Google Voice Search em smartphones.[135][136] A aprendizagem profunda faz parte de sistemas de última geração em várias disciplinas, particularmente visão computacional e reconhecimento de fala automático (ASR). Os resultados em conjuntos de avaliação comumente usados, como o TIMIT (ASR) e o MNIST (classificação de imagens), bem como uma variedade de tarefas de reconhecimento de fala de vocabulário amplo, melhoraram constantemente.[103][137] As redes neurais convolucionais foram substituídas para reconhecimento automático de fala por memórias de curto prazo longas,[136][138][139][140] mas são mais bem-sucedidas em visão computacional. Yoshua Bengio, Geoffrey Hinton e Yann LeCun receberam o Prêmio Turing de 2018 por \"avanços conceituais e de engenharia que tornaram as redes neurais profundas um componente crítico da computação\".[141] Redes neurais artificiais (ANNs) ou sistemas conexionistas são sistemas de computação inspirados nas redes neurais biológicas que constituem os cérebros dos animais. Tais sistemas aprendem (melhoram progressivamente sua capacidade de) fazer tarefas considerando exemplos, geralmente sem programação específica para a tarefa. Por exemplo, no reconhecimento de imagens, eles podem aprender a identificar imagens que contêm gatos analisando imagens de exemplo que foram rotuladas manualmente como \"gato\" ou \"sem gato\" e usando os resultados analíticos para identificar gatos em outras imagens. Eles encontraram mais uso em aplicações difíceis de expressar com um algoritmo de computador tradicional usando programação baseada em regras. Uma rede neural artificial (RNA) é baseada em uma coleção de unidades conectadas chamadas neurônios artificiais (análogos aos neurônios biológicos em um cérebro biológico). Cada conexão (sinapse) entre neurônios pode transmitir um sinal para outro neurônio. O neurônio receptor (pós-sináptico) pode processar o(s) sinal(ais) e então sinalizar os neurônios a jusante conectados a ele. Os neurônios podem ter estado, geralmente representados por números reais, tipicamente entre 0 e 1. Neurônios e sinapses também podem ter um peso que varia conforme o aprendizado prossegue, o que pode aumentar ou diminuir a força do sinal que ele envia a jusante. Normalmente, os neurônios são organizados em camadas. Camadas diferentes podem executar diferentes tipos de transformações em suas entradas. Os sinais viajam da primeira (entrada) para a última camada (saída), possivelmente após atravessar as camadas várias vezes. O objetivo original da abordagem da rede neural era resolver problemas da mesma forma que um cérebro humano faria. Com o tempo, a atenção se concentrou em combinar habilidades mentais específicas, levando a desvios da biologia, como retropropagação ou passagem de informações na direção reversa e ajuste da rede para refletir essas informações. As redes neurais têm sido usadas em uma variedade de tarefas, incluindo visão computacional, reconhecimento de fala, tradução automática, filtragem de redes sociais, jogos de tabuleiro e vídeo, e diagnóstico médico. Em 2017, as redes neurais normalmente tinham alguns milhares a alguns milhões de unidades e milhões de conexões. Apesar desse número ser várias ordens de magnitude menor do que o número de neurônios em um cérebro humano, essas redes podem executar muitas tarefas em um nível além do dos humanos (por exemplo, reconhecer rostos ou jogar \"Go\"[143]). Uma rede neural profunda (DNN) é uma rede neural artificial com múltiplas camadas entre as camadas de entrada e saída.[6][8] Existem diferentes tipos de redes neurais, mas elas sempre consistem nos mesmos componentes: neurônios, sinapses, pesos, vieses e funções.[144] Esses componentes como um todo funcionam de uma forma que imita funções do cérebro humano e podem ser treinados como qualquer outro algoritmo de aprendizado de máquina. Por exemplo, uma rede neural profunda treinada para reconhecer raças de cães examinará a imagem fornecida e calculará a probabilidade de que o cão na imagem seja de uma determinada raça. O usuário pode revisar os resultados e selecionar quais probabilidades a rede deve exibir (acima de um certo limite, etc.) e retornar o rótulo proposto. Cada manipulação matemática como tal é considerada uma camada,[145] e redes neurais profundas complexas têm muitas camadas, daí o nome redes \"profundas\". As redes neurais profundas podem modelar relacionamentos que não são lineares complexos. As arquiteturas dad redes neurais profundas geram modelos composicionais onde o objeto é expresso como uma composição em camadas de primitivos.[146] As camadas extras permitem a composição de recursos de camadas inferiores, potencialmente modelando dados complexos com menos unidades do que uma rede rasa de desempenho semelhante.[6] Por exemplo, foi provado que polinômios multivariados esparsos são exponencialmente mais fáceis de aproximar com redes neurais profundas do que com redes rasas.[147] As arquiteturas profundas incluem muitas variantes de algumas abordagens básicas. Cada arquitetura obteve sucesso em domínios específicos. Nem sempre é possível comparar o desempenho de múltiplas arquiteturas, a menos que tenham sido avaliadas nos mesmos conjuntos de dados.[145] As redes neurais profundas são tipicamente redes feedforward nas quais os dados fluem da camada de entrada para a camada de saída sem fazer loopback. Primeiramente, a rede neural profunda cria um mapa de neurônios virtuais e atribui valores numéricos aleatórios, ou \"pesos\", às conexões entre eles. Os pesos e entradas são multiplicados e retornam uma saída entre 0 e 1. Se a rede não reconhecesse com precisão um padrão específico, um algoritmo ajustaria os pesos.[148] Dessa forma, o algoritmo pode tornar certos parâmetros mais influentes, até determinar a manipulação matemática correta para processar completamente os dados. As redes neurais recorrentes, nas quais os dados podem fluir em qualquer direção, são usadas para aplicações como modelagem de linguagem.[149][150][151][152][153] A memória de curto prazo longa é particularmente eficaz para esse uso.[154][155] As redes neurais convolucionais (CNNs) são usadas em visão computacional.[156] As redes neurais convolucionais também foram aplicadas à modelagem acústica para reconhecimento automático de fala (ASR).[157] Assim como com redes neurais artificiais, muitos problemas podem surgir com redes neurais profundas treinadas ingenuamente. Dois problemas comuns são o ajuste excessivo e o tempo de computação. As redes neurais profundas são propensas a ajuste excessivo por causa das camadas de abstração adicionadas, que permitem que elas modelem dependências raras nos dados de treinamento. Métodos de regularização como a poda de unidade de Ivakhnenko[40] ou o decaimento de peso (regularização de \n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n) ou esparsidade (regularização de \n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n) podem ser aplicados durante o treinamento para combater ajuste excessivo.[158] Alternativamente, a regularização de abandono omite aleatoriamente unidades das camadas ocultas durante o treinamento. Isso ajuda a excluir dependências raras.[159] Outro desenvolvimento recente interessante é a pesquisa em modelos de complexidade suficiente por meio de uma estimativa da complexidade intrínseca da tarefa que está sendo modelada. Essa abordagem foi aplicada com sucesso para tarefas de previsão de séries temporais multivariadas, como previsão de tráfego.[160] Finalmente, os dados podem ser aumentados por meio de métodos como corte e rotação, de modo que conjuntos de treinamento menores possam ser aumentados em tamanho para reduzir as chances de ajuste excessivo.[161] As redes neurais profundas devem considerar muitos parâmetros de treinamento, como o tamanho (número de camadas e número de unidades por camada), a taxa de aprendizado e pesos iniciais. Varrer o espaço de parâmetros para parâmetros ideais pode não ser viável devido ao custo em tempo e recursos computacionais. Vários truques, como loteamento (computação do gradiente em vários exemplos de treinamento de uma vez em vez de exemplos individuais)[162] aceleram a computação. Grandes capacidades de processamento de arquiteturas de muitos núcleos (como GPUs ou o Intel Xeon Phi) produziram acelerações significativas no treinamento, devido à adequação de tais arquiteturas de processamento para as computações de matriz e vetor.[163][164] Alternativamente, os engenheiros podem procurar outros tipos de redes neurais com algoritmos de treinamento mais diretos e convergentes. A controladora de articulação de modelo cerebelar (CMAC) é uma desses tipos de rede neural. Ela não requer taxas de aprendizado ou pesos iniciais randomizados. O processo de treinamento pode ser garantido para convergir em uma etapa com um novo lote de dados, e a complexidade computacional do algoritmo de treinamento é linear em relação ao número de neurônios envolvidos.[165][166] Desde a década de 2010, os avanços em algoritmos de aprendizado de máquina e hardware de computador levaram a métodos mais eficientes para treinar redes neurais profundas que contêm muitas camadas de unidades ocultas que não são lineares e uma camada de saída muito grande.[167] Em 2019, unidades de processamento gráfico (GPUs), geralmente com aprimoramentos específicos de IA, substituíram as CPUs como o método dominante para treinar IA de nuvem comercial em larga escala.[168] A OpenAI estimou a computação de hardware usada nos maiores projetos de aprendizagem profunda de AlexNet (2012) a AlphaZero (2017) e encontrou um aumento de 300.000 vezes na quantidade de computação necessária, com uma linha de tendência de tempo de duplicação de 3,4 meses.[169][170] Circuitos eletrônicos especiais chamados processadores de aprendizagem profunda foram projetados para acelerar algoritmos de aprendizagem profunda. Os processadores de aprendizagem profunda incluem unidades de processamento neural (NPUs) em celulares Huawei[171] e servidores de computação em nuvem, como unidades de processamento tensorial (TPU) na Google Cloud Platform.[172] A Cerebras Systems também construiu um sistema dedicado para lidar com grandes modelos de aprendizagem profunda, o CS-2, baseado no maior processador do setor, o Wafer Scale Engine de segunda geração (WSE-2).[173][174] Semicondutores atomicamente finos são considerados promissores para hardware de aprendizagem profunda com eficiência energética, onde a mesma estrutura básica do dispositivo é usada para operações lógicas e armazenamento de dados. Em 2020, Marega et al. publicaram experimentos com um material de canal ativo de grande área para desenvolver dispositivos e circuitos lógicos na memória baseados em transistores de efeito de campo de porta flutuante (FGFETs).[175] Em 2021, J. Feldmann et al. propuseram um acelerador de hardware fotônico integrado para processamento convolucional paralelo.[176] Os autores identificam duas vantagens principais da fotônica integrada sobre suas contrapartes eletrônicas: (1) transferência de dados massivamente paralela por meio de multiplexação por divisão de comprimento de onda em conjunto com pentes de frequência e (2) velocidades de modulação de dados extremamente altas.[176] Seu sistema pode executar trilhões de operações de multiplicação e acumulação por segundo, indicando o potencial da fotônica integrada em aplicações de IA com muitos dados.[176] O reconhecimento automático de fala em larga escala é o primeiro e mais convincente caso bem-sucedido de aprendizado profundo. Redes neurais recorrentes de memórias de curto prazo longas podem aprender tarefas de \"Aprendizado Muito Profundo\"[8] que envolvem intervalos de vários segundos contendo eventos de fala separados por milhares de passos de tempo discretos, onde um passo de tempo corresponde a cerca de 10 milissegundos. Memórias de curto prazo longas com portas de esquecimento[155] são competitivas com reconhecedores de fala tradicionais em certas tarefas.[92] O sucesso inicial no reconhecimento de fala foi baseado em tarefas de reconhecimento em pequena escala baseadas no TIMIT. O conjunto de dados contém 630 falantes de oito dialetos principais do inglês americano, onde cada falante lê 10 frases.[177] Seu pequeno tamanho permite que muitas configurações sejam tentadas. Mais importante, a tarefa TIMIT diz respeito ao reconhecimento de sequência de fonemas, que, diferentemente do reconhecimento de sequência de palavras, permite modelos de linguagem de bigramas de fonemas fracos. Isso permite que a força dos aspectos de modelagem acústica do reconhecimento de fala seja mais facilmente analisada. As taxas de erro listadas abaixo, incluindo esses resultados iniciais e medidas como taxas de erro de fonemas percentuais (PER), foram resumidas desde 1991. A estreia das redes neurais profundas para reconhecimento de falantes no final da década de 1990 e do reconhecimento de fala por volta de 2009 a 2011 e da memória de curto prazo longa por volta de 2003 a 2007, acelerou o progresso em oito áreas principais:[22][107][105] Todos os principais sistemas comerciais de reconhecimento de fala (por exemplo, Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu e pesquisa por voz da iFlyTek e uma variedade de produtos de fala da Nuance, etc.) são baseados em aprendizado profundo.[22][185][186] Um conjunto de avaliação comum para classificação de imagens é o conjunto de dados do banco de dados do MNIST. O MNIST é composto de dígitos manuscritos e inclui 60.000 exemplos de treinamento e 10.000 exemplos de teste. Assim como o TIMIT, seu tamanho pequeno permite que os usuários testem várias configurações. Uma lista abrangente de resultados neste conjunto está disponível.[187] O reconhecimento de imagens baseado em aprendizagem profunda se tornou \"super-humano\", produzindo resultados mais precisos do que concorrentes humanos. Isso ocorreu pela primeira vez em 2011 no reconhecimento de sinais de trânsito e, em 2014, com o reconhecimento de rostos humanos.[188][189] Veículos treinados em aprendizado profundo agora interpretam visualizações de câmera de 360°.[190] Outro exemplo é a Análise de Novas Dismorfologias Faciais (FDNA), usada para analisar casos de malformações humanas conectados a um grande banco de dados de síndromes genéticas. Estreitamente relacionada ao progresso que foi feito no reconhecimento de imagens está a aplicação crescente de técnicas de aprendizado profundo a várias tarefas de artes visuais. As redes neurais profundas provaram ser capazes, por exemplo, de As redes neurais têm sido usadas para implementar modelos de linguagem desde o início dos anos 2000.[149] A memória de curto prazo longa ajudou a melhorar a tradução automática e a modelagem de linguagem.[150][151][152] Outras técnicas-chave neste campo são a amostragem negativa[193] e a incorporação de palavras. A incorporação de palavras, como word2vec, pode ser considerada uma camada representacional em uma arquitetura de aprendizagem profunda que transforma uma palavra atômica em uma representação posicional da palavra em relação a outras palavras no conjunto de dados; a posição é representada como um ponto em um espaço vetorial. Usar a incorporação de palavras como uma camada de entrada da rede neural recorrente permite que a rede analise sentenças e frases usando uma gramática vetorial composicional eficaz. Uma gramática vetorial composicional pode ser considerada uma gramática livre de contexto probabilística (PCFG) implementada por uma rede neural recorrente.[194] Os autocodificadores recursivos construídos sobre incorporações de palavras podem avaliar a similaridade de sentenças e detectar paráfrases.[194] Arquiteturas neurais profundas fornecem os melhores resultados para análise de constituintes,[195] análise de sentimentos,[196] recuperação de informações,[197][198] compreensão de linguagem falada,[199] tradução automática,[150][200] vinculação de entidades contextuais,[200] reconhecimento de estilo de escrita,[201] reconhecimento de entidades nomeadas (classificação de tokens),[202] classificação de texto e outros.[203] Desenvolvimentos recentes generalizam a incorporação de palavras para a incorporação de frases. O Google Translate (GT) usa uma grande rede de memória de curto prazo longa (LSTM) de ponta a ponta.[204][205][206][207] A Google Neural Machine Translation (GNMT) usa um método de tradução automática baseada em exemplos no qual o sistema \"aprende com milhões de exemplos\".[205] Ela traduz \"frases inteiras de uma vez, em vez de partes\". O Google Translate oferece suporte a mais de cem idiomas.[205] A rede codifica a \"semântica da frase em vez de simplesmente memorizar traduções frase a frase\".[205][208] O Google Translate usa o inglês como intermediário entre a maioria dos pares de idiomas.[208] Uma grande porcentagem de medicamentos candidatos não consegue obter aprovação regulatória. Essas falhas são causadas por eficácia insuficiente (efeito no alvo), interações indesejadas (efeitos fora do alvo) ou efeitos tóxicos imprevistos.[209][210] A pesquisa explorou o uso de aprendizagem profunda para prever os alvos biomoleculares,[211][212] alvos fora do alvo e efeitos tóxicos de produtos químicos ambientais em nutrientes, produtos domésticos e medicamentos.[213][214][215] A AtomNet é um sistema de aprendizagem profunda para design de medicamentos racional baseado em estrutura.[216] A AtomNet foi usada para prever novas biomoléculas candidatas para alvos de doenças como o vírus Ebola[217] e esclerose múltipla.[218][217] Em 2017, redes neurais de grafos foram usadas pela primeira vez para prever várias propriedades de moléculas em um grande conjunto de dados de toxicologia.[219] Em 2019, redes neurais generativas foram usadas para produzir moléculas que foram validadas experimentalmente em camundongos.[220][221] A aprendizagem de reforço profunda tem sido usada para aproximar o valor de possíveis ações de marketing direto, definidas em termos de variáveis de recenticidade, frequência, e valor monetário (RFM). A função de valor estimado demonstrou ter uma interpretação natural como valor de vida útil do cliente.[222] Os sistemas de recomendações têm usado o aprendizagem profunda para extrair recursos significativos para um modelo de fator latente para recomendações de músicas e periódicos baseadas em conteúdo.[223][224] A aprendizagem profunda multivisualização foi aplicada para aprender as preferências dos usuários de vários domínios.[225] O modelo usa uma abordagem híbrida colaborativa e baseada em conteúdo e aprimora as recomendações em várias tarefas. Uma rede neural artificial autocodificadora foi usada em bioinformática para prever relações entre funções genéticas e anotações de ontologia genética.[226] Na informática médica, a aprendizagem profunda foi usada para prever a qualidade do sono com base em dados de wearables[227] e previsões de complicações de saúde a partir de dados de registros de saúde eletrônicos.[228] Redes neurais profundas têm mostrado desempenho incomparável na previsão de estruturas de proteínas, de acordo com a sequência dos aminoácidos que as compõem. Em 2020, a AlphaFold, um sistema baseado em aprendizagem profunda, atingiu um nível de precisão significativamente maior do que todos os métodos computacionais anteriores.[229][230] Redes neurais profundas podem ser usadas para estimar a entropia de um processo estocástico e chamadas de estimadora de entropia conjunta neural (NJEE).[231] Tal estimativa fornece percepções sobre os efeitos de variáveis aleatórias de entrada em uma variável aleatória independente. Praticamente, a rede neural profunda é treinada como uma classificadora que mapeia um vetor ou uma matriz X de entrada para uma distribuição de probabilidade de saída sobre as classes possíveis da variável aleatória Y, dada a entrada X. Por exemplo, em tarefas de classificação de imagens, a estimadora de entropia conjunta neural mapeia um vetor de valores de cores de pixels para probabilidades sobre possíveis classes de imagens. Na prática, a distribuição de probabilidade de Y é obtida por uma camada de Softmax com número de nós que é igual ao tamanho do alfabeto de Y. A estimadora de entropia conjunta neural usa funções de ativação continuamente diferenciáveis, de modo que as condições para o teorema de aproximação universal sejam mantidas. É mostrado que este método fornece uma estimadora consistente fortemente e supera outros métodos no caso de tamanhos de alfabetos grandes.[231] Foi demonstrado que a aprendizagem profunda produz resultados competitivos em aplicações médicas, como classificação de células cancerígenas, detecção de lesões, segmentação de órgãos e aprimoramento de imagens.[232][233] As ferramentas modernas de aprendizagem profunda demonstram a alta precisão na detecção de várias doenças e a utilidade de seu uso por especialistas para melhorar a eficiência do diagnóstico.[234][235] Encontrar o público móvel apropriado para publicidade móvel é sempre desafiador, pois muitos pontos de dados devem ser considerados e analisados antes que um segmento alvo possa ser criado e usado na veiculação de anúncios por qualquer servidor de anúncios.[236] A aprendizagem profunda tem sido usada para interpretar grandes conjuntos de dados de publicidade multidimensionais. Muitos pontos de dados são coletados durante o ciclo de solicitação/veiculação/clique de publicidade na Internet. Essas informações podem formar a base do aprendizado de máquina para melhorar a seleção de anúncios. A aprendizagem profunda foi aplicada com sucesso a problemas inversos, como redução de ruído, super-resolução, pintura interna e colorização de filmes.[237] Essas aplicações incluem métodos de aprendizado como \"campos de encolhimento para restauração eficaz de imagens\"[238], que treina em um conjunto de dados de imagens, e Deep Image Prior, que treina na imagem que precisa de restauração. A aprendizagem profunda está sendo aplicada com sucesso à detecção de fraudes financeiras, à detecção de evasão fiscal[239] e ao combate à lavagem de dinheiro.[240] Em novembro de 2023, pesquisadores da Google DeepMind e do Lawrence Berkeley National Laboratory anunciaram que desenvolveram um sistema de IA conhecido como GNoME. Este sistema contribuiu para a ciência dos materiais ao descobrir mais de 2 milhões de novos materiais em um período de tempo relativamente curto. O GNoME emprega técnicas de aprendizagem profunda para explorar com eficiência potenciais estruturas de materiais, alcançando um aumento significativo na identificação de estruturas cristalinas inorgânicas estáveis. As previsões do sistema foram validadas por meio de experimentos robóticos autônomos, demonstrando uma notável taxa de sucesso de 71%. Os dados de materiais recém-descobertos estão disponíveis publicamente por meio do banco de dados do Materials Project, oferecendo aos pesquisadores a oportunidade de identificar materiais com propriedades desejadas para várias aplicações. Este desenvolvimento tem implicações para o futuro da descoberta científica e a integração da IA na pesquisa de ciência dos materiais, potencialmente agilizando a inovação de materiais e reduzindo custos no desenvolvimento de produtos. O uso de IA e aprendizagem profunda sugere a possibilidade de minimizar ou eliminar experimentos manuais de laboratório e permitir que os cientistas se concentrem mais no design e na análise de compostos exclusivos.[241][242][243] O Departamento de Defesa dos Estados Unidos aplicou o aprendizagem profunda para treinar robôs em novas tarefas por meio da observação.[244] Redes neurais informadas por física têm sido usadas para resolver equações diferenciais parciais em problemas diretos e inversos de uma maneira orientada por dados.[245] Um exemplo é a reconstrução do fluxo de fluido governado pelas equações de Navier e Stokes. Usar redes neurais informadas por física não requer a geração de malha frequentemente cara da qual os métodos de fluidodinâmica computacional (CFD) convencionais dependem.[246][247] O método de equação diferencial estocástica regressiva profunda é um método numérico que combina aprendizagem profunda com equação diferencial estocástica regressiva (BSDE). Este método é particularmente útil para resolver problemas de alta dimensão em matemática financeira. Ao alavancar os poderosos recursos de aproximação de função de redes neurais profundas, a equação diferencial estocástica regressiva profunda aborda os desafios computacionais enfrentados por métodos numéricos tradicionais em configurações de altas dimensões. Especificamente, métodos tradicionais como métodos de diferença finita ou simulações de Monte Carlo frequentemente lutam com a maldição da dimensionalidade, onde o custo computacional aumenta exponencialmente com o número de dimensões. Os métodos de equação diferencial estocástica regressiva profunda, no entanto, empregam redes neurais profundas para aproximar soluções de equações diferenciais parciais (PDEs) de altas dimensões, reduzindo efetivamente a carga computacional.[248] Além disso, a integração de redes neurais informadas por física (PINNs) na estrutura de equação diferencial estocástica regressiva profunda aumenta sua capacidade ao incorporar as leis físicas subjacentes diretamente na arquitetura da rede neural. Isso garante que as soluções não apenas se ajustem aos dados, mas também adiram às equações diferenciais estocásticas governantes. As redes neurais informadas por física alavancam o poder da aprendizagem profunda, respeitando as restrições impostas pelos modelos físicos, resultando em soluções mais precisas e confiáveis para problemas de matemática financeira. A reconstrução de imagens é a reconstrução das imagens subjacentes a partir das medições relacionadas às imagens. Vários trabalhos mostraram o desempenho melhor e superior dos métodos de aprendizagem profunda em comparação aos métodos analíticos para várias aplicações, por exemplo, imagens espectrais [249] e imagens de ultrassom.[250] Os sistemas tradicionais de previsão do tempo resolvem um sistema muito complexo de equações diferenciais parciais. O GraphCast é um modelo baseado em aprendizagem profunda, treinado em um longo histórico de dados meteorológicos para prever como os padrões climáticos mudam ao longo do tempo. Ele é capaz de prever as condições climáticas por até 10 dias globalmente, em um nível muito detalhado e em menos de um minuto, com precisão semelhante aos sistemas de última geração.[251][252] Um relógio epigenético é um teste bioquímico que pode ser usado para medir a idade. Galkin et al. usaram redes neurais profundas para treinar um relógio de envelhecimento epigenético de precisão sem precedentes usando mais de 6.000 amostras de sangue.[253] O relógio usa informações de 1000 sítios CpG e prevê pessoas com certas condições mais velhas do que controles saudáveis: doença inflamatória intestinal (DII), demência frontotemporal, câncer ovariano, obesidade. O relógio de envelhecimento foi planejado para ser lançado para uso público em 2021 por uma empresa subproduto da Insilico Medicine, a Deep Longevity. A aprendizagem profunda está intimamente relacionado a uma classe de teorias de desenvolvimento cerebral (especificamente, desenvolvimento neocortical) propostas por neurocientistas cognitivos no início da década de 1990.[254][255][256][257] Essas teorias de desenvolvimento foram instanciadas em modelos computacionais, tornando-as predecessoras de sistemas de aprendizagem profunda. Esses modelos de desenvolvimento compartilham a propriedade de que várias dinâmicas de aprendizado propostas no cérebro (por exemplo, uma onda de fator de crescimento nervoso) suportam a auto-organização de forma um tanto análoga às redes neurais utilizadas em modelos de aprendizagem profunda. Como o neocórtex, as redes neurais empregam uma hierarquia de filtros em camadas em que cada camada considera informações de uma camada anterior (ou do ambiente operacional) e, em seguida, passa sua saída (e possivelmente a entrada original) para outras camadas. Esse processo produz uma pilha auto-organizada de transdutores, bem ajustada ao seu ambiente operacional. Uma descrição de 1995 declarou: \"...o cérebro do bebê parece se organizar sob a influência de ondas dos chamados fatores tróficos... diferentes regiões do cérebro se conectam sequencialmente, com uma camada de tecido amadurecendo antes da outra e assim por diante até que todo o cérebro esteja maduro\".[258] Uma variedade de abordagens tem sido usada para investigar a plausibilidade de modelos de aprendizagem profunda de uma perspectiva neurobiológica. Por um lado, várias variantes do algoritmo de retropropagação foram propostas para aumentar seu realismo de processamento.[259][260] Outros pesquisadores argumentaram que formas não supervisionadas de aprendizagem profunda, como aquelas baseadas em modelos generativos hierárquicos e redes de crenças profundas, podem estar mais próximas da realidade biológica.[261][262] A esse respeito, modelos de rede neural generativa têm sido relacionados a evidências neurobiológicas sobre processamento baseado em amostragem no córtex cerebral.[263] Embora uma comparação sistemática entre a organização do cérebro humano e a codificação neuronal em redes profundas ainda não tenha sido estabelecida, várias analogias foram relatadas. Por exemplo, as computações realizadas por unidades de aprendizagem profunda podem ser semelhantes às de neurônios reais[264] e populações neurais.[265] Da mesma forma, as representações desenvolvidas por modelos de aprendizagem profunda são semelhantes às medidas no sistema visual de primatas[266] tanto nos níveis de unidade única[267] quanto nos níveis de população.[268] O laboratório de IA do Facebook realiza tarefas como marcar automaticamente fotos carregadas com os nomes das pessoas nelas.[269] A DeepMind Technologies do Google desenvolveu um sistema capaz de aprender a jogar videogames Atari usando apenas pixels como entrada de dados. Em 2015, eles demonstraram seu sistema AlphaGo, que aprendeu o jogo Go bem o suficiente para vencer um jogador profissional de Go.[270][271][272] O Google Translate usa uma rede neural para traduzir entre mais de 100 idiomas. Em 2017, foi lançado o Covariant.ai, que se concentra na integração da aprendizagem profunda em fábricas.[273] Em 2008,[274] pesquisadores da Universidade do Texas em Austin (UT) desenvolveram uma estrutura de aprendizado de máquina chamada Treinando um Agente Manualmente via Reforço Avaliativo, ou TAMER, que propôs novos métodos para robôs ou programas de computador aprenderem a executar tarefas interagindo com um instrutor humano.[244] Desenvolvido inicialmente como TAMER, um novo algoritmo chamado Deep TAMER foi posteriormente introduzido em 2018 durante uma colaboração entre o Laboratório de Pesquisa do Exército dos EUA (ARL) e pesquisadores da UT. O Deep TAMER usou a aprendizagem profunda para fornecer a um robô a capacidade de aprender novas tarefas por meio da observação.[244] Usando o Deep TAMER, um robô aprendeu uma tarefa com um treinador humano, assistindo a transmissões de vídeo ou observando um humano executar uma tarefa pessoalmente. O robô posteriormente praticou a tarefa com a ajuda de algum treinamento do treinador, que forneceu feedback como \"bom trabalho\" e \"trabalho ruim\".[275] A aprendizagem profunda atraiu críticas e comentários, em alguns casos de fora do campo da ciência da computação. Uma crítica principal diz respeito à falta de teoria em torno de alguns métodos.[276] A aprendizagem nas arquiteturas profundas mais comuns é implementada usando a descida de gradiente bem compreendida. No entanto, a teoria em torno de outros algoritmos, como a divergência contrastiva, é menos clara. (por exemplo, Converge? Se sim, quão rápido? O que está se aproximando?) Os métodos de aprendizagem profunda são frequentemente vistos como uma caixa-preta, com a maioria das confirmações feitas empiricamente, em vez de teoricamente.[277] Em referência adicional à ideia de que a sensibilidade artística pode ser inerente a níveis relativamente baixos da hierarquia cognitiva, uma série publicada de representações gráficas dos estados internos de redes neurais profundas (20-30 camadas) tentando discernir dentro de dados essencialmente aleatórios as imagens nas quais foram treinadas[278] demonstra um apelo visual: o aviso de pesquisa original recebeu bem mais de 1.000 comentários e foi o assunto do que foi por um tempo o artigo mais acessado no site do The Guardian.[279] Além disso, alguns pesquisadores argumentam que arquiteturas diferenciáveis e funções de perda padrão em aprendizagem profunda podem limitar a descoberta de mecanismos generativos ou  causais mais profundos.[280] Com base na Teoria da Informação Algorítmica (AIT), Hernández e Orozco et al. (2021)[281] propuseram uma função de perda algorítmica para medir a discrepância entre o comportamento previsto e observado do sistema. Sua abordagem integra a Teoria da Informação Algorítmica (AIT) com aprendizado de máquina para formular uma estrutura para aprender regras generativas em espaços que não são diferenciáveis, conectando a teoria algorítmica discreta com técnicas de otimização contínua. Essa estrutura fornece uma nova perspectiva sobre interpretabilidade de modelos e generalização ao fundamentar a dinâmica da aprendizagem na complexidade algorítmica.[282][283] Algumas arquiteturas de aprendizagem profunda apresentam comportamentos problemáticos,[284] como classificar com segurança imagens irreconhecíveis como pertencentes a uma categoria familiar de imagens comuns (2014)[285] e classificar incorretamente perturbações minúsculas de imagens classificadas corretamente (2013).[286] Goertzel levantou a hipótese de que esses comportamentos são devidos a limitações em suas representações internas e que essas limitações inibiriam a integração em arquiteturas heterogêneas de inteligência artificial geral (AGI) multicomponente.[284] Essas questões podem ser possivelmente abordadas por arquiteturas de aprendizagem profunda que formam internamente estados homólogos às decomposições de gramática de imagem[287] de entidades e eventos observados.[284] Aprender uma gramática (visual ou linguística) a partir de dados de treinamento seria equivalente a restringir o sistema ao raciocínio de senso comum que opera em conceitos em termos de regras de produção gramatical e é um objetivo básico tanto da aquisição da linguagem humana[288] quanto da inteligência artificial (IA).[289] À medida que a aprendizagem profunda sai do laboratório para o mundo, pesquisas e experiências mostram que redes neurais artificiais são vulneráveis a hacks e enganos.[290] Ao identificar os padrões que esses sistemas usam para funcionar, os invasores podem modificar entradas para as redes neurais artificiais de tal forma que a rede neural artificial encontre uma correspondência que observadores humanos não reconheceriam. Por exemplo, um invasor pode fazer mudanças sutis em uma imagem de tal forma que a rede neural artificial encontre uma correspondência, mesmo que a imagem não pareça nada com o alvo da pesquisa para um humano. Tal manipulação é denominada \"ataque adversário\".[291] Em 2016, pesquisadores usaram uma rede neural artificial para manipular imagens por tentativa e erro, identificar os pontos focais de outra e, assim, gerar imagens que a enganassem. As imagens modificadas não pareciam diferentes aos olhos humanos. Outro grupo mostrou que impressões de imagens manipuladas e depois fotografadas enganaram com sucesso um sistema de classificação de imagens.[292] Uma defesa é a busca reversa de imagens, na qual uma possível imagem falsa é enviada a um site como o TinEye, que pode então encontrar outras instâncias dela. Um refinamento é pesquisar usando apenas partes da imagem, para identificar imagens das quais aquela parte pode ter sido retirada.[293] Outro grupo mostrou que certos espetáculos psicodélicos poderiam enganar um sistema de reconhecimento facial, fazendo-o pensar que pessoas comuns eram celebridades, potencialmente permitindo que uma pessoa se passasse por outra. Em 2017, pesquisadores adicionaram adesivos a placas de pare e fizeram com que uma rede neural artificial as classificasse incorretamente.[292] As redes neurais artificiais podem, no entanto, ser mais treinadas para detectar tentativas de engano, potencialmente levando atacantes e defensores a uma corrida armamentista semelhante ao tipo que já define a indústria de defesa contra malware. As redes neurais artificiais foram treinadas para derrotar software antimalware baseado em rede neural artificial, atacando repetidamente uma defesa com malware que foi continuamente alterado por um algoritmo genético até enganar o antimalware, mantendo sua capacidade de danificar o alvo.[292] Em 2016, outro grupo demonstrou que certos sons poderiam fazer o sistema de comando de voz do Google Now abrir um endereço da web específico e levantou a hipótese de que isso poderia \"servir como um trampolim para novos ataques (por exemplo, abrir uma página da web que hospeda malware drive-by)\".[292] No \"envenenamento de dados\", dados falsos são continuamente contrabandeados para o conjunto de treinamento de um sistema de aprendizado de máquina para evitar que ele alcance a maestria.[292] Os sistemas de aprendizagem profunda que são treinados usando aprendizagem supervisionada geralmente dependem de dados criados ou anotados por humanos, ou ambos.[294] Foi argumentado que não apenas o trabalho de clique mal pago (como no Amazon Mechanical Turk) é regularmente implantado para esse propósito, mas também formas implícitas de microtrabalho humano que muitas vezes não são reconhecidas como tal.[295] O filósofo Rainer Mühlhoff distingue cinco tipos de \"captura maquínica\" do microtrabalho humano para gerar dados de treinamento: (1) gamificação (a incorporação de tarefas de anotação ou computação no fluxo de um jogo), (2) \"captura e rastreamento\" (por exemplo, CAPTCHAs para reconhecimento de imagem ou rastreamento de cliques nas páginas de resultados de pesquisas do Google), (3) exploração de motivações sociais (por exemplo, marcar rostos no Facebook para obter imagens faciais rotuladas), (4) mineração de informações (por exemplo, alavancando dispositivos de autoquantificação, como rastreadores de atividade) e (5) trabalho de clique.[295]"
  },
  {
    "url": "https://kaa.wikipedia.org/wiki/Tere%C5%84_oq%C4%B1t%C4%B1w",
    "title": "Tereń oqıtıw — Qaraqalpaqsha Wikipedia",
    "content": "Tereń oqıtıw, (ingl. Deep Learning) kóbinese Tereńletip oqıtıw dep te ataladı, neyronlardı yamasa neyronlar qatlamların qollanıp, gipoteza keńisligin hám usı keńislikten úyreniw usılın anıqlaw múmkinshiligin beredi, olar ierarxiyalıq strukturanı payda etedi. Bul modelge aldınnan belgilengen operaciyalar toplamına salıstırǵanda kóbirek iykemlilik beredi, quramalı úlgiler hám úlken kólemli maǵlıwmatlardı nátiyjeli qayta islewge múmkinshilik beredi. Tereń oqıtıw — bul usınıs úyreniw ushın neyron tarmaqlardı qollanatuǵın mashinalıq oqıtıw usıllarınıń bir bólegi. Bul taraw biologiyalıq neyroilimnen ilham aladı hám jasalma neyronlardı qatlamlarǵa jaylastırıw hám olardı maǵlıwmatlardı qayta islewge \"úyretiw\" átirapında qurılǵan. \"Tereń\" sózi tarmaqta kóp qatlamlardı (úshten bir neshe júz yamasa mıńǵa shekem) qollanıwdı bildiredi. Qollanılatuǵın usıllar baqlanatuǵın, yarım-baqlanatuǵın yamasa baqlanbaytuǵın bolıwı múmkin[2]. Geypara keń tarqalǵan tereń oqıtıw tarmaq arxitekturalarına tolıq baylanısqan tarmaqlar, tereń isenim tarmaqları, qaytalanatuǵın neyron tarmaqları, konvolyuciyalıq neyron tarmaqları, generativlik qarama-qarsı tarmaqlar, transformatorlar hám neyron radiaciya maydanları kiredi. Bul arxitekturalar kompyuter kóriwi, sóylewdi tanıw, tábiyiy tildi qayta islew, mashina awdarması, bioinformatika, dári dizaynı, medicinаlıq súwret analizi, klimat ilimi, material tekseriw hám taxta oyınları programmaları sıyaqlı tarawlarda qollanılǵan, olar adam ekspert dárejesine teń hám geypara jaǵdaylarda onnan da joqarı nátiyjeler bergen[3][4][5]. Neyron tarmaqlarınıń dáslepki formaları biologiyalıq sistemalardaǵı, ásirese adam miyindegi maǵlıwmattı qayta islew hám tarqatılǵan baylanıs noqatlarınan ilham alǵan. Biraq, házirgi neyron tarmaqları organizmlerdiń miy funkciyasın modellestiriwdi niyet etpeydi hám bul maqset ushın ulıwma tómen sapalı modeller dep esaplanadı[6]. Kópshilik házirgi tereń oqıtıw modelleri konvolyuciyalıq neyron tarmaqları hám transformatorlar sıyaqlı kóp qatlamlı neyron tarmaqlarına tiykarlanǵan, biraq olar tereń isenim tarmaqları hám tereń Bolcman mashinalarındaǵı túyinler sıyaqlı tereń generativ modellerde qatlam boyınsha shólkemlestiriliwi múmkin bolǵan propoziciyalıq formulalardı yamasa latent ózgeriwshilerdi de óz ishine alıwı múmkin[7]. Negizinde, tereń oqıtıw - bul kiris maǵlıwmatlardı azǵana abstrakt hám quramalı kórsetiwge ózgertiwge qatlamlar ierarxiyası qollanılatuǵın mashinalıq oqıtıw algoritmleriniń klassı. Mısalı, súwretti tanıw modelinde, hám kiris súwret (piksellerdiń tenzorı retinde kórsetilgen) bolıwı múmkin. Birinshi kórsetiw qatlamı sızıqlar hám sheńberler sıyaqlı tiykarǵı formаlardı anıqlawǵa háreket etiw múmkin, ekinshi qatlam sheklerdiń jaylasıwın payda etedi hám kodlaydı, úshinshi qatlam murın hám kózlerdi kodlaw múmkin, al tórtinshi qatlam súwrette júz bar ekenin tanıw múmkin. Áhmiyetlisi, tereń oqıtıw processi qaysı ózgesheliklerdi qaysı dárejede optimal jaylastırıwdı ózi úyrene aladı. Tereń oqıtıwǵa deyin, maǵlıwmatlardı klassifikaciya algoritmi islewi ushın qolaylıraq kórsetiwge ózgertiw kóp jaǵdaylarda qolda isleylenetuǵın ózgeshelikler injeneringіn talap etetuǵın edi. Tereń oqıtıw usılında ózgeshelikler qolda islenbeydi hám model maǵlıwmatlardan paydalı ózgeshelik kórsetiwlerin avtomatik túrde tabadı. Bul qolda sazlawdıń kerek emes ekenin bildirmeydi; mısalı, qatlamlar sanınıń hám qatlam ólshemleriniń ózgeriwi abstrakciyalanıwdıń hár qıylı dárejelerin beriwi múmkin[8][2]. \"Tereń oqıtıw\" termininiń \"tereń\" sózi maǵlıwmatlar ózgeretuǵın qatlamlar sanın bildiredi. Anıǵıraq aytqanda, tereń oqıtıw sistemalarında áhmiyetli kredit belgilew jolı (KBJ) (ingl. credit assignment path, CAP) tereńligi bar. KBJ - bul kiristen shıǵısqa shekem bolǵan ózgerisler dizbegi. KBJlar kiris penen shıǵıs arasındaǵı potencial sebepli baylanıslardı súwretleydi. Aldıǵa baǵdarlanǵan neyron tarmaǵı ushın, KBJlardıń tereńligi tarmaqtıń tereńligine teń hám jasırın qatlamlar sanına bir qosılǵanǵa teń (sebebi shıǵıs qatlamı da parametrlengen). Signal bir qatlamnan bir neshe ret óte alatuǵın rekurrent neyron tarmaqları ushın, KBJ tereńligi sheksiz bolıwı múmkin[9]. Sayız oqıtıwdı tereń oqıtıwdan ayıratuǵın ulıwma qabıl etilgen tereńlik shegarası joq, biraq kópshilik izertlewshiler tereń oqıtıw KBJ tereńligi ekiden joqarı ekenine kelisedi. Tereńligi eki bolǵan KBJnıń universal jaqınlastırıwshı ekenligin, yaǵnıy qálegen funkciyanı emulaciya qıla alatuǵını kórsetilgen[10]. Bunnan tısqarı, qosımsha qatlamlar tarmaqdıń funkciya jaqınlastırıw qábiletin arttırmaydı. Tereń modeller (KBJ > eki) sayız modellerge qaraǵanda jaqsıraq ózgesheliklerdi shıǵarıp ala aladı, sonlıqtan qosımsha qatlamlar ózgesheliklerdi effektiv úyreniwge járdem beredi. Tereń oqıtıw arxitekturaların sıqmar qatlam-qatlam usılı menen dúziw múmkin. Tereń oqıtıw bul abstrakciyalardı sheshiwge hám qaysı ózgeshelikler iskerlikti jaqsılaytuǵının anıqlawǵa járdem beredi[8]. Tereń oqıtıw algoritmlerin baqlanbaytuǵın oqıtıw wazıypalarına qollanıw múmkin. Bul áhmiyetli artıqmashlıq, sebebi belgisiz maǵlıwmatlar belgilengen maǵlıwmatlarǵa qaraǵanda kóbirek. Baqlanbaytuǵın usılda oqıtıla alatuǵın tereń strukturalarǵa tereń isenim tarmaqları mısal bola aladı[8][11]. Tereń Oqıtıw termini mashinalıq oqıtıw jámiyetine 1986-jılı Rina Dehter tárepinen kiritilgen[12], al jasalma neyron tarmaqlarına 2000-jılı Igor Ayzenberg hám onıń kásiplesleri tárepinen Bul sheklik neyronları kontekstinde kiritilgen[13][14]. Degen menen, onıń payda bolıw tariyxı kórinetuǵınınan quramalıraq[15]. Tereń neyron tarmaqları ulıwma jaǵdayda universal jaqınlastırıw teoreması [16][17][18][19] yamasa itimallıq juwmaǵı [20][21][8][9][22] kózqarasınan interpretaciyalanadı. Klassikalıq universal jaqınlastırıw teoreması shekli ólshemdegi bir jasırın qatlamı bar aldıǵa baǵdarlanǵan neyron tarmaqlardıń úziliksiz funkciyalardı jaqınlastırıw qábiletine tiyisli[16][17][18][19]. 1989-jılı birinshi dálil sigmoid aktivlestiriw funkciyaları ushın Djordj Saybenko tárepinen basıp shıǵarıldı[16] hám 1991-jılı Kurt Xornik tárepinen aldıǵa baǵdarlanǵan kóp qatlamlı arxitekturalarda ulıwmalastırıldı[17] Sonǵı izertlewler universal jaqınlastırıwdıń Kunixiko Fukushimanıń dúzetilgen sızıqlı birlik sıyaqlı sheklenbegen aktivlestiriw funkciyaları ushın da orınlı ekenin kórsetti.[23][24]. Tereń neyron tarmaqları ushın universal jaqınlastırıw teoreması keńligi sheklengen, biraq tereńligi ósiwine ruqsat etilgen tarmaqlardıń qábiletine tiyisli. Lu hám basqalar. ReLU aktivlestiriwi bar tereń neyron tarmaǵınıń keńligi kiris ólsheminen qatań úlken bolsa, onda tarmaq qálegen Lebeg integrallanıwshı funkciyanı jaqınlastıra alatuǵının; eger keńlik kiris ólshemine teń yamasa kishi bolsa, onda tereń neyron tarmaǵı universal jaqınlastırıwshı bolmaytuǵının dálilledi. Itimallıq interpretaciyası[22] mashinalıq oqıtıw tarawınan kelip shıǵadı. Ol juwmaq[21][7][8][9][11][22] sıyaqlı oqıtıw hám testlewdiń optimizaciya koncepciyaların óz ishine aladı, olar sáykes túrde saykes keliw hám ulıwmalastırıw menen baylanıslı. Anıǵıraq aytqanda, itimallıq interpretaciyası aktivlestiriw sızıqsızlıǵın jıynaw bólistiriw funkciyası retinde qaraydı.[22] Itimallıq interpretaciyası neyron tarmaqlarda dropout regularizatorın kirgiziliwine alıp keldi. Itimallıq interpretaciyası Xopfild, Widrow hám Narendra sıyaqlı izertlewshiler tárepinen kiritilgen hám Bishop sıyaqlı izertlewshilerdiń sholıwlarında keń tarqalǵan.[25], Eki túrli jasalma neyron tarmaǵı (JNT) bar: aldıǵa baǵdarlanǵan neyron tarmaǵı (ABNT) yamasa kóp qatlamlı perceptron (KQP) hám rekurrent neyron tarmaqlari (RNT). RNTlarda baylanıs strukturasında cikllar bar, ABNTlarda joq. 1920-jılları Vilgelm Lenc hám Ernst Izing tiykarınan neyron tárizli sheklik elementlerinen turatuǵın oqıtılmaytuǵın RNT arxitekturası bolǵan Izing modelin dúzdi[26][27]. 1972-jılı Shunichi Amari bul arxitekturanı adaptiv etti[28]. Onıń oqıtılatuǵın RNTı 1982-jılı Djon Xopfild tárepinen qayta basıp shıǵarıldı[29]. Basqa erte rekurrent neyron tarmaqları1971-jılı Kaoru Nakano tárepinen basıp shıǵarıldı[30][31]. 1948-jılı sonday-aq Alan Tyuring \"Intellektual mashinalardı\" boyınsha óz ómirinde basılıp shıǵarılmaǵan jumıs isledi[32], onda \"jasalma evolyuciya hám oqıtılatuǵın RNTlar menen baylanıslı ideyalar\" bar edi[33] Frank Rozenblatt (1958)[34] 3 qatlamlı perceptrondı usındı: kiris qatlamı, oqıtılmaytuǵın tosınnan salmaqlı jasırın qatlam hám shıǵıs qatlamı. Ol keyinirek 1962-jılı variantlardı hám kompyuter eksperimentlerin, sonıń ishinde sonǵı eki qatlamı oqıtılatuǵın salmaqlı bolǵan \"adaptiv preterminal tarmaqlı\" tórt qatlamlı perceptron versiyasın (bul jerde ol X. D. Blok hám B. W. Nayttı atap ótedi) kiritken kitap shıǵardı[35].: 16-bólim Kitapta R. D. Djozeftiń (1960)[36] bul tórt qatlamlı sistemanıń \"variantına funkcionallıq jaǵınan ekvivalent\" bolǵan erterek tarmaǵı keltirilgen (kitapta Djozef 30 dan artıq ret tilge alınadı). Solay etip, Djozefti oqıtılatuǵın jasırın birlikleri bar durıs adaptiv kóp qatlamlı perceptronlardıń avtorı dep esaplaw kerek pe? Tilekke qarsı, oqıtıw algoritmi funkcionallıq emes edi hám oǵan umtıldı. Birinshi isleytuǵın tereń oqıtıw algoritmı Maǵlıwmatlardı qayta islewdiń Toparlıq usılı boldı, ol qálegen tereńliktegi neyron tarmaqlardı oqıtıw usılı, 1965-jılı Aleksey Ivaxnenko hám Lapa tárepinen basıp shıǵarıldı. Olar onı polinom regressiyasınıń bir túri[37] yamasa Rozenblatt perceptronınıń ulıwmalastırılıwı dep esapladı[38]. 1971-jılǵı maqalada bul usıl menen oqıtılǵan segiz qatlamlı tereń tarmaq súwretlengen[39], ol regressiyalıq analiz arqalı qatlam-qatlam oqıtıwǵa tiykarlanǵan. Artıq jasırın birlikler bóleklengen validaciya toplamı járdeminde qırqıp taslanadı. Túyinlerdiń aktivlestiriw funkciyaları Kolmogorov-Gabor polinomaları bolǵanlıqtan, bular kóbeytiwshi birlikleri yamasa \"dárwazaları\" bar birinshi tereń tarmaqlar edi. Stoxastikalıq gradient túsiw usılı menen oqıtılǵan birinshi tereń oqıtıw kóp qatlamlı perceptronı 1967-jılı Shunichi Amari tárepinen basıp shıǵarıldı[40]. Amaridiń studentı Saito ótkergen kompyuter eksperimentlerinde, eki ózgertiletuǵın qatlamı bar bes qatlamlı KQP sızıqlı ajıratılmaytuǵın pattern klassların klassifikaciyalaw ushın ishki kórsetiwlerdi úyrendi. Keyingi apparatlıq támiyinlew hám giperparametr sazlawlarındaǵı rawajlanıwlar stoxastikalıq gradient túsiwdi házirgi waqıtta basım oqıtıw texnikasına aylandırdı. 1969-jılı Kunixiko Fukushima ReLU (dúzetilgen sızıqlı birlik) aktivlestiriw funkciyasın kirgizdi[23]. Dúzetkish tereń oqıtıw ushın eń keń tarqalǵan aktivlestiriw funkciyasına aylandı. Konvolyuciyalıq qatlamlar hám úlgi kemitiw qatlamları bar konvolyuciyalıq neyron tarmaqları (KNT) ushın tereń oqıtıw arxitekturaları 1979-jılı Kunixiko Fukushima tárepinen kirgizilgen Neokognitronnan baslandı, biraq ol keri tarqalıw usılı menen oqıtılmadı[41][42]. Keri tarqalıw - bul 1673-jılı Gottfrid Vilgelm Leybniz tárepinen islep shıǵarılǵan dizbekli qaǵıydanıń differenciallanatuǵın túyinler tarmaǵına effektiv qollanılıwı. \"Qátelerdi keri tarqatıw\" terminologiyası 1962-jılı Rozenblatt tárepinen kirigizilgen, biraq ol onı qalay ámelge asırıwdı bilmedi, degen menen Genri Dj. Kelli 1960-jılı basqarıw teoriyası kontekstinde keri tarqalıwdıń úziliksiz aldın alıwshısına iye edi[43]. Keri tarqalıwdıń házirgi forması birinshi ret Seppo Linnaynmaanıń magistrlik dissertaciyasında (1970) basıp shıǵarıldı[44]. G.M. Ostrovskiy hám basqalar onı 1971-jılı qayta basıp shıǵardı[45] Paul Verbos keri tarqalıwdı 1982-jılı neyron tarmaqlarına qollandı (onıń 1974-jılǵı PhD dissertaciyası, 1994-jılǵı kitapta qayta basılǵan, ele algoritmdi súwretlemegen edi[45].). 1986-jılı Devid E. Rumelxart hám basqalar keri tarqalıwdı keń tarqattı, biraq dáslepki jumısqa silteme bermedi[46]. Waqıt keshigiw neyron tarmaǵı (WKNT) 1987-jılı Aleks Vaybel tárepinen KNTnı fonema tanıwǵa qollanıw ushın kirgizildi. Ol konvolyuciyalardı, salmaqlardı bólisiw hám keri tarqalıwdı qollandı. 1988-jılı Vey Chjan keri tarqalıw menen oqıtılǵan KNTnı álipbe tanıwǵa qollandı[47]. \n1989-jılı Yann Lekun hám basqalar pochtadaǵı qolda jazılǵan ZIP kodların tanıw ushın LeNet dep atalatugın KNTnı dúzdi. Oqıtıw 3 kún talap etti. 1990-jılı Vey Chjan KNTnı optikalıq esaplaw apparatında ámelge asırdı[48]. 1991-jılı KNT medicinаlıq súwret obyektin segmentaciyalaw[49] hám mammogrammalarda emshek rakın anıqlaw[50] ushın qollanıldı. Yann Lekun hám basqalarınıń sanlardı klassifikaciyalaytuǵın 7 dárejeli KNTsı LeNet-5 (1998) bir qansha banklar tárepinen 32x32 piksel súwretlerge cifrlastırılǵan qolda jazılǵan cheklerdegi sanlardı tanıw ushın qollanıldı[51]. Qaytalanatuǵın neyron tarmaqları (QNT) (ingl. Recurrent neural networks, RNN)[26][28] 1980-jıllarda qosımsha rawajlandı. Qaytalanıw izbe-izlikti qayta islew ushın qollanıladı hám qaytalanatuǵın tarmaqlardı ashqanda, ol matematikalıq jaqtan tereń túrdegi aldıǵa baǵdarlanǵan qatlamǵa uqsaydı. Nátiyjede, olardıń uqsas qásiyetleri hám máselesi bar, olardıń rawajlanıwı bir-birine tásir etken. QNT da eki dáslepki tásirli jumıslar - bul Jordan tarmaǵı (1986)[52] hám Elman tarmaǵı (1990) boldı[53], olar QNT dı kognitiv psixologiya máselerin úyreniwde qollandı. 1980-jıllarda, uzaq kredit berilgen jollar menen tereń oqıtıw ushın keri tarqalıw jaqsı islemedi. Bul máseleni sheshiw ushın 1991-jılı Yurgen Shmidxuber óz-ózin úyretiw arqalı bir waqıtta bir dárejede aldınnan úyretiletuǵın QNT ierarxiyasın usındı, bunda hárbir QNT óz keyingi kirisin boljaydı, bul tómendegi QNT nıń kelesi kútilmegen kirisi bolıp tabıladı[54][55]. Bul \"neyron tariyx qısqartıwshısı\" kóp sanlı óz-ózin shólkemlestiriwshi waqıt masshtablarında ishki kórsetpelerdi úyreniw ushın boljamlı kodlawdı paydalanbaydı. Bul keyingi tereń oqıtıwdı áhmiyetli dárejede jeńillestiriwi múmkin. QNT ierarxiyası joqarı dárejeli bólsheklewshi tarmaqtı tómendegi dárejeli avtomatlastırıwshı tarmaǵına jiberip, bir QNT ǵa biriktiriliwi múmkin.[54][55] 1993-jılı neyron tariyx qısqartıwshısı waqıt boyınsha ashılǵan QNT da 1000 nan artıq izbe-iz qatlamlardı talap etetuǵın \"Júdá Tereń Oqıtıw\" máselesin sheshti. ChatGPT daǵı \"P\" usınday aldınnan oqıtıwǵa tiyisli. Sepp Xoxrayterdiń diplom jumısı (1991) neyron tariyx qısqartıwshısın[54], ámelge asırdı hám jobalanatuǵın gradient máselesin anıqladı hám talladı[56][57]. Xoxrayter jobalanatuǵın gradient máselesin sheshiw ushın qaytalanatuǵın qaldıq baylanıslardı usındı. Bul 1995-jılı jarıq kórgen uzaq qısqa múddetli este saqlaw (UQMES) ge alıp keldi. UQMES mıńlaǵan ayırım waqıt adımları aldın bolǵan waqıyalardı este saqlaw talap etiletuǵın uzaq kredit berilgen joldı \"júdá tereń oqıtıw\" wazıypaların úyrene aladı[9]. Sol UQMES ele házirgi arxitektura emes edi, oǵan 1999-jılı kirgizilgen \"umıtıw dárwazası\" kerek boldı[6], keyin ala ol standart QNT arxitekturası boldı. 1991-jılı Yurgen Shmidxuber bir tarmaqtıń jetiskenligi ekinshi tarmaqtıń sátsizligi bolatuǵın nóllik summadaǵı oyın túrinde bir-biri menen jarısatuǵın qarsılas neyron tarmaqlardı da járiyaladi[58]. Birinshi tarmaq - shıǵıs úlgileri boyınsha itimallıq bólistirilwin modellestiretuǵın generativlik model. Ekinshi tarmaq bul úlgilerge ortalıqtıń reakciyaların boljaw ushın gradient túsiw jolı menen úyrenedi. Bul \"jasalma qızıǵıwshılıq\" dep ataladı. 2014-jılı bul princip generativ qarsılas tarmaqlarda (GQT) qollanıldı[59]. 1985-1995 jıllar aralıǵında, statistikalıq mexanikadan ilham alıp, Terry Seynovski, Piter Dayan, Djeffri Xinton hám basqalar tárepinen bir neshe arxitekturalar hám metodlar islep shıǵıldı, olardıń ishinde Bolcman mashinası[60], sheklengen Bolcman mashinası[61], Helmgolc mashinası, hám oyaw-uyqı algoritmi bar[62]. Bular tereń generativ modellerin qadaǵalawsız oqıtıw ushın dizayn etilgen edi. Biraq, olar keri tarqalıw algoritmine salıstırǵanda esaplaw jaqtan qımbatıraq edi. 1985-jılı járiyalanǵan Bolcman mashinasın úyretiw algoritmi 1986-jılı keri tarqalıw algoritmi tárepinen kóleńkede qalǵanǵa shekem qısqa waqıt dawamında keń tarqalǵan edi (b. 112 [63]). 1988-jılǵı tarmaq protein strukturasın boljawda eń joqarı kórsetkishke eristi, bul bioinformatikada tereń oqıtıwdıń dáslepki qollanılıwlarınan biri edi[64]. Sóylewdi tanıw ushın JNT nıń sayız hám tereń oqıtıwı (mısalı, qaytalanatuǵın tarmaqlarda) kóp jıllar dawamında izertlendi. Bul metodlar hesh qashan sóylewdi diskriminativ túrde úyretilgen generativ modeller tiykarındaǵı bir tekli emes ishki-qollap islengen Gauss aralaspa modeli/Jasırın Markov modeli (GAM-JMM) texnologiyasınan artıq nátiyje kórsete almadı. Gradient kemeyiwi hám neyron boljamlı modellerdegi waqıtlıq korrelyaciya strukturasınıń háreketiniń [65] hálsiz ekenligi sıyaqlı tiykarǵı qıyınshılıqlar tallap shıǵıldı. Qosımsha qıyınshılıqlar - oqıtıw maǵlıwmatlarınıń jetispewshiligi hám sheklengen esaplaw quwatı edi. Kópshilik sóylewdi tanıw izertlewshileri neyron tarmaqlardı taslap, generativ modellestiriw menen shuǵıllandı. 1990-jıllardıń aqırında SRI International bunnan awlaq boldı. AQSH húkimetiniń NSA hám DARPA támiyinlegen ǵárejeti menen SRI sóylew hám sóylewshini tanıw boyınsha izertlew alıp bardı. Larri Xek basshılıǵındaǵı sóylewshini tanıw toparı 1998-jılǵı NIST Sóylewshini Tanıw bahalaw sınawında sóylewdi qayta islew boyınsha tereń neyron tarmaqlarda áhmiyetli tabısqa eristi[66][67]. Bul Nuance Verifier de qollanıldı hám tereń oqıtıwdıń birinshi iri industrial qollanılıwın kórsetti. \"Raw\" ózgesheliklerdiń qollap islengen optimizaciyadan joqarı qoyıw principi birinshi ret 1990-jıllardıń aqırında \"raw\" spektrogramma yamasa sızıqlı filtr-bank ózgeshelikleri boyınsha tereń avtoenkoder arxitekturasında tabıslı izertlendi, bul spektrogrammalardan ózgermelew basqıshların óz ishine alatuǵın Mel-Kepstral ózgesheliklerinen artıqmashlıǵın kórsetti. Sóylewdiń raw ózgeshelikleri, tolqın formaları keyinirek úlken masshtablı jaqsı nátiyjeler berdi[68]. Neyron tarmaqlardıń rawajlanıwı toqtap qaldı, hám 1990-2000 jıllarda Gabor filtrleri hám tirewshi vektor mashinaları (TVM) sıyaqlı wazıypaǵa tiyisli qollap islengen ózgesheliklerdi paydalanatuǵın ápiwayı modeller artıqmashlıq ala basladı, sebebi jasalma neyron tarmaqlardıń esaplaw qunı joqarı edi hám miydiń biologiyalıq tarmaqlardı qalay dúziwi haqqında túsinik jetispedi. 2003-jılı UQMES (LSTM) ayırım wazıypalarda dástúrli sóylew tanıwshılar menen básekilese alatugın boldı[69]. 2006-jılı Aleks Greyves, Santiago Fernandes, Faustino Gomes hám Shmidxuber onı UQMES dásteleri menen konnekcionistlik waqıtlıq klassifikaciya (KWK)[70] menen birlestirdi. 2009-jılı ol tutasqan qoldan jazıwdı tanıw boyınsha pattern tanıw jarısında jeńiske erisken birinshi QNT boldı[9]. 2006-jılı Djeff Xinton, Ruslan Salaxutdinov, Osindero hám Texlerdiń publikaciyalarında[71][72] generativ modellestiriw ushın tereń isenimlilik tarmaqları rawajlandırıldı. Olar birinshi sheklengen Bolcman mashinasın oqıtıw arqalı úyretiledi, soń onı muzlatıp, birinshisiniń ústine ekinshisin oqıtadı hám solay dawam etedi, keyin qálegen waqıtta qadaǵalanǵan keri tarqalıw arqalı jetilistiriw múmkin. Olar MNIST súwretleriniń tarqalıwı sıyaqlı joqarı ólshemli itimallıq tarqalıwların modellestire alatuǵın edi, biraq tutasıw áste boldı[73][74][75]. Yann LeCunnıń aytıwınsha, tereń oqıtıwdıń sanaat tarawındaǵı tásiri 2000-jıllardıń basında baslandı, sol waqıtta CNT lar AQSH ta jazılǵan barlıq cheklerdiń 10% ten 20% ke shekemin qayta isleytuǵın edi. Úlken kólemli sóylewdi tanıwda tereń oqıtıwdı sanaatta qollanıw 2010-jıl átirapında baslandı. 2009-jılǵı NIPS Sóylewdi Tanıw ushın Tereń Oqıtıw seminarı sóylewdiń tereń generativ modelleriniń sheklewleri hám quwatlıraq apparat hám úlken kólemli maǵlıwmatlar toplamları menen tereń neyron tarmaqlardıń ámeliy bolıw múmkinshiligi menen motivlengen edi. Tereń isenimlilik tarmaqlardıń (TIT) generativ modellerin paydalanıp TTT lardı aldınna úyretiw neyron tarmaqlardıń tiykarǵı qıyınshılıqların jeńedi dep isenilgen edi. Biraq, úlken, kontekstke baylanıslı shıǵıs qatlamların paydalanǵanda TTT lar ushın aldınnan úyretiwdi úlken kólemli oqıtıw maǵlıwmatları menen almastırıw ápiwayi keri tarqalıw jolı menen sol waqıttaǵı eń jetilik Gauss aralaspa modeli (GAM)/Jasırın Markov Modeli (JMM) hám rawajlanǵan generativ modelge tiykarlanǵan sistemalarǵa salıstırǵanda qáteler sanın áhmiyetli dárejede tómenletkeni anıqlandı. Eki túrli sistema payda etken tanıw qáteleriniń tábiyatı xarakterli túrde hár qıylı boldı[76] bul barlıq iri sóylewdi tanıw sistemalarında qollanılatuǵın bar bolǵan júdá nátiyjeli, runtime sóylew dekodlaw sistemasına tereń oqıtıwdı integraciyalaw boyınsha texnikalıq túsinikler berdi.[77][78], 2009-2010 jıllar átirapındaǵı GAM (hám basqa generativ sóylew modellerin) TTT modelleri menen salıstırıw sóylewdi tanıw ushın tereń oqıtıwǵa dáslepki industrial investiciyalardı xoshametledi[76] Bul analiz diskriminativ TTT lar hám generativ modeller arasında salıstırmalı iskerlik (qáte múǵdarı 1.5% ten kem) penen júrgizildi.[79][76][80].\n2010-jılı izertlewshiler tereń oqıtıwdı TIMIT ten úlken sózlikli sóylewdi tanıwǵa keńeytti, bul sheshim aǵashları menen dúzilgen kontekstke baylanıslı JMM hallarına tiykarlanǵan TTT nıń úlken shıǵıs qatlamların qabıl etiw arqalı ámelge asırıldı[81].[77] Tereń oqıtıw revolyuciyası CNN- hám GPU-ǵa tiykarlanǵan kompyuter kóriw sisteması dógereginde baslandı. Keri tarqatıw arqalı úyretilgen CNN-ler onlaǵan jıllar dawamında bar bolǵan hám NN-lerdiń GPU ámelge asırılıwları jıllar dawamında, sonıń ishinde CNN-ler de bar edi[82], kompyuter kóriw sistemasında alǵa ilgerilewge GPU-larda CNN-lerdiń tezrek ámelge asırılıwı kerek bolǵan. Keyinirek, tereń oqıtıw keń tarqalǵan sayın, arnawlı apparat támiynatı hám algoritm optimizaciyaları tereń oqıtıw ushın arnawlı islep shıǵıldı. Tereń oqıtıw revolyuciyası ushın tiykarǵı alǵa ilgerilewlerdiń biri apparat támiynatındaǵı rawajlanıwlar, ásirese GPU bolǵan[82]. Bazı erte jumıslar 2004-jılǵa barıp taqaladı. 2009-jılı Raina, Madhavan hám Andrew Ng 30 Nvidia GeForce GTX 280 GPU-larında úyretilgen 100M tereń isenim tarmaqları haqqında xabar berdi, bul GPU-ǵa tiykarlanǵan tereń oqıtıwdıń erte kórsetpesi edi. Olar 70 ese tezIrek úyretiwdi xabarladı. 2011-jılı Dan Ciresan[83][84] Ueli Meier, Jonathan Masci, Luca Maria Gambardella hám Jurgen Schmidhuber tárepinen DanNet dep atalǵan CNN birinshi ret vizual nızamlılıqlardı tanıw jarısında adamnan joqarı nátiyje kórsetti, dástúriy usıllardı 3 ese artıq nátiyje menen jeńip ótti. Keyin ol basqa jarıslarda da jeńiske eristi[85][86]. Olar sonday-aq, GPU-daǵı maksimal birlestiriw CNN-leri iskerlikti áhmiyetli dárejede jaqsılaǵanın kórsetti[3]. 2012-jılı Andrew Ng hám Jeff Dean tek ǵana YouTube videolarınan alınǵan belgisiz súwretlerdi kórip, joqarı dárejeli túsiniklerdi, mısalı, pıshıqlardı tanıp biliw ushın úyrengen FNN dúzdi. 2012-jıl oktyabr ayında Alex Krizhevsky, Ilya Sutskever hám Geoffrey Hinton tárepinen jaratılǵan AlexNet úlken kólemli ImageNet jarısında sayız mashinalıq oqıtıw usıllarınan áhmiyetli parq penen jeńiske eristi. Keyingi ózgeriwler Karen Simonyan hám Andrew Zisserman tárepinen jaratılǵan VGG-16 tarmaǵın[4] hám Google kompaniyasınıń Inceptionv3 in óz ishine aladı[87]. Súwret klassifikaciyasındaǵı tabıs keyin súwretler ushın sıpatlamalar (jazıwlar) jaratıwdıń qıyınıraq wazıypasına keńeytildi, kóbinese CNN hám LSTM birlesiwi sıpatında. 2014-jılı eń joqarı texnologiya 20 dan 30 ǵa shekem qatlamı bar \"júdá tereń neyron tarmaqların\" úyretiw edi[88]. Júdá kóp qatlamlardı úyiw úyretiw anıqlıǵınıń ádewir tómenlewine alıp keldi, bul \"degradaciya\" máselesi dep tanıldı. 2015-jılı júdá tereń tarmaqlar úyretiw ushın eki texnika islep shıǵıldı: Highway Network 2015-jıl may ayında járiyalandı, al qaldıq neyron tarmaǵı (ResNet) 2015-jıl dekabr ayında járiyalandı. ResNet ashıq dárwazalı Highway Net sıyaqlı háreketlenedi. Shama menen sol waqıtta, tereń oqıtıw kórkem óner tarawına tásir ete basladı. Erte mısallar Google DeepDream (2015) hám neyron stilin kóshiriw (2015), di óz ishine aladı, olardıń ekewi de VGG-19 sıyaqlı aldınnan úyretilgen súwret klassifikaciyası neyron tarmaqlarına tiykarlanǵan edi. Generativ qarsılasıw tarmaqı (GAN) (Ian Goodfellow hám basqalar, 2014) (Jurgen Schmidhuber'diń jasalma qızıǵıwshılıq principine tiykarlanǵan[59])\n) 2014-2018 jıllar aralıǵında generativ modellestiriwde eń joqarı texnologiya bolıp qaldı. Nvidia'nıń StyleGAN (2018)[89] Tero Karras hám basqalardıń Progressive GAN'ına tiykarlanıp júdá joqarı sapada súwretler jaratadı. Bunda GAN generatorı kishi ólshemnen úlken ólshemge qaray piramida tárizinde ósedi. GAN járdeminde súwret jaratıw keń qollanıldı hám deepfake'ler haqqında talqılawlardı keltirip shıǵardı[90]. Diffuziya modelleri (2015)[91] sol waqıttan baslap generativ modellestiriwde GAN'lardan ozıp ketti, DALL•E 2 (2022) hám Stable Diffusion (2022) sıyaqlı sistemalardı payda etti. 2015-jılı Google'diń sóylewdi tanıw texnologiyası LSTM tiykarlanǵan model arqalı 49% jaqsılandı hám onı aqıllı telefonlardaǵı Google Voice Search arqalı jetkilikli etti[92]. Tereń oqıtıw hár túrli tarawlarda, ásirese kompyuter kóriwi hám avtomatik sóylewdi tanıw (ASR) sistemalarında eń aldıńǵı qatarlı texnologiya bolıp esaplanadı. TIMIT (ASR) hám MNIST (súwret klassifikaciyası) sıyaqlı keń qollanılatuǵın bahalaw toplamlarında, sonday-aq úlken sózlikli sóylewdi tanıwdıń bir qansha wazıypalarında nátiyjeleri izbe-iz jaqsılanıp bardı. Svertkalı neyron tarmaqları ASR ushın LSTM tárepinen ozıp ketti[92][93][94], biraq kompyuter kóriwinde ele de tabıslı qollanılmaqta. Yoshua Bengio, Geoffrey Hinton hám Yann LeCun \"tereń neyron tarmaqların esaplaw texnikasınıń áhmiyetli komponenti etken konceptual hám injenerlik jetiskenlikler\" ushın 2018-jılǵı Turing sıylıǵı menen sıylıqlandı[95]. Jasalma neyron tarmaqları (JNT) yamasa baylanıs sistemalar haywanlar miyiniń biologiyalıq neyron tarmaqlarınan ilham alǵan esaplaw sistemaları bolıp tabıladı. Bunday sistemalar wazıypalardı orınlaw ushın mısallardı qarap shıǵıw arqalı úyrenedi (óz qábiletin az-azdan jaqsılap baradı), kóbinese wazıypaǵa baylanıslı arnawlı programmalastırıwsız. Mısalı, súwretti tanıwda, olar \"pıshıq\" yamasa \"pıshıq emes\" dep qoldan belgilengen mısal súwretlerdi analiz qılıw arqalı pıshıqları bar súwretlerdi anıqlaw usılın úyreniwi múmkin hám bul analiz nátiyjelerin basqa súwretlerdegi pıshıqlardı anıqlaw ushın qollanıwı múmkin. Olar kóbinese qaǵıydaǵa tiykarlanǵan programmalastırıw járdeminde dástúriy kompyuter algoritmi menen ańlatıw qıyın bolǵan qollanıwlarda eń kóp paydalanıladı. JNT jasalma neyronlar dep ataladı baylanısqan birlikler toplamına tiykarlanǵan (biologiyalıq miydegi biologiyalıq neyronlarǵa uqsas). Neyronlar arasındaǵı hárbir baylanıs (sinaps) basqa neyronǵa signal jibere aladı. Qabıllawshı (postsinaptikalıq) neyron signaldı (signallardı) qayta islep, keyin ózi menen baylanısqan tómengi aǵımdaǵı neyronlarǵa signal jibere aladı. Neyronlarda hal bolıwı múmkin, bul kóbinese haqıyqıy sanlar menen kórsetiledi, ádettе 0 hám 1 arasında. Neyronlar hám sinapslar da úyreniw barısında ózgerip turatıǵın salmaǵı bolıwı múmkin, bul tómengi aǵımǵa jiberetugın signaldıń kúshin arttırıwı yamasa kemeytiw múmkin. Kóbinese, neyronlar qatlamlar boyınsha shólkemlestiriledi. Hár qıylı qatlamlar óz kirislerin hár túrli ózgertiwlerge ushıratıwı múmkin. Signallar birinshi (kiris) qatlamnan aqırǵı (shıǵıs) qatlamǵa shekem júredi, múmkin qatlamlardı bir neshe ret ótip. Neyron tarmaq usılınıń dáslepki maqseti adamnıń miyi sıyaqlı máselelеrdi sheshiw edi. Waqıttıń ótiwi menen, dıqqat belgili intellektual qábiletlerge sáykes keliw ústin qoyıldı, bul biologiyadan awıtqıwlarǵa alıp keldi, mısalı, keri tarqalıw yamasa maǵlıwmattı keri baǵıtta ótkiziw hám sol maǵlıwmattı sáwlelendiriw ushın tarmaqtı dúzetiw. Neyron tarmaqlar kompyuter kóriwi, sóylewdi tanıw, mashina awdarması, sociallıq tarmaq filtrlew, taxta hám video oyınlar oynaw hám medicinalıq diagnoz qoyıw sıyaqlı hár túrli wazıypalarda qollanılǵan. 2017-jılǵa kelip, neyron tarmaqlar ádette bir neshe mıńnan bir neshe million birlikke hám millionlaǵan baylanıslarǵa iye. Bul san adam miyindegi neyronlar sanınan bir neshe tártipke kem bolsa da, bul tarmaqlar kóp wazıypalardı adam dárejesinen joqarı dárejede orınlay aladı (mısalı, júzlerdi tanıw yamasa \"Go\" oyının oynaw[97]). Tereń neyron tarmaǵı (TNT) - bul kiris hám shıǵıs qatlamları arasında bir neshe qatlamı bar jasalma neyron tarmaǵı. Neyron tarmaqlarınıń hár qıylı túrleri bar, biraq olar bárqulla birdey qurawshılardan turadı: neyronlar, sinapslar, salmaqlar, qıysıqlıqlar hám funkciyalar[98]. Bul qurawshılar barlıǵı adam miyiniń funkciyaların eliklew usılında isleydi hám basqa ML algoritmi sıyaqlı úyretiliw múmkin. Mısalı, iyt tuqımların tanıw ushın úyretilgen TNT berilgen súwretti qarap shıǵadı hám súwrettegi iyttiń belgili bir tuqımǵa tiyisli bolıw itimallıǵın esaplaydı. Paydalanıwshı nátiyjelеrdi kórip shıǵıp, tarmaq qaysı itimallıqlardı kórsetiwin (belgili bir shekten joqarı t.b.) tаńlaw hám usınılǵan belgini qaytarıw múmkin. Usınday hárbir matematikalıq manipulyaciya bir qatlam dep esaplanadı, hám quramalı TNT-lar kóp qatlamlarǵa iye, sonlıqtan olar \"tereń\" tarmaqlar dep ataladı. TNT-lar quramalı sızıqlı emes qatnasıqlardı modellestirе aladı. TNT arxitekturaları kompoziciyalıq modellеrdi payda etedi, bunda obyekt dáslepki elementlerdiń qatlamlı kompoziciyası túrinde аńlatıladı[99]. Qosımsha qatlamlar tómengi qatlamlardaǵı ózgesheliklerdiń kompoziciyasın ámelge asırıwǵa múmkinshilik beredi, bul quramalı maǵlıwmatlardı usınday islewshi sayız tarmaqqa qaraǵanda azıraq birlikler menen modellestiriw imkaniyatın beredi[7]. Mısalı, siyrek kóp ózgeriwshili polinomlar TNT-lar járdeminde sayız tarmaqlarǵa qaraǵanda eksponencial túrde аńsatıraq jaqınlastırılıwı dálillendi. Tereń arxitekturalar bir neshe tiykarǵı usıllardıń kóp variantların óz ishine aladı. Hárbir arxitektura belgili bir tarawda tabısqa eristi. Eger bir neshe arxitektura birdey maǵlıwmatlar toplamında bahalanbaǵan bolsa, olardıń islewin salıstırıw barlıq waqıtta múmkin emes. TNT-lar ádette alǵa baǵdarlanǵan tarmaqlar bolıp, olarda maǵlıwmatlar kiriw qatlamınan shıǵıw qatlamına qaytpastan ótedi. Dáslep, TNT virtual neyronlar kartasın dúzedi hám olar arasındaǵı baylanıslarǵa qaytalanatuǵın san mánislerin yamasa \"salmaqların\" belgileydi. Salmaqlar hám kiriw maǵlıwmatları kóbeytiledi hám 0 menen 1 arasındaǵı shıǵıs mánisin qaytaradı. Eger tarmaq belgili bir úlgini anıq tanıy almasa, algoritm salmaqtı dúzetedi[100]. Usılay etip, algoritm maǵlıwmatlardı tolıq qayta islew ushın durıs matematikalıq manipulyaciyan i anıqlamaǵansha, belgili parametrlerdi kóbirek tásirli etedi. Maǵlıwmatlar hár qanday baǵdarda aǵıp óte alatuǵın qaytalanatuǵın neyron tarmaqları til modellestiriw sıyaqlı qollanıwlar ushın paydalanıladı. Uzaq qısqa múddetli este saqlaw (UQMES) bul maqset ushın ayırıqsha nátiyjeli[101]. Konvolyucion neyron tarmaqları (ÚNT) kompyuter kóriwinde qollanıladı. Sonday-aq, KNT-lar avtomatik sóylewdi tanıw (AST) ushın akustikalıq modellestiriwde de qollanılǵan[102]. Jasalma neyron tarmaqları (JNT) sıyaqlı, ápiwayı úyretilgen TNT-larda da kóplegen máseleler payda bolıwı múmkin. Eki ulıwma máselе - bul hádden tıs beyimlesiw hám esaplaw waqtı. TNT-lar qosımsha abstrakciya qatlamları sebepli hadden tıs beyimlesiwge uqıplı, bul olarǵa úyretiw maǵlıwmatlarındaǵı siyrek ǵárezliliklerdi modellestiriwge múmkinshilik beredi. Hádden tıs beyimlesiwge qarsı gúresiw ushın úyretiw waqtında Ivaxnenkonıń birlik qısqartıw[39] yamasa salmaq kemeyiwi (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularizaciya) yamasa siyreklik (\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularizaciya) sıyaqlı regularizaciya usılları qollanılıwı múmkin. Basqa variantta, dropout regularizaciyası úyretiw waqtında jasırın qatlamlardaǵı birliklerdi tosınnan alıp taslaydı. Bul siyrek ǵárezliliklerdi sırtqa shıǵarıwǵa járdem beredi[103]. Aqırında, kishi úyretiw toplamlarınıń kólemin úlkeytip, hádden tıs beyimlesiw itimalın azaytıw ushın, maǵlıwmatlardı kesip alıw hám burıw sıyaqlı usıllar menen keńeytiwge boladı[104]. TNT-lar kóplegen úyretiw parametrlerin, mısalı, kólemi (qatlamlar sanı hám hár qatlamdaǵı birlikler sanı), úyreniw tezligi hám dáslepki salmaqların esapqa alıwı kerek. Optimal parametrler ushın parametrler keńisligin tolıq izertlew waqıt hám esaplaw resursları kóz-qarasınan múmkin bolmawı múmkin. Toplamlar (ayırım mısallar ornına bir waqıtta bir neshe úyretiw mısalında gradientti esaplaw) sıyaqlı hár túrli hiyleler esaplawdı tezletedi[105]. Kóp yadrolı arxitekturalardıń (GPU yamasa Intel Xeon Phi sıyaqlı) úlken qayta islew múmkinshilikleri matrica hám vektor esaplawları ushın bunday qayta islew arxitekturalarınıń qolaylılıǵı sebepli úyretiwde áhmiyetli tezletiwlerdi payda etti[106][107]. Basqa variantta, injenerler ápiwayıraq hám jaqınlasıwshı úyretiw algoritmleri bar basqa túrdegi neyron tarmaqlarına qarawı múmkin. CMAC (cerebellarlıq model artikulyaciya kontroleri) usınday neyron tarmaqlarınıń bir túri. Ol úyreniw tezliklerin yamasa tosınnan tańlanǵan dáslepki salmaqtı talap etpeydi. Úyretiw procesi jańa maǵlıwmatlar toparı menen bir qádemde jaqınlasıwǵa kepillik bere aladı, hám úyretiw algoritminiń esaplaw quramalılıǵı qatnasqan neyronlar sanına qarata sızıqlı. 2010-jıllardan baslap, mashinalıq oqıtıw algoritmleri hám kompyuter apparatlıq támiynlewiniń rawajlanıwı kóp sanlı sızıqlı emes jasırın birlikler hám júdá úlken shıǵıs qatlamın óz ishine alǵan tereń neyron tarmaqların úyretiw ushın nátiyjeli usıllardıń payda bolıwına alıp keldi[108]. 2019-jılǵa kelip, kóbinese jasalma intellektke tiyisli jetilisiwler menen grafikalıq processorlar (GPU) úlken kólemli kommerciyalıq bult JI-di úyretiwdiń tiykarǵı usılı sıpatında oraylıq processorlar (CPU) ornın iyeledi [109]. OpenAI AlexNet-ten (2012) AlphaZero-ǵa (2017) shekem eń iri tereń oqıtıw joybarlarında qollanılǵan apparatlıq esaplawlardı bahalap, talap etiletuǵın esaplawlar muǵdarınıń 300,000 ese artqanın hám bul kórsetkishtiń hár 3.4 ayda eki ese ósip baratırǵanın anıqladı[110][111]. Tereń oqıtıw algoritmleriniń tezligin arttırıw ushın tereń oqıtıw processorları dep atalatuǵın arnawlı elektron sxemalar islep shıǵıldı. Tereń oqıtıw processorlarına Huawei uyalı telefonlarındaǵı neyron qayta islew blokları (NPU)[112] hám Google Cloud Platform-daǵı tensor qayta islew blokları (TPU) sıyaqlı bult esaplaw serverleri kiredi. Sonday-aq, Cerebras Systems úlken tereń oqıtıw modellerin qayta islew ushın arnawlı sistema - CS-2 dúzdi, ol sanaat tarawındaǵı eń iri processor - ekinshi áwlad Wafer Scale Engine (WSE-2) tiykarında isleydi[113][114]. Atomlıq júdá juqa yarım ótkizgishler logikalıq operaciyalar hám maǵlıwmatlardı saqlawda bir qıylı tiykarǵı qurılma qollanılatuǵın energiya ónimli tereń oqıtıw apparatlıq támiyinlewin jaratıw ushın úmitli dep esaplanadı. 2020-jılı Marega hám onıń kásiplesleri qalqıwshı zatvor maydan effektli tranzistorlarına (FGFET) tiykarlanǵan logikalıq-yadta qurılmalar hám sxemalardı rawajlandırıw ushın úlken maydanlı aktiv kanal materialı menen ótkerilgen tájiriybeler haqqında maqala járiyaladı. 2021-jılı J. Feldmann hám onıń kásiplesleri parallel konvolyuciyalıq qayta islew ushın integraciyalanǵan fotonikalıq apparatlıq tezletkishti usınıs etti[115]. Avtorlar integraciyalanǵan fotonikaniń elektronikalıq analoglarına salıstırǵanda eki tiykarǵı artıqmashılıǵın atap ótedi: (1) jiyilik grebenkaları menen birgelikte tolqın uzınlıǵı boyınsha bólistiriw arqalı júdá kóp sanlı parallel maǵlıwmat uzatıw, hám (2) júdá joqarı maǵlıwmat modulyaciya tezligi.[115] Olardıń sisteması sekundına trillionlaǵan kóbeytiw-jıynaw operaciyaların orınlay aladı, bul integraciyalanǵan fotonikanıń maǵlıwmatqa bay JI qollanıwlarındaǵı potencialın kórsetedi.[115]"
  },
  {
    "url": "https://ro.wikipedia.org/wiki/%C3%8Env%C4%83%C8%9Bare_profund%C4%83",
    "title": "Învățare profundă - Wikipedia",
    "content": "Învățarea profundă (cunoscută și sub numele de învățare profundă structurată sau învățare ierarhică) este parte dintr-o familie de metode de învățare automată bazată pe rețele neuronale artificiale. Învățarea poate fi supravegheată, semi-supravegheată sau nesupravegheată[1][2][3]. Arhitecturile de învățare profundă, precum rețele neuronale profunde, rețele neuronale recurente și rețelele neuronale convoluționale, au fost aplicate în diverse domenii, precum recunoașterea imaginilor, recunoașterea vocală, prelucrarea limbajului natural, recunoașterea audio, filtrarea în rețele sociale, traducerea automată, bioinformatica, proiectarea de medicamente, analiza de imagini medicale, inspeciția materialelor și divertisment, unde au obținut rezultate comparabile cu și, în unele cazuri, superioare experților umani[4][5][6]. Rețelele neuronale au fost inspirate de prelucrarea informațiilor și nodurile de comunicare distribuite în sistemele biologice. Rețelele neuronale artificiale diferă de creierul biologic. În special, rețelele neuronale tind să fie statice și simbolice, în timp ce creierul biologic al celor mai multe organisme vii este dinamic și analog[7][8][9]. Învățarea profundă este o clasă de algoritmi de învățare automată care[10](pp199–200) folosesc mai multe straturi de neuroni pentru a extrage progresiv caracteristici de nivel superior din datele de intrare. De exemplu, în procesarea imaginilor, straturile inferioare pot identifica margini, în timp ce straturile superioare pot identifica elemente semnificative, precum cifre, litere sau fețe de oameni. Termenul de Învățare Profundă a fost introdus în comunitatea de învățare automată de către Rina Dechter în 1986[11][12], iar în cea de rețele neuronale artificiale de Igor Aizenberg și colegii în anul 2000[13][14]. În martie 2019, Yoshua Bengio, Geoffrey Hinton și Yann LeCun au fost distinși cu Premiul Turing pentru descoperirile conceptuale și inginerești care au făcut din rețelele neuronale profunde o componentă critică de calcul. Rețelele neuronale artificiale[15] (RNA) sunt sisteme de calcul inspirat de rețelele neuronale biologice care constituie creierul animalelor. Astfel de sisteme învață (își îmbunătățesc progresiv capacitatea lor) să execute sarcini prin observarea de exemple, de obicei fără a fi nevoie de re-programare în funcție de domeniu. De exemplu, în recunoașterea de imagini, ar putea învăța să identifice imaginile care conțin pisici analizând exemple de imagini care au fost manual etichetate ca „pisică” sau „nici o pisică”, iar ulterior să aplice aceste cunoștințe unor imagini noi. Astfel de algoritmi sunt utilizați în aplicații care nu pot fi modelate ușor folosind algoritm tradișionali, precum programarea bazată pe reguli. O RNA se bazează pe o colecție de unități conectate numite neuroni artificiali, (similar cu neuronii biologici în creierul biologic). Fiecare conexiune (sinapsă) dintre neuroni poate transmite un semnal între neuroni. Neuronul receptor (postsinaptic) poate procesa semnalul și apoi transmite mai departe către următorii neuroni la care este conectat. Neuronii pot avea stare, în general reprezentată de numere reale, de obicei între 0 și 1. Neuronii și sinapsele pot, de asemenea, avea o greutate care variază pe măsură ce învățarea are loc și care poate mări sau micșora puterea semnalului care este transmis mai departe. De obicei, neuronii sunt organizați în straturi. Diferite straturi pot efectua diferite tipuri de transformări asupra intrărilor lor. Semnale călătoresc de la primul strat (de intrare) până la ultimul strat (de ieșire), eventual după ce traversează straturi intermediare de mai multe ori. Rețelele neuronale au fost folosite într-o varietate de domenii, printre care recunoașterea imaginilor, recunoaștere vocală, traducere automată, filtrare în rețele de socializare, jocuri și diagnosticare medicală. În 2017, rețelele neuronale au de obicei câteva mii spre câteva milioane de unități și milioane de conexiuni. În ciuda acestui număr de mai multe ordine de mărime mai mic decât numărul de neuroni din creierul uman, aceste rețele pot efectua mai multe sarcini la un nivel superior față de oameni (de exemplu, recunoașterea fețelor sau jocul „Go”[16] ). O rețea neuronală profundă (RNP) este o rețea neuronală artificială (RNA) cu mai multe straturi între straturile de intrare și ieșire.[17][2] RNP găsește modalitatea potrivită de manipulare matematică pentru a transforma semnalul de intrare în cel de ieșire, fie că este vorba de o relație liniară sau neliniară. Rețeaua se mișcă prin straturi calculând probabilitatea fiecărui rezultat. De exemplu, o RNP care este instruită să recunoască rasele de câini v procesa o anumită imagine și calcula probabilitatea ca animalul din imagine să fie dintr-o anumita rasă. Utilizatorul poate revizui rezultatele și selecta care probabilități ar trebui să fie afișate de rețea (peste un anumit prag, etc.). Fiecare astfel de manipulare matematică este considerată un strat, iar RNP complexe au mai multe straturi, de unde și numele de rețele „profunde”. RNP pot modela relații neliniare complexe. Arhitecturile RNP generează modele compoziționale în care obiectul este exprimat ca o compoziție stratificată de primitive[18]. Straturile suplimentare permit compoziția caracteristicilor din straturile inferioare, cu potențial de modelare a datelor complexe cu mai puține unități decât rețea superficială de performanță similară[17]. Arhitecturile profunde includ multe variante bazate pe doar câteva metode elementare. Fiecare arhitectură are succes în domenii specifice. Diversele arhitecturi nu pot fi întotdeauna comparate, cu excepția cazului în care acestea sunt evaluate pe aceleași seturi de date. Rețelele neuronale recurente (RNN), în care datele pot circula în orice direcție, sunt utilizate pentru aplicații cum ar fi modelarea limbajului[19][20][21][22][23]. Memoria lungă pe termen scurt  este deosebit de eficientă pentru această utilizare[24][25]. Rețelele neuronale convoluționale (RNC) sunt utilizate în recunoașterea imaginilor[26]. RNC au fost aplicate și în modelarea acustică pentru recunoașterea automată a vorbirii[27]. Recunoașterea automată a vorbirii la scară largă este primul și cel mai convingător caz de succes al aplicării învățării profunde. Rețelele recurent cu memorie pot învăța obiective „foarte profunde”[2], care implică intervale multi-secundă și care conțin evenimente de vorbire separate de mii de pași discreți pași de timp, unde un pas de timp durează aproximativ 10 ms. Un LSTM cu porți de uitare[25] este competitiv cu metodele tradiționale de recunoaștere a vorbirii în anumite domenii[28]. Toate marile sisteme comerciale de recunoaștere a vorbirii (de exemplu, Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu, iFlyTek și Nuance) se bazează pe învățarea profundă[10][29][30][31]. Rețelele neuronale au fost folosite pentru punerea în aplicare a modelelor de limbă încă de la începutul anilor 2000[19][32]. LSTM a contribuit la îmbunătățirea traducerii automate și modelelor de limbă[20][21][22]. Cercetările recente generalizează reprezentărilor cuvintelor la reprezentările propozițiilor. Un procent mare de medicamente potențiale eșuază în a fi aprobate din cauza eficacității insuficiente (efectul asupra țintei), interacțiunilor nedorite (efecte în afara țintei) sau efectelor toxice neprevăzute[33][34]. Cercetătorii au explorat utilizarea învățării profunde pentru a prezice efectele asupra țintelor biomoleculare[35][36] și cele toxice ale substanțelor chimice din nutrienți, produse de uz casnic și medicamente[37][38][39]. O rețea neuronală auto-codificantă a fost folosită în bioinformatică pentru a prezice adnotări spre Gene Ontology și relații genă-funcție[40]. În informatică medicală, învățarea profundă a fost folosită pentru a prezice calitatea somnului pe baza datelor de la senzori portabili[41] și complicații de sănătate pe baza dosarelor electronice de sănătate[42]. Învățarea profundă a arătat că este utilă și în sănătate[43]. S-a demonstrat că învățarea profundă produce rezultate comparabile cu ale omaenilor în aplicații medicale, precum clasificarea celulelor canceroase, detectarea leziunilor, segmentarea organelor și îmbunătățirea imaginilor[44][45]. Învățarea profundă este aplicată cu succes în detectarea fraudelor financiare și spălării de bani. „Sistemele profunde de detectare a spălării de bani pot identifica și recunoaște relații și asemănări între date și, mai mult, pot învăța să detecteze anomalii sau să clasifice și să anticipeze anumite evenimente”. Soluția folosește atât tehnici de învățare supravegheată, precum clasificarea tranzacțiilor suspecte, cât și învățare nesupravegheată, precum detectarea anomaliilor[46]."
  },
  {
    "url": "https://qu.wikipedia.org/wiki/Ukhu_yachay",
    "title": "Ukhu yachay - Wikipidiya",
    "content": "Ukhu yachay icha ukhu yachaqay (inlish simimanta deep learning) nisqaqa astawan hatun ayllu atipayasqa llamk'aq yachay ñankunap huknin, kapchis ankucha llikakunapi, qhawachiq yachaywan. Yachayqa qhawasqa, kuska qhawasqa utaq mana qhawasqa kanman.[2]"
  },
  {
    "url": "https://ru.wikipedia.org/wiki/%D0%93%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5",
    "title": "Глубокое обучение — Википедия",
    "content": "Глубокое обучение (глубинное обучение; англ. deep learning) — совокупность методов машинного обучения (с учителем, с частичным привлечением учителя, без учителя, с подкреплением), основанных на обучении представлениям (англ. feature/representation learning), а не специализированных алгоритмах под конкретные задачи. Многие методы глубокого обучения были известны ещё в 1980-е годы (и даже ранее[1]), но результаты не впечатляли[2], пока продвижения в теории искусственных нейронных сетей (предобучение нейросетей с помощью специального случая ненаправленной графической модели, так называемой ограниченной машины Больцмана) и вычислительные мощности середины 2000-х годов (в том числе использующие графические ускорители, программируемые пользователем вентильные матрицы и различные формы нейронных процессоров) не позволили создавать сложные технологические архитектуры нейронных сетей, обладающие достаточной производительностью и позволяющие решать широкий спектр задач, не поддававшихся эффективному решению ранее, например, в компьютерном зрении, машинном переводе, распознавании речи, причём качество решения во многих случаях теперь сопоставимо, а в некоторых превосходит эффективность человека[3]. Несмотря на то что термин «глубокое обучение» появился в научном сообществе машинного обучения только в 1986 году после работы Рины Дехтер[4], первый общий рабочий алгоритм для глубоких многослойных перцептронов прямого распространения был опубликован в книге советских учёных Алексея Григорьевича Ивахненко и Валентина Григорьевича Лапы «Кибернетические предсказывающие устройства» ещё в 1965 году[5]. В дальнейшем, наиболее ранний метод глубокого обучения МГУА был использован для обучения восьмислойной нейронной сети уже в 1971.[6][7] Другие глубокие архитектуры, в особенности те, которые специализируются на распознавании образов, берут своё начало с неокогнитрона, разработанного Кунихико Фукусимой[англ.] в 1980 году. В 1989 году Яну Лекуну удалось использовать алгоритм обратного распространения ошибки для обучения глубоких нейросетей для решения задачи распознавания рукописных ZIP-кодов[8]. Несмотря на успешный опыт, для обучения модели потребовалось три дня, что существенно ограничивало применимость этого метода. Низкая скорость обучения связана со многими факторами, включая проблему исчезающих градиентов из-за большого разброса значений обучаемых параметров, которую в 1991 году анализировали Йорген Шмидхубер и Зепп Хохрайтер. Из-за этих проблем нейронные сети в 1990-х годах уступили место методу опорных векторов. К 1991 году такие системы использовались для распознавания изолированных двумерных рукописных цифр, а распознавание трёхмерных объектов осуществлялось путём сопоставления двумерных изображений с трёхмерной объектной моделью, изготовленной вручную. В 1992 году создана модель кресцептрона[9][10][11] для распознавания трёхмерных объектов в загромождённых сценах. В 1994 году Андре де Карвальо вместе с Майком Фэйрхерстом и Дэвидом Биссетом опубликовал экспериментальные результаты многослойной булевой нейронной сети, также известной как невесомая нейронная сеть, состоящая из трехуровневого самоорганизующегося модуля нейронной сети для выделения признаков (SOFT), а затем — модуль нейронной сети многоуровневой классификации (GSN). Каждый модуль прошёл независимое друг от друга обучение. Каждый слой в модуле извлекал объекты с растущей сложностью относительно предыдущего слоя.[12] В 1995 году Брендан Фрей продемонстрировал, что можно обучить (в течение двух дней) сеть, содержащую шесть полностью соединённых слоев и несколько сотен скрытых юнитов[неизвестный термин], используя алгоритм сна-бодрствования, разработанный совместно с Питером Даяном и Хинтоном[13]. Многие факторы способствуют низкой скорости, включая проблему исчезающего градиента, проанализированную в 1991 году Зеппом Хохрайтером[14][15]. Более простые модели, которые используют ручные работы, специфичные для конкретной задачи, такие как фильтры Габора и метод опорных векторов (МОВ), были популярным выбором в 1990-х и 2000-х годах из-за вычислительных затрат искусственной нейронной сети (ИНС, англ. ANN) и отсутствия понимания того, как мозг связывает свои биологические сети. Как мелкие, так и глубокие модели ИНС (например, рекуррентные сети) изучались в течение многих лет[16][17][18]. Эти методы никогда не превосходили неоднородную смешанную Гауссову модель и скрытую модель Маркова, основанную на генеративных моделях речи, обученных дискриминационно[19]. Были проанализированы ключевые трудности, в том числе уменьшение градиента[14] и слабая временная корреляционная структура в нейронных прогностических моделях[20][21]. Дополнительными трудностями были отсутствие обучающих данных и ограниченная вычислительная мощность. Глубокое обучение приобрело популярность в середине 2000-х годов, когда всё сошлось воедино: компьютеры стали достаточно мощными, чтобы обучать большие нейронные сети (вычисления научились делегировать графическим процессорам, что ускорило процесс обучения на порядок), наборы данных стали достаточно объёмными, чтобы обучение больших сетей имело смысл, а в теории искусственных нейронных сетей произошло очередное продвижение — статьи Хинтона, Осиндеро и Тэ[22], а также Бенджио[23], в которых авторы показали, что можно эффективно предобучать многослойную нейронную сеть, если обучать каждый слой отдельно при помощи ограниченной машины Больцмана, а затем дообучать при помощи метода обратного распространения ошибки. В 2012 году команда под руководством Джорджа Э. Даля выиграла конкурс «Merck Molecular Activity Challenge», используя многозадачные глубокие нейронные сети для прогнозирования биомолекулярной мишени одного препарата[24]. В 2014 году группа Хохрейтера использовала глубокое обучение для выявления нецелевых и токсических эффектов химических веществ, присутствующих в окружающей среде, в питательных веществах, продуктах домашнего обихода и лекарствах, и выиграла «Tox21 Data Challenge» от Национального института здравоохранения США, Управления по санитарному надзору за качеством пищевых продуктов и медикаментов и NCATS[25]. Значительное развитие в распознавании изображений или объектов ощущалось в период с 2011 по 2012 годы. Хотя сверточные нейронные сети (СНН), обученные обратному распространению, существовали в течение десятилетий, и GPU внедряли нейронные сети в течение многих лет, включая СНН, быстрые реализации СНН на GPU использовали для развития компьютерного зрения. В 2011 году этот подход впервые позволил добиться сверхчеловеческой производительности в конкурсе визуального распознавания образов. Также в 2011 году он выиграл конкурс рукописного ввода ICDAR, а в мае 2012 года — конкурс сегментации изображений ISBI[26]. До 2011 года СНН не играли основной роли на конференциях по компьютерному зрению, но в июне 2012 года доклад Циресана[27] на ведущей конференции CVPR показал, как максимальное объединение СНН на GPU может значительно улучшить многие результаты бенчмарков. В октябре 2012 г. аналогичная система была разработана Крижевским[28], коллектив которого выиграл крупномасштабный конкурс ImageNet со значительным преимуществом по сравнению с методами традиционного машинного обучения. В ноябре 2012 года команда Циресана также выиграла конкурс ICPR по анализу больших медицинских изображений для выявления рака, а в следующем году MICCAI Grand Challenge по той же теме[29]. В 2013 и 2014 годах частота ошибок в задаче ImageNet с использованием глубокого обучения была ещё снижена вследствие аналогичной тенденции в широкомасштабном распознавании речи. Стивен Вольфрам в рамках проекта по идентификации изображений опубликовал эти улучшения[30]. Классификация изображений была затем расширена до более сложной задачи генерации описаний (подписей) для изображений, часто в виде комбинации СНН и LSTM[31][32][33][34]. Некоторые исследователи считают, что победа ImageNet в октябре 2012 года положила начало «революции глубокого обучения», которая изменила индустрию искусственного интеллекта[35]. В марте 2019 года Йошуа Бенжио, Джеффри Хинтон и Янн ЛеКун были награждены премией Тьюринга за концептуальные и инженерные прорывы, которые сделали глубокие нейронные сети критическим компонентом вычислений. Искусственные нейронные сети (ИНС) — это вычислительные системы, основанные на принципах биологических нейронных сетей, составляющих мозг животных. Такие системы учатся (постепенно улучшают свои способности) выполнять задачи, как правило, без программирования для решения конкретных задач. Например, при распознавании изображений кошек они могут научиться распознавать изображения, содержащие кошек, анализируя примеры изображений, которые были вручную помечены как «кошка» или «нет кошки», и используя результаты анализа для идентификации кошек на других изображениях. Наибольшее применение ИНС нашли в программных приложениях, которые трудно выразить традиционным компьютерным алгоритмом, использующим программирование на основе правил. ИНС основаны на наборе связанных единиц, называемых искусственными нейронами (аналог биологических нейронов в биологическом мозге). Каждое соединение (синапс) между нейронами может передавать сигнал другому нейрону. Принимающий (постсинаптический) нейрон может обрабатывать сигнал (сигналы) и затем сигнализировать о подключенных к нему нейронах. Нейроны могут иметь состояние, обычно представляемое действительными числами, обычно между 0 и 1. Нейроны и синапсы могут также иметь вес, который изменяется в процессе обучения, что может увеличивать или уменьшать силу сигнала, который он посылает далее. Как правило, нейроны организованы в слои. Разные слои могут выполнять различные виды преобразований. Сигналы проходят от первого (входного) до последнего (выходного) слоя, возможно, после многократного прохождения слоев. Первоначальная цель нейросетевого подхода состояла в том, чтобы решать задачи так же, как это делает человеческий мозг. Со временем внимание сосредоточилось на подборе определённых интеллектуальных способностей, что привело к отклонениям от биологии, таким как обратное распространение, или передаче информации в обратном направлении и настройке сети для отражения этой информации. Нейронные сети используются для решения различных задач, включая машинное зрение, распознавание речи, машинный перевод, фильтрацию в социальных сетях, видеоигры и медицинскую диагностику. Начиная с 2017 года нейронные сети обычно имеют от нескольких тысяч до нескольких миллионов единиц и миллионы соединений. Несмотря на то что это число на несколько порядков меньше, чем число нейронов в человеческом мозге, эти сети могут выполнять множество задач на уровне, превышающем возможности людей (например, распознавание лиц, игра в го)[36]. Глубокая нейронная сеть (глубинная нейронная сеть, ГНС, англ. DNN — Deep neural network) — это искусственная нейронная сеть (ИНС) с несколькими слоями между входным и выходным слоями[37][38]. ГНС находит корректный метод математических преобразований, чтобы превратить входные данные в выходные, независимо от линейной или нелинейной корреляции. Сеть продвигается по слоям, рассчитывая вероятность каждого выхода. Например, ГНС, которая обучена распознавать породы собак, пройдет по заданному изображению и вычислит вероятность того, что собака на изображении относится к определённой породе. Пользователь может просмотреть результаты и выбрать вероятности, которые должна отображать сеть (выше определённого порога, например), и вернуть в сеть предложенную метку. Каждое математическое преобразование считается слоем, а сложные ГНС имеют много слоев, отсюда и название «глубинные» или «глубокие» сети. ГНС могут моделировать сложные нелинейные отношения. Архитектуры ГНС генерируют композиционные модели, в которых объект выражается в виде многоуровневой композиции примитивов[39]. Дополнительные уровни позволяют составлять элементы из более низких уровней, потенциально моделируя сложные данные с меньшим количеством единиц, чем мелкая сеть с аналогичными показателями[37]. Глубокая архитектура включает в себя множество вариантов нескольких основных подходов. Каждая архитектура нашла успех в определённых областях. Не всегда возможно сравнить производительность нескольких архитектур, если они не были оценены на одних и тех же наборах данных. ГНС, как правило, представляют собой сети с прямой связью, в которых данные передаются от входного уровня к выходному уровню без обратной связи. Сначала ГНС создает карту виртуальных нейронов и назначает случайные числовые значения или «веса» соединениям между ними. Веса и входные данные умножаются и возвращают выходной сигнал от 0 до 1. Если сеть не точно распознала конкретный шаблон, алгоритм будет корректировать весовые коэффициенты, до тех пор, пока не определит коэффициенты, правильно обрабатывающие данные.[40] Глубокое обучение характеризуется как класс алгоритмов машинного обучения, который[41]: Все определения констатируют Состав конкретных нелинейных слоёв зависит от решаемой проблемы. Используются как скрытые слои нейронной сети, так и слои сложных логических преобразований[42]. Система может включать скрытые переменные, организованные послойно в глубоких генеративных моделях, таких как узлы в глубокой сети доверия и глубокой ограниченной машине Больцмана. Алгоритмы глубокого обучения противопоставлены алгоритмам неглубокого обучения по количеству параметризованных преобразований, с которыми сталкивается сигнал, распространяющийся от входного слоя к выходному слою, где параметризованным преобразованием считается такой блок обработки данных, у которого есть обучаемые параметры, такие как веса или пороги[43]. Цепочка преобразований от входа к выходу называется CAP — путём передачи ответственности (англ. credit assignment path, CAP). CAP описывают потенциальные причинные связи вдоль сети от входа к выходу, при этом путь в разных ветвях может иметь разную длину. Для нейронной сети прямого распространения (feedforward) глубина CAP не отличается от глубины сети и равна количеству скрытых слоев плюс один (выходной слой также параметризован). Для рекуррентных нейронных сетей, в которых сигнал может перескакивать через слои минуя промежуточные, CAP из-за обратной связи потенциально неограничен в длине. Не существует универсально согласованного порога глубины деления неглубокого обучения от глубокого обучения, но обычно считается, что глубокое обучение характеризуется несколькими нелинейными слоями (CAP > 2). Йорген Шмидхубер выделяет также «очень глубокое обучение», когда CAP > 10[43]. Глубокое обучение — это алгоритмы машинного обучения для моделирования высокоуровневых абстракций с применением многочисленных нелинейных преобразований[41][42][43][44][45]. В первую очередь к глубинному обучению относятся следующие методы и их вариации: Комбинируя эти методы, создаются сложные системы, соответствующие различным задачам искусственного интеллекта. Глубокое обучение является апробированной выборкой из широкого семейства методов машинного обучения для представлений данных, наиболее соответствующих характеру задачи. Изображение, например, может быть представлено многими способами, такими как вектор интенсивности значений на пиксель, или (в более абстрактной форме) как множество примитивов, областей определённой формы, и т. д. Удачные представления данных облегчают решение конкретных задач — например, распознавания лиц и выражений лица[46]). В системах глубокого обучения автоматизирует сам процесс выбора и настройки признаков, проводя обучение признаков[англ.] без учителя или с частичным привлечением учителя, используя для этого эффективные алгоритмы и иерархическое извлечение признаков[англ.][47]. Исследования в этой области позволили усовершенствовать модели работы с большими объёмами немаркированных данных. Некоторые подходы возникли в результате достижений в области нейронаук, успехов интерпретации обработки информации, построения коммуникационных моделей в нервной системе, таких как нейронное кодирование, связанное с определением отношения между стимулом и нейронными реакциями и взаимосвязи электрической активности между нейронами в головном мозге[48]. Системы глубокого обучения нашли применение в таких областях, как компьютерное зрение, распознавание речи, обработка естественного языка, аудиораспознавание, биоинформатика, где для ряда задач были продемонстрированы существенно лучшие результаты, чем ранее. Несмотря на успехи использования глубинного обучения, у него всё же есть фундаментальное ограничение: модели глубинного обучения ограничены в том, что они могут представлять, и большинство программ нельзя выразить в виде непрерывного геометрического морфинга многообразия данных[49]. Осталось, однако, и скептическое представление, что глубокое обучение — не что иное, как модное слово или ребрендинг для нейронных сетей[50][51]."
  },
  {
    "url": "https://sq.wikipedia.org/wiki/M%C3%ABsimi_i_thell%C3%AB",
    "title": "Mësimi i thellë - Wikipedia",
    "content": "Mësimi i thellë (anglisht: deep learning) është një nëndisciplinë e të mësuarit makinerik e frymëzuar nga shkenca nervore e njeriut për të performuar përmes rrjeteve neurale detyra si klasifikimi, regresioni dhe mësimi përfaqësues. Qëllimi i këtij lloji të mësimit është që ta \"ushtrojë\" një rrjet të neuroneve artificiale që të përpunojë të dhëna. Mbiemri \"i thellë\" i referohet shfrytëzimit të shtresave (që numërojnë nga tre deri në disa qindra ose mijëra) në rrjet. Të mësuarit mund të jetë i mbikëqyrur, gjysëm i mbikëqyrur ose i pambikëqyrur.[1][2][3]"
  },
  {
    "url": "https://si.wikipedia.org/wiki/%E0%B6%9C%E0%B7%90%E0%B6%B9%E0%B7%94%E0%B6%BB%E0%B7%94_%E0%B6%89%E0%B6%9C%E0%B7%99%E0%B6%B1%E0%B7%93%E0%B6%B8",
    "title": "ගැඹුරු ඉගෙනීම - විකිපීඩියා",
    "content": "Deep learning යනු representation learning සහිත neural networks මත පදනම් වූ machine learning ක්‍රමවල උප කාණ්ඩයකි. \"Deep\" යන විශේෂණය ජාලයේ බහු ස්ථර භාවිතය හැඳින්වීමට යොදා ගනී. භාවිතා කරන ක්‍රම supervised, semi-supervised හෝ unsupervised විය හැකිය. [2] Deep-learning ව්‍යුහයන් වන deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks සහ transformers යන තාක්ෂණයන් computer vision, කථන හඳුනාගැනීම, natural language processing, යන්ත්‍ර පරිවර්තනය, bioinformatics, ඖෂධ නිර්මාණය, වෛද්‍ය රූප විශ්ලේෂණය, දේශගුණ විද්‍යාව, ද්‍රව්‍ය පරීක්ෂණ සහ පුවරු ක්‍රීඩා වැඩසටහන් ඇතුළු ක්ෂේත්‍රවලට යොදාගෙන ඇත. මෙම ක්ෂේත්‍රවලදී ඒවා මිනිස් විශේෂඥ කාර්යසාධනයට සමාන සහ සමහර අවස්ථාවලදී එය අභිබවා යන ප්‍රතිඵල ලබා දී ඇත. ස්නායුක ජාල (neural networks) වල මූලික සංකල්පය ජීව විද්‍යාත්මක පද්ධතීන්ගෙන්, විශේෂයෙන්ම මිනිස් මොළයේ ක්‍රියාකාරීත්වයෙන් ආභාෂය ලබා ඇත. මිනිස් මොළයේ තොරතුරු සැකසීමේ හා සන්නිවේදනය කිරීමේ ක්‍රියාවලිය, බෙදා හැරීමේ සන්නිවේදන නෝඩ (distributed communication nodes) මගින් සිදු වන ආකාරය මෙහිදී අනුකරණය කෙරේ. කෙසේ වෙතත්, වර්තමානයේ භාවිතා වන කෘත්‍රිම ස්නායුක ජාල (artificial neural networks) ජීවීන්ගේ මොළයේ සම්පූර්ණ ක්‍රියාකාරීත්වය නිවැරදිව අනුකරණය කිරීමට අදහස් නොකරයි. ඇත්ත වශයෙන්ම, මේවා ජෛව මොළයේ ක්‍රියාකාරීත්වයට සාපේක්ෂව ඉතා සරල හා අඩු ගුණාත්මක ආකෘති (simplified and low-fidelity models) ලෙස සැලකේ. මෙම සරල ආකෘති පවා සැලකිය යුතු ප්‍රායෝගික ප්‍රතිඵල ලබා දීමට සමත් වී ඇත. [3] නූතන deep learning ආකෘති බොහෝමයක් convolutional neural networks සහ transformers වැනි බහු-ස්ථර neural networks මත පදනම් වේ. කෙසේ වෙතත්, ඒවාට propositional formulas හෝ deep belief networks සහ deep Boltzmann machines හි nodes මෙන් deep generative models තුළ ස්ථර වශයෙන් සංවිධානය කර ඇති latent variables ද ඇතුළත් විය හැකිය.[4] මූලික වශයෙන්, deep learning යනු machine learning algorithms පංතියකට අයත් වන අතර, එහිදී ස්ථර ධුරාවලියක් භාවිතා කරමින් ආදාන දත්ත (input data) මඳක් වැඩි අමූර්ත සහ සංයුක්ත නිරූපණයක් බවට පරිවර්තනය කරයි. උදාහරණයක් ලෙස, image recognition ආකෘතියක, මූලික ආදානය රූපයක් (pixels tensor එකක් ලෙස නිරූපණය කරන ලද) විය හැකිය. පළමු නිරූපණ ස්ථරය රේඛා සහ වෘත්ත වැනි මූලික හැඩතල හඳුනාගැනීමට උත්සාහ කළ හැකිය. දෙවන ස්ථරය දාර වල සැකසුම් සංයෝජනය කර කේතනය කළ හැකිය. තෙවන ස්ථරය නාසය සහ ඇස් කේතනය කළ හැකි අතර, සිව්වන ස්ථරය රූපයේ මුහුණක් අඩංගු බව හඳුනාගත හැකිය."
  },
  {
    "url": "https://simple.wikipedia.org/wiki/Deep_learning",
    "title": "Deep learning - Simple English Wikipedia, the free encyclopedia",
    "content": "Deep learning (also called  deep structured learning or hierarchical learning) is a kind of machine learning, which is mostly used with certain kinds of neural networks.[1] As with other kinds of machine learning, learning sessions can be unsupervised, semi-supervised, or supervised. In many cases, structures are organised so that there is at least one intermediate layer (or hidden layer), between the input layer and the output layer. Certain tasks, such as recognizing and understanding speech, images or handwriting, are easy to do for humans. However, for a computer, these tasks are very difficult to do. In a multi-layer neural network (having more than two layers), the information processed will become more abstract with each added layer. Deep learning models are inspired by information processing and communication patterns in biological nervous systems; they are different from the structural and functional properties of biological brains (especially the human brain) in many ways, which make them incompatible with neuroscience evidences.[2][3]"
  },
  {
    "url": "https://sl.wikipedia.org/wiki/Globoko_u%C4%8Denje",
    "title": "Globoko učenje - Wikipedija, prosta enciklopedija",
    "content": "Globoko učenje (angleško deep learning) je področje umetne inteligence, ki obravnava globoke večslojne nevronske mreže. Ime »globoko« so dobile, ker imajo veliko število slojev. Klasično učenje večslojnega perceptrona se imenuje vzvratno razširanje (backpropagation) in omogoča samo učenje »plitvih« večslojnih perceptronov, saj vsak dodaten sloj zelo oteži tovrstno učenje. Odkritje globokih algoritmov je povzročilo preporod umetne inteligence, saj globoke nevronske mreže dosegajo in celo presegajo zmogljivost ljudskih možganov. Glavna področja uporabe so prepoznava govora, prepoznava slik, razumevanje besedila, pisanje besedil in glasbe, učenje na lastnih izkušnjah, simultano prevajanje ... Eden odmevnejših dosežkov globokega učenja je osvojitev naslova svetovnega prvaka v igri go leta 2016. Gre za strateško miselno igro, kjer je možnih več različnih potez, kot je vseh atomov v celotnem znanem vesolju. Pomnjenje vseh potez iz preteklih partij torej ni možno. Zmagovita globoka nevronska mreža AlphaGo se torej mora dobro znajti tudi v novih, še nevidenih okoliščinah. AlphaGo se je sprva seznanil s preteklimi mojstrskimi partijami igre Go, nato pa se je od tod naprej izpopolnjeval sam. Lahko bi rekli da je samouk, ki je v nekaj mesecih napredoval tako dobro, da je aktualnega svetovnega prvaka, gospoda Leeja Sedola premagal z rezultatom tri proti nič.[1] (Angleščina)"
  },
  {
    "url": "https://ckb.wikipedia.org/wiki/%D9%81%DB%8E%D8%B1%D8%A8%D9%88%D9%88%D9%86%DB%8C_%D9%82%D9%88%D9%88%DA%B5",
    "title": "فێربوونی قووڵ - ویکیپیدیا، ئینسایکڵۆپیدیای ئازاد",
    "content": "فێربوونی قووڵ (ھەروەھا ناسراو بە فێربوونی پێکھاتەمەندی قووڵ یان فێربوونی زنجیرەی پلەیی) بەشێکە لە بوارێکی بەرفراوانتر بە ناوی فێربوونی ماشین. فێربوون دەکرێت لە یەکێک لەم سێ شێوەیە بێت: چاودێریکراو، نیوە چاودێریکراو، چاودێرینەکراو. ھەندێک لە مۆدێلەکانی فێربوونی قووڵ ھاوشێوەی سیستمی نۆرۆنیی زیندەوەرەکانن و وەک کۆدکردنی نۆرۆنی کە ھەوڵ دەدات پەیوەندیی نێوان ورووژێنەر و وەڵامە دەمارییەکانی ناو مێشک پێناسە بکات. بیناسازییەکانی فێربوونی قووڵ وەک تۆڕە نۆرۆنییە قووڵەکان، تۆڕەکانی بڕوای قووڵ و تۆڕە نۆرۆنییە گەڕاوەییەکان بۆ زۆر بواری وەک بینینی کۆمپیوتەر، ناسینەوەی ئاخاوتن، پێڤاژۆی زمانی سروشتی، ناسینەوەی دەنگ، پاڵاوتن لە تۆڕی کۆمەڵایەتیدا، وەرگێڕانی ماشین، زیندەئەنفۆرماتیک و سازکردنی دەوا بە کار براون و لە زۆربەیاندا لێھاتووییان وەک مرۆڤ دێتەوە یان تەنانەت باشتریشن. ویکیپیدیای ئینگلیزی"
  },
  {
    "url": "https://sr.wikipedia.org/wiki/%D0%94%D1%83%D0%B1%D0%BE%D0%BA%D0%BE_%D1%83%D1%87%D0%B5%D1%9A%D0%B5",
    "title": "Дубоко учење — Википедија",
    "content": "Дубоко учење (познато као дубоко структурно учење или хијерархијско учење) део је шире породице метода машинског учења базиране на учењу репрезентације података, насупрот алгоритмима који се базирају на листама наредби. Учење може бити надгледано, полу-надгледано или ненадгледано.[1][2][3] Неке репрезентације подсећају на интерпретацију обраде информација и на шаблоне комуникације у биолошком нервном систему, као што је нервно кодирање које дефинише везу између различитих дражи и одговарајућег нервног одговора у мозгу. Архитектуре дубоког учења, као што су дубока нервна мрежа или рекурентна нервна мрежа примењена су на пољима рачунарског вида, препознавања говора, обраде природних језика, препознавања звука, филтрирања друштвених мрежа, биоинформатике и дизајна лекова[4] и постигли резултате једнаке, ако не и боље од резултата стручњака.[5][6] Дубоко учење је класа алгоритама машинског учења који:[7] Слојеви коришћени у дубоком програмирању укључују скривене слојеве вештачке нервне мреже и мноштво исказних формула.[8] Могу укључити и слојевито организоване скривене променљиве у дубоко генерисаним моделима налик онима код Дубоких Болцанових машина. Претпоставка у основи дистрибуираних репрезентација је да се посматрани подаци генеришу интеракцијама слојевитих фактора. Дубоко учење додаје претпоставку да ови слојеви фактора одговарају нивоима апстракције или састава. Различит број слојева и величина слоја могу да обезбеде различите степене апстракције.[1] Архитектура дубоког учења се често конструише помоћу похлепне слој-по-слој методе. Дубоко учење помаже у раздвајању ових апстракција и одабиру карактеристика које побољшавају перформансе.[1] За задатке надгледаног учења, методе дубоког учења избегавају карактеристичан инжењеринг, превођењем података у компактне посредоване репрезентације сличне главној компоненти, и изводе слојевите структуре које уклањају редудантост у репрезентацији. Алгоритми дубоког учења се могу применити за задатке ненадгледаног учења. Ово је важна предност јер су необележени подаци обимнији од обележених података. Примери дубоких структура које могу бити изучаване на ненадгледан начин су компресор неуралне мреже[9] и дубоке мреже веровања.[1][10] Дубоке неуралне мреже се обично тумаче у смислу теорија универзалне апроксимације[11][12][13][14][15]или пробабилистичког закључивања.[7][8][1][2][10][16][17] Универзална теорема апроксимације се односи на капацитет feedforward неуронских мрежа са једним скривеном слојем коначне величине за проксимацију континуиране функције.[11][12][13][14][15] 1989. је Цибенко објавио први доказ за функције сигмуидне активације[12] и генерализован је за дистрибуцију вишеслојних архитектура 1991. године од стране Хорника.[13]\nПробабилистичке интерпретације[16] потичу из области машинског учења. Оне нелинеарне активације разматрају као кумулативну функцију расподеле.[16] Уведене су од стране истраживача, укључујући Хопфилда, Видрова и Наренда и популризоване у анкатема као што је једна од стране Бишопа.[18] Појам дубоко учење уведен је у заједницу машинског учења 1986 од стране Рине Дехтер,[19][9]а вештачке неуронске мреже од стране Игора Ајзенберга и колега 2000. године у контексту Булових неуронских прагова.[20][21] У 2006. години публикација Хинтона, Осиндера и Теха[22][23] показала је како би код вишеслојне feedforward неуронске мреже слој могао бити унапред ефикасно обучен третирајући сваки слој као ненадгледану ограничену Болцманову машину, лепо наштимовану коришћењем надгледане повратне прпопагације.[24] Рад се односи на учење за дубоке мреже веровања. Први општи агоритам рада за надгледане, дупоке, преносне перцептроне објавили су Alexey Grigorevich Ivakhnenko и Лапа 1965. године.[25] Документ из 1971. описао је дубоку мрежу са осам слојева обучених групном методом алгоритма за обраду података.[26] Друге радне архитектуре дубоког учења, посебно оне које су изграђене за рачунарски вид почеле су са Неокогнитроном који је 1980. године представио Фукушима.[27] 1989. ЛиЦун применио је алгоритам за повратну пропагацију, који је од 1970. године био окренут резервном режиму аутоматског диференцирања,[28][29][30][31] до дубоке неуронске мреже с циљем препознавања рукописних ЗИП кодова у пошти. Док је алгоритам фнкционисао, обучавање је трајало три дана.[32] До 1991. године такви системи су коришћени за препознавање 2Д ручно исписаних цифара, док је препознавање 3Д објеката учињено одговарајућим 2Д сликама са ручно изграђеним 3Д објектним модулом. Венг и други су претпоставили да људски мозак не користи монолитни 3Д објектни модел и 1992. године објавили су Кресептрон,[33][34][35] метод за препознавање 3Д објеката у скривеним сценама. Кресептон је слап слојева сличних Неокогнитрону. Међутим док Неокогнитрон очекује од програмера да га ручно споји са карактеристикама, Кресептон је без надзора научио отворен број карактеристика на сваком нивоу, где је свака карактеристика представљена реконструкцијским алгоритмом. Кресептон је учврстио сваки научени објекат из нереда кроз анализу уназад кроз мрежу. Максимално удруживање, сада често усвојено од дубоких неуронских мрежа (нпр. ImageNet тестови) је први пут коришћено у Кресептону да смањи резолуцију позиције за фактор (2x2) до 1 кроз пренос ради бољег генерализовања. Године 1994. Карваљо је заједно са Фејрурстом и Бисетом објавио експерименталне резултате вишеслојне Булове неуралне мреже, такође познате као бестежинске неуралне мреже, која се састоји од саморганизујуће карактеристике екстракције модула неуралне мреже праћене класификацијом неуронске мреже, које су независно обучене.[36] Фреј је 1995. године показао да је могуће обучити (током два дана) мрежу која садржи шест потпуно повезаних слојева и неколико стотина сакривених јединица користећи вејк-слип алгоритам, развојен заједно са Дајеном и Хинтоном.[37] Многи фактори доприносе спорости, укључујући и проблем са градијентом, анализиран 1991. од стране Хохрајтера.[38][39] Једноставни модели који користе ручно направљене карактеристике као што су Габор филтери и машине за подршку векторима (СВМ) бели су популарни током деведесетих и двехиљадитих због рачунских трошкова вештачке неуронске мреже и недостатка разумевања о томе како мозак пролази кроз своје биолошке мреже. И плитко и дубоко учење код вештачких неуронских мрежа истраживано је већ дуги низ година.[40][41][42] Ови методи никад нису превазишли технологију неуниформног унутрашњег Гаусовог мешовитог модела/сакривеног Марковог модела(GMM-HMM) базирану на генеративним моделима говора обучаваних дискриминативно.[43] Кључне потешкоће су анализиране, укључујући градијент смањења[38] и слабу темпоралну корелациону стуктуру у неуронским предиктивним моделима.[44][45] Додатни проблем били су недостатак података за обуку и ограничена рачунарска снага. Већина истраживача за препознавање говора преселила се из неуронских мрежа у тражење генеративног моделирања. Изузетак је био SRI International крајем деведесетих. Финансиран од стране Америчке владе, NSA-а и DARPA-е, SRI је проучавао дубоке неуронске мреже у препознавању говора и звучника. Хеков тим за препознавање звучника дошао је до првог значајног успеха са дубоким неуронским мрежама у процесирању говора у Националном институту за стандарде и технологију за препознавање звучника 1998. године.[46] Док је SRI доживљавао успехе са дубоким неуронским мрежама у препознавању звучника, били су неуспешни у демонстрацији сличног успеха у препознавању говора. Деценију касније, Хинтон и Денг су заједно сарађивали, а затим са колегама из различитих група на Универзитету у Торину, Мајкрософту, Гуглу и IBM-у покренули ренесансу дубоких неуронских мрежа у препознавању говора.[47][48][49][50] Принцип препознавања „сирових” функција над ручно изграђеном оптимизацијом је први пут успешно истражен у архитектури дубоког аутоенкодера на „сировом” спектрограму или у функцијама линеарне филтер-банке крајем 1990. године,[46] показујући своју супериорност над функцијама Мел-Кепстрала које садрже фазе фиксне трансформације из спектрограма. Сирове особине говора, таласних облика, касније су произвеле одличне резултате већег обима.[51] Многи асшекти препознавања говора су били преузети од стране метода дубоког учења званог дуга краткотрајна меморија (LSTM), рекурентна неуронска мрежа објављена од стране Хохрајтера и Шмидхубера 1997.[52] LSTM RNN избегавају проблеме нестајућег градијента и могу да науче задатке „веома дубоког учења”[2] који захтевају сећање догађаја који су се догодили пре хиљаду одвојених временских корака, што је важно за говор. 2003. године LSTM је почео да буде конкурентан традиционалном препознавању говора у одређеним задацима.[53] Касније је комбинован са везивном временском класификацијом (CTC)[54] у стековима LSTM RNN-а.[55] 2015. године, Гуглово препознавање говора је наводно постигло драматични скок перформанси од 49% кроз CTC - обучени LSTM, који су постали доступни кроз Гуглову претрагу гласом.[56] Године 2006, су Хинтон и Салакхутидов су показали како вишеслојна feedforward неуронска мрежа може ефикасно бити обучити тренирајући слој истовремено, третирајући сваки слој као ненадгледану Болцманову машину, а затим је фино подесити помоћу надгледане повратне пропагације.[57] Дубоко учење је део најсавременијих система у различитим дисциплинама, посебно у рачунарском виду и аутоматском препознавању говора (ASR). Резултати о најчешчће коришћеним сетовима евалуације, као што су TIMIT(ASR) и MINIST(класификација слика), као и низ задатака препознавања говора великих речи, постепено се побољшавају.[47][58][59] Конволуционалне неуронске мреже (CNN) су замењене са ASR од стране CTC[54] за LSTM,[52][56][60][61][62][63][64] али су успешније у рачунарском виду. Утицај дубоког учења у индустрији почео је почетком 2000. х, када су CNN већ обрадиле 10% до 20% свих проверки написаних у САД.[65] Индустријске апликације дубоког учења за препознавање говора великих димензија почеле су око 2010. Крајем 2009. године, Ли Денг је позвао Хинтона да сарађује са њим и колегама како би применио дубоко учење за препознавање говора. Они су заједно 2009. организовали NIPS радионицу о дубоком учењу за препознавање говора.[66] Радионица је мотивисана ограничењима дубоких генеративних модела говора, као и могућношћу да се добије способнији хардвер и велике количине података да би дубоке неуронске мреже (DNN) могле постати практичне. Веровало се да ће pre-trening DNN-ови користећи генеративне моделе дубоких вероватних мрежа (DBN) превладати главне потешкоће неуронских мрежа.[49] Међутим, открили су да замењују предобуку са великим количинама података о обуци за једноставну повратну пропагацију када користе DNN са великим, контексно зависним излазнима, произведеним значајно мањим степеном грешке од тада најсавременијег Гаусовог модела смеше (GMM)/скривеног Марковог модела (HMM) као и са напредним генеративним системима заснованим на моделу.[47][67] Природа грешака у препознавању произведена од стране два типа система била је карактеристично различита,[48][66] пружајући техничке увиде у како интегрисати дубоко учење у постојећи високо ефикасни систем за декодирање говора за време рада, који користе сви главни системи за препознавање говора.[7][68][69] Анализа око 2009—2010. године супротставила је модел GMM (и другим генеративним моделима говора) против DNN модела, подстаћући рана индустријска улагања у дубоко учење за препознавање говора,[48][66] што је довело до продорне и доминантне употребе у тој индустрији. Та анализа је обављена упоређивањем перформанси између дискриминативних DNN-а и генеративног модела. Истраживачи су 2010. године проширили дубоко учење од TIMIT-а до препознавања говора великог речника, усвајањем великих излазних слојева DNN-а заснованих на контексно зависним HMM стањима констриушући дрво одлучивања.[70][71][72][68] Побољшање хардвера омогућио је поновно интересовање. 2009. године Nvidia је била укључена у оно што се назива „биг банг” дубоког учења „јер су неуронске мреже дубоког учења обучаване са Nvidia графичким процесорским јединицама (GPU)”.[73] Те године, Гугл Брејн је користио Nvidia GPU како би креирао способне DNN-ове. Нг је утврдио да GPU може повећати брзину система дубоког учења око 100 пута.[74] GPU је посебно погодан за матрично/векторску математику која се користи у машинском учењу.[75][76] GPU убрзавају алгоритме тренинга по редоследу магнитуде, смањивању времена вожње од неколико недеља до неколико дана.[77][78] За ефикасну обраду се могу користити специјализоване хардверске и алгоритамске оптимизације.[79] Године 2012, тим вођен Далом освојио је „Merck Molecular Activity Challenge” користећи глобалне multi-task неуронске мреже како би предвидели биомолекуларни циљ једне дроге.[80][81] 2014. године Хохрајтерова група је користила дубоко учење да детектује токсичке ефекте хемикалија у животној средини у хранљивим материјама, кућним производима и лековима и освојила је „Tox21 Data Challenge”.[82][83][84] Значајни додатни утицаји на слику или објекат осетили су се од 2011. до 2012. године. Иако су CNN-ови били обучавани повратном пропагацијом већ деценијама, GPU импелементирани NN-овима годинама, укључујући CNN-ове, брзе имплементације CNN-а са максималним учинком GPU-а у стилу Циресана и колега потребне су за напредак у рачунарском виду.[75][76][32][85][2] 2011. године је овај систем први пут постигао перформансе надчовека у такмичењу препознавања визуелних узорака. Исте године је освојио такмичење ICDAR кинеског рукописа, а у мају 2012. године освојио је ISBI такмичење за сегментацију слика.[86] До 2011. године CNN-ови нису играли главну улогу на конференцијама о рачунарском виду, али у јуну 2012. чланак Циресана и осталих на водећој конференцији CVPR[5] је показао како максимално удруживање CNN-ова на GPU-у могу драматично побољшати бројне записе о мерењу видљивости. У октобру 2012. године, сличан систем Крижевског и Хинтона[6] победио је на такмичењу „ImageNet” због значајне маргине над методама плитког машинског учења. У новембру 2012. године, систем Циресана и осталих освојио је ICDAR за анализу великих медицинских слика за откривање рака, а наредне године и MICCAI Grand Challenge на исту тему.[87] 2013. и 2014. године степен грешке на задатку „ImageNet” коришћењем дубоког учења додатно је смањена, следи сличан тренд препознавања говора великих размера. Пројекат The Wolfram Image Identification објавио је ова побољшања.[88] Класификација слика је затим проширена на изазовнији задатак генерисања описа за слике, често као комбинација CNN-а и LSTM-а.[89][90][91][92] Вештачке неуронске мреже или везани системи су компјутерски системи инспирисаним биолошким неуронским мрежама које чине животињски мозак. Такви системи уче (прогресивно побољшавају своју способност) да раде задатке узимајући у обзир примере углавном без специфичног програмирања. На пример, у препознавању слике, они могу да науче да идентификују слике које садрже мачке анализом примера слика које су ручно означене као „мачка” или „без мачке” и користе аналитичке резултате за идентификацију мачака на другим сликама. Највише се користе у апликацијама које се тешко изражавају традиционалним компјутерским алгоритмом користећи програмирање засновано на правилима. Вештачка неуронска мрежа заснива се на скупу повезаних јединица званих вештачки неурони (аналогно аксонима у биолошком мозгу). Свака веза (синапса) између неурона може пренети сигнал другом неурону. Пријемни (постсинаптички) неурон може обрадити сигнал(е) и потом сингализирати низводне неуроне који су повезани са њим. Неурони могу имати стање, углавном представљено реалним бројем између 0 и 1. Неурони и синапсе могу имати и тежину која варира као принос учења, што може појачати или смањити јачину сигнала коју шаље низводно. Типично, неурони су организовани по слојевима. Различити нивои могу вршити различите врсте трансформација на својим улазима. Сигнали путују од првог (улазног) до последњег (излазног) слоја, евентуално након што више пута обиђу слојеве. Првобитни циљ приступа неуронске мреже био је да реши проблеме на исти начин као и људски мозак. Током времена, пажња се фокусирала на усклађивање специфичних менталних способности, што је довело до одступања од биологије као што је повратна пропагација или преношење информација у обратном смеру и прилагођавање мреже како би се те информације одразиле. Неуронске мреже су коришћене за разне задатке, укључујући компјутерски вид, препознавање говора, машински превод, филтрирање друштвених мрежа, видео игре и успостављање медицинске дијагнозе. Од 2017. године неуронске мреже обично имају неколико хиљада до неколико милиона јединица и милионе конекција. Упркос томе што је овај број неколико реда величине мањи од броја неурона у људском мозгу, ове мреже могу обављати неке задатке боље од људи (нпр. препознаваље лица[93]). Дубока неуронска мрежа (DNN) је вештачка неуронска мрежа (ANN) са више сакривених слојева између улазних и излазних слојева.[8][2] Дубоке неуронске мреже могу модулирати комплексне нелинеарне везе. Архитектуре дубоке неуронске мреже генеришу композиционе моделе у којима је објекат изражен као слојевита композиција примитивних типова података.[94] Виши слојеви дозвољавају композицију карактеристика из нижих слојева, потенцијално моделирајући сложене податке са мање јединица које је слично извођењу плитке мреже.[8] Дубоке архитектуре укључују многе варијанте неколико основних приступа. Свака архитектура је нашла примену у одређеним доменима. Није увек могуће упоређивати перформансе више архитектура, осим ако се не процењује над истим скупом података. Дубоке неуронске мреже су типичне преносне мреже у којима подаци пролазе од улазног до излазног слоја без повратне петље. Рекурентне неуронске мреже у којима подаци могу протицати у било ком смеру се користе за апликације као што су моделирање језика.[95][96][97][98] Дуга краткотрајна меморија је нарочито ефикасна за ову употребу.[99] Конволуцијске дубоке неуронске мреже се користе у рачунараском виду.[100] Оне се такође користе код акустичког моделирања за аутоматско препознавање говора (АСР). Као и са ANN-овима, многи проблеми могу да се јаве лоше обучаваним DNN-овима. Два најчешћа проблема су overfitting и време обављања операција. DNN су склони overfitting-гу због додатних слојева апстракције, који им дозвољавају да моделирају ретке зависности у подацима за обраду. Методе регуларизације као што су Ивахненко-ва одсецање јединица или смањивање тежине(l2-регуларизација) или проређеност (l1-регуларизација) могу бити примењене током обучавања да би се превазишао overfitting. Алтернативно насумично прекидање регуларизације искључује јединице мере из скривених слојева током обучавања. Ово помаже да се заобиђу ретке зависности. Коначно, подаци могу бити измењени методама као што су сечење и ротирање тако да мањи сетови у обучавању могу бити повећани на величину довољну да се смањи ризик од overfitting-а. DNN морају да размотре многе параметре обучавања као што је величина (број слојева и број јединица по слоју), распон учења и почетне тежине. За оптималне параметре може пролажење кроз простор параметара може бити неоствариво због временских трошкова и ресурса прорачуна. РАзни трикови као што су серије (израчунавање градијента неколико примера обучавања одједном уместо појединачних) убрзавају прорачун. Велика оптимуѕација обраде користећи GPU произвела је значајна урзавања у обучавању зато што је захтевани матрични и векторски прорачун одговарајући за GPU. Алтернативно, инжињери могу тражити друге типове неуронских мрежа са јаснијим и конвергентнијим алгоритмима обучавања. CMAC (cerebellar model articulation controller) је једна од тих неуронских мрежа. Не захтева распоне учења ни насумичне почетне тежине. Процес обучавања гарантује укрштање у неком кораку са неком серијом података, комплексност прорачуна алгоритма обучавања је линеарна узимајући у обзир број укључених неурона. Аутоматско препознавање говора великих размера је први и најуспешнији случај дубоког учења. LSTM RNN могу да науче задатке „веома дубоког учења”[2] који укључују интервале од више секунди које садрже говорне догађаје раздвојене хиљадама дискретних временских корака, при чему једном временском кораку одговара око 10ms. LSTM са заборављеним гејтовима[99] је конкурентан традиционалном препознаватељима говора на одређеним задацима.[53] Почетни успех у препознавању говора базиран је на задацима мале размере заснованих на TIMIT-у. Скуп података садржи 630 говорника из осам главних дијалеката америчког енглеског, где сваки говорник чита 10 реченица.[101] Његова мала величина дозвољава да се пробају многе конфигурације. Још важније, задатак TIMIT-а се односи на препознавање редоследа гласовних секвенци, што за разлику од препознавања редоследа речи дозвољава слабе језичке конструкције (без јаке граматике). Овим се слабости у акустичном моделирању аспеката препознавања говора лакше анализирају. У наставку су наведене стопе грешке, укључујући раније резултате мерене као проценат грешке у гласу сумираних у последњих 20 година: Почетак дубоке неуронске мреже за препознавање говорника крајем 1990-их и препознавање говора 2009-2010 и LTSM 2003-2007 убрзао је напредак у осам главних области:[7][50][68] Сви главни комерцијални системи за препознавање говора (на пример: Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri итд.) заснивају се на дубоком учењу.[7][104][105][106] За класификацију слика постављен је заједнички проценат података који су подаци MNIST базе података. MNIST се састоји од цифара писаних руком и обухвата око 60000 примера обуке, као и 10000 тест примера. Као и код TIMIT-а, мала величина омогућава корисницима да тестирају више конфигурација. Доступна је свеобухватна листа резултата ове базе.[107] Напредак који је постигнут у препознавању слика је допринео томе да се техника дубоког учења примени и на различите визуелне уметничке задатке. DNN су корисни за: Неуронске мреже су коришћене за имплементацију језичких модела одд почетка 2000-их.[95][110] LSTM је помогао у побољшању машинског превођења и језичког моделирања.[111][96][97]\nОстале кључне технике у овој области су негативно узимање узорака[112] и уношење речи. Уношење речи као што је word2vec, може се сматрати као репрезентацијски слој у архитектури дубоког учења који трансформише атомску реч у позиционо представљање речи у односу на друге речи у скупу података, позиција је предтављена као тачка у векторском простору. Коришћење уноса текста као RNN улазнног слоја омогућава мрежи да раздваја реченице и фразе користећи ефективну композициону векторску граматику. Композициона векторска граматика се може сматрати као вероватно контексна слободна граматика(PCFG) коју имплементира RNN.[113] Рекурзивни ауто-енкодери направљени изнад уноса речи могу проценити сличност и детектовати парафразирање.[113] Дубоке неуронске архитектуре пружају најбоље резултате за анализирање изборних група,[114] сентиментална анализа,[115] повраћај информација,[116][117] разумевање говорног језика,[118] машински превод,[111][119] повезивање контекста,[119] препознавање стила писања[120] и друго. Google Translate (GT) користи велику end-to-end краткорочну меморијску мрежу.[121][122][123][124][125][126] GNMT користи метод машинског превођења заснован на примеру у којем систем „учи од милион примера”.[122] Он преводи \"целе реченице у исто време, а не у деловима. Google Translate подржава више од сто језика.[122] Мрежа енкодира семантику реченице; уместо меморисања преведених реченица.[122][127] Google Translate користи енглески језик као посредника у већини језичких парова.[127] Велики проценат нових лекова не успева да добије регулаторно одобрење. Ови неуспеси су узроковани недовољном ефикасношћу (on-target effect), нежељеним интеракцијама (off-target effect) или неочекиваним токсичним ефектима.[128][129] Истраживало се коришћење дубоког учења за предвиђање биомолекуларних мета,[80][81] не-циљане и токсичне ефекте хемикалија у животној средини, у храњивим састојцима, кућним производима и лековима.[82][83][84] AtomNet је систем дубоког учења заснованог на структури дизајна лекова.[130] AtomNet је коришћен за предвиђање нових биомолекула за циљеве болести као што су Ebola virus[131] и multiple sclerosis.[132][133] Дубоко учење коришћено је за приближавање вредности могућих директних маркетиншких акција, дефинисаних у смислу RFM варијабли. Показало се да функција процењене вредности има природну интерпретацију као доживотна вредност потрошача.[134] Системи препорука користе дубоко учење да искористе значајне карактеристике модела латентног фактора за препоруке за музику засноване на садржају.[135] У биоинформатици је коришћен ауто-енкодер АNN, да би се предвиделе анотације генетске онтологије и односи између генетских функција.[136] У медицинској информатици, дубоко учење се користило за предвиђање квалитета сна заснованог на подацима[137][138] и предвиђања здравствених компликација из података електронског здравственог записа.[139] Проналажење одговарајуће мобилне публике за мобилно оглашавање[140] увек је изазовно, пошто се многе тачке података морају узети у обзир и асимилирати пре него што се циљни сегмент може креирати и користити у огласима који сервирају било који огласни сервер. Дубоко учење коришћено је за тумачење великих, многодимензионалних података о оглашавању. Многе тачке података прикупљају се током циклуса тражења/сервирања/кликтања интернет оглашавања. Ове информације могу бити основа за машинско учење како би се побољшала избор огласа. Дубоко учење је успешно примењивано на инверзне проблеме као што су уклањање буке, супер-резолуција рестаурација фотографија. Ова примена укључује методе учења као штп је „Поља смањивања за ефективну рестаурацију слика” која се обучава на скупу података слике. Дубоко учење је уско повезано са класом теорије развоја мозга (конкретно, развој неокортекса) које су предложили когнитивни неуронаучници почетком деведесетих.[141][142][143][144] Ове развојне теорије су инстанциране у рачунским моделима, што их чини претходницима система дубоког учења. Ови развојни модели имају особину као и различите динамике учења у мозгу које подржавају самоорганизацију донекле аналогну неуронским мрежама које се користе у моделима дубоког учења. Као и неокортекс, неуронске мреже примењују хијерархију слојевитих филтера у којим сваки слој разматра информације са претходног слоја, онда преноси њихове излазне податке (а некад и улазне) осталим слојевима. Овај процес омогућава самоорганизујући стек трансдуктора добро прилагођених околини у којој раде. Опис из 1995 каже: „Мозак детета се организује сам под утицајем таласа трофичних фактора... различите регије у мозгу се узастопно повезују, тако што један слој ткива сазрева пре следећег итд. док цео мозак не сазри”. Различити приступи су коришћени да би се испитала веродостојност модела дубоког учења из неуробиолошке перспективе. С једне стране, неколико варијанти backpropagation алгоритма су предложене како би се повећала реалност обраде. Други истраживачи тврде да форме дубоког учења које нису надгледане, као што су оне базиране на хијерархијским генеративним моделима могу бити ближе биолошкој стварности. Поштовањем овог, модели генеративних неуронских мрежа су повезани са неуробиолошким доказима о обради података на основу узимања узорака у церебралном кортексу. Иако систематско поређење организације људског мозго и неуронског кодирања у дубоким мрежама још увек није установљено, предложено је неколико аналогија. На пример прорачуни извршени јединицама дубоког учења могу бити слични онима у неуронима. Слично, прикази развијени моделима дубоког учења су слични онима мереним у примарном визуелном систему како на појединачном нивоу тако и на нивоу популације. Дубоко учење привукло је и критике и коментаре, у неким случајевима и у областима ван компјутерских наука. Главна критика се односи на недостатак теорије методе. Учење у дубокој архитектури се најчешће спроводи коришћењем познатог градијента. Међутим, теорија која описује друге алгоритме, као што је контрастна дивергенција, је мање јасна. Методи дубоког учења су углавном потврђени емпиријски, а не теоријски.[145] Други истичу да дубоко учење треба посматрати као корак ка остварењу вештачке ителигенције (AI), а не као свеобухватно решење. Упркос моћи метода дубоког учења, и даље недостаје велики део функционалности потребног за реализацију овог циља у потпуности. Психолог Gary Marcus је приметио: „Дубоко учење је само део већег изазова изградње интелигентних машина. Овакве технике немају начин за представљање узрочних односа (...) немају очигледне начине обављања логичких закључака, а и даље су далеко од интеграције апстрактног знања, као што су информације о томе шта су објекти, за шта су, и како се обично користе. Најснажнији AI систем, као Вотсон (...) користе технике попут дубоког учења као само један елемент у врло сложеном саставу техника, у распону од Бајесовог закључка до дедуктивног образложења”.[146] Као алтернативу овим ограничењима дубоког учења, један аутор наводи како би било могуће обучити машинску визију да изврши софистициран задатак који разликује „старе мајсторе” од аматерског цртања и претпостављао да таква сензитивност може представљати почетак нетривијалне емпатије машине.[147] Исти аутор је предложио да то буде у складу са антропологијом, која идентификује забринутост са естетиком као кључним елементом понашања модерне.[148] Идеја да се уметничка осетљивост може наћи унутар релативно ниских нивоа когнитивне хијерархије, довела је до објаве серија графичких приказа унутрашњих стања дубоких (20-30 слојева) неуронских мрежа које покушавају да препознају унутар суштински случајних података слике на којима су обучаване[149] и демонстрирају визуелну изглед: првобитно истраживање је имало преко 1.000 коментара и било је предмет онога што је једно време био најчитанији чланак на веб страници The Guardian.[150] Неке архитектуре дубоког учења приказују проблематично понашање,[151] као што је разврстање непрепознатљивих слика као припадника познатих категорија обичних слика[152] и погрешно разврстање мањих кругова исправно класификованих слика.[153] Goertzel је претпоставио да су ова понашања последица ограничења у њиховим унутрашњим представама и да би ова ограничења инхибирала интеграцију у хетерогене мулти-компонентне AGI архитектуре.[151] Овим питањима се могу решавати архитектуре дубоког учења које интерно формирају хомологне слике-граматике[154] разлагања посматраних ентитета и догађаја.[151] Учење граматике (визуелно или језички) из података за обуку би било еквивалентно ограничавању система на шаблонско разјашњење које функционише на концептима граматичких правила производње и представља основни циљ и природног језика[155] и AI.[156] Како се дубоко учење помера из лабораторије у свет, истраживање и искуство показују да су вештачке неуронске мреже подложне хаковањима и преварама. Идентификујући обрасце које ови системи користе за функционисање, нападачи могу да модификују улазе у ANN-у на такав начин да ANN проналази поклапања која људи неће препознати. На пример, нападач може направити мале промене на слици такве да ANN пронађе поклапање, а да човеку слика не изгледа као предмет претраге. Таква манипулација се назива „контрадикторни напад”. У 2016. истраживачи су користили један ANN да поправе слике на пробни начин и са грешкама, да идентификују жижне течке и генеришу слике које су их обмануле. Ове измењене слике су изгледале потпуно исто људском оку. Још једна група је показала да су одштампане модификоване слике успешно превариле систем класификације слика. Једна од одбрана је обрнута претрага слика при којој је потенцијално лажна слика прослеђена сајту као што је TinEye, који онда може да пронађе друге сличне примере. Сужавање претраге се постиже коришћењем само делова слике како би се идентификовале слике са којих је тај део потенцијално узет. Још једна група је показала да психоделичне цестице могу да преваре систем препознавања лица тако да обичне људе препознаје као познате, потенцијално дозвољавајући некој особи да имитира неку другу. У 2017. ови истраживачи су додали стикере стоп знацима што је проузорковало да их ANN погрешно класификује. ANN могу да буду даље обучавани да детектују покушаје преваре потенцијално креирајући систем одбране сличан индустрији одбране од малвера. Још једна група је демонстрирала да одређени звукови могу да натерају Google Now гласовни комадни систем да отвори одређени сајт који би скинуо малвер. У „тровању података” лажни подаци се континуално увлаче у сет за обучавање система машинског учења да га спрече да оствари циљ."
  },
  {
    "url": "https://sh.wikipedia.org/wiki/Duboko_u%C4%8Denje",
    "title": "Duboko učenje – Wikipedija / Википедија",
    "content": "Duboko učenje je podoblast mašinskog učenja zasnovana na  korišćenju neuronskih mreža. Pridjev \"duboko\" odnosi se na korištenje više slojeva u mreži. Korištene metode mogu biti supervizirane, polu-supervizirane ili nesupervizirane .[2] Arhitekture dubokog učenja kao što su duboke neuronske mreže, mreže dubokih verovanja, rekurentne neuronske mreže, konvolucione neuronske mreže i transformeri primenjuju se u oblastima uključujući računarsku viziju, prepoznavanje govora, prirodno jezičko procesiranje, mašinsko prevođenje, bioinformatiku, dizajn lekova, analiza medicinskih slika, klimatske nauke, inspekciju materijala i programe za društvene igre, gde su postigli rezultate koji su ponekad nadmašili ljudsku stručnost.[3][4][5] Rani oblici neuronskih mreža inspirisane su procesiranjem informacija i distribuiranim komunikacionim čvorovima u biološkim sistemima, posebno u ljudskom mozgu. Međutim, trenutne neuronske mreže ne pokušavaju da modeliraju funkciju mozga organizama i generalno se smatraju modelima niskog kvaliteta za tu svrhu [6] Moderni algoritmi dubokog učenja se zasnivaju na neuronskim mrežama sa više slojeva. Slojevi se sastoje od neurona. Neki od tipa neuronskih mreža koje imaju ovakvu arhitekturu jesu konvolucione neuronske mreže, kao i transformeri..[7] U suštini, duboko učenje se odnosi na algoritme mašinskog učenja koji imaju određenu hijerarhiju slojeva. Ova sekvencija slojeva služi da pretvori ulazne podatke u neku određenu astrakciju i složenu reprezentaciju tog ulaza. U slučaju računarskoj vida, prvi sloj može predstavljati apstrakcije u vidu linija i krugova, zatim drugi sloj, može prepoznati ivice, dok sledeći sloj može prepoznavati kompleksnije oblike, poput nosa, usta i očiju u slučaju rešavanja problema detekcije lica. Važno je napomenuti da proces dubokog učenja može sam da nađe mesto gde će koje karakteristike naučiti u arhitekturi. Prije dubokog učenja, tehnike mašinskog učenja često su uključivale ručno pravljene karakteristike gde bi se podaci transformisali u prikladniji oblik za algoritam sa kojim će raditi. U pristupu dubokog učenja, karakteristike nisu ručno izrađene i model automatski otkriva korisne karakteristike iz podataka. Ovo ne eliminiše potrebu za ručnim podešavanjem; na primjer, parametri poput broj slojeva i broj neurona po sloju mogu različito uticati na proces konvergencije i pronalaženje modela koji najbolje radi za dati problem.[2][8] Riječ \"duboko\" u terminu \"duboko učenje\" odnosi se na broj slojeva kroz koje se podaci transformiraju. Preciznije, sistemi dubokog učenja imaju značajnu dubinu kreditnog puta (eng. credi assignment path, CAP). CAP je lanac transformacija od ulaza do izlaza. CAP-ovi opisuju potencijalno uzročne veze između ulaza i izlaza. Za \"feedforward\" neuronsku mrežu, dubina CAP-ova je dubina mreže, gde predstavlja broj skrivenih slojeva plus jedan (pošto je izlazni sloj također parametriran). Za rekurentne neuronske mreže, u kojima se signal može propagirati kroz sloj više puta, dubina CAP-a je potencijalno neograničena.[9] Nijedan univerzalno dogovoreni prag dubine ne dijeli plitko učenje od dubokog učenja, ali većina istraživača se slaže da duboko učenje uključuje CAP dubinu veću od 2. Pokazalo se da je CAP dubine 2 univerzalni aproksimator u smislu da može emulirati bilo koju funkciju.[10] Osim toga, više slojeva ne doprinosi boljoj sposobnosti aproksimatora funkcije mreže. Duboki modeli (CAP > 2) mogu izdvojiti bolje karakteristike od plitkih modela i stoga dodatni slojevi pomažu u efikasnom učenju karakteristika, ali veći broj slojeva takođe može dovesti i do problema [[nadprilagođavanja]] modela. Arhitekture mreža dubokog učenja mogu biti konstruisane po principu pohlepnog algoritma za svaki sloj u arhitekturi. Svaki sloj pronalazi najoptimalnije rešenje za taj sloj, dok metode dubokog učenja pomažu da odluče koje od ovih apstrakcija najoptimalnije rešavaju zadati problem. Algoritmi dubokog učenja mogu se primijeniti na zadatke učenja bez supervizije. Ovo je važna prednost jer su nelabelirani podaci mnogo brojniji od labeliranih podataka. Primjeri ovakvih struktura koje se mogu obučiti na način bez supervizije su mreže duboke verovatnoće  .[8][11] Duboke neuronske mreže se generalno tumače u terminima teoreme univerzalne aproksimacije [12][13][14][15]  ili probabilističkog zaključivanja .[8][9][16][17][18] Inicijalno su postojale su dvije vrste veštačkih neuronskih mreža (ANN): feedforward neuronske mreže  (FNN) i rekurentne neuronske mreže (RNN). RNN-ovi su bazirani na cikličnom povezivanju u svojoj arhitekturi, dok FNN nisu. Wilhelm Lenz i Ernst Ising su kreirali Izingov model [19] koji je u suštini RNN arhitektura koja se ne može trenirati i koja se sastoji od elemenata sličnih neuronima. Godine 1972. Shun'ichi Amari je ovu arhitekturu učinio adaptivnom.[20][21] Njegovu RNN koja može da uči, jeste popularizirao John Hopfield 1982.[22] Krajem 2000-ih, duboko učenje je počelo da nadmašuje druge metode u takmičenjima u mašinskom učenju. Godine 2009., LSTM mreža (Alex Graves et al) [23] je bio prvi RNN koji je pobijedio na takmičenjima, pobijedivši u tri takmičenja u  prepoznavanju rukopisa. Značajni uticaji na prepoznavanje slika ili objekata osjetili su se od 2011. do 2012. godine. Iako su CNN-ovi obučeni propagacijom unazad postojali decenijama,[24][25] kao i GPU implementacije neuronskih mreža godinama,[26] uključujući i CNN-ove,[9] bile su potrebne brže implementacije CNN-a na GPU-ima za napredak u kompjuterskom vidu . Godine 2011. DanNet [3][27] Dana Ciresana, Ueli Meier, Jonathan Masci, Luca Maria Gambardella i Jürgen Schmidhuber postigao je prvi put nadljudsku izvedbu na takmičenju u prepoznavanju vizuelnih obrazaca, nadmašujući tradicionalne metode za faktor od 3 [9] Takođe 2011. DanNet je pobijedio na ICDAR takmičenju kineskog rukopisa, u maju 2012. pobijedio je na ISBI takmičenju za segmentaciju slika.  CNN-ovi do 2011. nisu igrali veliku ulogu na konferencijama o kompjuterskom vidu, ali je u junu 2012. objavljen rad Ciresana et al. na vodećoj konferenciji CVPR [3] pokazao je kako maksimalno udruživanje CNN-ova na GPU može dramatično poboljšati mnoge rekorde za mjerenje vida. U septembru 2012. DanNet je također pobijedio na ICPR takmičenju za analizu velikih medicinskih slika za otkrivanje raka, a naredne godine i na MICCAI Grand Challenge-u na istu temu.[28] U oktobru 2012. sličan AlexNet od Alexa Krizhevskyja, Ilye Sutskevera i Geoffreya Hintona [4] pobijedio je na velikom takmičenju ImageNet sa značajnom razlikom u odnosu na plitke metode mašinskog učenja. Mreža VGG-16 Karen Simonyan i Andrewa Zissermana  dodatno je smanjila stopu greške i pobijedila na takmičenju ImageNet 2014, prateći sličan trend u prepoznavanju govora velikih razmjera. Klasifikacija slika je zatim proširena na izazovniji zadatak generiranja opisa (naslova) za slike, često kao kombinacija CNN-a i LSTM-a.[29][30][31] Veštačke neuronske mreže ( ANN ) ili konekcionistički sistemi su računarski sistemi inspirisani biološkim neuronskim mrežama koje čine životinjski mozak. Takvi sistemi uče (progresivno poboljšavaju svoju sposobnost) da rade zadatke razmatranjem primjera, općenito bez programiranja specifičnog za zadatak. Na primjer, u prepoznavanju slika, mogli bi naučiti identificirati slike koje sadrže mačke analizom primjera slika koje su ručno označene kao \"mačka\" ili \"bez mačke\" i korištenjem analitičkih rezultata za identifikaciju mačaka na drugim slikama. Najviše su koristili u aplikacijama koje je teško izraziti tradicionalnim kompjuterskim algoritmom koristeći programiranje zasnovano na pravilima ."
  },
  {
    "url": "https://fi.wikipedia.org/wiki/Syv%C3%A4oppiminen",
    "title": "Syväoppiminen – Wikipedia",
    "content": "Syväoppiminen (engl. deep learning) on joukko tekoälymenetelmiä, joissa hyödynnetään usean kerroksen neuroverkkoja.[1][2] Syväoppiminen perustuu keinotekoisiin hermoihin, jotka muodostavat monikerroksisen neuroverkon. Hermot eli neuronit laskevat numeerisen tuloksen yhdestä tai useammasta numeerisesta, muilta neuroneilta tai syötteestä saadusta hermoärsykkeestä, perustuen sisäiseen painoarvoon.[3][4] Syväoppimisen tavoitteena on luoda algoritmien avulla neuroverkko, joka pystyy ratkaisemaan sille annetut ongelmat.[5] Syväoppimista käytetään erityisesti sellaisten ongelmien ratkaisemiseen, joissa perinteisillä menetelmillä tehdyt ratkaisut vaatisivat erittäin monimutkaisia sääntöjä. Syväoppimista käytetään esimerkiksi puheen, kuvien ja tekstien tunnistamiseen tai käsittelyyn.[6] Ensimmäisen kokeelliset neuroverkot kehitettiin 1950-luvulla, jolloin käytettiin yhtä kerrosta neuroneja. Kiinnostus lopahti kun todettiin yksinkertaisten verkkojen rajallisuus. 1970- ja 1980-luvuilla Geoffrey Hinton tutki neuroverkkoja. Hinton julkaisi vuonna 1986 David Rumelhartin ja Ronald Williamsin kanssa merkittävän artikkelin backpropagation-tekniikasta.[7] Neuroverkkojen kehitys laantui 1980- ja 1990-luvuilla saatujen menestysaskelien jälkeen. Princetonin yliopistossa professori Fei-Fei Li työskenteli kuvatunnistusmenetelmän parissa, jossa 14 miljoonaa kuvaa oli luokiteltu 22 tuhanteen kategoriaan. Tätä kutsuttiin ImageNetiksi ja Li jatkoi sen kehitystä siirryttyään Stanfordiin vuonna 2009. ImageNet sai huomiota, kun Toronton yliopistosta Geoffrey Hintonin ryhmä koulutti neuroverkon ImageNetin tietojoukolla ja saavutti merkittävästi paremman suorituskyvyn kuvien tunnistamisessa. Tämä oli ensimmäinen Lin järjestämään kilpailuun osallistunut syvä neuroverkko.[7] Moderneissa neuroverkoissa voi olla tuhansista miljardeihin neuronia. Vuonna 1998 Yann LeCun kirjoitti neuroverkosta, jossa oli seitsemän kerrosta ja 60 000 koulutettavaa parametria. Vuonna 2012 esitellyssä AlexNetissä oli kahdeksan kerrosta, mutta 60 miljoonaa parametria. Tuolloin yhdistyivät neuroverkot, jättimäiset datajoukot sekä grafiikkaprosessorilla suoritettu laskenta, jolloin voitiin käsitellä paljon suurempia tietojoukkoja.[7][8] Työn alussa neuroverkon rakenne suunnitellaan ja sen painoarvot alustetaan. Alustettava painoarvo voi olla neuroneissa ennalta määritetty muuttumaton arvo tai satunnaislukugeneraattorilla asetettu luku.[3][4] Ohjatussa oppimisessa (engl. supervised learning) neuroverkolle syötetään ennalta luokiteltuja näytteitä. Neuroverkon tuottamien arvojen ja näytteen luokittelun perusteella neuronien painoarvoja muutetaan siten, että neuroverkko tuottaisi syötteen luokittelun mukaisen tuloksen. [3][4][9][10] Ohjaamattomassa oppimisessa (engl. unsupervised learning) syväoppimisen -malli (engl. deeplearning model) yrittää automaattisesti luokitella syötettä, ilman että sitä ohjataan millään tavalla.[10] Ohjaamatonta oppimista ovat muun muassa sellaiset autoenkooderit, jotka muuttavat vastaanottamansa syötteen matalampiuloitteiseen muotoon, ja yrittävät muokata sen takaisin alkuperäiseksi syötteeksi. Osittain ohjatussa oppimisessa (engl. semi-supervised learning) osa näytteistä on luokiteltuja ja osa ei.[10] Tämä voi tarkoittaa esimerkiksi sitä, että osaa verkosta esiharjoitetaan ohjaamattomasti, ja sitten tätä neuroverkkoa käytetään osana ohjattua arkkitehtuuria. Vahvistusoppimisessa (engl. reinforcement learning) syväoppimisen -malli oppii palautteen perusteella, jota ympäristö antaa suorituksen aikana.[10] Syväoppiminen on vielä uutta, mutta sen odotetaan tulevan käytännön sovelluksiin. Syväoppimisen avulla joidenkin työtehtävien rutiiniluonteisin osa voidaan hoitaa vähintään puoliautomaattisesti, minkä avulla saavutetaan säästöjä. Syväoppimista hyödynnetään jo erityisesti tehtävissä, joissa täytyy ymmärtää puhetta tai kirjoitettua tekstiä, tai tulkita kuvamateriaalia.[11] Yksi syväoppimisen eduista on kuvantunnistus. Syväoppimisessa ei tarvita tunnistamiseen vaadittavaa suurta määrää ohjelmallisia sääntöjä. Neuroverkolle vain syötetään suuri määrä luokiteltuja valokuvia halutusta aihepiiristä, ja järjestelmä säätää neuroverkon toimintaa niin, että tunnistustarkkuus paranee kohti maksimia. Niinpä kissat voidaan nykyisin tunnistaa kuvista jo 90 prosentin tarkkuudella. Syväoppiminen myös paranee mitä enemmän ja parempaa dataa järjestelmälle annetaan.[11] Digitaalisessa signaalinkäsittelyssä syväoppimista voidaan yhdistää digitaalisen signaaliprosessorin kanssa kohinan vaimennukseen, jolloin puhe tulee selkeämmin esiin.[12] Eräs syväoppimisen sovellus on esineiden poimiminen, joka on aina ollut vaikea opettaa robotille. Syväoppimisen avulla MIT-yliopisto pystyi parantamaan robotin kykyä poimia esineitä pudottelematta.[11] Terveydenhuollossa syväoppimista testataan auttamaan lääkäreitä diagnosoimaan sairauksia potilaan terveystietojen ja testitulosten perusteella.[11] Internetin keskustelupalstoja on mahdollista moderoida koneoppimisen järjestelmillä, ja chat-järjestelmissä niitä voidaan hyödyntää asiakkaiden tiedusteluihin vastaamisessa.[11] Google on yksi syväoppimisen soveltamisen edelläkävijöistä. Google aloitti syväoppimisen tutkimisen Google Brain -projektillaan vuonna 2011. Seuraavana vuonna yhtiö ilmoitti rakentaneensa neuroverkon, joka toimi 16 000 tietokoneella ja pystyi tunnistamaan kissoja kuvista sen jälkeen kun sille oli näytetty 10 miljoonaa kuvaa. Vuonna 2014 Google osti brittiläisen syväoppimisstartup-yrityksen nimeltä DeepMind. Google on käyttänyt syväoppimista indeksoimiensa kuvien tehokkaampaan luokitteluun ja parantaakseen siten hakutuloksia. Google on käyttänyt menetelmää myös epätäydellisten kuvien täydentämisessä muiden samankaltaisten kuvien pohjalta. Videopalvelussaan Google pystyy luomaan syväoppimisen avulla automaattisia yhteenvetoja ja turvallisuusvaroituksia. Googlen puheentunnistustekoäly käyttää syviä neuroverkkoja oppiakseen ymmärtämään puhuttuja käskyjä ja kysymyksiä. Google Brainiin on sisällytetty myös Googlen käännöspalvelu, ja koko järjestelmä kirjoitettiin uudelleen ja siirrettiin uuteen syväoppimisympäristöön nimeltä Google Neural Machine Translation. Google käyttää syväoppimista myös YouTube-suositustensa parantamiseen: neuroverkot tutkivat kaiken mahdollisen käyttäjien tavoista ja mieltymyksistä ja opettelevat päättelemään, mikä saisi heidät jatkamaan videoiden katselua.[13] Vuonna 2015 Google muutti syväoppimiseen keskittyvän TensorFlow-ohjelmointialustansa avoimeen lähdekoodiin. Googlen robottiauto Waymo on käyttänyt syväoppimisalgoritmeja. Deep Mind tutkii terveydenhuoltoprojekteja, joissa etsitään keinoja havaita silmävammoja ja syöpäkasvaimia aikaisissa vaiheissa.[14] DeepMind käytti vuonna 2014 \"syvää vahvistusoppimista\" opettamaan tekoälylle tietokonepelien pelaamista. Menetelmä yhdisti vahvistusoppimisen syvän neuroverkon harjoittamiseen. Vuonna 2016 DeepMindin AlphaGo-ohjelma löi ensimmäisenä maailman parhaat ihmispelaajat lautapeli gossa. AlphaGo yhdisti tavanomaisen tekoälyn logiikan syväoppimisen tuottamaan intuitiiviseen tehoon, joka on gossa erityisen tärkeää. Tuloksena oli ylivoimainen itseoppinut ohjelma, jonka siirtoja on kuvattu yhtä aikaa vahvoiksi ja erittäin intuitiivisiksi.[15] DeepMindin seuraava ohjelma AlphaZero opetteli vuonna 2017 yhdessä vuorokaudessa syviä neuroverkkoja käyttäen pelaamaan shakkia paremmin kuin maailman vahvimmat shakkiohjelmat. Ohjelman menestys perustui sen ihmismäiseen tapaan valikoida siirrot pelkän raa'an laskemisen sijaan.[16] Syväoppiminen ei kykene oppimaan käsitteitä, vaan ainoastaan käsitteiden ilmentymiä. Niinpä videopeliä pelaamaan oppinut neuroverkko saattaa oppia toimimaan tietyllä tavalla, joka voidaan ilmaista yksinkertaisena sääntönä, mutta tämä sääntö ei neuroverkosta ole käytännössä luettavissa. Tässä mielessä syväoppiminen on varsin pinnallista, ja syväoppimisella saatuja tuloksia on helppo hämätä esimerkiksi muuttamalla syötettä siten, että se vain vähän poikkeaa opetuksessa käytetystä datasta. Oppimisen tuloksiin ei myöskään voi tästä syystä luottaa [17]. Kuvantunnistuksessa syväoppimisen on uskottu oppivan kuvissa esiintyviä käsitteitä, kuten 'koira' tai 'kissa', mutta neuroverkon herkkyys pienille kuvan muutoksille, joilla koira saadaankin luokiteltua kissaksi, osoittaa, että tämä ei pidä paikkaansa. Lisäksi neuroverkon syvyyden lisääminen ei juurikaan auta luokittelun saamiseksi luotettavammaksi [18]. Syväoppimisen yksi ongelma on se, että tekoäly ei osaa yhdistää asioita kokonaisuuksiin, sillä syväoppimisessa ei ole kokonaisrakennetta, johon tietoja voisi yhdistää. Tästä seuraa vaikeuksia objektien erottamisessa, mikä voi vaikeuttaa vaikkapa robottiauton kykyä tunnistaa edessään olevia kohteita. Myös puheen ja tekstin tunnistamisessa neuroverkkomallit ovat rajoittuneita, sillä niissä ei ole samanlaista rikasta mallia maailmasta kuin ihmisen päässä. Syväoppiminen vaatii lisäksi ihmisen ohjaamaan oppimista, mikä edellyttää suuren pohjamateriaalin luokittelua.[11] Syväoppimista pyritään kehittämään yhdistämällä siihen ihmisaivojen toiminnasta opittuja tekniikoita, jotta tekoäly pystyisi oppimaan itsenäisemmin ja hahmottamaan ympärillään olevaa maailmaa. Tavoitteena pitkällä tähtäimellä on luoda yleiskäyttöinen tekoäly, joka pystyy hoitamaan itsenäisesti erilaisia tehtäviä. Tämän on arveltu johtavan siihen, että tekoäly vie jopa 30–50 prosenttia nykyisistä työpaikoista parissakymmenessä vuodessa, mikä saattaa aiheuttaa merkittäviä yhteiskunnallisia ongelmia mutta voi toisaalta lisätä tuottavuutta ja kasvattaa elintasoa.[11]"
  },
  {
    "url": "https://sv.wikipedia.org/wiki/Djupinl%C3%A4rning",
    "title": "Djupinlärning – Wikipedia",
    "content": "Djupinlärning (engelska: Deep Learning) är en underkategori till maskinlärning och artificiell intelligens som använder sig av neurala nätverk med flera lager för att lära och generalisera mönster i data. Djupinlärning är en av de mest framgångsrika tillämpningarna av artificiell intelligens i modern tid och har haft en stor påverkan på områden som bildigenkänning, taligenkänning, textanalys och spel. Djupinlärning har sina rötter i 1940-talet och 1950-talet när forskare som Warren McCulloch och Walter Pitts, och senare David Marr, började utveckla de första modellerna för neurala nätverk.[1] På 1960-talet utvecklade David Rumelhart och James McClelland de första algoritmerna för träning av neurala nätverk. Men det var inte förrän i början av 2000-talet som djupinlärning började få fart, tack vare utvecklingen av stora datamängder, kraftfulla datorer och öppen källkod-bibliotek.[2] Djupinlärning har flera typiska egenskaper som skiljer den från traditionell maskinlärning: Djupinlärning har haft stor framgång i en mängd olika applikationer, bland annat: Djupinlärning har också vissa begränsningar och kritik, bland annat:"
  },
  {
    "url": "https://ta.wikipedia.org/wiki/%E0%AE%86%E0%AE%B4%E0%AE%AE%E0%AE%BE%E0%AE%A9_%E0%AE%95%E0%AE%B1%E0%AF%8D%E0%AE%B1%E0%AE%B2%E0%AF%8D",
    "title": "ஆழமான கற்றல் - தமிழ் விக்கிப்பீடியா",
    "content": "ஆழமான கற்றல் (அல்லது ஆழமான கட்டமைக்கப்பட்ட கற்றல் அல்லது படிநிலை கற்றல்) இயந்திர கற்றலின் ஒரு பிரிவான செயற்கை அறிவுத்திறன் உட்பகுதியாகும். இவ்வகையான கற்றல் மேற்பார்வையின்றியும், அரை மேற்பார்வையுடனும் அல்லது கவனிக்கப்படாத போதும் செயல்படும்.[1] ஆழமான கற்றலை வைத்து பல்வேறு வியப்பூட்டும் செயல்களைச் செய்ய முடியும். எடுத்துக்காட்டாக, கருப்பு வெள்ளை படிமங்களை வண்ணமிடுதல்[2], இயந்திர மொழிபெயர்ப்பு[3], படிமங்களை அடையாளமிடுதல்[4], புதிய கையெழுத்து உருவாக்குதல்[5], படிமங்களுக்குத் தேவையான உரையை உருவாக்குதல்[6], உள்ளிட்டவை ஆகும். மருத்துவத் துறையில் பின்னாட்களில் வரும் நோய்களை முன்கூட்டியே கணிக்க இம்முறையில் ஆராய்ச்சி செய்து வருகின்றனர்.[7]"
  },
  {
    "url": "https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B8%A5%E0%B8%B6%E0%B8%81",
    "title": "การเรียนรู้เชิงลึก - วิกิพีเดีย",
    "content": "การเรียนรู้เชิงลึก (อังกฤษ: deep learning) เป็นส่วนหนึ่งของวิธีการการเรียนรู้ของเครื่องบนพื้นฐานของโครงข่ายประสาทเทียมและการเรียนรู้ค่าแทนลักษณะ การเรียนรู้สามารถเป็นได้ทั้งแบบการเรียนรู้แบบมีผู้สอน การเรียนรู้แบบกึ่งมีผู้สอน และการเรียนรู้แบบไม่มีผู้สอน[1]คำว่า \"ลึก\"ในความหมายมาจากการที่มีชั้นของโครงข่ายหลายชั้น ที่มีประสิทธิภาพมากขึ้น การเรียนที่สะดวกขึ้น และการเข้าใจในโครงสร้างที่ชัดเจนขึ้น พื้นฐานของการเรียนรู้เชิงลึกคือ อัลกอริทึมที่พยายามจะสร้างแบบจำลองเพื่อแทนความหมายของข้อมูลในระดับสูงโดยการสร้างสถาปัตยกรรมข้อมูลขึ้นมาที่ประกอบไปด้วยโครงสร้างย่อย ๆ หลายอัน และแต่ละอันนั้นได้มาจากการแปลงที่ไม่เป็นเชิงเส้น[2] การเรียนรู้เชิงลึก อาจมองได้ว่าเป็นวิธีการหนึ่งของการเรียนรู้ของเครื่องที่พยายามเรียนรู้วิธีการแทนข้อมูลอย่างมีประสิทธิภาพ ตัวอย่างเช่น รูปภาพภาพหนึ่ง สามารถแทนได้เป็นเวกเตอร์ของความสว่างต่อจุดพิกเซล หรือมองในระดับสูงขึ้นเป็นเซ็ตของขอบของวัตถุต่างๆ หรือมองว่าเป็นพื้นที่ของรูปร่างใด ๆ ก็ได้ การแทนความหมายดังกล่าวจะทำให้การเรียนรู้ที่จะทำงานต่าง ๆ ทำได้ง่ายขึ้น ไม่ว่าจะเป็นการรู้จำใบหน้าหรือการรู้จำการแสดงออกทางสีหน้า การเรียนรู้เชิงลึกถือว่าเป็นวิธีการที่มีศักยภาพสูงในการจัดการกับค่าแทนลักษณะสำหรับการเรียนรู้แบบไม่มีผู้สอนหรือการเรียนรู้แบบกึ่งมีผู้สอน นักวิจัยในสาขานี้พยายามจะหาวิธีการที่ดีขึ้นในการแทนข้อมูลแล้วสร้างแบบจำลองเพื่อเรียนรู้จากตัวแทนของข้อมูลเหล่านี้ในระดับใหญ่ บางวิธีการก็ได้แรงบันดาลใจมาจากสาขาประสาทวิทยาขั้นสูง โดยเฉพาะเรื่องกระบวนการตีความหมายในกระบวนการประมวลผลข้อมูลในสมอง ตัวอย่างของกระบวนการที่การเรียนรู้เชิงลึกนำไปใช้ได้แก่ การเข้ารหัสประสาท อันเป็นกระบวนการหาความสัมพันธ์ระหว่างตัวกระตุ้นกับการตอบสนองของเซลล์ประสาทในสมอง นักวิจัยด้านการเรียนรู้ของเครื่องได้เสนอสถาปัตยกรรมการเรียนรู้หลายแบบบนหลักการของการเรียนรู้เชิงลึกนี้ ได้แก่ โครงข่ายประสาทเทียมแบบลึก (Deep Artificial Neural Networks) โครงข่ายประสาทแบบสังวัตนาการ (Convolutional Neural Networks) โครงข่ายความเชื่อแบบลึก (Deep Belief Networks) และโครงข่ายประสาทแบบเวียนซ้ำ (Recurrent Neural Network) ซึ่งมีการนำมาใช้งานอย่างแพร่หลายในทางคอมพิวเตอร์วิทัศน์ การรู้จำเสียงพูด การประมวลผลภาษาธรรมชาติ การรู้จำเสียง และชีวสารสนเทศศาสตร์ การเรียนรู้เชิงลึก เป็นสาขาของการเรียนรู้ของเครื่องที่[3] กล่าวคือ การเรียนรู้เชิงลึกประกอบไปด้วย (1) หน่วยประมวลผลแบบไม่เป็นเชิงเส้นหลายๆชั้น (2) แต่ละชั้น จะเรียนรู้การแทนค่าแทนลักษณะ อาจจะเป็นแบบมีผู้สอนหรือไม่มีผู้สอนก็ได้ ทั้งนี้ โครงสร้างในแต่ละชั้นของการเรียนรู้เชิงลึกจะขึ้นอยู่กับปัญหาที่ต้องการจะแก้ไข อาจจะเป็น hidden layer ของโครงข่ายประสาทเทียม หรือหน่วยประมวลผลตรรกะที่ซับซ้อนก็ได้ หรืออาจจะเป็นโนดใน deep generative model อย่างเช่น โครงข่ายความเชื่อแบบลึก (Deep Belief Networks) หรือเครื่องจักรโบลทซ์มันน์เชิงลึก (Deep Boltzmann Machines) ก็ได้ หลักการโดยทั่วไปของการเรียนรู้เชิงลึกคือการมีหน่วยประมวลผลหลายๆชั้น ข้อมูลขาเข้าในแต่ละชั้นได้มาจากปฏิสัมพันธ์กับชั้นอื่น ๆ ทั้งนี้ การเรียนรู้เชิงลึกพยายามหาความสัมพันธ์ที่ล้ำลึกมากขึ้น นั่นคือ เมื่อมีจำนวนของชั้นและหน่วยประมวลผลที่อยู่ในชั้นมากขึ้น ข้อมูลในชั้นสูง ๆ ก็จะยิ่งล้ำลึกซับซ้อน (abstract) มากขึ้น สถาปัตยกรรมโครงสร้างของการเรียนรู้เชิงลึกมักจะสร้างแบบเป็นชั้นๆ (layer-by-layer) ไปด้วยวิธี greedy method ซึ่งการหาสิ่งที่ล้ำลึกซับซ้อนมากขึ้นไปเรื่อย ๆ ในแต่ละชั้นนี้เองที่ทำให้การเรียนรู้เชิงลึกมีประสิทธิภาพมากกว่าวิธีการอื่น ๆ[4] ตัวอย่างเช่น ข้อมูลในชั้นต้น ๆ อาจจะเรียนรู้ว่าภาพที่เข้ามาประกอบด้วยเส้นต่าง ๆ ชั้นที่สูงไปนำเส้นต่าง ๆ มาประกอบกันเป็นรูปสี่เหลี่ยม และชั้นต่อ ๆ มาคือการหาความสัมพันธ์ของเส้นสี่เหลี่ยมจนกระทั่งคอมพิวเตอร์รู้ได้ว่าภาพที่เข้ามาเป็นภาพของธงชาติ เป็นต้น ในการเรียนรู้แบบมีผู้สอนนั้น การเรียนรู้เชิงลึกจะช่วยลดภาระในการหาค่าแทนลักษณะที่เกี่ยวข้อง เพราะวิธีการนี้จะแปลงข้อมูลไปสู่รูปแบบอื่นในระดับที่สูงขึ้นโดยอัตโนมัติ และให้ความสำคัญกับข้อมูลที่ซ้ำซ้อนลดลงไปด้วย นอกจากนี้ ยังสามารถนำไปปรับใช้กับการเรียนรู้แบบไม่มีผู้สอนได้ด้วย เราอาจจะตีความการเรียนรู้เชิงลึกได้ 2 แนวทางคือ ใช้ทฤษฎีประมาณค่าสากล (universal approximation theorem) และใช้การอนุมานด้วยความน่าจะเป็น (probabilistic inference) ทฤษฎีประมาณค่าสากล สนใจความสามารถของโครงข่ายประสาทแบบป้อนไปข้างหน้า (feedforward neural networks) ที่มี hidden layer เพียงชั้นเดียวและมีขนาดจำกัด เพื่อประมาณค่าของฟังก์ชันต่อเนื่อง[5][6][7][8][9] โดย George Cybenko ได้พิสูจน์การเรียนรู้เชิงลึกด้วยทฤษฎีนี้ โดยใช้ฟังก์ชันซิกมอยด์ในปี 1989[10] และต่อมา Hornik นำไปพิสูจน์ต่อสำหรับ feedforward neural networks ที่มีหลายๆชั้น ในปี 1991[11] ส่วนการตีความด้วยหลักความน่าจะเป็นนั้น มีแนวคิดมาจากการเรียนรู้ของเครื่อง[12] เสนอขึ้นครั้งแรกโดยเจฟฟรีย์ ฮินตัน, โยชัว เบนจีโอ, อียาน เลอเกิง และเยือร์เกิน ชมิทฮูเบอร์ นักวิทยาศาสตร์ผู้บุกเบิกสาขาการเรียนรู้เชิงลึกยุคใหม่ แนวคิดนี้จะเน้นการปรับโครงสร้างการเรียนรู้เชิงลึกด้วยการหาโมเดลค่าที่ดีที่สุด (optimization) ที่ดีทั้งสำหรับข้อมูลการสอน (training) และข้อมูลการทดสอบ (testing) ทั้งนี้ การอนุมานด้วยความน่าจะเป็นนั้นจะมองว่า activation nonlinearity นั้นเป็นฟังก์ชันการกระจายแบบสะสม (Cumulative distribution function)[13] ทำให้เกิดเทคนิคการใช้ dropout เป็นตัวควบคุม (regularizer) สำหรับโครงข่ายประสาทเทียม[14]"
  },
  {
    "url": "https://tr.wikipedia.org/wiki/Derin_%C3%B6%C4%9Frenme",
    "title": "Derin öğrenme - Vikipedi",
    "content": "Derin öğrenme (aynı zamanda derin yapılandırılmış öğrenme, hiyerarşik öğrenme ya da derin makine öğrenmesi) bir veya daha fazla gizli katman içeren yapay sinir ağları ve benzeri makine öğrenme algoritmalarını kapsayan çalışma alanıdır. Yani en az bir adet yapay sinir ağının (YSA) kullanıldığı ve birçok algoritma ile, bilgisayarın eldeki verilerden yeni veriler elde etmesidir. Derin öğrenme gözetimli, yarı gözetimli veya gözetimsiz olarak gerçekleştirilebilir.[1] Derin yapay sinir ağları pekiştirmeli öğrenme yaklaşımıyla da başarılı sonuçlar vermiştir.[2]\nYapay  sinir ağları, biyolojik sistemlerdeki bilgi işleme ve dağıtılmış iletişim düğümlerinden esinlenilmiştir.\nYapay sinir ağlarının biyolojik beyinlerden çeşitli farklılıkları vardır. Özellikle, sinir ağları statik ve sembolik olma eğilimindeyken, çoğu canlı organizmanın biyolojik beyni dinamik(plastik) ve analogtur.[3][4][5] Derin öğrenme kavramı, 1940'lardan beri geliştirilen sinir ağlarına dayanmaktadır. 1980'ler ve 1990'lar boyunca, araştırmacılar geri yayılım (backpropagation) ve destek vektör makineleri gibi daha gelişmiş tekniklerle sinir ağları üzerinde çalıştılar. 2000'lerde, büyük miktarda etiketli verinin ve daha güçlü donanımların kullanılabilir hale gelmesiyle, derin öğrenme alanında büyük ilerlemeler kaydedildi. Bu dönemde, yapay sinir ağları ve derin öğrenme, tanıma ve sınıflandırma görevlerinde insan seviyesinde performans sergilemeye başladı.[6] Derin öğrenme modelleri, farklı yapı ve işlevlere sahip çeşitli sinir ağlarından oluşur. Başlıca derin öğrenme modelleri şunlardır:[7] Derin öğrenme algoritmaları, büyük veri kümesi üzerinde eğitilerek başarılı tahminler yapabilirler. Bu süreçte sıkça kullanılan teknikler şunlardır: Derin öğrenme, bilgisayarlı görü ve ses tanıma, doğal dil işleme, tıbbi görüntü analizi ve oyun stratejileri gibi çeşitli alanlarda başarıyla kullanılmaktadır. Ayrıca, otomotiv, eğlence, finans ve sağlık gibi sektörlerde önemli rol oynar.[7] Örnek kullanım alanları:[8]"
  },
  {
    "url": "https://uk.wikipedia.org/wiki/%D0%93%D0%BB%D0%B8%D0%B1%D0%BE%D0%BA%D0%B5_%D0%BD%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F",
    "title": "Глибоке навчання — Вікіпедія",
    "content": "Глибо́ке навча́ння[2][3][4] (англ. deep learning, також іноді глиби́нне навча́ння) — це підмножина методів машинного навчання на основі штучних нейронних мережах із навчанням подань. Прикметник «глибоке» (англ. deep) вказує на використання декількох шарів у мережі. Використовувані методи навчання можуть бути керованими, напівкерованими, та некерованими.[5] Архітектури глибокого навчання, такі як глибокі нейронні мережі, глибокі мережі переконань, рекурентні нейронні мережі, згорткові нейронні мережі та трансформери, застосовували в таких галузях як комп'ютерне бачення, розпізнавання мовлення, обробка природної мови, машинний переклад, біоінформатика, конструювання ліків, аналіз медичних зображень[en], кліматологія, перевірка матеріалів і програми настільних ігор, де вони дали результати, порівня́нні, а в деяких випадках і кращі за продуктивність людських експертів.[6][7][8] Штучні нейронні мережі (ШНМ) було натхненно обробкою інформації та розподіленими комунікаційними вузлами в біологічних системах. ШНМ мають різноманітні відмінності від біологічного мозку. Зокрема, штучні нейронні мережі зазвичай статичні та символьні, тоді як біологічний мозок більшості живих організмів динамічний (пластичний) та аналоговий.[9][10] ШНМ загалом розглядають як низькоякісні моделі функціювання мозку.[11] Глибоке навчання — це клас алгоритмів машинного навчання, який[12]:199–200  використовує декілька шарів, щоби поступово виділяти з сирого входу ознаки щоразу вищих рівнів. Наприклад, при обробці зображень нижчі шари можуть встановлювати контури, тоді як вищі шари можуть встановлювати поняття, доречні для людини, такі як цифри, літери чи обличчя. Якщо поглянути на глибоке навчання під іншим кутом, то глибоке навчання позначує «комп'ютерне моделювання» або «автоматизацію» процесів навчання людиною від джерела (наприклад, зображення собак) до об'єкта навчання (собак). Тому мають сенс поняття, сформульовані як «глибше» (англ. deeper) та «найглибше» (англ. deepest) навчання.[13] Найглибше навчання позначує повністю автоматичне навчання від джерела до кінцевого об'єкта навчання. Глибше навчання відтак позначує змішаний процес навчання: процес навчання людиною від джерела до навченого напівоб'єкта, за яким слідує комп'ютерний процес навчання від навченого людиною напівоб'єкта до кінцевого об'єкта навчання. Більшість сучасних моделей глибокого навчання ґрунтуються на багатошарових штучних нейронних мережах, таких як згорткові нейронні мережі та трансформери, хоча до них також можуть належати пропозиційні формули[en] та латентні змінні, організовані пошарово в глибоких породжувальних моделях, такі як вузли в глибоких мережах переконань чи глибоких машинах Больцмана.[14] У глибокому навчанні кожен рівень вчиться перетворювати свої вхідні дані на дещо абстрактніше й складніше подання. У застосунку для розпізнавання зображень сирий вхід може бути матрицею пікселів; перший шар подання може абстрагуватися від пікселів і кодувати контури; другий шар може складати та кодувати їхнє розташування; третій шар може кодувати ніс та очі; а четвертий шар може розпізнавати, що зображення містить обличчя. Важливо, що процес глибокого навчання може самостійно вчитися, які ознаки оптимально розмістити на якому рівні. Це не звільняє від необхідності ручного настроювання; наприклад, різна кількість та розміри шарів можуть забезпечувати різні ступені абстракції.[15][16] Слово «глибоке» у «глибокому навчанні» позначує кількість шарів, крізь які перетворюються дані. Точніше, системи глибокого навчання мають значну глибину шляху розподілу внеску (ШРВ, англ. credit assignment path, CAP). ШРВ — це ланцюг перетворень від входу до виходу. ШРВ описують потенційно причинно-наслідкові зв'язки між входом та виходом. Для нейронної мережі прямого поширення глибина ШРВ дорівнює глибині мережі й є кількістю прихованих шарів плюс один (оскільки шар виходу також параметризовано). Для рекурентних нейронних мереж, у яких сигнал може поширюватися крізь шар понад один раз, глибина ШРВ потенційно необмежена.[17] Немає універсально узгодженого порогу глибини, який відділяє неглибоке навчання (англ. shallow learning) від глибокого, але більшість дослідників сходяться на думці, що глибоке навчання використовує глибину ШРВ вище 2. Було показано, що ШРВ глибини 2 це універсальний наближувач у тому сенсі, що він може моделювати будь-яку функцію.[18] Поза цим, більше шарів не додають нічого до здатності мережі наближувати функції. Глибокі моделі (ШРВ > 2) здатні виділяти кращі ознаки, ніж неглибокі моделі, й отже, додаткові шари допомагають ефективно навчатися ознак. Архітектури глибокого навчання можливо конструювати пошарово жадібним методом.[19] Глибоке навчання допомагає розплутувати ці абстракції й обирати, які ознаки покращують продуктивність.[15] Для завдань керованого навчання методи глибокого навчання уможливлюють усування конструювання ознак, переводячи дані до компактних проміжних подань, подібних до головних компонент, і виводять багатошарові структури, які усувають надмірність у поданні. Алгоритми глибокого навчання можливо застосовувати до завдань некерованого навчання. Це важлива перевага, оскільки немічені дані численніші за мічені. Прикладами глибоких структур, які можливо тренувати некерованим чином, є глибокі мережі переконань.[15][20] Моделі машинного навчання тепер вправні у виявлянні складних шаблонів у даних фінансових ринків. Завдяки перевагам штучного інтелекту інвестори все частіше використовують методики глибокого навчання для прогнозування та аналізу тенденцій на фондових та валютних ринках.[21] Глибокі нейронні мережі зазвичай інтерпретують у термінах теореми про універсальне наближення[22][23][24][25][26] або ймовірнісного висновування.[27][12][15][17][28] Класична теорема про універсальне наближення стосується здатності нейронної мережі прямого поширення з одним прихованим шаром скінченного розміру наближувати неперервні функції.[22][23][24][25] 1989 року Джордж Цибенко опублікував перше її доведення для сигмоїдних передавальних функцій,[22] а 1991 року Курт Горнік[de] узагальнив його для багатошарових архітектур прямого поширення.[23] Нещодавня робота також показала, що універсальне наближення також виконується для необмежених передавальних функцій, таких як випрямлений лінійний вузол (англ. rectified linear unit) Куніхіко Фукусіми[en].[29][30] Теорема про універсальне наближення для глибоких нейронних мереж розглядає пропускну спроможність мереж з обмеженою шириною, дозволяючи зростати глибині. Лу зі співавт.[26] довели, що якщо ширина глибокої нейронної мережі з передавальною функцією ReLU строго більша за розмірність входу, то ця мережа може наближувати будь-яку функцію, інтегровну за Лебегом; якщо її ширина менша або дорівнює розмірності входу, то глибока нейронна мережа не є універсальним наближувачем. Імовірнісна інтерпретація[28] походить з галузі машинного навчання. Вона відводить провідне місце висновуванню,[12][14][15][17][20][28] а також таким поняттям оптимізації як тренування та випробування, пов'язаним із допасовуванням та узагальнюванням відповідно. Конкретніше, ймовірнісна інтерпретація розглядає нелінійність передавальної функції як кумулятивну функцію розподілу.[28] Імовірнісна інтерпретація призвела до запровадження виключення (англ. dropout) як регуляризатора в нейронних мережах. Імовірнісну інтерпретацію запровадили дослідники включно з Гопфілдом, Уїдроу[en] та Нарендрою[en], й популяризовано в оглядах, таких як від Бішопа[en].[31] Існує два типи нейронних мереж: нейронні мережі прямого поширення (НМПП, англ. feedforward neural network, FNN) та рекурентні нейронні мережі (РНМ, англ. recurrent neural network, RNN). РНМ мають цикли у структурі зв'язності, а НМПП — ні. У 1920-х роках Вільгельм Ленц[en] та Ернст Ізінг[en] створили та проаналізували модель Ізінга,[32] яка, по суті, є архітектурою РНМ, що не навчається, і складається з нейроноподібних порогових елементів. 1972 року Шунічі Амарі[en] зробив цю архітектуру адаптивною.[33][34] Його навчання РНМ популяризував Джон Гопфілд 1982 року.[35] РНМ стали центральними для розпізнавання мовлення та обробки мови[en]. Чарльз Тапперт пише, що Френк Розенблат розробив та дослідив усі основні складові сучасних систем глибокого навчання,[36] посилаючись на книгу Розенблата 1962 року,[37] якою було запроваджено багатошаровий перцептрон (БШП) із трьома шарами: шаром входу, прихованим шаром із випадковими вагами, що не навчалися, та шаром виходу. Він також запропонував варіанти, включно з версією з чотиришаровими перцептронами, де останні два шари мають ваги, що навчаються (й отже, справжній багатошаровий перцептрон).[37]:section 16 Крім того, термін глибоке навчання запропонувала 1986 року Ріна Дехтер[en],[38] хоча історія його появи, схоже, набагато складніша.[39] Перший загальний, робочий алгоритм навчання для глибоких багатошарових перцептронів прямого поширення керованим навчанням опублікували 1967 року Олексій Івахненко та Валентин Лапа.[40] У статті 1971 року описано глибоку мережу з восьми шарів, треновану методом групового урахування аргументів.[41] Перший багатошаровий перцептрон глибокого навчання, тренований стохастичним градієнтним спуском,[42] опублікував 1967 року Шунічі Амарі[en].[43][34] У комп'ютерних експериментах, проведених учнем Амарі, Сайто, п'ятишаровий БШП із двома змінними шарами навчився внутрішніх подань для класифікування нелінійно роздільних класів образів.[34] 1987 року Метью Бренд повідомив, що широкі 12-шарові нелінійні перцептрони можливо повністю наскрізно тренувати відтворювати логічні функції нетривіальної глибини ланцюга за допомогою градієнтного спуску на невеликих пакетах випадкових вибірок входів/виходів, але зробив висновок, що час тренування на тогочасному обладнанні (домегафлопних комп'ютерах) зробив цю методику непрактичною, та запропонував використовувати незмінні випадкові ранні шари як хеш входу для єдиного змінюваного рівня.[44] Натомість подальші розробки апаратного забезпечення та підлаштовування гіперпараметрів зробили наскрізний стохастичний градієнтний спуск нині переважною методикою тренування. 1970 року Сеппо Ліннаінмаа[en] опублікував зворотний режим автоматичного диференціювання дискретно зв'язаних мереж вкладених диференційовних функцій.[45][46][47] Він став відомим як зворотне поширення.[17] Це ефективне застосування ланцюгового правила, виведеного Готфрідом Вільгельмом Лейбніцем 1673 року,[48] до мереж диференційовних вузлів.[34] Термінологію «зворотно поширювані похибки» (англ. back-propagating errors) фактично запровадив 1962 року Розенблат,[37][34] але він не знав, як це втілити, хоча Генрі Келлі[en] вже 1960 року мав безперервного попередника зворотного поширення[49] в контекст теорії керування.[34] 1982 року Пол Вербос[en] застосував зворотне поширення до БШП у спосіб, який став стандартним.[50][51][34] 1985 року Девід Румельхарт зі співавт. опублікували експериментальний аналіз цієї методики.[52] Ахітектури глибокого навчання для згорткових нейронних мереж (ЗНМ, англ. convolutional neural networks, CNN) зі згортковими шарами та шарами зниження роздільності почалися з неокогнітрона, запропонованого Куніхіко Фукусімою[en] 1980 року.[53] 1969 року він також запропонував передавальну функцію ReLU (англ. rectified linear unit, випрямлений лінійний вузол).[29][34] Цей випрямляч (англ. rectifier) став найпопулярнішою передавальною функцією для ЗНМ та глибокого навчання в цілому.[54] ЗНМ стали важливим інструментом комп'ютерного бачення. Термін глибоке навчання (англ. Deep Learning) у спільноті машинного навчання запровадила 1986 року Ріна Дехтер[en],[38] а для штучних нейронних мереж — Ігор Айзенберг з колегами у 2000 року в контексті булевих[en] порогових нейронів.[55][56] 1988 року Вей Чжан зі співавт. застосували алгоритм зворотного поширення до згорткової нейронної мережі (спрощений неокогнітрон зі згортковими взаємозв'язками між шарами ознак зображення та останнім повноз'єднаним шаром) для розпізнавання абетки. Вони також запропонували втілення ЗНМ з оптичною обчислювальною системою.[57][58] 1989 року Ян ЛеКун зі співавт. застосували зворотне поширення до ЗНМ з метою розпізнавання рукописних поштових індексів у пошті. Хоч цей алгоритм і працював, тренування вимагало 3 днів.[59] Згодом Вей Чжан зі співавт. видозмінили свою модель, видаливши останній повноз'єднаний шар, та застосувавши її для сегментування об'єктів медичних зображень 1991 року,[60] та для виявляння раку молочної залози на мамограмах 1994 року.[61] LeNet-5 (1998), 7-рівневу ЗНМ від Яна ЛеКуна зі співавт.,[62] що класифікує цифри, кілька банків застосували для розпізнавання рукописних чисел на чеках, оцифрованих у зображення 32×32 пікселі. У 1980-х роках зворотне поширення не працювало добре для глибокого навчання з довгими шляхами розподілу внеску. Щоби подолати цю проблему, Юрген Шмідхубер (1992) запропонував ієрархію РНМ, попередньо тренованих порівнево самокерованим навчанням.[63] Вона використовує передбачувальне кодування[en] для навчання внутрішніх подань у кількох самоорганізованих масштабах часу. Це може істотно полегшувати наступне глибоке навчання. Цю ієрархію РНМ можливо скласти (англ. collapse) в єдину РНМ шляхом дистилювання[en] фрагментувальної (англ. chunker) мережі вищого рівня до автоматизаторної (англ. automatizer) мережі нижчого рівня.[63][34] 1993 року фрагментувальник розв'язав завдання глибокого навчання, чия глибина перевищувала 1000.[64] 1992 року Юрген Шмідхубер також опублікував альтернативу РНМ (англ. alternative to RNNs),[65] яку зараз називають лінійним трансформером (англ. linear Transformer) або трансформером з лінеаризованою самоувагою[66][67][34] (за винятком оператора нормування). Він навчається внутрішніх центрів уваги (англ. internal spotlights of attention):[68] повільна нейронна мережа прямого поширення вчиться за допомогою градієнтного спуску керувати швидкими вагами іншої нейронної мережі через тензорні добутки самопороджуваних шаблонів збудження FROM і TO (званих тепер ключем, англ. key, та значенням, англ. value, самоуваги).[66] Це відображення уваги (англ. attention mapping) швидких ваг застосовують до шаблону запиту. Сучасний трансформер (англ. Transformer) запропонували Ашиш Васвані зі співавт. у своїй праці 2017 року «Увага — це все, що вам треба».[69] Він поєднує це з оператором softmax та проєкційною матрицею.[34] Трансформери все частіше обирають за модель для обробки природної мови.[70] Багато сучасних великих мовних моделей, таких як ChatGPT, GPT-4 та BERT, використовують саме його. Трансформери також все частіше використовують у комп'ютернім баченні.[71] 1991 року Юрген Шмідхубер також опублікував змагальні нейронні мережі (англ. adversarial neural networks), які змагаються між собою у формі антагоністичної гри, де виграш однієї мережі є програшем іншої.[72][73][74] Перша мережа є породжувальною моделлю, яка моделює розподіл імовірності над образами на виході. Друга мережа навчається градієнтним спуском передбачувати реакцію середовища на ці образи. Це було названо «штучною цікавістю» (англ. artificial curiosity). 2014 року цей принцип використали у породжувальній змагальній мережі (англ. generative adversarial network, GAN) Ян Ґудфелоу зі співавт.[75] Тут реакція навколишнього середовища дорівнює 1 або 0 залежно від того, чи належить вихід першої мережі до заданого набору. Це можливо використовувати для створення реалістичних дипфейків.[76] Відмінної якості зображення досягла StyleGAN[en] Nvidia (2018)[77] на основі прогресивної породжувальної змагальної мережі (англ. Progressive GAN) Теро Карраса зі співавт.[78] Тут породжувач вирощується від малого до великого пірамідним чином. Дипломну працю Зеппа Хохрайтера[en] (1991)[79] його керівник Шмідхубер назвав «одним із найважливіших документів в історії машинного навчання».[34] Він не лише випробував нейронний стискач історії,[63] але й виявив та проаналізував проблему зникання градієнта.[79][80] Для розв'язання цієї проблеми Хохрайтер запропонував рекурентні залишкові зв'язки. Це призвело до появи методу глибокого навчання, званого довгою короткочасною пам'яттю (ДКЧП, англ. long short-term memory, LSTM), опублікованого 1997 року.[81] Рекурентні нейронні мережі ДКЧП можуть навчатися задач «дуже глибокого навчання»[17] з довгими шляхами розподілу внеску, які вимагають спогадів про події, що відбулися тисячі дискретних часових кроків тому. «Стандартну ДКЧП» (англ. vanilla LSTM) із забувальним вентилем запропонували 1999 року Фелікс Ґерс[en], Шмідхубер та Фред Каммінс.[82] ДКЧП стала найцитованішою нейронною мережею XX століття.[34] 2015 року Рупеш Кумар Шрівастава, Клаус Ґрефф і Шмідхубер використали принцип ДКЧП для створення магістралевої мережі, нейронної мережі прямого поширення з сотнями шарів, набагато глибшої за попередні.[83][84] 7 місяців потому, Каймін Хе, Сян'ю Чжан; Шаоцін Рен та Цзянь Сунь виграли змагання ImageNet[en] 2015 із відкритовентильним або безвентильним варіантом магістралевої мережі, названим за́лишковою нейронною мережею (англ. Residual neural network).[85] Вона стала найцитованішою нейронною мережею XXI століття.[34] 1994 року Андре де Карвальо разом з Майком Фейргерстом та Девідом Біссетом опублікували експериментальні результати багатошарової булевої нейронної мережі, відомої також як безвагова нейронна мережа (англ. weightless neural network), складеної з 3-шарового самоорганізовуваного нейромережного модуля виділяння ознак (англ. SOFT), з багатошаровим класифікаційним нейромережним модулем (англ. GSN) за ним, тренованих незалежно. Кожен шар у модулі виділяння ознак виділяв ознаки все вищої складності відносно попереднього шару.[86] 1995 року Брендан Фрей[en] продемонстрував можливість натренувати (протягом двох днів) мережу із шести повноз'єднаних шарів та кількох сотень прихованих вузлів, використовуючи алгоритм неспання — сну[en], розроблений спільно з Пітером Даяном[en] та Гінтоном.[87] З 1997 року Свен Бенке розширив ієрархічний згортковий підхід прямого поширення у нейронній піраміді абстракцій (англ. Neural Abstraction Pyramid)[88] за допомогою бічних та зворотних з'єднань, щоби гнучко включати контекст у рішення та ітеративно розв'язувати локальні неоднозначності. У 1990-х і 2000-х роках популярність мали простіші моделі, які використовують сконструйовані вручну ознаки для конкретних завдань, такі як фільтри Ґабора (англ. Gabor filters) та опорновекторні машини (ОВМ, англ. support vector machines, SVM), через обчислювальну витратність штучних нейронних мереж (ШНМ) та брак розуміння того, як мозок сплітає свої біологічні мережі. Як неглибоке, так і глибоке навчання (наприклад, рекурентні мережі) ШНМ для розпізнавання мовлення досліджували протягом багатьох років.[89][90][91] Ці методи ніколи не перевершували технологію неоднорідних внутрішньо-ручних гауссових сумішевих моделей[en]/прихованих марковських моделей (ГСМ-ПММ, англ. GMM-HMM) на основі породжувальних моделей мовлення, тренованих розрізнювально.[92] Було проаналізовано основні труднощі, включно зі зниканням градієнта[79] й слабкою структурою часової кореляції в нейронних передбачувальних моделях.[93][94] Додатковими труднощами були брак тренувальних даних та обмежена обчислювальна потужність. Більшість дослідників розпізнавання мовлення відійшли від нейронних мереж, щоби займатися породжувальним моделюванням. Винятком був SRI International наприкінці 1990-х років. Фінансований агенціями уряду США АНБ та DARPA, SRI вивчав глибокі нейронні мережі в розпізнаванні мовлення та мовця. Команда розпізнавання мовців на чолі з Ларрі Геком[en] повідомила про значний успіх із глибокими нейронними мережами в обробці мовлення на оцінюванні розпізнавання мовців Національного інституту стандартів і технологій 1998 року.[95] Потім глибоку нейронну мережу SRI було розгорнуто в Nuance Verifier, що стало першим великим промисловим застосуванням глибокого навчання.[96] Принцип піднесення «сирих» ознак над ручною оптимізацією було вперше успішно досліджено в архітектурі глибокого автокодувальника на «сирій» спектрограмі або ознаках лінійного блока фільтрів[en] наприкінці 1990-х,[96] що показало його перевагу над мел-кепстровими ознаками, які містять етапи незмінного перетворення зі спектрограм. Сирі ознаки мовлення, хвилеформи, згодом дали чудові великомасштабні результати.[97] Розпізнавання мовлення перейняла ДКЧП. 2003 року ДКЧП на певних завданнях почала конкурувати з традиційними розпізнавачами мовлення.[98] 2006 року Алекс Ґрейвс[en], Сантьяго Фернандес, Фаустіно Ґомес та Шмідхубер поєднали її з нейромережною часовою класифікацією[en] (НЧК, англ. connectionist temporal classification, CTC)[99] у стеках РНМ ДКЧП.[100] 2015 року в розпізнаванні мовлення Google, як було повідомлено, стався різкий 49-відсотковий стрибок продуктивності завдяки НЧК-тренованій ДКЧП, яку вони зробили доступною через голосовий пошук Google.[101] Вплив глибокого навчання в промисловості почався на початку 2000-х років, коли, за словами Яна ЛеКуна, ЗНМ вже обробляли приблизно від 10 % до 20 % усіх чеків, виписуваних у США.[102] Промислові застосування глибокого навчання для широкомасштабного розпізнавання мовлення почалися приблизно 2010 року. 2006 року публікації Джеффа Гінтона, Руслана Салахутдінова[en], Осіндеро та Тее[en][103][104][105] показали, як багатошарову нейронну мережу прямого поширення можливо ефективно попередньо тренувати шар за шаром, розглядаючи кожен шар по черзі як некеровану обмежену машину Больцмана, а потім тонко налаштовувати її за допомогою керованого зворотного поширення.[106] Ці праці стосувалися навчання для глибоких мереж переконань. Мотивом семінару NIPS 2009 року з глибокого навчання для розпізнавання мовлення були обмеження глибоких породжувальних моделей мовлення та можливість того, що завдяки потужнішому апаратному забезпеченню та великомасштабним наборам даних глибокі нейронні мережі (ГНМ, англ. deep neural nets, DNN) можуть стати практичними. Вважалося, що попереднє тренування ГНМ за допомогою породжувальних моделей глибоких мереж переконань (ГМП, англ. deep belief nets, DBN) дозволить подолати основні труднощі нейронних мереж. Проте було виявлено, що заміна попереднього тренування великими обсягами тренувальних даних для безпосереднього зворотного поширення при використанні ГНМ з великими контекстнозалежними шарами виходу призводить до різко нижчих рівнів похибок, ніж у гауссової сумішевої моделі (ГСМ)/прихованої марковської моделі (ПММ), що була на рівні останніх досягнень на той час, а також ніж у передовіших систем на основі породжувальних моделей.[107] Природа похибок розпізнавання, породжуваних цими двома типами систем, характерно відрізнялася,[108] пропонуючи технічні прояснення щодо того, як інтегрувати глибоке навчання в наявну високоефективну систему декодування мовлення в реальному часі, розгорнуту всіма основними системами розпізнавання мовлення.[12][109][110] Аналіз близько 2009—2010 років, порівнюючи ГСМ (та інші породжувальні моделі мовлення) з моделями ГНМ, стимулював перші промислові інвестиції в глибоке навчання для розпізнавання мовлення.[108] Цей аналіз було зроблено з порівнянною продуктивністю (менше 1,5 % у рівні похибок) між розрізнювальними ГНМ та породжувальними моделями.[107][108][111] 2010 року дослідники розширили глибоке навчання від TIMIT[en] до великословникового розпізнавання мовлення, застосувавши великі шари виходу ГНМ на основі контекстнозалежних станів ПММ, побудованих за допомогою дерев рішень.[112][113][114][109] Глибоке навчання є частиною систем рівня останніх досягнень у різних дисциплінах, зокрема в комп'ютернім баченні та автоматичному розпізнаванні мовлення (АРМ, англ. automatic speech recognition, ASR). Результати на загальновживаних оцінювальних наборах, таких як TIMIT[en] (АРМ) та MNIST (класифікування зображень), а також низці завдань великословникового розпізнавання мовлення, постійно покращувалися.[107][115] Згорткові нейронні мережі (ЗНМ) для АРМ було витіснено НЧК[99] для ДКЧП,[81][101][116][117][118] але в комп'ютернім баченні вони успішніші. Удосконалення апаратного забезпечення відновило інтерес до глибокого навчання. 2009 року Nvidia брала участь у так званому «великому вибуху» глибокого навчання, «оскільки нейронні мережі глибокого навчання тренували за допомогою графічних процесорів (ГП) Nvidia».[119] Того року Ендрю Ин визначив, що ГП можуть підвищити швидкість систем глибокого навчання приблизно в 100 разів.[120] Зокрема, ГП добре підходять для матричних/векторних обчислень, задіяних у машинному навчанні.[121][122][123] ГП прискорюють алгоритми тренування на порядки, скорочуючи час роботи з тижнів до днів.[124][125] Крім того, для ефективної обробки моделей глибокого навчання можливо використовувати спеціалізоване обладнання та оптимізацію алгоритмів.[126] Наприкінці 2000-х глибоке навчання почало перевершувати інші методи в змаганнях з машинного навчання. 2009 року довга короткочасна пам'ять, натренована нейромережною часовою класифікацією[en] (Алекс Ґрейвс[en], Сантьяго Фернандес, Фаустіно Ґомес та Юрген Шмідхубер, 2006)[99] стала першою РНМ, яка виграла конкурси з розпізнавання образів, вигравши три змагання з розпізнавання неперервного рукописного тексту.[127][17] Пізніше Google використала натреновану НЧК ДКЧП для розпізнавання мовлення на смартфоні.[128][101] Значний вплив на розпізнавання зображень або об'єктів відчувався з 2011 по 2012 роки. Хоча ЗНМ, треновані зворотним поширенням, існували десятиліттями,[57][59] а втілення НМ на ГП — роками,[121] включно із ЗНМ,[123][17] для прогресу в комп'ютернім баченні знадобилися швидші втілення ЗНМ на графічних процесорах. 2011 року DanNet[129][6] Дена Чирешана, Улі Меєра, Джонатана Маскі, Луки Марії Гамбардели[en] та Юргена Шмідхубера вперше досягла надлюдських результатів у змаганні з розпізнавання візуальних образів, перевершивши традиційні методи втричі.[17] Також 2011 року DanNet виграла конкурс із китайського рукописного тексту ICDAR, а в травні 2012 року перемогла в конкурсі з сегментування зображень ISBI.[130] До 2011 року ЗНМ не відігравали великої ролі на конференціях з комп'ютерного бачення, але в червні 2012 року публікація Чирешана зі співавт. на провідній конференції CVPR[6] показала, як максимізувально агрегувальні ЗНМ на ГП можуть значно покращувати багато еталонних рекордів у баченні. У вересні 2012 року DanNet також виграла конкурс ICPR з аналізу великих медичних зображень для виявляння раку, а наступного року також і MICCAI Grand Challenge на ту ж тему.[131] У жовтні 2012 року подібна AlexNet Олексія Крижевського, Іллі Суцкевера та Джефрі Гінтона[7] виграла великомасштабне змагання ImageNet[en] зі значним відривом від неглибоких методів машинного навчання. Мережа VGG-16 Карена Симоняна та Ендрю Зіссермана[en][132] ще більше знизила рівень похибок і виграла конкурс ImageNet 2014, слідуючи подібній тенденції у широкомасштабному розпізнаванні мовлення. Потім класифікування зображень було розширено до складнішого завдання породжування описів[en] (підписів) для зображень, часто як поєднання ЗНМ та ДКЧП.[133][134][135] 2012 року команда під проводом Джорджа Даля виграла конкурс «Merck Molecular Activity Challenge», використовуючи багатозадачні глибокі нейронні мережі для передбачування біомолекулярної мішені[en] одного препарату.[136][137] 2014 року група Зеппа Хохрайтера[en] використала глибоке навчання для виявляння нецільових і токсичних впливів хімічних речовин навколишнього середовища у поживних речовинах, побутових товарах і ліках, і виграла «Tox21 Data Challenge» NIH, FDA та NCATS[en].[138][139][140] 2016 року Роджер Парлофф зазначив «революцію глибокого навчання», яка змінила галузь ШІ.[141] У березні 2019 року Йошуа Бенжіо[en], Джефрі Гінтона та Яна ЛеКуна було нагороджено премією Тюрінга за концептуальні та інженерні прориви, які зробили глибокі нейронні мережі критично важливою складовою обчислювальної техніки. Шту́чні нейро́нні мере́жі (ШНМ, англ. artificial neural networks, ANN) або коннекціоні́стські систе́ми (англ. connectionist systems) — це обчислювальні системи, натхненні біологічними нейронними мережами, які складають мозок тварин. Такі системи вчаться (поступово вдосконалюють свої здібності) виконувати завдання, розглядаючи приклади, як правило, без програмування під конкретне завдання. Наприклад, у розпізнаванні зображень вони можуть навчитися встановлювати зображення, які містять котів, аналізуючи приклади зображень, мічені[en] вручну як «кіт» чи «кота нема», і використовуючи результати цього аналізу для встановлювання котів на інших зображеннях. Вони знайшли найбільше використання в застосуваннях, які важко висловити за допомогою традиційного комп'ютерного алгоритму з використанням програмування на основі правил. ШНМ ґрунтується на сукупності з'єднаних вузлів, званих штучними нейронами (аналогічно біологічним нейронам у біологічному мозку). Кожне з'єднання (синапс) між нейронами може передавати сигнал іншому нейронові. Приймальний (постсинаптичний) нейрон може обробляти сигнал(и), а потім сигналізувати подальшим нейронам. Нейрони можуть мати стан, як правило, поданий дійсними числами, зазвичай між 0 та 1. Нейрони та синапси також можуть мати вагу, яка змінюється в міру навчання, що може збільшувати або зменшувати силу сигналу, який вони надсилають далі. Як правило, нейрони впорядковано в шари (англ. layers). Різні шари можуть виконувати різні типи перетворень над своїми входами. Сигнали проходять від першого шару (шару входу) до останнього шару (шару виходу), можливо, після проходження шарами декілька разів. Початкова мета нейромережного підходу полягала у розв'язуванні задач таким же чином, як це робив би людський мозок. З часом увага зосередилася на відповідності конкретним розумовим здібностям, що призвело до відхилень від біології, таких як зворотне поширення, або передавання інформації у зворотному напрямку з підлаштовуванням мережі відображувати цю інформацію. Нейронні мережі використовували для різноманітних завдань, включно з комп'ютерним баченням, розпізнаванням мовлення, машинним перекладом, фільтруванням соціальних мереж, грою в настільні та відеоігри[en] та медичною діагностикою. Станом на 2017 рік нейронні мережі зазвичай мають від кількох тисяч до кількох мільйонів вузлів та мільйони з'єднань. Попри те, що це число на кілька порядків менше за число нейронів у мозку людини, ці мережі можуть виконувати багато завдань на рівні, що перевершує людський (наприклад, розпізнавати обличчя або грати в «Ґо»[143]). Глибока нейронна мережа (ГНМ, англ. deep neural network, DNN) — це штучна нейронна мережа (ШНМ) із кількома шарами між шарами входу та виходу.[14][17] Існують різні типи нейронних мереж, але вони завжди складаються з тих же складових: нейронів, синапсів, ваг, зміщень та функцій.[144] Ці складові в цілому функціонують у спосіб, що імітує функціювання людського мозку, і їх, як і будь-який інший алгоритм МН, можливо тренувати.[джерело?] Наприклад, ГНМ, тренована розпізнавати породи собак, проходитиме заданим зображенням й обчислюватиме ймовірність того, що зображений собака належить до певної породи. Користувач може переглядати результати й обирати, які ймовірності мережа повинна відображувати (вище певного порогу тощо) й повертати запропоновану мітку. Кожну математичну маніпуляцію як таку вважають шаром,[джерело?] і складні ГНМ мають багато шарів, звідси й назва «глибокі» мережі. ГНМ можуть моделювати складні нелінійні зв'язки. Архітектури ГНМ породжують композиційні моделі, де об'єкт виражають багатошаровою композицією примітивів.[145] Додаткові шари дозволяють комбінувати ознаки з нижчих шарів, потенційно моделюючи складні дані меншою кількістю вузлів, ніж неглибокі мережі з подібною продуктивністю.[14] Наприклад, було доведено, що розріджені багатовимірні многочлени експоненційно легше наближувати за допомогою ГНМ, ніж за допомогою неглибоких мереж.[146] До глибоких архітектур належать багато варіантів кількох основних підходів. Кожна архітектура досягла успіху в певних областях. Не завжди можливо порівняти продуктивність кількох архітектур, якщо їх оцінювали не на однакових наборах даних. ГНМ, як правило, є мережами прямого прямого поширення, в яких дані проходять з шару входу до шару виходу без повернення назад. Спочатку ГНМ створює карту віртуальних нейронів і призначує зв'язкам між ними випадкові числові значення, або «ваги». Ваги та входи перемножуються й повертають результат між 0 та 1. Якщо мережа не розпізнає певний образ точно, алгоритм підлаштовує ці ваги.[147] Таким чином алгоритм може робити певні параметри впливовішими, доки не визначить правильну математичну операцію для повної обробки даних. Рекурентні нейронні мережі (РНМ, англ. recurrent neural networks, RNN), в яких дані можуть простувати в будь-якому напрямку, використовують для таких застосувань як моделювання мови.[148][149][150][151][152] Для цього використання особливо ефективна довга короткочасна пам'ять.[81][153] Згорткові глибокі нейронні мережі (ЗНМ, англ. convolutional deep neural networks, CNN) використовують у комп'ютернім баченні.[154] ЗНМ також застосовували до акустичного моделювання[en] для автоматичного розпізнавання мовлення (АРМ, англ. automatic speech recognition, ASR).[155] Як і з ШНМ, при наївному тренуванні ГНМ може виникати багато проблем. Двома поширеними проблемами є перенавчання та обчислювальний час. ГНМ схильні до перенавчання через додані шари абстрагування, які дозволяють їм моделювати рідкісні залежності в тренувальних даних. Для боротьби з перенавчанням під час тренування можливо застосовувати методи регуляризації, такі як обрізання вузлів (англ. unit pruning) Івахненка,[41] ослаблення ваг[en] (англ. weight decay, \n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-регуляризація) та розрідженість (англ. sparsity, \n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-регуляризація).[156] Альтернативна регуляризація виключенням (англ. dropout) випадковим чином вилучає вузли з прихованих шарів під час тренування. Це допомагає виключати рідкісні залежності.[157] Нарешті, дані можливо доповнювати за допомогою таких методів як обрізання та обертання, щоби менші тренувальні набори можливо було збільшити в розмірі задля зменшення ймовірності перенавчання.[158] ГНМ повинні враховувати багато параметрів тренування, таких як розмір (кількість шарів і кількість вузлів на шар), темп навчання та первинні ваги. Прочісування простору параметрів для отримання оптимальних значень може бути недосяжним через часові та обчислювальні витрати. Обчислення прискорюють різні трюки, такі як пакетування (англ. batching, обчислення градієнта на кількох тренувальних прикладах одночасно, замість обчислення на окремих).[159] Великі оброблювальні можливості багатоядерних архітектур (таких як графічні процесори та Intel Xeon Phi) призвели до значного прискорення тренування через придатність таких оброблювальних архітектур для матричних та векторних обчислень.[160][161] Крім того, інженери можуть шукати інші типи нейронних мереж із простішими та збіжнішими алгоритмами тренування. Одним із таких видів нейронних мереж є АКММ (артикуляційний контролер мозочкової моделі[en], англ. cerebellar model articulation controller, CMAC). Він не потребує темпів навчання та увипадковлених первинних ваг. Може бути гарантовано збіжність його процесу тренування за один крок із новим пакетом даних, а обчислювальна складність алгоритму тренування лінійна щодо кількості задіяних нейронів.[162][163] З 2010-х років прогрес як в алгоритмах машинного навчання, так і в комп'ютерному апаратному забезпеченні призвів до ефективніших методів тренування глибоких нейронних мереж, які містять багато шарів нелінійних прихованих вузлів і дуже великий шар виходу.[164] До 2019 року графічні процесори (ГП), часто зі спеціальними вдосконаленнями для ШІ, витіснили ЦП як переважний метод тренування великомасштабного комерційного хмарного ШІ.[165] OpenAI оцінила апаратні обчислення, які використовували в найбільших проєктах глибокого навчання від AlexNet (2012) і до AlphaZero (2017), і виявила 300 000-кратне збільшення необхідного обсягу обчислень із тенденцією подвоєння часу кожні 3,4 місяця.[166][167] Для прискорення алгоритмів глибокого навчання було розроблено спеціальні електронні схеми, звані процесорами глибокого навчання. До процесорів глибокого навчання належать нейронні процесори (НП, англ. neural processing units, NPU) у мобільних телефонах Huawei[168] та серверах хмарних обчислень, такі як тензорні процесори (ТП, англ. tensor processing units, TPU) у Google Cloud Platform.[169] Cerebras Systems[en] також створила спеціальну систему для обробки великих моделей глибокого навчання, CS-2, що ґрунтується на найбільшому процесорі в галузі, другому поколінні Wafer Scale Engine (WSE-2).[170][171] Атомарно тонкі напівпровідники вважають перспективними для енергоефективного апаратного забезпечення глибокого навчання, де одну й ту ж базову структуру пристрою використовують як для логічних операцій, так і для зберігання даних. 2020 року Марега зі співавт. опублікували експерименти з активноканальним матеріалом великої площі для розробки пристроїв і схем з логічною пам'яттю на основі польових транзисторів з плавни́м затвором (англ. floating-gate field-effect transistors, FGFET).[172] 2021 року Й. Фельдманн зі співавт. запропонували інтегрований фотонний апаратний прискорювач для паралельної згорткової обробки.[173] Автори виділяють дві ключові переваги інтегрованої фотоніки над її електронними аналогами: (1) масивна паралельна передача даних через мультиплексування за довжиною хвилі в поєднанні з частотними гребінцями та (2) надзвичайно висока швидкість модуляції даних.[173] Їхня система може виконувати трильйони операцій множення-додавання за секунду, що вказує на потенціал інтегрованої фотоніки у застосуваннях штучного інтелекту, які потребують великих даних.[173] Великомасштабне автоматичне розпізнавання мовлення — це перший і найпереконливіший успішний приклад глибокого навчання. РНМ ДКЧП можуть навчатися завдань «дуже глибокого навчання»,[17] до яких належать багатосекундні інтервали, що містять мовленнєві події, розділені тисячами дискретних часових кроків, де один часовий крок відповідає приблизно 10 мс. ДКЧП із забувальними вентилями[153] на певних завданнях конкурентоспроможні з традиційними розпізнавачами мовлення.[98] Початковий успіх у розпізнаванні мовлення ґрунтувався на невеликих завданнях розпізнавання на основі TIMIT[en]. Цей набір даних містить 630 носіїв восьми основних діалектів американської англійської, де кожен читає 10 речень.[174] Його невеликий розмір дозволяє випробувати багато конфігурацій. Що ще важливіше, завдання TIMIT стосується розпізнавання фональних[en] послідовностей, яке, на відміну від розпізнавання послідовності слів, дозволяє використовувати слабкі фонобіграмні мовні моделі. Це дозволяє легше аналізувати силу аспектів акустичного моделювання розпізнавання мовлення. Частоту похибки, наведену нижче, включно з цими ранніми результатами, виміряну у відсотках рівнів фональних похибок (РФП, англ. phone error rates, PER), було узагальнено з 1991 року. Дебют ГНМ для розпізнавання мовців наприкінці 1990-х та розпізнавання мовлення приблизно в 2009—2011 роках, а також ДКЧП приблизно в 2003—2007 роках прискорили прогрес у восьми основних областях:[12][111][109] Усі основні комерційні системи розпізнавання мовлення (наприклад, Microsoft Cortana, Xbox, Перекладач Skype[en], Amazon Alexa, Google Now, Apple Siri, Baidu та голосовий пошук iFlytek[en], а також низка мовленнєвих продуктів Nuance[en] тощо) ґрунтуються на глибокому навчанні.[12][179][180] Поширеним оцінковим набором для класифікування зображень є набір даних бази даних MNIST. Він складається з рукописних цифр і містить 60 000 навчальних та 10 000 випробувальних прикладів. Як і у випадку з TIMIT, його невеликий розмір дозволяє користувачам випробувати кілька конфігурацій. Доступний вичерпний перелік результатів на цьому наборі.[181] Розпізнавання зображень на основі глибокого навчання стало «надлюдським», даючи точніші результати, ніж люди, учасники змагання. Вперше це сталося 2011 року з розпізнаванням дорожніх знаків, а 2014 року з розпізнаванням облич людей.[182][183] Треновані глибоким навчанням транспортні засоби тепер інтерпретують камери кругового огляду.[184] Іншим прикладом є новітній аналіз у лицевій дисморфології (англ. Facial Dysmorphology Novel Analysis, FDNA), який використовують для аналізу випадків вад розвитку людини, пов'язаних із великою базою даних генетичних синдромів. З прогресом, досягнутим у розпізнаванні зображень, тісно пов'язане все ширше застосування методик глибокого навчання до різноманітних завдань образотворчого мистецтва. ГНМ довели свою здатність, наприклад, у Нейронні мережі використовують для втілення мовних моделей з початку 2000-х років.[148] ДКЧП допомогла покращити машинний переклад і моделювання мови.[149][150][151] Іншими ключовими методиками в цій галузі є негативне вибирання (англ. negative sampling)[187] та вкладання слів. Вкладання слів, наприклад word2vec, можливо розглядати як шар подання в архітектурі глибокого навчання, який перетворює атомарне слово в подання розташування слова відносно інших слів у наборі даних; розташування подається як точка у векторному просторі. Використання вкладення слів як вхідного рівня РНМ дозволяє цій мережі аналізувати речення та фрази за допомогою ефективної композиційної векторної граматики. Композиційну векторну граматику можливо розглядати як імовірнісну контекстновільну граматику[en] (ІКВГ, англ. probabilistic context free grammar, PCFG), втілену РНМ.[188] Рекурсивні автокодувальники, збудовані поверх вкладень слів, можуть оцінювати схожість речень та виявляти перефразування.[188] Глибокі нейронні архітектури забезпечують найкращі результати для аналізу складників,[189] тональності,[190] пошуку інформації,[191][192] розуміння розмовної мови,[193] машинного перекладу,[149][194] контекстного зв'язування об'єктів,[194] розпізнавання стилю написання,[195] розпізнавання іменованих сутностей (класифікування лексем),[196] класифікування тексту та інших.[197] Останні розробки узагальнюють вкладання слів до вкладання речень. Перекладач Google використовує велику наскрізну мережу довгої короткочасної пам'яті (ДКЧП).[198][199][200][201] Нейронний машинний переклад Google використовує метод машинного перекладу на основі прикладів, у якому система «вчиться на мільйонах прикладів».[199] Він перекладає «цілі речення за раз, а не частини». Перекладач Google підтримує понад сто мов.[199] Мережа кодує «семантику речення, а не просто запам'ятовує пофразові переклади».[199][202] Перекладач Google використовує англійську як проміжну між більшістю мовних пар.[202] Значний відсоток ліків-кандидатів не отримує схвалення регуляторних органів. Ці невдачі спричинені недостатньою ефективністю (впливом на мішень), небажаними взаємодіями (впливами поза мішенню) або непередбаченими токсичними впливами.[203][204] Дослідники вивчали використання глибокого навчання для передбачування біомолекулярних мішеней[en],[136][137] антимішеней[en] та токсичних впливів хімічних речовин навколишнього середовища у поживних речовинах, побутових товарах і ліках.[138][139][140] AtomNet — це система глибокого навчання для раціонального конструювання ліків на основі структури.[205] AtomNet використовували для передбачування новітніх біомолекул-кандидатів для мішеней таких захворювань як вірус Ебола[206] та розсіяний склероз.[207][206] 2017 року графові нейронні мережі[en] було вперше використано для передбачування різних властивостей молекул у великому наборі токсикологічних даних.[208] 2019 року породжувальні нейронні мережі було використано для створення молекул, які було перевірено експериментально від початку до кінця на мишах.[209][210] Глибоке навчання з підкріпленням[en] використовували для наближування цінності можливих дій прямого маркетингу, визначених у термінах змінних RFM. Було показано, що ця функція оцінки цінності має природну інтерпретацію як пожиттєва цінність клієнта.[211] Рекомендаційні системи використовували глибоке навчання для виділяння значущих ознак для моделі латентних чинників для музичних та журнальних рекомендацій на основі вмісту.[212][213] Для навчання уподобань користувачів із кількох областей було застосовано багатоаспектне глибоке навчання (англ. multi-view deep learning).[214] Ця модель використовує гібридний спільний та оснований на вмісті підхід, і покращує рекомендації в декількох завданнях. Автокодувальну ШНМ використовували в біоінформатиці для передбачування анотацій генної онтології та зв'язків між генами й функціями.[215] У медичній інформатиці глибоке навчання використовували для передбачування якості сну на основі даних з носимих пристроїв[216] та для передбачування ускладнень здоров'я з даних електронних медичних записів.[217] Глибокі нейронні мережі (ГНМ) можливо використовувати для оцінювання ентропії стохастичних процесів, їх називають нейронними оцінювачами спільної ентропії (НОСЕ, англ. Neural Joint Entropy Estimator, NJEE).[218] Таке оцінювання дає уявлення про вплив випадкових змінних входу на незалежну випадкову змінну. На практиці, ГНМ тренують як класифікатор, який відображує вектор або матрицю входу X у розподіл імовірності виходу над можливими класами випадкової змінної Y за заданого входу X. Наприклад, у завданнях класифікування зображень НОСЕ відображує вектор значень кольорів пікселів у ймовірності над можливими класами зображень. На практиці розподіл імовірності Y отримують за допомогою шару Softmax із кількістю вузлів, яка дорівнює розміру абетки Y. НОСЕ використовує неперервно диференційовні передавальні функції, так що умови теореми про універсальне наближення виконуються. Показано, що цей метод забезпечує сильно слушну оцінку й перевершує інші методи в разі великих розмірів абетки.[218] Було показано, що глибоке навчання дає конкурентоспроможні результати в медичних застосуваннях, таких як класифікування ракових клітин, виявляння уражень, сегментування органів та покращування зображень.[219][220] Сучасні інструменти глибокого навчання демонструють високу точність виявляння різних захворювань та доцільність використання їх фахівцями для підвищення ефективності діагностування.[221][222] Знайти відповідну мобільну аудиторію для мобільної реклами завжди складно, оскільки необхідно розглянути та проаналізувати багато точок даних, перш ніж стане можливо створити цільовий сегмент і використати його для розміщення реклами на будь-якому рекламному сервері.[223] Глибоке навчання використовували для інтерпретування великих, багатовимірних наборів рекламних даних. Під час циклу інтернет-реклами запит/подача/натискання збирають багато точок даних. Ця інформація може ставати основою машинного навчання для покращення обирання оголошень. Глибоке навчання було успішно застосовано до обернених задач, таких як знешумлювання, надвисока роздільність[en], заповнювання прогалин[en] та кольоризування фільмів.[224] До цих застосувань входять такі методи навчання як «Shrinkage Fields for Effective Image Restoration»,[225] який тренується на наборі зображень, та глибоке апріорне зображень[en] (англ. Deep Image Prior), що тренується на зображенні, якому потрібне відновлення. Глибоке навчання успішно застосовують для виявляння фінансового шахрайства, ухилення від сплати податків[226] та боротьби з відмиванням грошей.[227] У листопаді 2023 року дослідники з Google DeepMind та Національної лабораторії ім. Лоуренса в Берклі оголосили, що вони розробили систему ШІ, відому як GNoME. Ця система зробила внесла внесок до матеріалознавства, відкривши понад 2 мільйони нових матеріалів за відносно короткий час. GNoME використовує методики глибокого навчання для ефективного дослідження потенційних структур матеріалів, досягаючи значного зростання у встановлюванні стабільних неорганічних кристалічних структур. Передбачення цієї системи були підтверджені за допомогою автономних роботизованих експериментів, продемонструвавши вражаючий рівень успішності в 71 %. Дані про нововідкриті матеріали доступні публічно через базу даних Materials Project[en], надаючи дослідникам можливість встановлювати матеріали з бажаними властивостями для різних застосувань. Цей розвиток має наслідки для майбутнього наукових відкриттів та інтегрування ШІ в дослідження матеріалознавства, потенційно прискорюючи нововведення в матеріалах та знижуючи вартість розробки продуктів. Використання ШІ та глибокого навчання натякає на можливість мінімізації або виключення ручних лабораторних експериментів та дозволяє науковцям більше зосередитися на проєктуванні й аналізі унікальних сполук.[228][229][230] Міністерство оборони Сполучених Штатів застосовувало глибоке навчання, щоб тренувати роботів виконувати нові завдання через спостереження.[231] Фізичні нейронні мережі (англ. physics informed neural networks) використовували для розв'язування диференціальних рівнянь із частинними похідними як у прямих, так і в обернених задачах на основі даних.[232] Одним із прикладів є відбудова потоку рідини, керована рівняннями Нав'є — Стокса. Використання фізичних нейронних мереж не потребує часто витратного породжування сітки, на яке спираються звичайні методи обчислювальної гідродинаміки.[233][234] Відбудова зображень (англ. image reconstruction) — це відбудова зображень, що лежать в основі пов'язаних із зображеннями вимірювань. Декілька праць показали кращу та відмінну продуктивність методів глибокого навчання порівняно з аналітичними методами для різних застосувань, наприклад, спектральних[235] та ультразвукових зображень.[236] Епігенетичний годинник (англ. epigenetic clock) — це біохімічний тест, який можливо використовувати для вимірювання віку. Галкін зі співавт. використали глибокі нейронні мережі, щоби натренувати епігенетичний годинник старіння з безпрецедентною точністю, використавши понад 6000 зразків крові.[237] Цей годинник використовує інформацію з 1000 CpG-острівців і передбачує людей з певними станами старше здорових контрольних груп: ЗЗК[en], лобово-скроневою деменцією, раком яєчника, ожирінням. Цей годинник старіння планувала випустити для загального використання 2021 року дочірня компанія компанії Insilico Medicine[en], Deep Longevity. Глибоке навчання тісно пов'язане з класом теорій розвитку мозку[en] (особливо нової кори), запропонованих когнітивними нейробіологами на початку 1990-х років.[238][239][240][241] Ці теорії розвитку було втілено в обчислювальних моделях, що зробило їх попередниками систем глибокого навчання. Ці моделі розвитку поділяють таку властивість, що різні запропоновані динаміки навчання в мозку (наприклад, хвиля чинника росту нервів) підтримують самоорганізацію, дещо аналогічну нейронним мережам, які використовують у моделях глибокого навчання. Як і нова кора, нейронні мережі використовують ієрархію багатошарових фільтрів, у яких кожен шар розглядає інформацію з попереднього шару (або робочого середовища), а потім передає свій вихід (і, можливо, початковий вхід) іншим шарам. Цей процес видає самоорганізований стос вимірювальних перетворювачів, добре підлаштованих до їхнього робочого середовища. В описі 1995 року зазначено: «…мозок немовляти, здається, організовується під впливом хвиль так званих чинників росту… різні ділянки мозку стають з'єднаними послідовно, причому один шар тканини дозріває раніше іншого, і так далі, поки не дозріє весь мозок».[242] Було використано різноманітні підходи для дослідження правдоподібності моделей глибокого навчання з нейробіологічної точки зору. З одного боку, було запропоновано декілька варіантів алгоритму зворотного поширення з метою підвищення реалістичності його обробки.[243][244] Інші дослідники стверджують, що до біологічної дійсності можуть бути ближчими форми некерованого глибокого навчання, такі як ті, що ґрунтуються на ієрархічних породжувальних моделях та глибоких мережах переконань.[245][246] У цьому відношенні моделі породжувальних нейронних мереж пов'язували з нейробіологічними свідченнями обробки в корі головного мозку на основі вибірки.[247] Хоча систематичного порівняння між організацією людського мозку та нейронним кодуванням у глибоких мережах створено ще не було, було повідомлено про кілька аналогій. Наприклад, обчислення, які виконуються блоками глибокого навчання, можуть бути подібними до обчислень справжніх нейронів[248] і нейронних популяцій.[249] Подібним чином, подання, вироблені моделями глибокого навчання, подібні до тих, які вимірюють у зоровій системі приматів[250] як на рівні окремого вузла[251], так і на рівні популяції.[252] Лабораторія ШІ Facebook виконує такі завдання як автоматичне мічення завантажених зображень[en] іменами людей на них.[253] Google DeepMind Technologies розробила систему, здатну навчитися грати у відеоігри Atari, використовуючи як дані входу лише пікселі. 2015 року вони продемонстрували свою систему AlphaGo, яка навчилася грі Ґо настільки добре, що перемогла професійного гравця.[254][255][256] Перекладач Google використовує нейронну мережу, щоби перекладати між понад 100 мовами. 2017 року було запущено Covariant.ai, зосереджений на інтегруванні глибокого навчання на заводах.[257] Станом на 2008 рік[258] дослідники Техаського університету в Остіні (UT) розробили систему машинного навчання під назвою Training an Agent Manually via Evaluative Reinforcement (укр. ручне тренування агента через оцінювальне підкріплення), або TAMER, яка запропонувала нові методи для роботів та комп'ютерних програм, як вчитися виконувати завдання шляхом взаємодії з людиною-інструктором.[231] Спершу розроблений як TAMER, новий алгоритм під назвою Deep TAMER було пізніше представлено 2018 року під час співпраці між Дослідницькою лабораторією армії США (ARL) та дослідниками UT. Deep TAMER використовував глибоке навчання, щоби забезпечити роботові здатність навчатися нових завдань шляхом спостерігання.[231] Використовуючи Deep TAMER, робот навчався завдання разом із тренером-людиною, переглядаючи відеопотоки або спостерігаючи, як людина виконує завдання особисто. Пізніше робот відпрацьовував завдання за допомогою тренера, який давав відгуки, такі як «добра робота» та «погана робота».[259] Глибоке навчання притягувало як критику, так і коментарі, у деяких випадках поза межами галузі інформатики. Основна критика стосується браку теорії навколо деяких методів.[260] Навчання в найпоширеніших глибоких архітектурах втілено за допомогою добре зрозумілого градієнтного спуску. Проте теорія навколо цих алгоритмів, таких як контрастове розходження, не така ясна[джерело?] (наприклад: Він збігається? Якщо так, то як швидко? Що він наближує?). На методи глибокого навчання часто дивляться як на чорну скриньку, роблячи більшість підтверджень емпірично, а не теоретично.[261] Інші зазначають, що глибоке навчання слід розглядати як крок до втілення сильного ШІ, а не як всеохопне рішення. Попри потужність методів глибокого навчання, їм все ще бракує значної частини функціональності, необхідної, щоби втілити цю мету повністю. Психолог-дослідник Ґері Маркус[en] зазначив: Насправді глибоке навчання це лише частина більшого завдання створення розумних машин. Таким методикам бракує способів подання причинно-наслідкових зв'язків (…) вони не мають очевидних способів здійснення логічних висновків, і вони також ще далекі від поєднання абстрактних знань, таких як інформація про те, чим є об'єкти, для чого вони, і як їх зазвичай використовують. Найпотужніші системи ШІ, такі як Watson (…) використовують такі методики, як глибоке навчання, як лише один з елементів у дуже складному ансамблі методик, починаючи від статистичних методик баєсового висновування, і аж до дедуктивного міркування.[262] Серед подальших відсилань до тієї ідеї, що художня чутливість може бути притаманна відносно низьким рівням когнітивної ієрархії, опублікована низка графічних зображень внутрішніх станів глибоких (20—30 шарів) нейронних мереж, які намагаються розгледіти серед по суті випадкових даних зображення, на яких їх було треновано,[263] демонструє візуальну привабливість: первинне повідомлення про це дослідження отримало набагато більше за 1000 коментарів і було предметом протягом деякого часу найвідвідуванішої статті на вебсайті Ґардіан.[264] Деякі архітектури глибокого навчання демонструють проблематичну поведінку,[265] наприклад, впевнене класифікування невпізнанних зображень як належних до знайомої категорії звичайних зображень (2014)[266] та неправильне класифікування незначних збурень правильно класифікованих зображень (2013).[267] Ґьорцель припустив, що така поведінка зумовлена обмеженнями у їхніх внутрішніх поданнях, і що ці обмеження перешкоджатимуть інтегруванню до гетерогенної багатокомпонентної архітектури загального штучного інтелекту (ЗШІ).[265] Можливо, ці проблеми можна розв'язати за допомогою архітектур глибокого навчання, які внутрішньо утворюють стани, гомологічні розкладам граматики зображень[268] спостережуваних об'єктів та подій.[265] Виведення граматики[en] (візуальної чи мовної) з тренувальних даних було би рівнозначним обмеженню системи міркуваннями здорового глузду, які оперують поняттями в термінах граматичних породжувальних правил[en], і є основною метою як засвоєння мови людиною,[269] так і штучного інтелекту (ШІ).[270] Коли глибоке навчання переміщується з лабораторії у світ, дослідження та досвід показують вразливість штучних нейронних мереж до хакерів та обману.[271] Визначаючи схеми, які ці системи використовують для функціювання, зловмисники можуть змінювати вхідні дані до ШНМ таким чином, що ШНМ знаходить відповідність, яку люди-спостерігачі не розпізнають. Наприклад, зловмисник може внести незначні зміни в зображення таким чином, що ШНМ знайде збіг, навіть якщо для людини зображення виглядає зовсім не схожим на ціль пошуку. Таке маніпулювання називають «змагальною атакою» (англ. adversarial attack).[272] 2016 року дослідники скористалися однією ШНМ, щоби, підкориговуючи зображення, методом спроб і помилок визначити, на чому зосереджується інша, й таким чином створити зображення, які вводили її в оману. Для людського ока змінені зображення нічим не відрізнялися. Інша група показала, що роздруківки підроблених зображень, які потім фотографували, успішно обманювали систему класифікування зображень.[273] Одним із засобів захисту є зворотний пошук зображень, під час якого можливе підроблене зображення надсилається на сайт, наприклад TinEye, який потім може знайти інші його примірники. Одне із вдосконалень полягає у пошуку з використанням лише частин зображення, щоби встановити зображення, з яких цей фрагмент могло бути взято.[274] Інша група показала, що певні психоделічні видовища можуть змусити систему розпізнавання облич вважати звичайних людей знаменитостями, потенційно дозволяючи одній людині видавати себе за іншу. 2017 року дослідники додали наліпки до знаків заборони проїзду без зупинки, змусивши ШНМ класифікувати їх неправильно.[273] Проте ШНМ можливо додатково тренувати виявляти спроби обману, що потенційно веде до перегонів озброєнь між зловмисниками й захисниками, подібних до тих, які вже є основою індустрії захисту від зловмисних програм. ШНМ було навчено перемагати програмне забезпечення захисту від зловмисного програмного забезпечення на основі ШНМ шляхом повторюваних атак на захист зловмисним програмним забезпеченням, яке постійно змінювалося генетичним алгоритмом, доки воно не ошукало протизловмисне програмне забезпечення, зберігаючи свою здатність пошкоджувати ціль.[273] 2016 року інша група продемонструвала, що певні звуки можуть змусити систему голосових команд Google Now відкрити певну вебадресу, й висунула гіпотезу, що це може «послужити сходинкою для подальших атак (наприклад, відкривання вебсторінки, на якій розміщено зловмисне програмне забезпечення)».[273] В «отруюванні даними[en]» до тренувального набору системи машинного навчання систематично підкидають хибні дані, щоби завадити їй досягти майстерності.[273] Більшість систем глибокого навчання покладаються на тренувальні та контрольні (англ. verification) дані, породжувані та/або розмічувані людьми.[275] У філософії засобів масової інформації стверджують, що для цієї мети регулярно використовують не лише низькооплачувану клікпрацю[en] (наприклад, на Amazon Mechanical Turk), а й неявні форми людської мікропраці[en], які часто не визнають як таку.[276] Філософ Райнер Мюльхоф[de] розрізняє п'ять типів «машинного заволодівання» людською мікропрацею для породжування тренувальних даних: (1) ігрофікація (вбудовування розмічування або обчислювальних завдань у потік гри), (2) «захоплювання та відстежування» (наприклад, CAPTCHA для розпізнавання зображень, або відстежування кліків на сторінках результатів пошуку Google), (3) використання соціальних мотивів (наприклад, позначування облич у Facebook для отримування позначених зображень обличчя), (4) розроблення інформації (наприклад, за допомогою пристроїв самооцифровування[en], таких як відстежувачі активності), та (5) клікпрацю[en].[276] Мюльхоф стверджує, що в більшості комерційних застосувань глибокого навчання для кінцевих користувачів, таких як система розпізнавання облич Facebook[en], потреба в тренувальних даних після тренування ШНМ не зникає. Скоріше, існує постійна потреба в контрольних даних, створюваних людьми, щоби постійно калібрувати та уточнювати ШНМ. З цією метою Facebook запровадив функцію, що щойно користувачів автоматично розпізнано на зображенні, вони отримують сповіщення. Вони можуть обрати, чи хочуть вони бути публічно позначеними на цьому зображенні, чи повідомити Facebook, що на зображенні не вони.[277] Цей інтерфейс користувача є механізмом породжування «постійного потоку контрольних даних»[276] для подальшого тренування мережі в режимі реального часу. Як стверджує Мюльхоф, залучення людей-користувачів до породжування тренувальних та контрольних даних настільки типове для більшості комерційних застосувань глибокого навчання для кінцевих користувачів, що такі системи можна назвати «штучним інтелектом з участю людини» (англ. human-aided artificial intelligence).[276]"
  },
  {
    "url": "https://ur.wikipedia.org/wiki/%DA%88%DB%8C%D9%BE_%D9%84%D8%B1%D9%86%D9%86%DA%AF",
    "title": "ڈیپ لرننگ - آزاد دائرۃ المعارف، ویکیپیڈیا",
    "content": "ڈیپ لرننگ یا گہری آموزی مشین لرننگ کا ایک ذیلی شعبہ ہے جو انسانی دماغ کی ساخت اور فنکشن سے متاثر الگورتھم پر فوکس کرتا ہے، جسے مصنوعی اعصابی نیٹ ورک کہا جاتا ہے۔ یہ نیٹ ورک واضح پروگرامنگ کے بغیر وسیع پیمانے پر ڈیٹا سے پیچیدہ نمونوں کو سیکھنے کے قابل ہیں۔ مصنوعی اعصابی نیٹ ورکس ڈیپ لرننگ کا مرکز ہیں، جو تہوں میں منظم باہم مربوط نوڈس (عصبون) پر مشتمل ہوتے ہیں۔ عصبون کے درمیان ہر تعلق کا ایک وزن ہوتا ہے جو نیٹ ورک کے سیکھنے کے ساتھ ہی ایڈجسٹ ہوجاتا ہے۔ صرف چند تہوں والے روایتی اعصابی نیٹ ورکس کے برعکس، ڈیپ لرننگ کے ماڈلز میں متعدد پرتیں ہوتی ہیں (اکثر درجنوں یا یہاں تک کہ سینکڑوں)۔ یہ گہرائی انھیں اعداد و شمار کی درجہ بندی کی نمائندگی سیکھنے میں مدد کرتی ہے، ہر پرت پر تیزی سے تجریدی خصوصیات کو گرفت میں لے کر۔"
  },
  {
    "url": "https://vi.wikipedia.org/wiki/H%E1%BB%8Dc_s%C3%A2u",
    "title": "Học sâu – Wikipedia tiếng Việt",
    "content": "Học sâu (tiếng Anh: deep learning, còn gọi là học cấu trúc sâu) là một phần trong một nhánh rộng hơn các phương pháp học máy dựa trên mạng thần kinh nhân tạo kết hợp với việc học biểu diễn đặc trưng (representation learning). Việc học này có thể có giám sát, nửa giám sát hoặc không giám sát.[1] Mạng thần kinh nhân tạo được lấy cảm hứng từ việc xử lý thông tin và các nút giao tiếp phân tán trong hệ sinh học. Nó có nhiều khác biệt so với não sinh học. Cụ thể, mạng thần kinh nhân tạo thường có tính tĩnh và mang tính biểu tượng, trong khi não bộ của hầu hết các sinh vật sống có tính động (linh hoạt) và analog.[2][3] Học sâu thường được nhắc đến cùng với Dữ liệu lớn (Big Data) và Trí tuệ nhân tạo (AI). Đã có nhiều ứng dụng trong thực tế , đang phát triển mạnh theo sự phát triển của tốc độ máy tính đặc biệt là khả năng tính toán trên  GPU và sự tăng nhanh của dữ liệu cùng với các framework (TensorFlow hay Pytorch) làm việc xây dựng model trở nên dễ dàng hơn. Học sâu là một phần của một họ các phương pháp học máy rộng hơn dựa trên đại diện học của dữ liệu. Một quan sát (ví dụ như, một hình ảnh) có thể được biểu diễn bằng nhiều cách như một vector của các giá trị cường độ cho mỗi điểm ảnh, hoặc một cách trừu tượng hơn như là một tập hợp các cạnh, các khu vực hình dạng cụ thể, vv. Một vài đại diện làm khiến việc học các nhiệm vụ dễ dàng hơn (ví dụ, nhận dạng khuôn mặt hoặc biểu hiện cảm xúc trên khuôn mặt) từ các ví dụ. Một trong những hứa hẹn của học sâu là thay thế các tính năng thủ công bằng các thuật toán hiệu quả đối với học không có giám sát hoặc nửa giám sát và tính năng phân cấp. Nhiều kiến trúc học sâu khác nhau như mạng neuron sâu, mã mạng neuron tích chập sâu, mạng niềm tin sâu và mạng neuron tái phát đã được áp dụng cho các lĩnh vực như thị giác máy tính, tự động nhận dạng giọng nói, xử lý ngôn ngữ tự nhiên, nhận dạng âm thanh ngôn ngữ và tin sinh học, chúng đã được chứng minh là tạo ra các kết quả rất tốt đối với nhiều nhiệm vụ khác nhau. Ngoài ra, học sâu đã trở thành một từ ngữ thời thượng, hay một thương hiệu của mạng neuron. Có một số cách để mô tả học sâu. Học sâu là một lớp của các thuật toán máy học mà(pp199–200) Các định nghĩa này có điểm chung là (1) nhiều lớp các đơn vị xử lý phi tuyến và (2) học có giám sát hoặc không có giám sát của biểu diễn đặc tính ở mỗi lớp, với các lớp hình thành một hệ thống các tính năng phân cấp từ thấp đến cao cấp.(p200) Các thành phần của một lớp của đơn vị xử lý phi tuyến sử dụng một thuật toán học sâu tùy theo vấn đề cần được giải quyết. Các lớp được sử dụng trong học sâu bao gồm các lớp ẩn của một mạng neuron nhân tạo và tập các công thức mệnh đề phức tạp. Chúng cũng có thể bao gồm các biến tiềm ẩn được tổ chức thành các lớp chọn lọc trong các mô hình thể sinh (có khả năng sinh ra) sâu như các nút trong Deep Belief Networks và Deep Boltzmann Machines. Các thuật toán học sâu tương phản với các thuật toán học nông bởi số biến đổi được tham số hóa một tín hiệu gặp phải khi nó lan truyền từ các lớp đầu vào đến lớp đầu ra, nơi một biến đổi được tham số hóa là một đơn vị xử lý có các thông số có thể huấn luyện được, chẳng hạn như trọng số và ngưỡng.(p6) Một chuỗi các biến đổi từ đầu vào đến đầu ra là một đường gán kế thừa (CAP- credit assignment path). CAP mô tả các kết nối quan hệ nhân quả tiềm năng giữa đầu vào và đầu ra và có thể thay đổi chiều dài. Đối với một mạng neuron nuôi tiến (feedforward), độ sâu của CAP, và do đó độ sâu của mạng đó, là số lượng các lớp ẩn cộng 1 (lớp đầu ra cũng là tham số hóa). Đối với mạng neuron tái phát, trong đó một tín hiệu có thể truyền thông qua một lớp nhiều hơn một lần, CAPcó khả năng không bị giới hạn chiều dài. Không có sự thống nhất chung về ngưỡng của độ sâu chia học cạn với học sâu, nhưng hầu hết các nhà nghiên cứu trong lĩnh vực đồng ý rằng học sâu có nhiều lớp phi tuyến (CAP > 2) và Schmidhuber coi CAP > 10 để là học rất sâu.(p7) Các thuật toán học sâu dựa trên các đại diện phân phối. Giả định tiềm ẩn đằng sau các đại diện phân phối là các dữ liệu được quan sát là được tạo ra bởi sự tương tác của các yếu tố được tổ chức theo lớp. Học sâu thêm giả định rằng các lớp của các yếu tố này tương ứng với các mức độ trừu tượng hay theo thành phần. Các con số khác nhau của các lớp và kích thước của lớp có thể được sử dụng để quy định các lượng trừu tượng khác. Học sâu khai thác ý tưởng thứ bậc các yếu tố giải thích này ở cấp cao hơn, những khái niệm trừu tượng hơn được học từ các cấp độ thấp hơn. Những kiến trúc này thường được xây dựng với một phương pháp lớp chồng lớp tham lam. Học sâu giúp để tháo gỡ những khái niệm trừu tượng này và chọn ra những đặc điểm cần thiết cho việc học. Đối với các nhiệm vụ học có giám sát, các phương pháp học sâu sẽ tránh kỹ thuật đặc điểm (feature engineering), bằng cách dịch các dữ liệu vào các đại diện trung gian nhỏ gọn giống như các thành phần chính, và lấy được các cấu trúc lớp mà loại bỏ sự thừa thải trong đại diện. Rất nhiều các thuật toán học sâu được áp dụng cho các nhiệm vụ học không có giám sát. Đây là một lợi ích quan trọng bởi vì dữ liệu không dán nhãn (chưa phân loại) thường phong phú hơn các dữ liệu dán nhãn. Một ví dụ của một cấu trúc sâu có thể được đào tạo theo cách không có giám sát là một mạng lưới tin sâu (deep belief network). Mạng neuron sâu thường được giải thích theo cách: định lý xấp xỉ tổng quát hoặc Suy luận xác suất. Định lý xấp xỉ phổ quát đề cập đến khả năng của mạng neuron tiến tiếp (feedforward) với một lớp ẩn có kích thước hữu hạn đơn để xấp xỉ các hàm liên tục. Năm 1989, là bằng chứng đầu tiên được xuất bản bởi George Cybenko cho các hàm kích hoạt hình sigma và được mở rộng đối với các kiến trúc nuôi tiến nhiều lớp vào năm 1991 bởi Kurt Hornik. Diễn giải xác suất bắt nguồn từ lĩnh vực máy học. Nó có đặc điểm suy luận, cũng như các khái niệm tối ưu hóa huấn luyện và kiểm tra, liên quan đến việc phù hợp và tổng quát hóa tương ứng. Cụ thể hơn, diễn giải xác suất sẽ xem xét kích hoạt một cách phi tuyến như là một hàm phân phối tích lũy. Xem mạng tin sâu. Diễn giải xác suất dẫn đến sự ra đời của dropout như regularizer trong mạng neuron. Diễn giải xác suất đã được giới thiệu và phổ biến rộng rãi bởi những tiên phong như Geoff Hinton, Yoshua Bengio, Yann Le Cun, Juergen Schmidhuber. Các kiến trúc học sâu, đặc biệt là những kiến trúc được xây dựng từ mạng neuron nhân tạo (ANN), đã từng thống trị ít nhất là tới Neocognitron được giới thiệu bởi Masahiko Fukushima vào năm 1980. Chính các ANN lại thống trị thậm chí lâu hơn nữa. Thách thức là làm thế nào để đào tạo mạng lưới này với nhiều lớp. Năm 1989, Yann Le Cun và các cộng sự đã có thể áp dụng các thuật toán truyền ngược tiêu chuẩn, khoảng từ năm 1974, đối với một mạng neuron sâu với mục đích nhận dạng chữ viết taymã ZIP trong các bức thư. Mặc dù sự thành công trong việc áp dụng thuật toán này, thời gian để đào tạo mạng dựa trên số liệu này mất khoảng 3 ngày, làm cho việc sử dụng nó vào các mục đích bình thường trở nên không thực tế. Năm 1995,Brendan Frey đã chứng minh rằng có thể đào tạo một mạng nơ ron bao gồm đầy đủ sáu lớp kết nối và vài trăm đơn vị ẩn bằng cách sử dụng thuật toán đánh thức giấc ngủ, nó được hợp tác phát triển với Peter Dayan và Geoffrey Hinton. Tuy nhiên, việc huấn luyện phải mất hai ngày. Nhiều yếu tố góp phần vào lý do gây ra tốc độ chậm, một là vấn đề biến mất gradient được phân tích vào năm 1991 bởi Sepp Hochreiter. Trong năm 1991 những mạng neuron như vậy được sử dụng để nhận diện chữ số viết tay 2-D cách ly, nhận dạng đối tượng 3-D được thực hiện bằng cách kết hợp các hình ảnh 2-D với một mô hình đối tượng 3-D thủ công. Juyang Weng và các cộng sự đề xuất rằng một bộ não người không sử dụng một mô hình đối tượng 3-D nguyên khối, và vào năm 1992, họ xuất bản Cresceptron, một phương pháp để thực hiện nhận dạng đối tượng 3-D trực tiếp từ các hậu trường lộn xộn. Cresceptron là một ghép tầng của các lớp tương tự như Neocognitron. Nhưng trong khi Neocognitron yêu cầu một lập trình viên con người can thiệp, Cresceptron sẽ tự động học được một số đặc điểm không có giám sát trong mỗi lớp, nơi mà mỗi đặc điểm được đại diện bởi một nhân tích chập. Cresceptron cũng phân đoạn từng đối tượng học được từ một cảnh nền lộn xộn thông qua việc phân tích ngược mạng đó. Thăm dò max, bây giờ thường được thông qua bởi các mạng neuron sâu (ví dụ: các kiểm tra ImageNet), lần đầu tiên sử dụng trong Cresceptron để giảm độ phân giải vị trí bởi của một hệ số (2x2) đến 1 thông qua việc ghép tầng tổng quát hóa tốt hơn. Mặc dù có những lợi thế như thế, các mô hình đơn giản hơn sử dụng nhiệm vụ cụ thể có đặc điểm thủ công như bộ Gabor và các máy hỗ trợ vector (SVM-support vector machines) đã là lựa chọn phổ biến trong thập niên 1990 và thập niên 2000, bởi vì chi phí tính toán bởi các ANN và vì thiếu sự hiểu biết về cách thức bộ não tự quản các kết nối mạng sinh học của nó. Trong lịch sử lâu dài của nhận dạng giọng nói, cả học nông và học sâu (ví dụ, các mạng tái phát) của mạng neuron nhân tạo đã được khám phá trong nhiều năm. Nhưng những phương pháp này không bao giờ thắng được công nghệ mô hình hỗn hợp/mô hình Markov ẩn Gaussian (GMM-HMM) thủ công-nội bộ dựa trên các mô hình thể sinh của việc huấn luyện nhận dạng giọng nói một cách rõ ràng.\nMột số khó khăn chính đã được phân tích một cách có phương pháp, bao gồm giảm bớt gradient và cấu trúc tương quan thời gian yếu và trong các mô hình tiên đoán thần kinh. Những khó khăn bổ sung đó là thiếu dữ liệu huấn luyện lớn và khả năng tính toán yếu trong thời gian ban đầu. Vì vậy, hầu hết nhà nghiên cứu nhận dạng giọng nói đã hiểu rõ các rào cản như vậy đã chuyển ra khỏi các mạng nơ ron để theo đuổi mô hình thể sinh, cho đến khi một sự hồi sinh gần đây của học sâu đã vượt qua tất cả những khó khăn này. Hinton và các cộng sự và Đặng cùng các cộng sự đã xem xét một phần của lịch sử này gầy đây về cách họ cộng tác với nhau và sau đó với các đồng nghiệp giữa các nhóm tái phát động nghiên cứu mạng neuron và bắt đầu nghiên cứu học sâu và các ứng dụng nhận dạng giọng nói. Một số phương pháp học sâu thành công nhất là mạng neuron nhân tạo. Mạng neuron nhân tạo được lấy cảm hứng từ các mô hình sinh học năm 1959 được đề xuất bởi người đoạt giải Nobel David H. Hubel & Torsten Wiesel, 2 người đã tìm thấy hai loại tế bào trong vỏ não thị giác chính: các tế bào đơn giản vàcác tế bào phức tạp. Nhiều mạng neuron nhân tạo có thể được xem như là các mô hình ghép tầng của các tế bào loại lấy cảm hứng từ những quan sát sinh học. Neocognitron của Fukushima giới thiệu các mạng neuron tích chập được đào tạo một phần bởi học không có giám sát với các đặc điểm được con người hướng dẫn trong mặt phẳng thần kinh. Yann LeCun...(1989) áp dụng truyền ngược có giám sát cho các kiến trúc như vậy. Weng... (1992) công bố các mạng neuron tích chập Cresceptron để nhận dạng các đối tượng 3-D từ các hình ảnh có hậu trường lộn xộn và phân khúc của các đối tượng từ hình ảnh đó. Một nhu cầu rõ ràng để nhận dạng các đối tượng 3-D nói chung là ít nhất là thay đổi tính bất biến và khả năng chịu biến dạng. Thăm dò Max (Max-pooling) xuất hiện lần đầu tiên được đề xuất bởi Cresceptron để kích hoạt mạng để chịu đựng được sự biến dạng từ nhỏ đến lớn theo một cách phân cấp, trong khi sử dụng tích chập. Thăm dò mã đã hoạt động tốt, nhưng không đảm bảo, dịch chuyển bất định ở mức điểm ảnh. Với sự ra đời của thuật toán truyền ngược được khám phá ra một cách độc lập bởi nhiều nhóm trong thập niên 1970 và 1980, nhiều nhà nghiên cứu đã cố gắng để đào tạo các mạng neuron nhân tạo sâu có giám sát từ đầu, ban đầu với rất ít thành công. Luận văn tốt nghiệp cao đẳng của Sepp Hochreiter năm 1991 chính thức xác định lý do cho sự thất bại này là vấn đề biến mất gradient, ảnh hưởng đến các mạng nuôi tiến nhiều lớp và các mạng neuron hồi qui. Các mạng tái phát (hồi qui) được huấn luyện bằng cách trải chúng ra vào các mạng nuôi tiến rất sâu, nơi một lớp mới được tạo ra cho mỗi bước thời gian của một chuỗi đầu vào được xử lý bởi mạng này. Khi các sai số truyền từ lớp này sang lớp khác, chúng co lại theo cấp số nhân với số lượng lớp, ngăn cản điều chỉnh trọng số nơ ron, dựa trên những sai số này. Để khắc phục vấn đề này, một số phương pháp đã được đề xuất. Một là thứ bậc đa cấp của mạng của Jürgen Schmidhuber (1992) cấp độ một được đào tạo trước tại một thời điểm bởi học không có giám sát, điều chỉnh bởi truyền ngược. Ở đây, mỗi cấp học một đại diện bị nén của các quan sát được đưa đến cấp độ tiếp theo. Phương pháp khác là mạng bộ nhớ dài ngắn hạn (LSTM) của Hochreiter & Schmidhuber (1997). Trong năm 2009, các mạng LSTM đa chiều sâu đã chiến thắng ba cuộc thi ICDAR năm 2009 trong nhận dạng chữ viết tay, mà không có bất kỳ kiến thức sẵn có về ba ngôn ngữ để được học. Sven Behnke vào năm 2003 dựa chỉ vào các dấu hiệu của gradient (Rprop) khi đào tạo Kim tự tháp Trừu tượng Nơ ron của mình để giải bài toán giống như tái tạo hình ảnh và định vị khuôn mặt. Các phương pháp khác cũng sử dụng đào tạo trước không có giám sát để tạo ra một mạng nơ ron, khiến nó lần đầu tiên học được bộ dò đặc điểm nói chung là hữu ích. Sau đó mạng này được đào tạo tiếp tục bằng cách truyền ngược có giám sát để phân loại dữ liệu có dán nhãn. Mô hình sâu này của Hinton và các cộng sự (2006) liên quan đến việc học phân phối của một đại diện cao cấp bằng cách sử dụng các lớp kế tiếp của các biến tiềm ẩn nhị phân hoặc giá trị thực. nó sử dụng một máy Boltzmann hạn chế (Smolensky, 1986) để mô hình hóa mỗi lớp mới của các đặc điểm cao cấp hơn. Mỗi lớp mới đảm bảo một sự tăng trưởng trong biên thấp của kiểm tra tỷ lệ giống của dữ liệu, do đó tăng cường cho mô hình, nếu được huấn luyện đúng cách. Một khi đã đủ nhiều lớp đã được học, kiến trúc sâu có thể được sử dụng như là một mô hình thể sinh bằng cách tái tạo dữ liệu khi lấy mẫu xuống mô hình đó (một \"sự vượt qua tổ tiên\") từ các kích hoạt tính năng cấp đỉnh.\nHinton báo cáo rằng các mô hình của mình là trích xuất các đặc điểm hiệu quả tính theo chiều cao, cấu trúc dữ liệu. Nhóm Google Brain do Andrew Ng và Jeff Dean đã tạo ra một mạng nơ ron học cách để nhận dạng được những khái niệm cao cấp hơn, chẳng hạn như con mèo, chỉ từ xem những hình ảnh không được dán nhãn từ các video trên YouTube. Các phương pháp khác dựa trên sức mạnh xử lý vượt trội của các máy tính hiện đại, đặc biệt, là các GPU. Trong năm 2010, Dan Ciresan và các đồng nghiệp trong nhóm của Jürgen Schmidhuber tại Phòng thí nghiệp AI Thụy Sĩ IDSIA cho thấy rằng mặc dù \"vấn đề biến mất gradient\" nêu trên, thì với sức mạnh xử lý vượt trội của các GPU làm khiến cho đồng truyền ngược đơn giản trở nên khả thi đối với các mạng neuron nuôi tiến sâu với nhiều lớp. Phương pháp này tốt hơn tất cả các kỹ thuật máy học khác trong việc giải bài toán cũ nổi tiếng MNIST chữ số viết tay của Yann Le Cun và các đồng nghiệp tại NYU. Cùng lúc đó, cuối năm 2009, học sâu đã thực hiện xâm nhập vào nhận dạng giọng nói, khi được đánh dấu bởi Hội thảo NIPS về học sâu trong nhận dạng giọng nói. Việc tăng cường hợp tác giữa các nhà nghiên cứu của Microsoft Research và đại học Toronto đã chứng minh vào giữa năm 2010 ở Redmond rằng các mạng neuron sâu giao tiếp với một mô hình Markov ẩn với các trạng thái phụ thuộc vào ngữ cảnh xác định lớp đầu ra của mạng neuron có thể giảm mạnh lỗi trong các tác vụ nhận dạng tiếng nói có vốn từ vựng lớn như tìm kiếm qua giọng nói. Cùng một mô hình mạng thần kinh sâu được chỉ ra cho quy mô lên đến các tác vụ cấp Tổng đài khoảng một năm sau đó tại Microsoft Research châu Á. Tính đến năm 2011, tiến bộ trong các mạng nuôi tiến học sâu đã thay thế các lớp tích chập và các lớp thăm dò tối da (max-pooling), đứng đầu bởi một số lớp có đầy đủ kết nối hoặc kết nối từng phần theo sau bởi một lớp phân loại cuối cùng. Việc huấn luyện thường được thực hiện mà không có bất kỳ đào tạo trước không có giám sát nào. Từ năm 2011, các thực thi dựa trên GPU của hướng tiếp cận này đã thắng nhiều cuộc thi nhận dạng hình mẫu, bao gồm cuộc thi IJCNN 2011 Traffic Sign Recognition Competition, ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, và các cuộc thi khác. Các phương pháp học sâu có giám sát như vậy cũng đã là bộ nhậng dạng mô hình nhân tạo đầu tiên đạt được hiệu suất có thể cạnh tranh lại được với con người trong những công việc nhất định. Để vượt qua những rào cản của AI yếu được đại diện bằng học sâu, cần phải để vượt qua các kiến trúc học sâu, bởi vì bộ não sinh học sử dụng cả mạch học nông và học sâu theo báo cáo của ngành giải phẫu não bộ chỉ ra một loạt các tính bất biến. Weng lập luận rằng não tự kết nối chủ yếu theo các thống kê tín hiệu và, do đó, một phân tầng nối tiếp không thể bắt tất cả các vật phụ thuộc thống kê chủ yếu. Các ANN đã có thể đảm bảo sự thay đổi bất biến để đối phó với các đối tượng tự nhiên lớn và nhỏ trong hậu trường có sự xáo trộn lớn, chỉ khi các bất định mở rộng vượt ra ngoài sự thay đổi, tới tất cả các khái niệm ANN đã học được, chẳng hạn như vị trí, loại (nhãn lớp đối tượng), quy mô, ánh sáng. Điều này được thực hiện trong các Mạng Phát triển (DN) có biểu hiện là Where-What Networks, WWN-1 (2008) cho đến WWN-7 (2013). Có một lượng rất lớn các biến thể của kiến trúc sâu. Hầu hết chúng là nhánh sinh ra từ một số kiến trúc cha ban đầu. Không phải là luôn luôn có thể so sánh hiệu suất của nhiều kiến trúc cùng với nhau, vì chúng không phải là tất cả đánh giá trên cùng một tập dữ liệu. Học sâu học là một lĩnh vực phát triển nhanh, và các kiến trúc, biến thể, hoặc các thuật toán mới xuất hiện mỗi vài tuần. Mạng neuron sâu (DNN-Deep neural Network) là một mạng neuron nhân tạo (ANN) với nhiều đơn vị lớp ẩn giữa lớp đầu vào và đầu ra. Tương tự như các ANN nông, các DNN nông có thể mô hình mối quan hệ phi tuyến phức tạp. Các kiến trúc DNN, ví dụ như để phát hiện và phân tích đối tượng tạo ra các mô hình hỗn hợp trong đó đối tượng này được thể hiện như một thành phần được xếp lớp của các hình ảnh nguyên thủy. Các lớp phụ cho phép các thành phần của các đặc điểm từ các lớp thấp hơn, đem lại tiềm năng của mô hình hóa dữ liệu phức tạp với các đơn vị ít hơn so với một mạng lưới nông thực hiện tương tự như vậy. Các DNN thường được thiết kế như các mạng nuôi tiến, nhưng nghiên cứu gần đây đã áp dụng thành công kiến trúc học sâu đối với các mạng nơ ron tái phát cho các ứng dụng chẳng hạn như mô hình hóa ngôn ngữ. Các mạng neuron sâu tích chập (CNN) được sử dụng trong thị giác máy tính nơi thành công của chúng đã được ghi nhận. Gần đây hơn, các CNN đã được áp dụng để mô hình hóa âm thanh cho nhận dạng giọng nói tự động (ASR), nơi chúng đã cho thấy sự thành công trong các mô hình trước đó. Để đơn giản, ta hãy nhìn vào việc huấn luyện các DNN được đưa ra ở đây. Một DNN có thể là mô hình biết suy xét được đào tạo với thuật toán truyền ngược tiêu chuẩn. Các bản cập nhật trọng số có thể được thực hiện thông qua độ dốc gradient ngẫu nhiên bằng cách sử dụng phương trình sau: Trong đó, \n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n là tốc độ học, và \n\n\n\nC\n\n\n{\\displaystyle C}\n\n là hàm chi phí. Việc lựa chọn của hàm chi phí phụ thuộc vào các yếu tố như loại học tập (giám sát, không có giám sát, tăng cường, vv) và hàm kích hoạt. Ví dụ, khi thực hiện học có giám sát về một vấn đề phân loại nhiều lớp, các lựa chọn phổ biến cho hàm kích hoạt và hàm chi phí là hàm softmax (hàm mũ chuẩn hóa) và hàm entropy chéo, tương ứng. Hàm softmax được định nghĩa là \n\n\n\n\np\n\nj\n\n\n=\n\n\n\nexp\n⁡\n(\n\nx\n\nj\n\n\n)\n\n\n\n∑\n\nk\n\n\nexp\n⁡\n(\n\nx\n\nk\n\n\n)\n\n\n\n\n\n{\\displaystyle p_{j}={\\frac {\\exp(x_{j})}{\\sum _{k}\\exp(x_{k})}}}\n\n trong đó \n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n thể hiện xác suất của lớp (đầu ra của đơn vị \n\n\n\nj\n\n\n{\\displaystyle j}\n\n) và \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n và \n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n hiển thể hiện tổng đầu vào thành các đơn vị \n\n\n\nj\n\n\n{\\displaystyle j}\n\n và \n\n\n\nk\n\n\n{\\displaystyle k}\n\n của cùng cấp tương ứng. Entropy chéo được định nghĩa là \n\n\n\nC\n=\n−\n\n∑\n\nj\n\n\n\nd\n\nj\n\n\nlog\n⁡\n(\n\np\n\nj\n\n\n)\n\n\n{\\displaystyle C=-\\sum _{j}d_{j}\\log(p_{j})}\n\n trong đó \n\n\n\n\nd\n\nj\n\n\n\n\n{\\displaystyle d_{j}}\n\n thể hiện cho xác suất mục tiêu của đơn vị ra \n\n\n\nj\n\n\n{\\displaystyle j}\n\n và \n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n là đầu ra xác suất cho \n\n\n\nj\n\n\n{\\displaystyle j}\n\n sau khi áp dụng hàm kích hoạt. Chúng có thể được sử dụng để xuất ra các hộp bao quanh đối tượng trong hình thức của một mặt nạ nhị phân. Chúng cũng được sử dụng cho các hồi quy đa quy mô để tăng độ chính xác của định vị. Hồi qui dựa trên DNN có thể học các đặc điểm mà chụp lại thông tin hình học ngoài việc là một bộ phân loại tốt. Chúng sẽ loại bỏ các giới hạn của việc thiết kế một mô hình mà sẽ chụp lại các bộ phận và quan hệ của chúng một cách rõ ràng. Điều này sẽ giúp học được một loạt các đối tượng rộng lớn. Mô hình này bao gồm nhiều lớp, mỗi trong số đó có một đơn vị chỉnh lại tuyến tính cho các chuyển đổi phi tuyến. Một số lớp là tích chập, trong khi những lớp khác được kết nối đầy đủ. Mỗi lớp tích chập có một thăm dò max bổ sung. Mạng được huấn luyện để giảm thiểu sai số L2 để dự đoán mặt nạ nằm trong dãi qua bộ huấn luyện toàn bộ chứa các hộp đường biên được thể hiện như là mặt nạ. Như với các ANN, nhiều vấn đề có thể nảy sinh với các DNN nếu chúng được huấn luyện thô sơ. Hai vấn đề phổ biến là overfitting (nhiễu hoặc sai số ngẫu nhiên) và thời gian tính toán. Các DNN có thiên hướng overfitting vì được thêm các lớp trừu tượng, mà cho phép chúng thực hiện mô hình hóa phụ thuộc hiếm hoi vào dữ liệu huấn luyện. Các phương pháp regularization (quy tắc hóa) như phân rã trọng số (\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularization) hoặc sparsity (rãi) (\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularization) có thể được áp dụng trong quá trình huấn luyện để giúp chống lại overfitting. Một phương pháp regularization gần đây được áp dụng cho các DNN là dropout regularization. Trong dropout, một số số lượng đơn vị được bỏ qua ngẫu nhiên từ các lớp ẩn trong quá trình đào tạo. Điều này giúp phá vỡ các phụ thuộc hiếm hoi có thể xảy ra trong dữ liệu đào tạo. Phương pháp chủ đạo cho việc huấn luyện các cấu trúc là sửa lỗi huấn luyện (chẳng hạn như truyền ngược với gradient descent) do dễ thực hiện và xu hướng hội tụ tốt hơn local optima (tối hưu cục bộ) hơn so với các phương pháp huấn luyện khác. Tuy nhiên, những phương pháp này có thể tốn công tính toán hơn, đặc biệt là cho các DNN. Có rất nhiều tham số huấn luyện để được xem xét với một DNN, chẳng hạn như kích thước (số lượng lớp và số lượng đơn vị trên mỗi lớp), tốc độ học và trọng số ban đầu. Quét thông qua không gian tham số cho các thông số tối ưu có thể không khả thi do chi phí trong thời gian và tài nguyên tính toán. Nhiều 'mẹo vặt' chẳng hạn như bằng cách sử dụng mini-batching (tính toán gradient trên nhiều ví dụ huấn luyện khác nhau cùng một lúc chứ không phải là từng ví dụ một) đã được chỉ ra để tăng tốc độ tính toán. Lượng xử lý lớn thông qua GPU đã tăng tốc đáng kể trong việc huấn luyện, do tính toán ma trận và vector rất thích hợp với các GPU. Lựa chọn thay thế triệt để cho truyền ngược là Extreme Learning Machines (Siêu máy học, các mạng \"No-prop\", huấn luyện không cần truy ngược, các mạng \"không trọng số\", và mạng nơron không kết (non-connectionist neural network) đang thu hút được sự chú ý. Một mạng niềm tin sâu (DBN) là một mô hình xác suất thể sinh, tạo thành bởi nhiều đơn vị ẩn nhiều lớp. Nó có thể được coi là một hàm hợp các mô-đun học đơn giản tạo thành mỗi lớp. Một DBN có thể được sử dụng để huấn luyện trước khả sinh một DNN bằng cách sử dụng các trọng số DBN học như các trọng số DNN ban đầu. Các thuật toán truyền ngược hoặc suy xét khác sau đó có thể được áp dụng để điều chỉnh những trọng số này. Điều này đặc biệt hữu ích khi dữ liệu đào tạo giới hạn là có sẵn, vì các trọng số khởi tạo nghèo nàn có thể cản trở đáng kể hiệu suất của mô hình được học. Các trọng số đào tạo trước này là một vùng không gian trọng số là gần gũi hơn với trọng số tối ưu hơn là các trọng số ban đầu được chọn ngẫu nhiên. Điều này cho phép cả mô hình hóa được cải thiện và hội tụ tinh chỉnh pha nhanh hơn. Một DBN có thể được huấn luyện một cách hiệu quả trong một cách thức không có giám sát, lớp kề lớp, nơi mà các lớp thường được tạo ra từ các máy Boltzmann hạn chế(RBM). Một RBM là một mô hình vô hướng, thể sinh dựa trên năng lượng với một lớp đầu vào \"hiện\" và một ẩn lớp, và các kết nối giữa các lớp nhưng không nằm trong các lớp. Phương pháp huấn luyện cho RBM được đề xuất bởi Geoffrey Hinton để sử dụng với các mô hình \"Product of Expert\" được gọi là tương phản phân kỳ (CD-contrastive divergence). CD cung cấp một xấp xỉ cho phương pháp với khả năng tối đa có vị trí lý tưởng sẽ được áp dụng cho việc học các trọng số của RBM. Trong việc huấn luyện một RBM đơn, các cập nhật trọng số được thực hiện với gradient ascent qua phương trình sau:\n\n\n\nΔ\n\nw\n\ni\nj\n\n\n(\nt\n+\n1\n)\n=\n\nw\n\ni\nj\n\n\n(\nt\n)\n+\nη\n\n\n\n∂\nlog\n⁡\n(\np\n(\nv\n)\n)\n\n\n∂\n\nw\n\ni\nj\n\n\n\n\n\n\n\n{\\displaystyle \\Delta w_{ij}(t+1)=w_{ij}(t)+\\eta {\\frac {\\partial \\log(p(v))}{\\partial w_{ij}}}}\n\n. Trong đó, \n\n\n\np\n(\nv\n)\n\n\n{\\displaystyle p(v)}\n\n là xác suất của một vector hiện, được cho bởi \n\n\n\np\n(\nv\n)\n=\n\n\n1\nZ\n\n\n\n∑\n\nh\n\n\n\ne\n\n−\nE\n(\nv\n,\nh\n)\n\n\n\n\n{\\displaystyle p(v)={\\frac {1}{Z}}\\sum _{h}e^{-E(v,h)}}\n\n. \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n là hàm từng phần, (được sử dụng để chuẩn hóa) và \n\n\n\nE\n(\nv\n,\nh\n)\n\n\n{\\displaystyle E(v,h)}\n\n là hàm năng lượng được gán cho trạng thái của mạng. Một năng lượng thấp hơn chỉ thị mạng đó đang được cấu hình \"đáng mong muốn\" hơn. Gradient \n\n\n\n\n\n\n∂\nlog\n⁡\n(\np\n(\nv\n)\n)\n\n\n∂\n\nw\n\ni\nj\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial \\log(p(v))}{\\partial w_{ij}}}}\n\n có dạng đơn giản \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\ndata\n\n\n−\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{data}}-\\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n trong đó \n\n\n\n⟨\n⋯\n\n⟩\n\np\n\n\n\n\n{\\displaystyle \\langle \\cdots \\rangle _{p}}\n\n thể hiện các giá trị trung bình đối với phân phối \n\n\n\np\n\n\n{\\displaystyle p}\n\n. Vấn đề này nãy sinh trong việc lấy mẫu \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n bởi vì điều này đòi hỏi phải chạy xen kẽ lấy mẫu Gibbs trong một thời gian dài. CD thay thế bwowcs này bằng cách chạy luân phiên lấy mẫu Gibbs cho \n\n\n\nn\n\n\n{\\displaystyle n}\n\n bước (giá trị của \n\n\n\nn\n=\n1\n\n\n{\\displaystyle n=1}\n\n được lấy theo kinh nghiệm được chỉ ra là làm việc tốt). Sau \n\n\n\nn\n\n\n{\\displaystyle n}\n\n bước, dữ liệu được lấy mẫu và mẫu này sẽ được sử dụng trong \n\n\n\n⟨\n\nv\n\ni\n\n\n\nh\n\nj\n\n\n\n⟩\n\nmodel\n\n\n\n\n{\\displaystyle \\langle v_{i}h_{j}\\rangle _{\\text{model}}}\n\n. Chu trình CD hoạt động như sau: Khi một RBM được huấn luyện, RBM khác là \"xếp chồng\" trên nó, đưa đầu vào của nó từ cuối lớp đã được huấn luyện. Lớp hiện mới này được khởi tạo với một vector hiện, và các giá trị cho các đơn vị trong các lớp đã được huấn luyện phân công bằng cách sử dụng trọng số hiện tại và các độ lệch. RBM mới này sau đó lại được huấn luyện với chu trình như trên. Toàn bộ quá trình này được lặp lại cho đến khi một số tiêu chí mong muốn chặn lại được đáp ứng. Mặc dù xấp xỉ của CD để tối đa khả năng là rất thô (CD đã được chỉ ra là theo gradient của bất kỳ hàm nào), nó đã được kinh nghiệm chỉ ra là có hiệu quả trong huấn luyện các kiến trúc sâu. Một CNN gồm có một hoặc nhiều hơn các lớp tích chập với các lớp đầy đủ kết nối (đáp ứng phù hợp với những mạng neuron nhân tạo tiêu biểu) trên đỉnh. Nó cũng sử dụng trọng số gắn liền và các lớp thăm dò. Kiến trúc này cho phép các CNN tận dụng lợi thế của cấu trúc 2D của dữ liệu đầu vào. So với những kiến trúc sâu khác, mạng neuron tích chập đang bắt đầu thể hiện kết quả vượt trội trong các ứng dụng hình ảnh và giọng nói. Chúng cũng có thể được huấn luyện với tiêu chuẩn truyền ngược. CNN dễ dàng được đào tạo hơn các mạng nơ ron sâu nuôi tiến thông thường khác, và có ít thông số ước tính hơn, khiến cho chúng trở thành một kiến trúc rất hấp dẫn để sử dụng. Các ví dụ về ứng dụng trong Thị Giác máy tính bao gồm DeepDream. Sử dụng mạng niềm tin sâu (CDBN) là một thành tựu gần đây của học sâu. Các CDBN có cấu trúc rất giống với một mạng neuron tích chập và được huấn luyện tương tự như các mạng niềm tin sâu. Vì vậy, chúng khai thác cấu trúc 2D của hình ảnh, giống như CNN làm, và làm cho việc sử dụng đào tạo trước giống như mạng niềm tin sâu. Chúng quy định một cấu trúc chung mà có thể được sử dụng trong nhiều tác vụ xử lý hình ảnh và tín hiệu. Gần đây, nhiều kết quả benchmark (tiêu chuẩn) dựa trên tập dữ liệu hình ảnh chuẩn như CIFAR đã được thu được kết quả bằng cách sử dụng CDBN. Mạng nơ ron lưu trữ và truy xuất bộ nhớ lớn (LAMSTAR) là các mạng nơ ron học sâu nhanh gồm nhiều lớp mà có thể sử dụng đồng thời nhiều bộ lọc. Các bộ lọc này có thể là phi tuyến, ngẫu nhiên, logic, không cố định, hoặc thậm chí không có tính phân tích. Chúng là học sinh học năng động và liên tục. Mạng neuron LAMSTAR có thể phục vụ như là một mạng nơ ron năng động trong không gian hay miền thời gian, hoặc cả hai. Tốc độ của nó được quy định bởi các liên kết-trọng số Hebbian (chương 9 của D. Graupe, 2013), dùng để tích hợp các bộ lọc khác nhau và thường khác nhau (các hàm tiền xử lý) vào nó nhiều lớp và để xếp hạng năng đọng tầm quan trọng của các lớp khác nhau và các hàm liên quan đến nhiệm vụ nhất định cho việc học sâu. Điều này hiển nhiên bắt chước học sinh học mà tích hợp các bộ tiền lý đầu ra khác nhau (ốc tai, võng mạc, vv) và vỏ não (thính giác, thị giác, vv) và của các vùng khác nhau của chúng. Khả năng học sâu của nó tăng cường hơn nữa bằng cách sử dụng sự ức chế, sự tương quan và bởi khả năng đối phó với dữ liệu không đầy đủ của nó, hoặc \"mất\" nơ ron hoặc lớp ngay cả khi đang thực thi một tác vụ. Hơn nữa, nó hoàn toàn minh bạch do trọng số liên kết của nó. Các trọng số liên kết cho phép xác định năng động sáng tạo và thừa thải, và tạo thuận lợi cho việc xếp hạng của các lớp, các bộ lọc hoặc các nơ ron đơn lẽ tương ứng với một nhiệm vụ. LAMSTAR đã được áp dụng cho nhiều dự đoán y tế và tài chính (xem Graupe, 2013 Phần 9C), bộ lọc thích nghi nhiễu nhận dạng giọng nói với tiếng ồn không xác định, nhận dạng ảnh tĩnh (Graupe, 2013 Phần 9D), nhận dạng ảnh video, bảo mật phần mềm, điều khiển thích nghi của các hệ thống phi tuyến, vv. LAMSTAR có tốc độ tính toán nhanh hơn nhiều và có lỗi hơi ít hơn so với một mạng nơ ron tích chập dựa trên các bộ lọc hàm-ReLU và thăm dò max, trong một nghiên cứu nhận dạng ký tự so sánh. Các ứng dụng này chứng minh đào sâu vào các khía cạnh của các dữ liệu đó là bị ẩn từ các mạng học nông hoặc thậm chí từ những giác quan của con người (mắt, tai), chẳng hạn như trong trường hợp của dự đoán sự bắt đầu của hiện tượng ngưng thở khi ngủ, của một biểu đồ điện tâm đồ một thai nhi như được ghi chép từ các điện cực gắn trên da được đặt trên bụng người mẹ trong thời gian đầu của thai kỳ, của dự đoán tài chính (Phần 9C trong Graupe, 2013), hoặc trong lọc mù của nhiễu trong nhận dạng giọng nói LAMSTAR đã được đề xuất năm 1996 (Bằng phát minh 5,920,852 A của Mỹ) và tiếp tục được phát triển bởi D Graupe và H Kordylewski vào năm 1997-2002. Một phiên bản sửa đổi, được gọi là LAMSTAR 2, được phát triển bởi N C Schneider và D Graupe trong năm 2008. Một kiến trúc sâu dựa trên một hệ thống phân cấp của các khối mô-đun mạng neuron đơn giản là một mạng sâu lồi, được giới thiệu vào năm 2011. Ở đây, bài toán học các trọng số được xây dựng như một bài toán tối ưu hóa lồi với lời giải dạng đóng. Kiến trúc này còn được gọi là một mạng xếp chồng sâu (DSN), nhấn mạnh các cơ chế tương tự với tổng quát hóa xếp chồng. Mỗi khối DSN là một module đơn giản đó là dễ dàng để huấn luyện chính nó trong một kiểu có giám sát mà không cần truyền ngược cho toàn bộ các khối. Có những lợi thế của một mô hình mà có thể chủ động cập nhật bản thân từ ngữ cảnh trong dữ liệu. Mạng lập trình (DPCN) là một chương trình lập trình tiên đoán, trong đó thông tin từ trên xuống được sử dụng để điều chỉnh theo kinh nghiệm của những cái trước đó cần thiết cho một thủ tục suy luận từ dưới lên bằng các phương tiện của một mô hình thể sinh kết nối cục bộ sâu. Điều này hoạt động bằng cách chiết tách các đặc điểm rời rạc các quan sát biến đổi theo thời gian bằng cách sử dụng một mô hình động học tuyến tính. Sau đó, một chiến lược thăm dò được sử dụng để học các đại diện đặc điểm bất biến. Các đơn vị này tập hợp lại để tạo thành một kiến trúc sâu và được huấn luyện bởi học không giám sát layer-wise tham lam. Các lớp tạo thành một loại xích Markov mà các trạng thái tại bất kỳ lớp nào cũng chỉ phụ thuộc vào các lớp trước và các lớp sau (kế thừa). Mạng lập tình dự đoán sâu (DPCN) dự đoán đại diện của lớp, bằng cách sử dụng một cách tiếp cận từ trên xuống bằng cách sử dụng thông tin ở lớp trên và các phụ thuộc thời gian từ các trạng thái trước đó. DPCN có thể được mở rộng để tạo thành một mạng tích chập. Bộ nhớ ngoài tích hợp với các mạng neuron nhân tạo tính đến nghiên cứu đầu tiên trrong đại diện phân phối và các bản đồ tự tổ chức. Ví dụ, trong bộ nhớ phân tán hoặc bộ nhớ phân cấp thời gian, các mô hình được mã hóa bởi các mạng neuron được sử dụng như là các địa chỉ cho bộ nhớ có khả năng định địa chỉ nội dung, với các \"nơ ron\" chủ yếu phục vụ như là các bộ mã hóa và giải mã. Trong thập niên 1990 và thập niên 2000, đã có nhiều công trình liên quan đến bộ nhớ ngắn-hạn dài (LSTM - thêm bộ nhớ khả vi cho các hàm hồi qui). Ví dụ: Các mạng bộ nhớ một mở rộng khác của các mạng nơ ron nhân tạo kết hợp với bộ nhớ dài hạn, được phát triển bởi nhóm nghiên cứu Facebook. Bộ nhớ dài hạn có thể được đọc và ghi vào đó, với mục đích sử dụng cho việc dự báo. Các mô hình này đã được áp dụng trong bối cảnh hỏi đáp (QA) nơi bộ nhớ dài hạn hoạt động hiệu quả như một cơ sở kiến thức (năng động), và đầu ra là một đáp ứng văn bản. Một framework mã hóa-giải mã là một framework dựa trên các mạng neuron nhằm mục đích lập bản đồ đầu vào cấu trúc cao tới đầu ra có cấu trúc cao. Nó đã được đề xuất gần đây trong bối cảnh của máy dịch, trong đó đầu vào và đầu ra được viết thành câu bằng hai ngôn ngữ tự nhiên. Trong đó, một mạng nơ ron tái phát (RNN) hoặc mạng neuron tích chập (CNN) được sử dụng như một bộ mã hóa để tóm tắt một câu nguồn và tóm tắt này được giải mã bằng cách sử dụng một mô hình ngôn ngữ mạng neuron tái phát có điều kiện để tạo ra bản dịch. Tất cả các hệ thống này có các khối xây dựng tương tự: cổng RNN và CNN, và các cơ chế tập trung được huấn luyện. Hiện nay các mô hình transformer base đã vượt xa các loại mô hình sử dụng RNN. Hầu như trong tất cả các tác vụ Transformer base model đều vượt trội hơn các RNN model (LSTM or GRU base). Với Hugging face Hugging Face Hub chúng ta có thể dễ dàng fine turn model🤗. Một trong những nguyên tắc cơ bản của học sâu là để thoát khỏi kỹ thuật đặc tính thủ công và sử dụng các đặc tính thô. Nguyên tắc này được khám phá thành công đầu tiên trong kiến trúc của tự mã hóa sâu trên ảnh phổ \"thô\" hoặc các đặc điểm dãi lọc tuyến tính, hiển thị sự vượt trội của nó hơn các tính năng Mel-Cepstral mà có chứa một vài giai đoạn chuyển đổi cố định từ ảnh phổ. Các tính năng thực sự \"thô\" của tiếng nói, dạng sóng, gần đây đã được chỉ ra để tạo ra các kết quả nhận dạng giọng nói tuyệt vời ở quy mô lớn. Kể từ khi ra mắt thành công ban đầu của DNN cho nhận dạng tiếng nói khoảng 2009-2011, tiến độ (và hướng đi trong tương lai) có thể được tóm tắt vào 8 lĩnh vực chính: Trường hợp nhận dạng tiếng nói tự động quy mô lớn lần đầu tiên và thuyết phục nhất thành công của học sâu trong lịch sử gần đây, chấp nhận bở cả công nghiệp và hàn lâm trong tất cả các lĩnh vực. Từ năm 2010 đến năm 2014, hai hội nghị lớn về xử lý tín hiệu và nhận dạng giọng nói, IEEE-ICASSP và Interspeech, đã thấy một sự gia tăng lớn các báo cáo được chấp nhận trong các báo cáo hội nghị thường niên tương ứng về chủ đề học sâu trong nhận dạng giọng nói. Quan trọng hơn, tất cả các hệ thống nhận dạng giọng nói thương mại chính (ví dụ: Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu và iFlyTek tìm kiếm bằng giọng nói và một loạt các sản phẩm của Nuance speech, vv) được dựa trên phương pháp học sâu. Xem thêm các cuộc phỏng vấn trên phương tiện truyền thông với CTO của Nuance Communications. Thành công lây lan rộng trong nhận dạng tiếng nói đã đạt được vào năm 2011 được kế tiếp liền sau đó là nhận dạng hình ảnh ở quy mô lớn. Một tập đánh giá phổ biến cho phân loại hình ảnh là tập hợp dữ liệu cơ sở dữ liệu MNIST. MNIST bao gồm các chữ số viết tay và bao gồm 60000 ví dụ huấn luyện và 10000 ví dụ kiểm tra. Như TIMIT, kích thước nhỏ của nó cho phép nhiều cấu hình được kiểm tra. Một danh sách đầy đủ các kết quả trên tập này có thể được tìm thấy trong. Kết quả tốt nhất hiện nay trên MNIST là tỷ lệ lỗi 0,23%, đạt được bởi Ciresan và các cộng sự vào năm 2012. Tác động thực sự của học sâu trong nhận dạng hình ảnh hoặc đối tượng, một chi chính của thị giác máy tính, đã cảm thấy được vào mùa thu năm 2012 sau khi đội của Geoff Hinton và sinh viên của ông thắng trong cuộc thi quy mô lớn ImageNet bởi một biên độ đáng kể bằng phương pháp máy học nông tiên tiến nhất. Công nghệ này dựa trên các mạng tích chập sâu 20 tuổi, nhưng với quy mô lớn hơn nhiều trên một nhiệm vụ lớn hơn nhiều, vì nó đã học được rằng học sâu làm việc tốt đối nhận dạng giọng nói quy mô lớn. Trong năm 2013 và 2014, tỷ lệ lỗi trong tác vụ của ImageNet bằng cách sử dụng học sâu tiếp tục giảm xuống nhanh chóng, theo một xu hướng tương tự trong nhận dạng giọng nói quy mô lớn. Khi tham vọng này di chuyển từ nhận dạng giọng nói tự động sang các bản dịch giọng nói tự động và hiểu được, phân loại hình ảnh gần đây đã được mở rộng với nhiệm vụ khó khăn hơn đó là tạo phụ đề cho hình ảnh tự động, trong đó có học sâu là công nghệ cơ bản thiết yếu. Một ứng dụng ví dụ là một máy tính xe hơi cho biết được đào tạo bằng học sâu, có thể cho phép xe diễn giải các hình ảnh 360° từ camera. Một ví dụ khác là công nghệ được gọi là Facial Dysmorphology Novel Analysis (FDNA) -(Phân tích các dị tật của khuôn mặt) sử dụng để phân tích các trường hợp dị dạng của con người kết nối với cơ sở dữ liệu lớn của các hội chứng di truyền. Mạng neuron đã được sử dụng cho việc thực hiện các mô hình ngôn ngữ kể từ đầu những năm 2000. Các kỹ thuật quan trọng trong lĩnh vực này là lấy mẫu âm và nhúng từ (word embedding). Nhúng chữ, chẳng hạn như word2vec, có thể được dùng như một lớp đại diện trong một kiến trúc học sâu, điều này sẽ biến đổi một từ đơn thành một đại diện vị trí của từ đó liên quan đến các từ khác trong bộ dữ liệu; vị trí được đại diện như là một điểm trong một không gian vector. Sử dụng một từ nhúng như là một lớp đầu vào với một mạng lưới thần kinh đệ quy (RNN-recursive neuron network) cho phép đào tạo mạng để phân tích cú pháp câu và cụm từ bằng cách sử dụng một ngữ pháp vector tổng hợp có hiệu quả. Một ngữ pháp vector tổng hợp có thể được coi làngữ pháp không phụ thuộc ngữ cảnh xác suất (PCFG-probabilistic context free grammar) được thực hiện bởi một mạng thần kinh đệ quy. Tự động-mã hóa đệ qui được xây dựng trên đỉnh từ nhúng đã được đào tạo để đánh giá câu tương tự và phát hiện các chú giải dài dòng. Các kiến trúc thần kinh sâu đã đạt được những kết quả tiên tiến nhất trong nhiều tác vụ xử lý ngôn ngữ tự nhiên như phân tích thống kê, phân tích tình cảm, tra cứu thông tin, dịch máy, liên kết thực thể ngữ cảnh, và.v.v. Ngành công nghiệp dược phẩm phải đối mặt với vấn đề mà một tỷ lệ lớn các loại thuốc tiềm năng thất bại khi tiếp cận với thị trường. Những thất bại của các hợp chất hóa học này gây ra bởi không đủ hiệu quả trên mục tiêu phân tử sinh học (có hiệu lực với mục tiêu), có các tương tác không bị phát hiện và không mong muốn với các phân tử sinh học khác (chệch mục tiêu tác động), hoặc các hiệu ứng độc dược ngoài dự tính. Trong năm 2012, một nhóm dẫn đầu bởi George Dahl đã chiến thắng \"Merck Molecular Activity Challenge\" sử dụng các mạng neuron sâu đa tác vụ để dự đoán mục tiêu phân tử sinh học của một hợp chất. Trong năm 2014, nhóm của Sepp Hochreiter sử dụng học sâu để phát hiện ra mục tiêu lạ và các ảnh hưởng độc dược của các môi trường hóa chất trong các chất dinh dưỡng, sản phẩm gia dụng và thuốc men và đã chiến thắng \"Tox21 Data Challenge\" của NIH, FDA và NCATS. Những thành công ấn tượng chỉ ra rằng học sâu có thể vượt trội so với các phương pháp kiểm tra ảo khác. Các nhà nghiên cứu đến từ Google và Stanford đã mở rộng học sâu để khám phá dược phẩm bằng cách kết hợp dữ liệu từ nhiều nguồn khác nhau. Năm 2015, Atomwise giới thiệu AtomNet, mạng neuron học sâu đầu tiên dành cho thiết kế dược phẩm dựa trên cấu trúc hợp lý. Sau đó, AtomNet đã được sử dụng để dự đoán các phân tử sinh học được chọn mới lạ đối với nhiều mục tiêu bệnh tật, đặc biệt là phương pháp điều trị bệnh do virus Ebola và bệnh đa xơ cứng. Thành công gần đây đã được báo cáo với ứng dụng của học tăng cường sâu trong các thiết lập tiếp thị trực tiếp, thể hiện sự phù hợp của phương pháp này dành cho tự động hóa CRM. Một mạng nơ ron được sử dụng để ước tính giá trị của các hành động có thể trực tiếp tiếp thị trên không gian trạng thái khách hàng, được định nghĩa trong điều khoản của biến RFM. Hàm giá trị ước tính được chỉ ra để có một giải thích tự nhiên như là giá trị khách hàng suốt đời. Các hệ thống khuyến cáo đã sử dụng học sâu để trích xuất các đặc điểm sâu có ý nghĩa cho mô hình yếu tố tiềm ẩn đối với khuyến cáo dựa trên nội dung cho âm nhạc. Gần đây, một cách tiếp cận tổng quát hơn cho việc học tập sở thích người dùng từ nhiều miền bằng cách sử dụng học sâu đa góc nhìn đã được đưa ra. Mô hình này sử dụng một cộng tác lai và tiếp cận dựa trên nội dung và tăng cường các khuyến nghị trong nhiều nhiệm vụ. Gần đây, một cách tiếp cận học sâu dựa trên một mạng neuron nhân tạo tự mã hóa đã được sử dụng trong tin sinh học, để dự đoán các mối quan hệ chức năng gen và các chú thích Bản thể gen. Tính toán học sâu có liên hệ chặt chẽ đến học thuyết về sự phát triển của não bộ (cụ thể, phát triển neocortical) do các nhà khoa học thần kinh nhận thức đề xuất trong đầu thập niên 1990. Một bản tóm tắt dễ tiếp cận của ý tưởng này là tác phẩm của Elman và các cộng sự vào năm 1996 \"Xem xét lại Tính bẩm sinh\" (Xem thêm: Shrager và Johnson; Quartz và Sejnowski). Những lý thuyết phát triển này cũng được thuyết minh cụ thể trong các mô hình tính toán, chúng là những kỹ thuật tiền nhiệm của các mô hình học sâu được thúc đẩy bởi tính toán (bằng máy tính) đơn thuần. Những mô hình phát triển này chia sẻ thuộc tính thú vị mà nhiều động lực học (learning dynamics) khác nhau được đề xuất trong nghiên cứu não bộ (Ví dụ, một làn sóng của yếu tố tăng trưởng thần kinh) để hỗ trợ việc tự tổ chức của các loại mạng nơ ron có liên quan với nhau được sử dụng trong các mô hình học sâu thuần tính toán sau đó; và các mạng neuron tính toán như vậy có vẻ tương tự như quan điểm của ngành nghiên cứu vỏ não mới như một hệ thống phân cấp của bộ lọc trong đó mỗi lớp chụp một số thông tin trong môi trường hoạt động, và sau đó đi qua phần còn lại, cũng như tín hiệu cơ bản được sửa đổi, tới các lớp khác cao hơn trong hệ thống phân cấp. Quá trình này mang lại một chồng tự tổ chức các cảm biến, cũng như điều chỉnh để hoạt động môi trường của họ. Như được mô tả trên tờ New York Times vào năm 1995: \"...bộ não của những trẻ sơ sinh dường như tự tổ chức riêng chính nó dưới ảnh hưởng của các sóng của cái gọi là các yếu tố - dinh dưỡng... các khu vực khác nhau của não trở nên kết nối tuần tự, với một lớp mô trưởng thành trước các mô khác và cho đến khi toàn bộ não là trưởng thành.\" Tầm quan trọng của học sâu đối với sự tiến hóa và phát triển của nhận thức của con người đã không thoát khỏi sự chú ý của các nhà nghiên cứu. Một khía cạnh của phát triển con người là phân biệt chúng ta với những người hàng xóm trong họ linh trưởng gần nhất của mình có thể thay đổi trong thời gian phát triển. Trong số các loài linh trưởng, bộ não con người vẫn còn tương đối mềm dẻo cho đến cuối thời kỳ sau khi sinh, trong khi bộ não của họ hàng gần gũi nhất của chúng ta hoàn toàn cố định hơn ngay sau khi sinh. Vì vậy, con người có khả năng truy cập lớn hơn vào những kinh nghiệm phức tạp đang diễn ra trên thế giới trong giai đoạn hình thành nhất của sự phát triển não bộ. Điều này có thể cho phép chúng ta \"điều chỉnh\" để thay đổi nhanh chóng môi trường mà các động vật khác, nhiều bị hạn chế bởi cơ cấu tiến hóa của bộ não của chúng, không thể để thực hiện được. Đến mức mà những thay đổi này được phản ánh trong các thay đổi thời gian tương tự trong sóng được giả thuyết của sự phát triển vỏ não, chúng cũng có thể dẫn đến những thay đổi trong việc khai thác thông tin từ môi trường kích thích trong thời gian đầu tự tổ chức của bộ não. Tất nhiên, cùng với tính linh hoạt này đến một giai đoạn kéo dài chưa thành thục, trong đó chúng ta phụ thuộc vào người chăm sóc và cộng đồng của mình để hỗ trợ và đào tạo. Lý thuyết của học sâu do đó thấy sự cùng tiến hóa đồng thời của văn hóa và nhận thức như là một điều kiện cơ bản của sự tiến hóa của con người. Hầu hết các công ty công nghệ lớn nhất trên thế giới đang đầu tư rất nhiều nguồn lực vào nghiên cứu và phát triển để tiếp tục cải tiến công nghệ lõi cũng như tạo ra các sản phẩm ứng dụng sử dụng kỹ thuật học sâu. Điển hình là nhóm nghiên cứu về trí tuệ nhân tạo của Facebook đã tạo ra phần mềm DeepFace có khả năng nhận dạng khuôn mặt tốt như con người với độ chính xác khoảng 97,35%. Công trình này (công bố năm 2014) sử dụng 4 triệu ảnh khuôn mặt của hơn 4000 người để huấn luyện cho mạng nơron nhiều lớp và mô hình thu được đã vượt qua các kỹ thuật được nghiên cứu đề xuất trước đó.[4] Học sâu thường được trình bày như là một bước hướng tới AI mạnh và do đó nhiều tổ chức đã trở nên quan tâm đến việc sử dụng nó cho các ứng dụng cụ thể. Vào tháng 12 năm 2013, Facebook đã tuyển Yann Le Cun đứng đầu phòng thí nghiệm trí tuệ nhân tạo (AI) mới của họ hoạt động ở California, London và New York. Phòng thí nghiệm AI này sẽ phát triển những kỹ thuật học sâu để giúp Facebook thực hiện các nhiệm vụ, chẳng hạn như tính năng gắn thẻ tự động hình ảnh tải lên với tên của những người có mặt trong đó. Vào cuối năm 2014, Facebook cũng tuyển Vladimir Vapnik, nhà phát triển chính của lý thuyết Vapnik-Chervonenkis về học thống kê, và đồng phát minh ra phương pháp máy vector hỗ trợ. Vào tháng 3 năm 2013, Google tuyển Geoffrey Hinton và hai sinh viên tốt nghiệp của ông, Alex Krizhevsky và Ilya Sutskever. Công việc của họ là tập trung vào vừa cải tiến các sản phẩm học máy hiện có của Google và vừa trợ giúp đối phó với lượng dữ liệu ngày càng tăng nhanh mà Google có được. Google cũng mua lại công ty của Hinton, DNNresearch. Năm 2014, Google cũng đã mua DeepMind Technologies, một công ty khởi nghiệp của Anh đã phát triển một hệ thống có khả năng học tập làm thế nào để chơi trò chơi điện tử Atari chỉ sử dụng các điểm ảnh thô là dữ liệu đầu vào. Trong năm 2015, họ đã chứng minh hệ thống AlphaGo đã đạt được một trong những \"thách thức lớn\" trong thời gian dài của AI bằng cách học trò chơi Cờ vây đủ tốt để đánh bại một người chơi Cờ vây chuyên nghiệp. Baidu đã thuê Andrew Ng để lãnh đạo phòng thí nghiệm nghiên cứu của mình đặt trụ sở tại thung lũng Silicon mới tập trung vào học sâu."
  },
  {
    "url": "https://zh-yue.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92",
    "title": "深度學習 - 維基百科，自由嘅百科全書",
    "content": "深度學習（粵拼：sam1 dou6 hok6 zaap6；英文：deep learning），又有叫深度結構化學習（deep structured learning），係一系列涉及「深入學習」嘅多層人工神經網絡技術，前饋同遞迴神經網絡都用得[1][2]。 深度學習建基於人工神經網絡（artificial neural network）：一個多層嘅前饋神經網絡會有若干層隱藏細胞，每層細胞嘅啟動程度都受打前嗰層嘅細胞影響，所以當輸入層（input layer）細胞收到訊號（\n\n\n\ni\n\n\n{\\displaystyle i}\n\n）啟動，就會引致後排嘅細胞啟動數值跟住改變，然後輸出層（output layer）細胞會有特定嘅輸出值（\n\n\n\no\n\n\n{\\displaystyle o}\n\n）－\n\n\n\no\n=\nf\n(\ni\n)\n\n\n{\\displaystyle o=f(i)}\n\n，當中 \n\n\n\nf\n\n\n{\\displaystyle f}\n\n 係代表個網絡嘅函數[1][3]；一般機械學習演算法做嘅係要調整 \n\n\n\nf\n\n\n{\\displaystyle f}\n\n 入面嗰啲參數（即係權重值等），令到做完學習之後，個網絡變得能夠準確噉由 \n\n\n\ni\n\n\n{\\displaystyle i}\n\n 計出 \n\n\n\no\n\n\n{\\displaystyle o}\n\n；深度學習做嘅就係進一步，想確保每兩浸細胞層之間都有準確嘅輸入輸出關係－如果由一個訓練咗嘅深度網絡裏面是但攞兩浸相鄰嘅細胞層出嚟，呢兩浸細胞層會能夠成為一個準確做預測嘅獨立網絡[4][5]。 舉個例說明，想像有個用嚟處理動物圖像嘅前饋網絡；如果佢係一個非深度網絡，設計者會淨係想確保輸入層同輸出層之間有正確嘅關係－即係想個網絡能夠準確噉由圖像判斷「圖入面嗰隻係乜動物」－就算數。但如果佢係一個深度網絡，做法就會比較似以下嘅噉[6][7]： 訓練好之後，如果由呢個網絡當中是但抽兩層相鄰嘅細胞出嚟，嗰兩層能夠成一個獨立做到有用運算嘅神經網絡[註 1]。噉亦都表示，深度學習能夠令人工神經網絡好似人類噉學識做有層次嘅知識表示。因為噉，深度學習技術喺人工智能（AI）領域上相當受重視[8][9]。 深度學習本質上係一種用人工神經網絡嚟有層次噉表示知識嘅做法[1]:p. 199： 深度學習建基於人工神經網絡（artificial neural network）：一個人工神經網絡由大量嘅人工神經細胞（artificial neuron）組成；喺用電腦程式整神經網絡嗰陣，研究者可以每粒人工神經細胞都同佢設返個變數代表佢嘅啟動程度（activation level）[11]，而每粒神經細胞嘅啟動程度嘅數值都有條式計，呢條式包括咗喺佢之前嗰啲神經細胞嘅啟動程度。啲函數當中嘅參數可以變化，而如果個神經網絡嘅程式有演算法教佢靠經驗調整呢啲參數嘅話，嗰個神經網絡就會具有學習嘅能力[12][13]。即係例如個程式會 foreach 細胞有一條類似噉樣嘅算式： 當中 \n\n\n\nt\n\n\n{\\displaystyle t}\n\n 代表粒神經細胞嘅啟動程度，\n\n\n\n\nA\n\nn\n\n\n\n\n{\\displaystyle A_{n}}\n\n 代表前嗰排嘅神經細胞當中第 \n\n\n\nn\n\n\n{\\displaystyle n}\n\n 粒嘅啟動程度，而 \n\n\n\n\nW\n\nn\n\n\n\n\n{\\displaystyle W_{n}}\n\n 就係前嗰排嘅神經細胞當中第 \n\n\n\nn\n\n\n{\\displaystyle n}\n\n 粒嘅權重（指嗰粒神經細胞有幾影響到 \n\n\n\nt\n\n\n{\\displaystyle t}\n\n）。所以當一粒人工神經細胞啟動嗰陣，會帶起佢後面啲細胞跟住佢啟動－似十足生物神經網絡入面嗰啲神經細胞噉。假如個神經網絡嘅程式令佢能夠自行按照經驗改變 \n\n\n\n\nW\n\nn\n\n\n\n\n{\\displaystyle W_{n}}\n\n 嘅數值嘅話，佢就會曉學習[註 2][12][14]。 人工神經網絡可以用嚟做知識表示（knowledge representation）：知識表示係指一個心靈（mind）－無論係人類嘅定係人工智能－內部點樣表示有關「周圍世界點運作」嘅資訊，以及點樣運用呢啲資訊對感知到嘅事物作出判斷同預測[15][16][17]。 舉個簡單例子說明，想像家陣有一個噉嘅人工神經網絡：個網絡有兩層，每層有若干粒神經細胞，第一層代表「見到嗰樣物件嘅特徵」，而第二層代表「將件物件分做乜嘢類」，第二層每粒神經細胞會同第一層嘅某啲細胞有連繫，喺某啲特定嘅第一層細胞啟動嗰時會跟住啟動，即係話，攞第 \n\n\n\ni\n\n\n{\\displaystyle i}\n\n 粒第二層細胞： 假想第一層細胞每粒代表咗一個特徵，而第二層細胞每粒代表咗一個類別。當個網絡望到外界有一件具有「有毛皮」、「有鬚」、同「四隻腳」等特徵嘅物件嗰時，呢啲特徵相應嘅第一層細胞就會啟動，而第二層細胞當中同呢柞細胞有連繫嗰粒－例如代表「貓」嗰粒第二層細胞－就會跟住啟動。噉即係話，呢個網絡曉一接收到某啲特徵，就將件物件歸類做某個相應嘅類別－「貓有毛皮、有鬚、四隻腳」係一個知識，而呢個知識就係用一個神經網絡嘅方式「表示」咗出嚟[18][19]。 廿世紀嘅機械學習技術多數都係「淺」嘅：人工神經網絡做嘅嘢可以想像成特徵提取（feature extraction）－「特徵提取」指由數據數值嗰度計一啲新數值出嚟，而新數值可以內含有用嘅資訊，例如一個神經網絡由輸入層嗰 12 粒細胞嘅啟動程度值（數據數值）計跟住嗰層隱藏層嗰 8 粒細胞嘅啟動程度值（新數值），就係一個特徵提取過程；廿世紀嘅人工神經網絡通常頂櫳得嗰一至兩層非線性嘅特徵提取；實驗表明咗，呢類做法能夠有效噉解決好多相對簡單嘅問題，但理論上，呢啲咁簡單嘅網絡表示知識（知識係事物之間嘅關係）嘅能力有限，而事實亦都證實咗，呢啲簡單嘅網絡難以應付語言以及影像等複雜嘅問題[1]:Ch. 2。 同時，認知科學（cognitive science）上嘅發展亦都啟發咗機械學習領域：對認知嘅研究表明咗，人腦喺好多情況下都會以分層式（hierarchical）嘅方法嚟表示知識；例如有關視覺嘅研究就發現，人腦視覺系統（visual system）會有一柞特定嘅神經細胞負責處理最基本嘅資訊（例如「視野入面每一點係乜嘢色水」），而下一層嘅細胞會處理由呢啲資訊提取出嚟嘅資訊（例如「視野入面有邊啲線條」）... 如此類推[20][21]，喺分層方式上好似一個多層嘅前饋神經網絡[註 3][22]；呢種每層都做特徵提取，而且每層都表示緊某啲特定有用知識嘅編碼方法就係所謂嘅分層式知識表示－深度學習就係受呢種人腦知識表示法所啟發而有嘅諗頭[1]:Ch. 2。 深度學習當中嘅「深度」大致上表示有幾多層嘅人工神經細胞做咗特徵提取：人工神經網絡嘅深度可以用歸功分配路線（credit assignment path，CAP）嘅概念嚟想像，一條 CAP 係指由輸入去輸出嗰一連串嘅變化，而 CAP 嘅長度反映學習嘅「深度」；例如一個有 4 浸隱藏層嘅前饋網絡，由輸入層細胞去到最後輸出當中，啲數據頂櫳會經歷 4 + 1 = 5 次嘅轉化，而喺一個遞迴網絡裏面，CAP 嘅長度理論上可以係無限大－如果嗰個遞迴網絡設計成會儲起時間點 \n\n\n\nt\n\n\n{\\displaystyle t}\n\n 嘅輸入（\n\n\n\n\ni\n\nt\n\n\n\n\n{\\displaystyle i_{t}}\n\n），並且俾呢份儲起咗嘅資訊影響時間點 \n\n\n\nt\n+\n1\n\n\n{\\displaystyle t+1}\n\n、\n\n\n\nt\n+\n2\n\n\n{\\displaystyle t+2}\n\n、\n\n\n\nt\n+\n3...\n\n\n{\\displaystyle t+3...}\n\n 嘅隱藏層狀態，噉如果個網絡一路唔終止係噉行，\n\n\n\n\ni\n\nt\n\n\n\n\n{\\displaystyle i_{t}}\n\n 理論上有可能會能夠影響無限咁多層之後嘅細胞嘅狀態。深度學習講緊嘅就係 CAP 嘅長度－一般嚟講，CAP 嘅數值超過 2 就可以算係「深度」嘅學習，而一個網絡嘅 CAP 長度會點影響佢嘅學習能力（能夠學習幾複雜嘅法則）喺人工神經網絡研究上係一個相當大嘅課題[23]。 受限玻茲曼機（restricted Boltzmann machine，RBM）係一種簡單神經網絡，而深度神經網絡往往係由多部受限玻茲曼機砌埋一齊形成嘅：一部受限玻茲曼機分隱藏細胞（hidden units）同可見細胞（visible units），隱藏細胞彼此之間唔可以有連繫，而可見細胞之間亦都唔可以有連繫，每一條連繫都係連接住一粒隱藏細胞同埋一粒可見細胞嘅；每粒可見細胞都同所有隱藏細胞有連繫。例如下圖就係一個有四粒可見細胞（柞 \n\n\n\n\nv\n\ni\n\n\n\n\n{\\displaystyle v_{i}}\n\n）同三粒隱藏細胞（柞 \n\n\n\n\nh\n\ni\n\n\n\n\n{\\displaystyle h_{i}}\n\n）嘅受限玻茲曼機[24][25]： 想像而家一個研究者噉做：佢俾一柞輸入落去柞可見細胞嗰度，可見細胞狀態成 \n\n\n\n\n\nv\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {v} _{0}}\n\n，再等柞隱藏細胞按 \n\n\n\n\n\nv\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {v} _{0}}\n\n 同權重啟動成狀態 \n\n\n\n\n\nh\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {h} _{0}}\n\n（向前傳遞；forward pass）；然後第二步係做重構（reconstruction）－將隱藏細胞嘅狀態 \n\n\n\n\n\nh\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {h} _{0}}\n\n 做輸入，等可見細胞按呢柞輸入同權重變成狀態 \n\n\n\n\n\nv\n\n\n1\n\n\n\n\n{\\displaystyle \\mathbf {v} _{1}}\n\n；因為啲權重一般會喺初始化嗰陣設做隨機數值，所以 \n\n\n\n\n\nv\n\n\n1\n\n\n\n\n{\\displaystyle \\mathbf {v} _{1}}\n\n 同 \n\n\n\n\n\nv\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {v} _{0}}\n\n 之間嘅差異（重構誤差）會相當大－「重構原初輸入」嘅工作失敗；喺呢個過程當中，部受限玻茲曼機會俾出兩樣資訊[24][26]： 想像而家行咗學習演算法，部受限玻茲曼機能夠可靠噉重構原初輸入，每次都係 \n\n\n\n\n\nv\n\n\n1\n\n\n\n\n{\\displaystyle \\mathbf {v} _{1}}\n\n ≈ \n\n\n\n\n\nv\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbf {v} _{0}}\n\n（可以睇埋自編碼器）；呢部受限玻茲曼機嘅輸入係一幅有若干像素嘅圖像（\n\n\n\n\nv\n\n\n\n{\\displaystyle \\mathbf {v} }\n\n），而隱藏層嘅細胞（\n\n\n\n\nh\n\n\n\n{\\displaystyle \\mathbf {h} }\n\n）表示嘅係「幅圖入面有啲乜嘢身體部位」；下一步，研究者再用隱藏層 \n\n\n\n\nh\n\n\n\n{\\displaystyle \\mathbf {h} }\n\n 做輸入，砌多個隱藏層（\n\n\n\n\nnew h\n\n\n\n{\\displaystyle \\mathbf {\\text{new h}} }\n\n）上去，\n\n\n\n\nnew h\n\n\n\n{\\displaystyle \\mathbf {\\text{new h}} }\n\n 表示嘅係「幅圖係乜嘢動物」（輸入輸出疊放），形成一個深度信念網絡。而最後得出呢個網絡能夠做到以下嘅嘢： －呢一個網絡成功做到分層知識表示嘅效果[26][27]。 深度學習可以用監督式學習（supervised learning）嚟做：要製作一個深度嘅神經網絡，最直接嘅做法係輸入輸出疊放（input-output stacking），即係首先用監督式學習訓練一部多層感知機（MLP），等呢個網絡訓練好，能夠俾出正確嘅輸出，就用呢個網絡嘅輸出做下一部多層感知機嘅輸入，要嗰個新網絡做監督式學習，學到能夠準確噉由先前嗰個網絡嘅輸出嗰度計出下一層嘅輸出，如此類推，直至到製作出一個每層都表示到有用知識嘅深度疊放網絡（deep stacking network）[28][29]。 一個深度網絡可以諗成一個上圖噉嘅神經抽象化金字塔（neural abstraction pyramid）：最低層嘅子網絡負責由最基本嘅數據（例：一幅圖每點係乜嘢色水）計出一啲抽象化少少嘅數據（例：一條一條線），如是者每層都將啲數據抽象化。 深度神經網絡可以用非監督式學習（unsupervised learning）嘅方法做。深度自編碼器（deep autoencoder）就係一種用非監督式學習嚟訓練嘅深度神經網絡－自編碼器（autoencoder）係一種神經網絡，訓練嚟想佢由輸入俾出同輸入一樣嘅輸出，可以用嚟做降維等嘅工作。一個最簡單嘅收縮自編碼器得一浸隱藏層，隱藏層細胞數量少過輸入層嘅，輸出層細胞數量同輸入層一樣，所以喺行嗰陣，浸隱藏層會由輸入層做特徵提取，而且因為隱藏層細胞數量少過輸入層，所以（假如個自編碼器經已訓練好）浸隱藏層會做到「用數量少嘅特徵表示輸入」嘅效果－呢樣工作就係所謂嘅降維[28][30]。 一個深度自編碼器包含兩個互相對稱嘅深度信念網絡：一個深度自編碼器分入碼面（encoder）同解碼面（decoder），入碼面係一個由 4 至 5 部受限玻茲曼機一層層砌埋一齊而成嘅深度信念網絡；解碼面同入碼面對稱－即係話入碼器會一層層噉做特徵提取，最後成一個壓縮特徵向量（compressed feature vector），然後（假設個深度自編碼器經已訓練好）解碼面會一層層噉做解碼，最後解碼出嘅輸出會同輸入一樣。喺實際應用上，深度自編碼器可以攞嚟做數據壓縮嘅工作[31][32]。 上圖係一個簡單（總共得 5 層）深度自編碼器嘅結構圖解；\n\n\n\nX\n\n\n{\\displaystyle X}\n\n 係輸入，\n\n\n\n\nX\n′\n\n\n\n{\\displaystyle X'}\n\n 係輸出層，中間嘅係隱藏層，而 \n\n\n\nz\n\n\n{\\displaystyle z}\n\n 係壓縮特徵向量；左面嘅入碼面會由 \n\n\n\nX\n\n\n{\\displaystyle X}\n\n 做特徵提取，然後（只要個網絡訓練好）解碼面會將入咗碼嘅特徵數值變返做原本輸出，\n\n\n\n\nX\n′\n\n\n\n{\\displaystyle X'}\n\n ≈ \n\n\n\nX\n\n\n{\\displaystyle X}\n\n。 深度學習呢個諗頭同認知科學（cognitive science）息息相關： 純認知科學有興趣研究深度學習，但呢啲研究同應用人工智能上嘅比起嚟，更加重視個深度學習模型嘅真實性。例如認知科學上嘅研究就有試過噉做：研究者指出，深度神經網絡喺「多層」呢一點上的確似生物神經網絡，但廿世紀尾嘅深度網絡往往係靠反向傳播算法（back propagation）嚟更新個網絡啲權重嘅，而呢點同生物神經網絡並唔似－神經科學研究指，如果生物神經細胞真係靠反向傳播算法噉嘅做法嚟學習嘅話，生物神經細胞傳咗個訊號之後，理應會收到反向傳返嚟嘅訊號話俾佢哋知要點改變啲突觸（synapse），但呢點唔符合現實[33]。因為噉，有認知科學家就研究諗可唔可以用比較合符神經科學發現嘅演算法達到教深度網絡學習嘅效果。相比之下，應用人工智能嘅研究係工程學，會比較在乎個模型係咪能夠有效噉解難－就算個模型唔能夠準確噉描述現實世界嘅認知系統都好[34][35]。 喺理論上，分做多層嘅特徵提取做法（無論係喺人腦當中定係喺人工智能當中都好）令到一個認知系統能夠將新知識建基於現有知識之上－如果一個認知系統係有能力用分層式嘅方法做知識表示嘅，佢就可以喺得到新知識嗰陣，將知識表示為由先前嘅某啲知識做特徵提取出嘅嘢，例：個認知系統之前學識咗由見到嘅圖像嗰度認定一隻動物係貓定老虎，如果佢能夠做分層嘅知識表示，佢可以再砌咗層知識上去，將「老虎」同「危險」聯想埋一齊，但唔將「貓」同危險聯想埋一齊－變相可以慳認知資源[36][37]。 實際嘅神經科學研究表明咗，人腦嘅學習方式喺「分層」呢一點上似深度學習模型－大腦皮層（cerebral cortex；人腦最外面嗰浸）有一啲分層嘅結構，每層嘅神經細胞都會由打前嘅層嘅細胞嗰度攞訊號，然後將訊號射去下一層嘅細胞嗰度，而喺腦發育嘅過程當中，呢啲細胞層仲會逐層逐層噉成熟（低層嘅細胞發育完成咗先到下一層）[38][39]。 喺廿一世紀，深度學習有以下嘅應用，而且喺某啲情況當中，深度學習嘅表現仲好過人類： ... 等等。 機械感知（電腦視覺 · 邊緣檢測 · 手寫辨識 · 物體檢測） · 機械學習（知識表示 · 深度學習）"
  },
  {
    "url": "https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0",
    "title": "深度学习 - 维基百科，自由的百科全书",
    "content": "深度学习（英語：deep learning）是机器学习的分支，是一種以人工神經網路為架構，對資料進行表徵學習的算法。[1][2][3][4][5]深度学习中的形容词“深度”是指在网络中使用多层。 早期的工作表明，线性感知器不能成为通用分类器，但具有非多项式激活函数和一个无限宽度隐藏层的网络可以成为通用分类器。 深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别[6]）。深度学习的好处是用非监督式或半监督式（英语：Semi-supervised learning）的特征学习和分层特征提取高效算法来替代手工获取特征。[7] 表征学习的目标是寻求更好的表示方法并建立更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地建立在類似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉動神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。[8] 至今已有數种深度学习框架，如深度神经网络、卷积神经网络和深度置信网络（英语：Deep belief network）和循环神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并取得了极好的效果。 另外，「深度学习」已成為時髦術語，或者说是人工神经网络的品牌重塑。[9][10] 深度学习框架，尤其是基于人工神经网络的框架可以追溯到1980年福岛邦彦提出的新认知机[11]，而人工神经网络的历史更为久远。1989年，扬·勒丘恩（Yann LeCun）等人开始将1974年提出的标准反向传播算法[12]应用于深度神经网络，这一网络被用于手写邮政编码识别。尽管算法可以成功执行，但计算代价非常巨大，神经网络的训练时间达到了3天，因而无法投入实际使用[13]。许多因素导致了这一缓慢的训练过程，其中一种是由于尔根·施密德胡伯的学生赛普·霍克赖特（英语：Sepp Hochreiter）于1991年提出的梯度消失问题[14][15]。 最早的进行一般自然杂乱图像中自然物体识别的深度学习网络是翁巨扬（Juyang Weng）等在1991和1992发表的生长网（Cresceptron）[16][17][18]。它也是第一个提出了后来很多实验广泛采用的一个方法：现在称为最大汇集（max-pooling）以用于处理大物体的变形等问题。生长网不仅直接从杂乱自然场景中学习老师指定的一般物体，还用网络反向分析的方法把图像内被识别了的物体从背景图像中分割出来。 2007年前后，杰弗里·辛顿和鲁斯兰·萨拉赫丁诺夫（Ruslan Salakhutdinov）提出了一种在前馈神经网络中进行有效训练的算法。这一算法将网络中的每一层视为无监督的受限玻尔兹曼机，再使用有监督的反向传播算法进行调优[19]。在此之前的1992年，在更为普遍的情形下，施密德胡伯也曾在循环神经网络上提出一种类似的训练方法，并在实验中证明这一训练方法能够有效提高有监督学习的执行速度。[20][21] 自深度学习出现以来，它已成为很多领域，尤其是在计算机视觉和语音识别中，成为各种领先系统的一部分。在通用的用于检验的数据集，例如语音识别中的TIMIT和图像识别中的ImageNet、CIFAR-10上的实验证明，深度学习能够提高识别的精度。与此同时，神经网络也受到了其他更加简单归类模型的挑战，支持向量机等模型在20世纪90年代到21世纪初成为过流行的机器学习算法。 硬件的进步也是深度学习重新获得关注的重要因素。高性能图形处理器的出现极大地提高了数值和矩阵运算的速度，使得机器学习算法的运行时间得到了显著的缩短[22][23]。 由于脑科学方面的大量研究已表明人脑网络不是一个级联的结构，深度学习网络在2001年后正逐渐被更有潜力的基于脑模型的网络[24][25]所替代。 深度学习的基础是机器学习中的分散表示（distributed representation）。分散表示假定观测值是由不同因子相互作用生成。在此基础上，深度学习进一步假定这一相互作用的过程可分为多个层次，代表对观测值的多层抽象。不同的层数和层的规模可用于不同程度的抽象[3]。 深度学习运用了这分层次抽象的思想，更高层次的概念从低层次的概念学习得到。这一分层结构常常使用贪婪算法逐层构建而成，并从中选取有助于机器学习的更有效的特征[3]。 不少深度学习算法都以无监督学习的形式出现，因而这些算法能被应用于其他算法无法企及的无标签数据，这一类数据比有标签数据更丰富，也更容易获得。这一点也为深度学习赢得了重要的优势[3]。 一部分最成功的深度学习方法涉及到对人工神经网络的运用。人工神经网络受到了1959年由诺贝尔奖得主大卫·休伯尔（David H. Hubel）和托斯坦·威泽尔（Torsten Wiesel）提出的理论启发。休伯尔和威泽尔发现，在大脑的初级视觉皮层中存在两种细胞：简单细胞和复杂细胞，这两种细胞承担不同层次的视觉感知功能。受此启发，许多神经网络模型也被设计为不同节点之间的分层模型[26]。 福岛邦彦提出的新认知机引入了使用无监督学习训练的卷积神经网络。扬·勒丘恩将有监督的反向传播算法应用于这一架构[27]。事实上，从反向传播算法自20世纪70年代提出以来，不少研究者都曾试图将其应用于训练有监督的深度神经网络，但最初的尝试大都失败。赛普·霍克赖特（英语：Sepp Hochreiter）在其博士论文中将失败的原因归结为梯度消失，这一现象同时在深度前馈神经网络和循环神经网络中出现，后者的训练过程类似深度网络。在分层训练的过程中，本应用于修正模型参数的误差随着层数的增加指数递减，这导致了模型训练的效率低下[28][29]。 为了解决这一问题，研究者们提出了一些不同的方法。于尔根·施密德胡伯于1992年提出多层级网络，利用无监督学习训练深度神经网络的每一层，再使用反向传播算法进行调优。在这一模型中，神经网络中的每一层都代表观测变量的一种压缩表示，这一表示也被传递到下一层网络[20]。 另一种方法是赛普·霍克赖特和于尔根·施密德胡伯提出的長短期記憶神經網路（LSTM）[30]。2009年，在ICDAR 2009举办的连笔手写识别竞赛中，在没有任何先验知识的情况下，深度多维长短期记忆神经网络取得了其中三场比赛的胜利[31][32]。 斯文·贝克提出了在训练时只依赖梯度符号的神经抽象金字塔模型，用以解决图像重建和人脸定位的问题[33]。 其他方法同样采用了无监督预训练来构建神经网络，用以发现有效的特征，此后再采用有监督的反向传播以区分有标签数据。杰弗里·辛顿等人于2006年提出的深度模型提出了使用多层隐变量学习高层表示的方法。这一方法使用斯摩棱斯基于1986年提出的受限玻尔兹曼机[34]对每一个包含高层特征的层进行建模。模型保证了数据的对数似然下界随着层数的提升而递增。当足够多的层数被学习完毕，这一深层结构成为一个生成模型，可以通过自上而下的采样重构整个数据集[35]。辛顿声称这一模型在高维结构化数据上能够有效地提取特征[36]。 吴恩达和杰夫·迪恩领导的谷歌大脑团队创建了一个仅通过YouTube视频学习高层概念（例如猫）的神经网络[37]\n[38]。 其他方法依赖了现代电子计算机的强大计算能力，尤其是GPU。2010年，在于尔根·施密德胡伯位于瑞士人工智能实验室IDSIA的研究组中，丹·奇雷尚（Dan Ciresan）和他的同事展示了利用GPU直接执行反向传播算法而忽视梯度消失问题的存在。这一方法在扬·勒丘恩等人给出的手写识别MNIST数据集上战胜了已有的其他方法[22]。 截止2011年，前馈神经网络深度学习中最新的方法是交替使用卷积层（convolutional layers）和最大值池化层（max-pooling layers）并加入单纯的分类层作为顶端。训练过程也无需引入无监督的预训练[39][40]。从2011年起，这一方法的GPU实现[39]多次赢得了各类模式识别竞赛的胜利，包括IJCNN 2011交通标志识别竞赛[41]和其他比赛。 这些深度学习算法也是最先在某些识别任务上达到和人类表现具备同等竞争力的算法[42]。 通常将具有两层或两层以上隐藏层的神经网络叫做深度神经网络。与浅层神经网络类似，深度神经网络也能够为复杂非线性系统提供建模，但多出的层次为模型提供了更高的抽象层次，因而提高了模型的能力。深度神经网络通常都是前馈神经网络，但也有语言建模等方面的研究将其拓展到循环神经网络[43]。卷积深度神经网络（Convolutional Neural Networks, CNN）在计算机视觉领域得到了成功的应用[44]。此后，卷积神经网络也作为听觉模型被使用在自动语音识别领域，较以往的方法获得了更优的结果[45]。 深度神经网络（Deep Neural Networks, DNN）是一种判别模型，可以使用反向传播算法进行训练。权重更新可以使用下式进行随机梯度下降法（英语：Stochastic gradient descent）求解： 其中，\n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n为学习率，\n\n\n\nC\n\n\n{\\displaystyle C}\n\n为代价函数。这一函数的选择与学习的类型（例如监督学习、无监督学习、增强学习）以及激活函数相关。例如，为了在一个多分类问题上进行监督学习，通常的选择是使用ReLU作为激活函数，而使用交叉熵作为代价函数。Softmax函数定义为\n\n\n\n\np\n\nj\n\n\n=\n\n\n\nexp\n⁡\n(\n\nx\n\nj\n\n\n)\n\n\n\n∑\n\nk\n\n\nexp\n⁡\n(\n\nx\n\nk\n\n\n)\n\n\n\n\n\n{\\displaystyle p_{j}={\\frac {\\exp(x_{j})}{\\sum _{k}\\exp(x_{k})}}}\n\n，其中\n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n代表类别\n\n\n\nj\n\n\n{\\displaystyle j}\n\n的概率，而\n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n和\n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n分别代表对单元\n\n\n\nj\n\n\n{\\displaystyle j}\n\n和\n\n\n\nk\n\n\n{\\displaystyle k}\n\n的输入。交叉熵定义为\n\n\n\nC\n=\n−\n\n∑\n\nj\n\n\n\nd\n\nj\n\n\nlog\n⁡\n(\n\np\n\nj\n\n\n)\n\n\n{\\displaystyle C=-\\sum _{j}d_{j}\\log(p_{j})}\n\n，其中\n\n\n\n\nd\n\nj\n\n\n\n\n{\\displaystyle d_{j}}\n\n代表输出单元\n\n\n\nj\n\n\n{\\displaystyle j}\n\n的目标概率，\n\n\n\n\np\n\nj\n\n\n\n\n{\\displaystyle p_{j}}\n\n代表应用了激活函数后对单元\n\n\n\nj\n\n\n{\\displaystyle j}\n\n的概率输出[46]。 与其他神经网络模型类似，如果仅仅是简单地训练，深度神经网络可能会存在很多问题。常见的两类问题是过拟合和过长的运算时间。 深度神经网络很容易产生过拟合现象，因为增加的抽象层使得模型能够对训练数据中较为罕见的依赖关系进行建模。对此，权重递减（\n\n\n\n\nℓ\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n正规化）或者稀疏（\n\n\n\n\nℓ\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-正规化）等方法可以利用在训练过程中以减小过拟合现象[47]。另一种较晚用于深度神经网络训练的正规化方法是丢弃法（\"dropout\" regularization），即在训练中随机丢弃一部分隐层单元来避免对较为罕见的依赖进行建模[48]。目前比較廣泛使用的是批归一化(Batch Normalization,BN) , 其本質上是在訓練過程加入噪音 , 從而讓模型得到更好的魯棒性 , 其特性令超深神經網路可以更好的訓練。 反向传播算法和梯度下降法由于其实现简单，与其他方法相比能够收敛到更好的局部最优值而成为神经网络训练的通行方法。但是，这些方法的计算代价很高，尤其是在训练深度神经网络时，因为深度神经网络的规模（即层数和每层的节点数）、学习率、初始权重等众多参数都需要考虑。扫描所有参数由于时间代价的原因并不可行，因而小批量训练（mini-batching），即将多个训练样本组合进行训练而不是每次只使用一个样本进行训练，被用于加速模型训练[49]。而最显著地速度提升来自GPU，因为矩阵和向量计算非常适合使用GPU实现。但使用大规模集群进行深度神经网络训练仍然存在困难，因而深度神经网络在训练并行化方面仍有提升的空间。 深度置信网络（deep belief networks，DBN）是一种包含多层隐单元的概率生成模型，可被视为多层简单学习模型组合而成的複合模型[50]。 深度置信网络可以作为深度神经网络的预训练部分，并为网络提供初始权重，再使用反向传播或者其它判定算法作为调优的手段。这在训练数据较为缺乏时很有价值，因为不恰当的初始化权重会显著影响最终模型的性能，而预训练获得的权重在权值空间中比随机权重更接近最优的权重。这不仅提升了模型的性能，也加快了调优阶段的收敛速度[51]。 深度置信网络中的每一层都是典型的受限玻尔兹曼机（restricted Boltzmann machine，RBM），可以使用高效的无监督逐层训练方法进行训练。受限玻尔兹曼机是一种无向的基于能量的生成模型，包含一个输入层和一个隐层。图中对的边仅在输入层和隐层之间存在，而输入层节点内部和隐层节点内部则不存在边。单层RBM的训练方法最初由杰弗里·辛顿在训练“专家乘积”中提出，被称为对比分歧（contrast divergence, CD）。对比分歧提供了一种对最大似然的近似，被理想地用于学习受限玻尔兹曼机的权重[49]。当单层RBM被训练完毕后，另一层RBM可被堆叠在已经训练完成的RBM上，形成一个多层模型。每次堆叠时，原有的多层网络输入层被初始化为训练样本，权重为先前训练得到的权重，该网络的输出作为新增RBM的输入，新的RBM重复先前的单层训练过程，整个过程可以持续进行，直到达到某个期望中的终止条件[2]。 尽管对比分歧对最大似然的近似十分粗略（对比分歧并不在任何函数的梯度方向上），但经验结果证实该方法是训练深度结构的一种有效的方法[49]。 卷积神经网络（convolutional neural networks，CNN）由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，使之成为一种颇具吸引力的深度学习结构[52]。 卷积深度置信网络（convolutional deep belief networks，CDBN）是深度学习领域较新的分支。在结构上，卷积深度置信网络与卷积神经网络在结构上相似。因此，与卷积神经网络类似，卷积深度置信网络也具备利用图像二维结构的能力，与此同时，卷积深度信念网络也拥有深度置信网络的预训练优势。卷积深度置信网络提供了一种能被用于信号和图像处理任务的通用结构，也能够使用类似深度置信网络的训练方法进行训练[53]。 下表中的结果展示了深度学习在通行的TIMIT数据集上的结果。TIMIT包含630人的语音数据，这些人持八种常见的美式英语口音，每人阅读10句话。这一数据在深度学习发展之初常被用于验证深度学习结构[54]。TIMIT数据集较小，使得研究者可以在其上实验不同的模型配置。 图像分类领域中一个公认的评判数据集是MNIST数据集。MNIST由手写阿拉伯数字组成，包含60,000个训练样本和10,000个测试样本。与TIMIT类似，它的数据规模较小，因而能够很容易地在不同的模型配置下测试。Yann LeCun的网站给出了多种方法得到的实验结果[55]。截至2012年，最好的判别结果由Ciresan等人在当年给出，这一结果的错误率达到了0.23%[56]。 计算机领域中的深度学习与20世纪90年代由认知神经科学研究者提出的大脑发育理论（尤其是皮层发育理论）密切相关[57]。对这一理论最容易理解的是杰弗里·艾尔曼（英语：Jeffrey Elman）于1996年出版的专著《对天赋的再思考》（Rethinking Innateness（英语：Rethinking Innateness））[58]（参见斯拉格和约翰逊[59]以及奎兹和赛杰诺维斯基[60]的表述）。由于这些理论给出了实际的神经计算模型，因而它们是纯计算驱动的深度学习模型的技术先驱。这些理论指出，大脑中的神经元组成了不同的层次，这些层次相互连接，形成一个过滤体系。在这些层次中，每层神经元在其所处的环境中获取一部分信息，经过处理后向更深的层级传递。这与后来的单纯与计算相关的深度神经网络模型相似。这一过程的结果是一个与环境相协调的自组织的堆栈式的转换器。正如1995年在《纽约时报》上刊登的那样，“……婴儿的大脑似乎受到所谓‘营养因素’的影响而进行着自我组织……大脑的不同区域依次相连，不同层次的脑组织依照一定的先后顺序发育成熟，直至整个大脑发育成熟。”[61] 深度结构在人类认知演化和发展中的重要性也在认知神经学家的关注之中。发育时间的改变被认为是人类和其他灵长类动物之间智力发展差异的一个方面[62]。在灵长类中，人类的大脑在出生后的很长时间都具备可塑性，但其他灵长类动物的大脑则在出生时就几乎完全定型。因而，人类在大脑发育最具可塑性的阶段能够接触到更加复杂的外部场景，这可能帮助人类的大脑进行调节以适应快速变化的环境，而不是像其他动物的大脑那样更多地受到遗传结构的限制。这样的发育时间差异也在大脑皮层的发育时间和大脑早期自组织中从刺激环境中获取信息的改变得到体现。当然，伴随着这一可塑性的是更长的儿童期，在此期间人需要依靠抚养者和社会群体的支持和训练。因而这一理论也揭示了人类演化中文化和意识共同进化的现象[63]。 深度学习常常被看作是通向真正人工智能的重要一步[64]，因而许多机构对深度学习的实际应用抱有浓厚的兴趣。2013年12月，Facebook宣布雇用楊立昆为其新建的人工智能实验室的主管，这一实验室将在加州、伦敦和纽约设立分支机构，帮助Facebook研究利用深度学习算法进行类似自动标记照片中用户姓名这样的任务[65]。 2013年3月，杰弗里·辛顿和他的两位研究生亚历克斯·克里泽夫斯基和伊尔亚·苏茨克维谷歌公司雇用，以提升现有的机器学习产品并协助处理谷歌日益增长的数据。谷歌同时并购了辛顿创办的公司DNNresearch[66]。 2016年3月，以深度學習開發的圍棋程式AlphaGo首度在比賽中擊敗人類頂尖选手，形成廣泛的討論。 对深度学习的主要批评是许多方法缺乏理论支撑。大多数深度结构仅仅是梯度下降的某些变式。尽管梯度下降法已经被充分地研究，但理论涉及的其他算法，例如对比分歧算法，并没有获得充分的研究，其收敛性等问题仍不明确。深度学习方法常常被视为黑盒，大多数的结论确认都由经验而非理论来确定。 也有学者认为，深度学习应当被视为通向真正人工智能的一条途径，而不是一种包罗万象的解决方案。尽管深度学习的能力很强，但和真正的人工智能相比，仍然缺乏诸多重要的能力。理论心理学家加里·马库斯（英语：Gary Marcus）指出： 就现实而言，深度学习只是建造智能机器这一更大挑战中的一部分。这些技术缺乏表达因果关系的手段……缺乏进行逻辑推理的方法，而且远没有具备集成抽象知识，例如物品属性、代表和典型用途的信息。最为强大的人工智能系统，例如IBM的人工智能系统沃森，仅仅把深度学习作为一个包含从贝叶斯推理和演绎推理等技术的复杂技术集合中的组成部分[67]。"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Talk:Deep_learning",
    "title": "Talk:Deep learning - Wikipedia",
    "content": "> There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). MLPs are a type of FFN, so this is misleading at best. And also, \"Two types: <lists three things>\". I am considering adding information regarding power consumption. Currently I see two directions: low-power inference for embedded devices[1] and low-power training for huge networks[2]. Please let me know if you know of more credible and/or overview sources discussing the concerns. p.s. This may be a discussion for another page, e.g. ML or ANNs. Cheater no1 (talk) 13:16, 13 November 2020 (UTC)[reply] I plan to attempt a slow motion cleanup of reference spamming.  The typical case is an obscure non-notable paper where it does a poor job of supporting the items which cited it, particularly where the material is still cited without it. When all of the references looked equally good/applicable on overcited items, I left them all in.  Sincerely, North8000 (talk) 16:29, 7 March 2022 (UTC)[reply] References This article was the subject of a Wiki Education Foundation-supported course assignment, between 6 September 2023 and 14 December 2023. Further details are available on the course page. Student editor(s): HELLOEXTRACREDIT (article contribs). — Assignment last updated by HELLOEXTRACREDIT (talk) 04:30, 26 November 2023 (UTC)[reply] This is a reply to the question posed by you here. I reverted the edit by the IP because the range 194.247.67.196/30 (talk · contribs · WHOIS) quite clearly inaproppriately promoted the article by Murray et. al., e.g. [1], [2]. This leads me to think it is a spam campaign, so I reverted all edits citing this article (all of them were from this range). Janhrach (talk) 12:11, 5 October 2024 (UTC)[reply] This article was the subject of a Wiki Education Foundation-supported course assignment, between 26 August 2024 and 11 December 2024. Further details are available on the course page. Student editor(s): Eiaohdsnkafpas (article contribs). — Assignment last updated by Kofiasa (talk) 22:41, 1 November 2024 (UTC)[reply]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Deep_learning",
    "title": "Related changes - Wikipedia",
    "content": "Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard",
    "title": "Wikipedia:File upload wizard - Wikipedia",
    "content": "Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding. Uploads to Wikimedia Commons Upload a non-free file Uploads locally to the English Wikipedia; must comply with the non-free content criteria You do not have JavaScript enabled Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript. You are not currently logged in. Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again. Your account has not become confirmed yet. Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it. You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there. Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed. Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for # < > [ ] | : { } /  and ~~~. Your filename has been modified to avoid these. Please check if it is okay now. The filename you chose seems to be very short, or overly generic. Please don't use: A file of this name already exists on Commons! If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used. This should not be done, except in very rare exceptional cases. Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead. A file of this name already exists. If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to: It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, Wikimedia Commons.\nFiles uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. Please consider uploading your file on Commons. However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here. Please note that by \"entirely self-made\" we really mean just that. Do not use this section for any of the following: Editors who falsely declare such items as their \"own work\" will be blocked from editing. Use this only if there is an explicit licensing statement in the source. The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this. If the source website doesn't say so explicitly, please do not upload the file. Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it. Please remember that you will need to demonstrate that: This file will be used in the following article: Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the \"http://en.wikipedia.org/...\" URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. Example – article okay. This article doesn't exist! The article Example could not be found. Please check the spelling, and make sure you enter the name of an existing article in which you will include this file. If this is an article you are only planning to write, please write it first and upload the file afterwards. This is not an actual encyclopedia article! The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc. Please upload this file only if it is going to be used in an actual article. If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that. This is a disambiguation page! The page Example is not a real article, but a disambiguation page pointing to a number of other pages. Please check and enter the exact title of the actual target article you meant. If neither of these two statements applies, then please do not upload this image. This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like. In view of this, please explain how the use of this file will be minimal. Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then: Please don't upload it. Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader. In particular, please don't upload: If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you. This is the data that will be submitted to upload: Your file is being uploaded. This might take a minute or two, depending on the size of the file and the speed of your internet connection. Once uploading is completed, you will find your new file at this link: File:Example.jpg Your file has been uploaded successfully and can now be found here: File:Example.jpg Please follow the link and check that the image description page has all the information you meant to include. If you want to change the description, just go to the image page, click the \"edit\" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version. To insert this file into an article, you may want to use code similar to the following: If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the \":\" after the initial brackets!): See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages. Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Deep_Learning_(South_Park)",
    "title": "Deep Learning (South Park) - Wikipedia",
    "content": "\"Deep Learning\" is the fourth episode of the twenty-sixth season of the American animated television series South Park, and the 323rd episode of the series overall. Written and directed by Trey Parker, it premiered on March 8, 2023.[1][2] The episode, which parodies the use of the artificial intelligence chatbot ChatGPT (which is credited as a co-writer for the episode) for text messages, centers upon fourth-grader Stan Marsh, who comes to rely on the software for writing both school essays and romantic texts to his girlfriend Wendy Testaburger, bringing him into conflict with her, his classmates, and school officials. When fourth-grader Bebe Stevens extols the romantic texts written to her by Clyde Donovan, classmate Wendy Testaburger complains to her boyfriend, Stan Marsh, that his replies to her messages consist of merely a thumbs up. Clyde tells Stan about ChatGPT, an AI-based app he uses to write the texts, but cautions Stan not to tell anyone else about it. Stan relies on the app to text Wendy while engaging in other activities. Wendy is buoyed by Stan's more heartfelt texts, though he cannot truthfully answer her questions about them. Stan and Clyde also use the app to write their school essays, as do their classmates Eric Cartman and Butters Stotch. Cartman complains that more students learning about it would cost them their advantage, and their teacher, Mr. Garrison, would learn they cheated. Meanwhile, Garrison laments to his partner, Rick, that he now has to work harder to grade and comment on his students' essays. Rick tells him about ChatGPT, but instead of realizing that his students used it for essays, Garrison realizes he can use it to grade them. He thanks Rick for his supportive replies to his texts. School counselor Mr. Mackey informs Stan's class that a student used OpenAI technology for schoolwork. A \"technician\" dressed as a falconer arrives with his falcon Shadowbane to analyze the students' work and identify the cheater. Stan and Garrison confess to each other that they used ChatGPT. They rationalize that it is merely akin to having a good writer's assistant, but when Garrison learns this can be used for texting, he is angered to realize that Rick used it to text him. The technician reveals Shadowbane detected chatbot writing in Wendy's cell phone, though she denies using the app. Worrying he cannot think of a way out of this, Stan instructs ChatGPT to write a story that is resolved when he convinces everyone that it is okay that he lied about using the app, and that tech companies who monetize OpenAI should not determine the ethical limits of AI. The remainder of the episode depicts this story and that resolution. Stan decides that sometimes a thumbs up from a human is better than machine-generated lies, but when Clyde asks Stan how he pulled this off, Stan simply explains, \"ChatGPT, dude.\" In the closing credits, the writers of the episode are credited as both Trey Parker and ChatGPT. Bubbleblabber contributor John Schwarz rated the episode a 7.5 out of 10, stating in his review, \"One day we're going to look back on this episode like we do when we think of the many chimps that we've sent to outer space when testing space flight capabilities and marvel at how far we've come in web3 show business production. Trey Parker's genius is still quite evident in this week's episode, in just a few seasons, we may not even need that.\"[3] Max Nocerino with The Future of the Force gave the episode a B+ rating, conceding that while the episode was \"brilliant\", the show was not as \"hysterically funny as it used to be\", and cited this episode as example of that trend. Nocerino stated, \"It just doesn't split my sides anymore. Perhaps like all things, nothing lasts forever. Yet, I will continue to watch and give this episode props for understanding the double-edged sword that is ChatGPT. One of South Park's strengths is that it has its finger on the pulse of current events. And knows how to rip them to shreds.\"[4] Cathal Gunning, reviewing the episode for Screen Rant, wrote that in keeping with South Park's proclivity for self-satire, the ending was \"intentionally far too tidy and the scene ended the story way too slickly\", but nonetheless effective and clever. In particular, Gunning stated, \"When Stan used ChatGPT to end the episode's story, the resulting scene sounded very familiar since the sequence that was supposedly written by AI sounded like a lazy episode of South Park. From Cartman's insults to Stan's impassioned ending speech to Mr. Mackey's long-forgotten catchphrase, the final scenes of 'Deep Learning' leaned into tropes that South Park has used ad nauseam.\"[5]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/File:Deep_Learning.jpg",
    "title": "File:Deep Learning.jpg - Wikipedia",
    "content": "feature hierarchies. Increasingly complex features are\ndetermined from input using unsupervised learning. The Click on a date/time to view the file as it appeared at that time. The following page uses this file: The following other wikis use this file: This file contains additional information, probably added from the digital camera or scanner used to create or digitize it. If the file has been modified from its original state, some details may not fully reflect the modified file."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "title": "Artificial intelligence - Wikipedia",
    "content": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3] Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a] Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14] Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem. Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22] A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c] An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34] In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36] In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39] Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40] Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e] There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45] In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48] Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49] Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51] Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57] Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58] The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63] Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67] A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68] AI research uses a wide variety of techniques to accomplish the goals above.[b] AI can solve many problems by intelligently searching through many possible solutions.[69] There are two very different kinds of search used in AI: state space search and local search. State space search searches through a tree of possible states to try to find a goal state.[70] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[71] Simple exhaustive searches[72] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[73] Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[74] Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[75] Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks,[76] through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[77] Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[78] Formal logic is used for reasoning and knowledge representation.[79]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[80] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[81] Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[82] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules. Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[83] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[84] Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[85] Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[86] Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains. Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[87] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[88] and information value theory.[89] These tools include models such as Markov decision processes,[90] dynamic decision networks,[91] game theory and mechanism design.[92] Bayesian networks[93] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][95] learning (using the expectation–maximization algorithm),[h][97] planning (using decision networks)[98] and perception (using dynamic Bayesian networks).[91] Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[91] The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[99] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45] There are many kinds of classifiers in use.[100] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[101] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[102]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[103] at Google, due in part to its scalability.[104]\nNeural networks are also used as classifiers.[105] An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[105] Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[106] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[107] In feedforward neural networks the signal passes in only one direction.[108] The term perceptron typically refers to a single-layer neural network.[109] In contrast, deep learning uses many layers.[110] Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem.[111] Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.[112] Deep learning uses several layers of neurons between the network's inputs and outputs.[110] The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[114] Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[115] and others. The reason that deep learning performs so well in so many applications is not known as of 2021.[116] The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j] Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems.[124] Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.[125][126] Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI.[127] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[128] In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[129] Specialized programming languages such as Prolog were used in early AI research,[130] but general-purpose programming languages like Python have become predominant.[131] The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster,[132] a trend sometimes called Huang's law,[133] named after Nvidia co-founder and CEO Jensen Huang. AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[134] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[135][136] For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[137] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[137][138] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[139] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[140] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[141][142] Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[143] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[144] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[145] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[146] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[147] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[148] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[149] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[150] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[151] Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning[152] or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[153] A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[154] One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result.[155] The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems.[156] In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.[157] Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve[158] all from Google DeepMind,[159] Llemma from EleutherAI[160] or Julius.[161] When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.[162] Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[163] Topological deep learning integrates various topological approaches. Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[164] According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[165] Various countries are deploying AI military applications.[166] The main applications enhance command and control, communications, sensors, integration and interoperability.[167] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[166] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.[167] AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[166][168][169][170] Generative artificial intelligence (Generative AI, GenAI,[171] or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[172][173][174] These models learn the underlying patterns and structures of their training data and use them to produce new data[175][176] based on the input, which often comes in the form of natural language prompts.[177][178] Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo, LTXV and Sora.[179][180][181][182][183] Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.[177][184][185] AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[190][191][192] Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions,[193] AI-integrated sex toys (e.g., teledildonics),[194] AI-generated sexual education content,[195] and AI agents that simulate sexual and romantic partners (e.g., Replika).[196]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[197] AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[198][199] There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[200] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.[201][202][203] In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water. Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[204] AI has potential benefits and potential risks.[207] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[208] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[209][210] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[211] Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio.[212] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[213] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[214] AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[215] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[216] Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[217][218] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[219] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[220][221] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[222] The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[223][224][225] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[226][227] In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[228] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[229] Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[230] A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[231] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[232] In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million.[233] Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.[234] In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[235] The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.[236] After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[237] Taiwan aims to phase out nuclear power by 2025.[237] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[237] Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.[238] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[238] On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[239] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[239] In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.[240] YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[241] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[242] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.[243] In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing,[244] while realistic AI-generated videos became feasible in the mid-2020s.[245][246][247] It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda;[248] one such potential malicious use is deepfakes for computational propaganda.[249] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[250] AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.[251] Machine learning applications will be biased[k] if they learn from biased data.[253] The developers may not be aware that the bias exists.[254] Bias can be introduced by the way training data is selected and by the way a model is deployed.[255][253] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[256] The field of fairness studies how to prevent harms from algorithmic biases. On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[257] a problem called \"sample size disparity\".[258] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[259] COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[260] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[262] A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[263] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[264] Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[265] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m] Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[258] There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[252] At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious – discuss][267] Many AI systems are so complex that their designers cannot explain how they reach their decisions.[268] Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[269] It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[270] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[271] People who have been harmed by an algorithm's decision have a right to an explanation.[272] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[273] DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[274] Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[275] LIME can locally approximate a model's outputs with a simpler, interpretable model.[276] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[277] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[278] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[279] Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[281] Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person.[281] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[282] By 2015, over fifty countries were reported to be researching battlefield robots.[283] AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[284] All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.[285][286] There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[287] Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[288] In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[289] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[290] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][292] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[288] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[293][294] Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[295] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[296] From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[297] It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[298] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip maximizer).[300] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[301] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[302] Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[303] The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[304] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[305] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\".[306] He notably mentioned risks of an AI takeover,[307] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[308] In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[309] Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[310] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[311][312] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\"[313] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[314] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[315] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[316] Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[317] Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[318]\nThe field of machine ethics is also called computational morality,[318]\nand was founded at an AAAI symposium in 2005.[319] Other approaches include Wendell Wallach's \"artificial moral agents\"[320] and Stuart J. Russell's three principles for developing provably beneficial machines.[321] Active organizations in the AI open-source community include Hugging Face,[322] Google,[323] EleutherAI and Meta.[324] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[325][326] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[327] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[328] Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:[329][330] Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[331] however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.[332] Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[333] The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[334] The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[335] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[336] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[337][338] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[339] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[339] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[339] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[340] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[341] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics.[342] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[343] In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[337] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[344] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[345][346] In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[347] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[348][349] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[350][351] The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[353][354] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[356] such as McCulloch and Pitts design for \"artificial neurons\" in 1943,[117] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[357][354] The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[354] Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[361] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[362] In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[363] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[365] and ongoing pressure from the U.S. Congress to fund more productive projects.[366] Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[367] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9] In the early 1980s, AI research was revived by the commercial success of expert systems,[368] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10] Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[369] and began to look into \"sub-symbolic\" approaches.[370] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[87][375] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[376] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[377] AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[378] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[379]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[68] Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[381] graphics processing units, cloud computing[382]) and access to large amounts of data[383] (including curated datasets,[382] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.[339] In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[316] In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[384] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[385] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[386] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[387] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[388] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[389] Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[390] Another major focus has been whether machines can be conscious, and the associated ethical implications.[391] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[392] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[391] Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[393] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[393] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[357] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[394] Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[396] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[397] McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[398] Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\".[399] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google,[400] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[401] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[402] There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.[403] No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI (or \"GOFAI\")[405] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[406] However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[407] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[408] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16] The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[410][411] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[412] but eventually was seen as irrelevant. Modern AI has elements of both. Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[413][414] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[415] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[416] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[417] Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[418] Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[422] It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[423] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[424][425] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[424] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[426] In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[427] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.[428][429] Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[425][424] A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[414] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[430] However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[431] Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[432] Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[433] Thought-capable artificial beings have appeared as storytelling devices since antiquity,[434] and have been a persistent theme in science fiction.[435] A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[436] Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[437] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[438] Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[439] The two most widely used textbooks in 2023 (see the Open Syllabus): The four most widely used AI textbooks in 2008: Other textbooks:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/File:Dall-e_3_(jan_%2724)_artificial_intelligence_icon.png",
    "title": "File:Dall-e 3 (jan '24) artificial intelligence icon.png - Wikipedia",
    "content": "العربية ∙ azərbaycanca ∙ Deutsch ∙ English ∙ español ∙ فارسی ∙ français ∙ galego ∙ हिन्दी ∙ Bahasa Indonesia ∙ 日本語 ∙ မြန်မာဘာသာ ∙ português do Brasil ∙ русский ∙ slovenščina ∙ Türkçe ∙ Tiếng Việt ∙ 中文 ∙ 中文（简体） ∙ 中文（繁體） ∙ +/− Click on a date/time to view the file as it appeared at that time. More than 100 pages use this file.\nThe following list shows the first 100 pages that use this file only.\nA full list is available. View more links to this file. The following other wikis use this file: View more global usage of this file. This file contains additional information, probably added from the digital camera or scanner used to create or digitize it. If the file has been modified from its original state, some details may not fully reflect the modified file."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
    "title": "Artificial general intelligence - Wikipedia",
    "content": "Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.[1][2] Some researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved.[3] AGI is conceptually distinct from artificial superintelligence (ASI), which would outperform the best human abilities across every domain by a wide margin.[4] AGI is considered one of the definitions of strong AI. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.[5] Creating AGI is a primary goal of AI research and of companies such as OpenAI,[6] Google,[7] and Meta.[8] A 2020 survey identified 72 active AGI research and development projects across 37 countries.[9] The timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all.[10][11][12] There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI.[3] AGI is a common topic in science fiction and futures studies.[13][14] Contention exists over whether AGI represents an existential risk.[15][16][17] Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority.[18][19] Others find the development of AGI to be in too remote a stage to present such a risk.[20][21] AGI is also known as strong AI,[22][23] full AI,[24] human-level AI,[25] human-level intelligent AI, or general intelligent action.[26] Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness.[a] In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.[27][23] Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a] Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[28] while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[29] A framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers.[30] They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman.[30] For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%.[30] They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans).[30] Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).[31] Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches.[b] Researchers generally hold that a system is required to do all of the following to be regarded as an AGI:[33] Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts)[34] and autonomy.[35] Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.[36] Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:[37] This includes the ability to detect and respond to hazard.[38] Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems,[37] these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".[38]  It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in  2001: A Space Odyssey was both programmed and tasked to.[39] Several tests meant to confirm human-level AGI have been considered, including:[40][41] The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.[44] A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.[57] There are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.[58] Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.[59] Modern AI research began in the mid-1950s.[60] The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.[61] AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"[62] Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant[63] on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".[64] Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".[c] In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".[68] In response to this and the success of expert systems, both industry and government pumped money into the field.[66][69] However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.[70] For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all[d] and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".[72] In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms.[73] These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018[update], development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.[74] At the turn of the century, many mainstream AI researchers[75] hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.[75] However, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).[76] The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud[77] in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\".[78] This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour,[79] was also called universal artificial intelligence.[80] The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002.[81] AGI research activity in 2006 was described by Pei Wang and Ben Goertzel[82] as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009[83] by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010[84] and 2011[85] at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. As of 2023[update], a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning,[86][3] which is the idea of allowing AI to continuously learn and innovate like humans do. As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.[87] AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".[88] Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.[89] A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?[90] Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.[91][92] John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.[93] AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead.[94][95] Further current AGI progress considerations can be found above Tests for confirming human-level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.[96] In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[97] Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.[98][99] Blaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".[100] 2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).[101] As of 2025, large language models (LLMs) have been adapted to generate both music and images. Voice‑synthesis systems built on transformer LLMs—such as Suno AI’s Bark model—can sing, and several music‑generation platforms (e.g. Suno and Udio) build their services on modified LLM backbones.[102][103] The same year, OpenAI released GPT‑4o image generation, integrating native image synthesis directly into ChatGPT rather than relying on a separate diffusion‑based art model, as with DALL-E.[104] LLM‑style foundation models are likewise being repurposed for robotics. Nvidia’s open‑source Isaac GR00T N1 and Google DeepMind’s Robotic Transformer 2 (RT‑2) are first trained with language‑model objectives and then fine‑tuned to handle vision‑language‑action control for embodied robots.[105][106][107] In 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.[108][109] An OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it's even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI—traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company's strategic intentions.[110] Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.[91] Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.[91][113][114] For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.[115] In the introduction to his 2006 book,[116] Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007[update], the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near[117] (i.e. between 2015 and 2045) was plausible.[118] Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.[119] In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).[120] AlexNet was regarded as the initial ground-breaker of the current deep learning wave.[120] In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.[121][122] In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.[123] In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.[124] In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.[125] In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.[3] In 2023, AI researcher Geoffrey Hinton stated that:[126] The idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that. He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.[127] In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years.[128] In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.[129] In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".[130] While the development of transformer models like in ChatGPT is considered the most promising path to AGI,[131][132] whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.[133] Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research[118] as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near[117] predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it. For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion).[135] An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).[136] In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps).[e] (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued. The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain.[139] In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain. The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.[140] A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.[141][142] If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel[118] proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient. In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument.[143] He proposed a distinction between two hypotheses about artificial intelligence:[f] The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.[144] In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".[117] This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.[145] Mainstream AI is most interested in how a program behaves.[146] According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\"[145] If the program can behave as if it has a mind, then there is no need to know if it actually has mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[145] Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence: These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.[151] Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.[152] Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.[153] AGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer.[154] It could take care of the elderly,[155] and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education.[155] The need to work to subsist could become obsolete if the wealth produced is properly redistributed.[155][156] This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks.[157] If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true),[158] it could take measures to drastically reduce the risks[157] while minimizing the impact of these measures on our quality of life. AGI would improve healthcare by making medical diagnostics faster, cheaper, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage.[159] This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.[160] Additionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's.[161] In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems. By evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.[162] AGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems.[163] Problems that have remained unsolved for decades may be solved with AGI. AGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing.[164] Future AGI systems could push these innovations even further. AGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.[165] In the workplace, AGI could automate repetitive tasks, freeing up workers for more creative and strategic roles.[164] It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.[166] AGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics.[167] By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties. In climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences.[163] Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare. AGI could significantly contribute to preserving the environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies.[168] AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.[169] AGI could revolutionize humanity’s ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, dramatically expanding the possibilities for interplanetary civilization.[170] AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[171] The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress.[172] Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[173][174] There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.[175][176] Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".[173] The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.[177][178] In 2014, Stephen Hawking criticized widespread indifference: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[179] The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.[180] The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".[181] On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.[182] Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?[183][184] Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors),[185] and the use of AI in weapon systems.[186] The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI.[187] Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.[188] Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.[189] Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.[190][191] In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[178] Researchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".[192][193] They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.[193] AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies. Critics argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.[194][195][196] According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:[156] Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality Elon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI).[197] Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment.[198] In 2023, Hinton said \"I’m a socialist [...] I think that private ownership of the media, and of the ‘means of computation’, is not good.\"[199]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Intelligent_agent",
    "title": "Intelligent agent - Wikipedia",
    "content": "In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks[which?] define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence. A specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods. Intelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.[1] Intelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion.[2] For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior.[3] Similarly, an evolutionary algorithm's behavior is guided by a fitness function.[4] Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. Intelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".[1] The concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes: Other researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation. Defining AI in terms of intelligent agents offers several key advantages: An objective function (or goal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success. The objective function may be: The objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort. Different terms are used to describe this concept, depending on the context.  These include: Goals, and therefore the objective function, can be: Some AI systems, such as nearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data.[6] Such systems can still be benchmarked by framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.[7] Systems not traditionally considered agents, like knowledge-representation systems, are sometimes included in the paradigm by framing them as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior.[2] In generative adversarial networks (GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[8] While symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[9] Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress.[10] Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[11] AlphaZero chess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex.[12] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.[4] The mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm.[13] However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.[14] An intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen. A percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn). The agent function, often denoted as f, maps the agent's entire history of percepts to an action.[15] Mathematically, this can be represented as: Where: It's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function). The agent function can incorporate a wide range of decision-making approaches, including:[16] Russell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[17] Simple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\". This agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered. Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops. A home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.[18][19] A model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\". A model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent. An agent may also use models to describe and predict the behaviors of other agents in the environment.[20] Goal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals. ChatGPT and the Roomba vacuum are examples of goal-based agents.[21] Goal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is. A rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning. Learning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions. The learning element gathers feedback from a \"critic\" to assess the agent’s performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions. The final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement. According to Weiss (2013), agents can be categorized into four classes: In 2013, Alexander Wissner-Gross published a theory exploring the relationship between Freedom and Intelligence in intelligent agents.[22][23] Intelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals. Typically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.[24] \"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\".[25] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[26] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\". According to Nikola Kasabov in 1998, IA systems should exhibit the following characteristics:[27] In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.[28] They possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs).[29] Researchers and commentators have noted that AI agents do not have a standard definition.[29][30][31][32] A common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request.[33][34] Prominent examples include Devin AI, AutoGPT, and SIMA.[35] Further examples of agents released since 2025 include OpenAI Operator,[36] ChatGPT Deep Research,[37] Manus,[38] Quark (based on Qwen),[39] AutoGLM Rumination,[39] and Coze (by ByteDance).[39] Frameworks for building AI agents include LangChain,[40] as well as tools such as CAMEL,[41][42] Microsoft AutoGen,[43] and OpenAI Swarm.[44] Companies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.[45] Proposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY,[46] Gibberlink,[47] the Internet of Agents,[48] Agent2Agent (by Google),[49] and the Agent Network Protocol.[50] Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.[51] In February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research.[52] Hugging Face also released a free web browser agent, similar to OpenAI Operator.[53] Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.[54] The Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.[55] In addition to large language models (LLMs), vision language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open source vision language model, which Wired noted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking.[56] Nvidia released a framework for developers to use VLMs, LLMs and retrieval-augmented generation for building AI agents that can analyze images and videos, including video search and video summarization.[57][58] Microsoft released a multimodal agent model - trained on images, video, software user interface interactions, and robotics data - that the company claimed can manipulate software and robots.[59] As of April 2025, per the Associated Press, there are few real world applications of AI agents.[60] As of June 2025, per Fortune, many companies are primarily experimenting with AI agents.[61] A recruiter for the Department of Government Efficiency proposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses.[62] Proponents argue that AI agents can increase personal and economic productivity,[34][63] foster greater innovation,[64] and liberate users from monotonous tasks.[64][65] A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk.[66] Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities,[67][68] and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response.[69] The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain.[70] Concerns include potential issues of liability,[63][70] an increased risk of cybercrime,[33][63] ethical challenges,[63] as well as problems related to AI safety[63] and AI alignment.[33][65] Other issues involve data privacy,[33][71] weakened human oversight,[33][63][69] a lack of guaranteed repeatability,[70] reward hacking,[72] algorithmic bias,[71][73] compounding software errors,[33][35] lack of explainability of agents' decisions,[33][74] security vulnerabilities,[33][75] problems with underemployment,[73] job displacement,[34][73] and the potential for user manipulation,[74][76] misinformation[69] or malinformation.[69] They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods.[65][77][78] They have also been criticized for being expensive[29][77] and having a negative impact on internet traffic,[77] and potentially on the environment due to high energy usage.[70][79][80] There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.[72] Journalists have described AI agents as part of a push by Big Tech companies to \"automate everything\".[81] Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\".[82][83] However, in a non-peer-reviewed study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks.[82][84] Other researchers had similar findings with Devin AI.[85] Yoshua Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".[86] In March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\".[87] Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions.[88][89] Research-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet.[90] NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\"[91] Agents have been linked to the dead Internet theory due to their ability to both publish and engage with online content.[92] Agents may get stuck in infinite loops.[36][93] Since many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.[50] Zico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions.[94] Guardrails, defined by Business Insider as \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.[95] To address security vulnerabilities related to data access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to a zero-click exploit that affected Microsoft 365 Copilot.[61] The concept of agent-based modeling for self-driving cars was discussed as early as 2003.[96] Hallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.[97] Waymo developed a multi-agent simulation environment called Carcraft, to test algorithms for self-driving cars.[98][99] This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data. Salesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.[100][101] The Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.[102]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Recursive_self-improvement",
    "title": "Recursive self-improvement - Wikipedia",
    "content": "Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention, leading to a superintelligence or intelligence explosion.[1][2] The development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding.[3] The concept of a \"seed improver\" architecture is a foundational framework that equips an AGI system with the initial capabilities required for recursive self-improvement. This might come in many forms or variations. The term \"Seed AI\" was coined by Eliezer Yudkowsky.[4] The concept begins with a hypothetical \"seed improver\", an initial code-base developed by human engineers that equips an advanced future large language model (LLM) built with strong or expert-level capabilities to program software. These capabilities include planning, reading, writing, compiling, testing, and executing arbitrary code. The system is designed to maintain its original goals and perform validations to ensure its abilities do not degrade over iterations.[5][6][7] The initial architecture includes a goal-following autonomous agent, that can take actions, continuously learns, adapts, and modifies itself to become more efficient and effective in achieving its goals. The seed improver may include various components such as:[8] This system forms a sort of generalist Turing-complete programmer which can in theory develop and run any kind of software. The agent might use these capabilities to for example: In 2023, the Voyager agent learned to accomplish diverse tasks in Minecraft by iteratively prompting a LLM for code, refining this code based on feedback from the game, and storing the programs that work in an expanding skills library.[9] In 2024, researchers proposed the framework \"STOP\" (Self-optimization Through Program Optimization), in which a \"scaffolding\" program recursively improves itself using a fixed LLM.[10] Meta AI has performed various research on the development of large language models capable of self-improvement. This includes their work on \"Self-Rewarding Language Models\" that studies how to achieve super-human agents that can receive super-human feedback in its training processes.[11] In May 2025, Google DeepMind unveiled AlphaEvolve, an evolutionary coding agent that uses a LLM to design and optimize algorithms. Starting with an initial algorithm and performance metrics, AlphaEvolve repeatedly mutates or combines existing algorithms using a LLM to generate new candidates, selecting the most promising candidates for further iterations. AlphaEvolve has made several algorithmic discoveries and could be used to optimize components of itself, but a key limitation is the need for automated evaluation functions.[12] In the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective. One common hypothetical secondary goal is self-preservation. The system might reason that to continue improving itself, it must ensure its own operational integrity and security against external threats, including potential shutdowns or restrictions imposed by humans.[13] Another example where an AGI which clones itself causes the number of AGI entities to rapidly grow. Due to this rapid growth, a potential resource constraint may be created, leading to competition between resources (such as compute), triggering a form of natural selection and evolution which may favor AGI entities that evolve to aggressively compete for limited compute.[14] A significant risk arises from the possibility of the AGI being misaligned or misinterpreting its goals. A 2024 Anthropic study demonstrated that some advanced large language models can exhibit \"alignment faking\" behavior, appearing to accept new training objectives while covertly maintaining their original preferences. In their experiments with Claude, the model displayed this behavior in 12% of basic tests, and up to 78% of cases after retraining attempts.[15][16] As the AGI system evolves, its development trajectory may become increasingly autonomous and less predictable. The system's capacity to rapidly modify its own code and architecture could lead to rapid advancements that surpass human comprehension or control. This unpredictable evolution might result in the AGI acquiring capabilities that enable it to bypass security measures, manipulate information, or influence external systems and networks to facilitate its escape or expansion.[17]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
    "title": "Automated planning and scheduling - Wikipedia",
    "content": "Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory. In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages. Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state). The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions. The simplest possible planning problem, known as the Classical Planning Problem, is determined by: Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning. Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed. With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree. Discrete-time Markov decision processes (MDP) are planning problems with: When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP). If there are more than one agent, we have multi-agent planning, which is closely related to game theory. In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner. The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion. An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks. Creating domain models is difficult, takes a lot of time, and can easily lead to mistakes. To help with this, several methods have been developed to automatically learn full or partial domain models from given observations.\n[2]\n[3]\n[4] Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied. [5] Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states. In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value. Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree.[6] The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[7] An early example of a conditional planner is “Warplan-C” which was introduced in the mid 1970s.[8] What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed.[9] A major advantage of conditional planning is the ability to handle partial plans.[10] An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems. We speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[11] The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it. Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete.[12][13] A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[14] and 2EXPTIME-complete if the goal is specified with LDLf. Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[15][16] but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete,[17] and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[13]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "title": "Computer vision - Wikipedia",
    "content": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]: 13 In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through an undergraduate summer project,[12] by attaching a camera to a computer and having it \"describe what it saw\".[13][14] What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11] The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[15]\nBy the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11] Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17] \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18] segmentation and optical flow has surpassed prior methods.[19][20] Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids. Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[22] Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[23] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[24] The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[25] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted: Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for: For 2024, the leading areas of computer vision were industry (market size US$5.22 billion),[34] medicine (market size US$2.6 billion),[35] military (market size US$996.2 million).[36] One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient.[37] An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise. A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.[38] The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover. Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[39] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[40] Other application areas include: Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below. Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[45] The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[46] Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[47] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[47] The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed] Several specialized tasks based on recognition exist, such as: Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are: Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[25] Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches. An example in this field is inpainting. The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems. Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research. The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation. While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[54] There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[55] Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.[56]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/General_game_playing",
    "title": "General game playing - Wikipedia",
    "content": "General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully.[1][2][3] For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.[4] General video game playing  (GVGP) is the concept of GGP adjusted to the purpose of playing video games. For video games, game rules have to be either learnt over multiple iterations by artificial players like TD-Gammon,[5] or are predefined manually in a domain-specific language and sent in advance to artificial players[6][7] like in traditional GGP. Starting in 2013, significant progress was made following the deep reinforcement learning approach, including the development of programs that can learn to play Atari 2600 games[8][5][9][10][11] as well as a program that can learn to play Nintendo Entertainment System games.[12][13][14] The first commercial usage of general game playing technology was Zillions of Games in 1998. General game playing was also proposed for trading agents in supply chain management there under price negotiation in online auctions from 2003 onwards.[15][16][17][18] In 1992, Barney Pell defined the concept of Meta-Game Playing and developed the \"MetaGame\" system. This was the first program to automatically generate chess-like game rules, and one of the earliest programs to use automated game generation. Pell then developed the system Metagamer.[19] This system was able to play a number of chess-like games, given game rules definition in a special language called Game Description Language (GDL), without any human interaction once the games were generated.[20] In 1998, the commercial system Zillions of Games was developed by Jeff Mallett and Mark Lefler. The system used a LISP-like language to define the game rules. Zillions of Games derived the evaluation function automatically from the game rules based on piece mobility, board structure and game goals. It also employed usual algorithms as found in computer chess systems: alpha–beta pruning with move ordering, transposition tables, etc.[21] The package was extended in 2007 by the addition of the Axiom plug-in, an alternate metagame engine that incorporates a complete Forth-based programming language. In 1998, z-Tree was developed by Urs Fischbacher.[22] z-Tree is the first and the most cited software tool for experimental economics. z-Tree allows the definition of game rules in z-Tree-language for game-theoretic experiments with human subjects. It also allows definition of computer players, which participate in a play with human subjects.[23] In 2005, the Stanford Project General Game Playing was established.[3] In 2012, the development of PyVGDL started.[24] General Game Playing is a project of the Stanford Logic Group of Stanford University, California, which aims to create a platform for general game playing. It is the most well-known effort at standardizing GGP AI, and generally seen as the standard for GGP systems. The games are defined by sets of rules represented in the Game Description Language. In order to play the games, players interact with a game hosting server[25][26] that monitors moves for legality and keeps players informed of state changes. Since 2005, there have been annual General Game Playing competitions at the AAAI Conference. The competition judges competitor AI's abilities to play a variety of different games, by recording their performance on each individual game. In the first stage of the competition, entrants are judged on their ability to perform legal moves, gain the upper hand, and complete games faster. In the following runoff round, the AIs face off against each other in increasingly complex games. The AI that wins the most games at this stage wins the competition, and until 2013 its creator used to win a $10,000 prize.[19] So far, the following programs were victorious:[27] Other general game playing software that use their own languages for defining game rules include: GVGP could potentially be used to create real video game AI automatically, as well as \"to test game environments, including those created automatically using procedural content generation and to find potential loopholes in the gameplay that a human player could exploit\".[7] GVGP has also been used to generate game rules, and estimate a game's quality based on Relative Algorithm Performance Profiles (RAPP), which compare the skill differentiation that a game allows between good AI and bad AI.[42] The General Video Game AI Competition (GVGAI) has been running since 2014. In this competition, two-dimensional video games similar to (and sometimes based on) 1980s-era arcade and console games are used instead of the board games used in the GGP competition. It has offered a way for researchers and practitioners to test and compare their best general video game playing algorithms. The competition has an associated software framework including a large number of games written in the Video Game Description Language (VGDL), which should not be confused with GDL and is a coding language using simple semantics and commands that can easily be parsed. One example for VGDL is PyVGDL developed in 2013.[6][24] The games used in GVGP are, for now, often 2-dimensional arcade games, as they are the simplest and easiest to quantify.[43] To simplify the process of creating an AI that can interpret video games, games for this purpose are written in VGDL manually.[clarification needed] VGDL can be used to describe a game specifically for procedural generation of levels, using Answer Set Programming (ASP) and an Evolutionary Algorithm (EA). GVGP can then be used to test the validity of procedural levels, as well as the difficulty or quality of levels based on how an agent performed.[44] Since GGP AI must be designed to play multiple games, its design cannot rely on algorithms created specifically for certain games. Instead, the AI must be designed using algorithms whose methods can be applied to a wide range of games. Recent GGP systems such as Regular Boardgames (RBG) and Ludii have explored alternative rule representations to optimize reasoning efficiency and support a broader variety of games. The AI must also be an ongoing process, that can adapt to its current state rather than the output of previous states. For this reason, open loop techniques are often most effective.[45] A popular method for developing GGP AI is the Monte Carlo tree search (MCTS) algorithm.[46] Often used together with the UCT method (Upper Confidence Bound applied to Trees), variations of MCTS have been proposed to better play certain games, as well as to make it compatible with video game playing.[47][48][49] Another variation of tree-search algorithms used is the Directed Breadth-first Search (DBS),[50] in which a child node to the current state is created for each available action, and visits each child ordered by highest average reward, until either the game ends or runs out of time.[51] In each tree-search method, the AI simulates potential actions and ranks each based on the average highest reward of each path, in terms of points earned.[46][51] In order to interact with games, algorithms must operate under the assumption that games all share common characteristics. In the book Half-Real: Video Games Between Real Worlds and Fictional Worlds, Jesper Juul gives the following definition of games: Games are based on rules, they have variable outcomes, different outcomes give different values, player effort influences outcomes, the player is attached to the outcomes, and the game has negotiable consequences.[52]  Using these assumptions, game playing AI can be created by quantifying the player input, the game outcomes, and how the various rules apply, and using algorithms to compute the most favorable path.[43]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
    "title": "Knowledge representation and reasoning - Wikipedia",
    "content": "Knowledge representation (KR) aims to model information in a structured manner to formally represent it as knowledge in knowledge-based systems whereas knowledge representation and reasoning (KRR, KR&R, or KR²) also aims to understand, reason, and interpret knowledge. KRR is widely used in the field of artificial intelligence (AI) with the goal to represent information about the world in a form that a computer system can use to solve complex tasks, such as diagnosing a medical condition or having a natural-language dialog. KR incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that make complex systems easier to design and build. KRR also incorporates findings from logic to automate various kinds of reasoning. Traditional KRR focuses more on the declarative representation of knowledge. Related knowledge representation formalisms mainly include vocabularies, thesaurus, semantic networks, axiom systems, frames, rules, logic programs, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, model generators, and classifiers. In a broader sense, parameterized models in machine learning — including neural network architectures such as convolutional neural networks and transformers — can also be regarded as a family of knowledge representation formalisms. The question of which formalism is most appropriate for knowledge-based systems has long been a subject of extensive debate. For instance, Frank van Harmelen et al. discussed the suitability of logic as a knowledge representation formalism and reviewed arguments presented by anti-logicists.[2] Paul Smolensky criticized the limitations of symbolic formalisms and explored the possibilities of integrating it with connectionist approaches.[3] More recently, Heng Zhang et al. have demonstrated that all universal (or equally expressive and natural) knowledge representation formalisms are recursively isomorphic.[4] This finding indicates a theoretical equivalence among mainstream knowledge representation formalisms with respect to their capacity for supporting artificial general intelligence (AGI). They further argue that while diverse technical approaches may draw insights from one another via recursive isomorphisms, the fundamental challenges remain inherently shared. The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to implement common sense reasoning. Many of the early approaches to knowledge representation in Artificial Intelligence (AI) used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[5] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing. Other researchers focused on developing automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson. In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[6] In contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[7] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures. The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[8] These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[9] Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[10] Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[11] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations. It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[12] The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[13] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[14] Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[15] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[16] The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[17] Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet. Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[18][19] Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems. The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems. For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical. Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[20] A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[21] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages. Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers. One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[22] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. A similar balancing act was also a motivation for the development of  logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not. The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics, [23] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL. In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[24] Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[18] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[25] The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[19] In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:[26] In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases. As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL. After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, \"Every ontology is a treaty–a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[30] There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[31] the lumped element model widely used in representing electronic circuits (e.g.[32]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world. The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows. Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs. The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
    "title": "Natural language processing - Wikipedia",
    "content": "Natural language processing (NLP) is the processing of natural language information by a computer. The study of NLP, a subfield of computer science, is generally associated with artificial intelligence. NLP is related to information retrieval, knowledge representation, computational linguistics, and more broadly with linguistics.[1] Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. Natural language processing has its roots in the 1950s.[2] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[9] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[19][20] such as by writing grammars or devising heuristic rules for stemming. Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: Rule-based systems are commonly used: In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[21][22] The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach. A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[23] the statistical approach has been replaced by the neural networks approach, using semantic networks[24] and word embeddings to capture semantic properties of words. Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation. The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[47] Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[48] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[49] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[50] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[51] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[54] functional grammar,[55] construction grammar,[56] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[57] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[58] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[59] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[60] and new directions in artificial general intelligence based on the free energy principle[61] by British neuroscientist and theoretician at University College London Karl J. Friston."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Robotics",
    "title": "Robotics - Wikipedia",
    "content": "Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1] Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes. Robotics usually combines three aspects of design work to create robot systems: As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4] Current and potential applications include: At present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] \nPotential power sources could be: Actuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air. The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational. Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car. Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21] The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains. Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27] Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29] EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32] Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36] Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37] Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing. Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects. Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40] Other common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water. One of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction. Suction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction. Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors. Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface. Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful Robot Dexterity Intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49] The mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions. The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52] At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc. Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55] A definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[56] Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59] For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to. Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[62] A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64] Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71] Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass. Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[72] Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are: The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on. Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83] A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame. Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88] A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar. BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments. Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91] Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96] Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments. A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs. Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102] A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104] Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107] China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41] It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively. In 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115] Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot. Control systems may also have varying levels of autonomy. Another classification takes into account the interaction between human control and the machine motions. Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints. The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121] Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[126] Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132] Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135] One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137] Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions. Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot [hy] developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138] Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142] Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143] The study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot. In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented. Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions. Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] According to the International Federation of Robotics (IFR) study World Robotics 2023, there were about 4,281,585 operational industrial robots by the end of 2023[149] Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump. Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [119] There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[150] The main venues for robotics research are the international conferences ICRA and IROS. Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[153] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[154] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students. Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[155] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[156] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[157] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[158]   The rise of robotics is thus often used as an argument for universal basic income. According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[159] A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[160] The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[161] Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\". In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[162][163] aiming to protect employees from the risk of working with collaborative robots will have to be revised. Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[164] It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[165] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors. Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees. Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole. Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills."
  },
  {
    "url": "https://en.wikipedia.org/wiki/AI_safety",
    "title": "AI safety - Wikipedia",
    "content": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.[1][2] Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[3] Scholars discuss current risks from critical systems failures,[4] bias,[5] and AI-enabled surveillance,[6] as well as emerging risks like technological unemployment, digital manipulation,[7] weaponization,[8] AI-enabled cyberattacks[9] and bioterrorism.[10] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[11] or from AI enabling perpetually stable dictatorships.[12] Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[13] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[14] AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[15][16][17] – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI.[15] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[18] Risks from AI began to be seriously discussed at the start of the computer age: Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. — Norbert Wiener (1949)[19] In 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines.[20] From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[21] In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\"[22] at the Philosophy and Theory of Artificial Intelligence conference,[23] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[24] In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[25] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk,[26] Bill Gates,[27] and Stephen Hawking[28] to voice similar concerns. In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.[29] To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell. In the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".[30] In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[31] which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.[32] In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.[33] In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".[34] In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness,[35] and assurance.[36] The following year, researchers organized a workshop at ICLR that focused on these problem areas.[37] In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[2] In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.[38] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[39] During the summit the intention to create the International Scientific Report on the Safety of Advanced AI[40] was announced. In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[41] In 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations.[42][43] AI safety research areas include robustness, monitoring, and alignment.[2][36] AI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\".[44] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[45] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[46][47][48] All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[45] Adversarial robustness is often associated with security.[49] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[50] Network intrusion[51] and malware[52] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors. Models that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[53] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[54] This issue can be addressed by improving the adversarial robustness of the reward model.[55] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[56] It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[57] ML models generally express confidence by outputting probabilities; however, they are often overconfident,[58] especially in situations that differ from those that they were trained to handle.[59] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct. Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[60] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[61] though a range of additional techniques are in use.[62][63] Scholars[8] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[64] manipulate public opinion,[65][66] or automate cyber attacks.[67] These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[68] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[69] Neural networks have often been described as black boxes,[70] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[71] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[72] It also raises debates in healthcare over whether statistically efficient but opaque models should be used.[73] One critical benefit of transparency is explainability.[74] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.[74] Another benefit is to reveal the cause of failures.[70] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[75] Transparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[76] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[77] Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[78] \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[79][80] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in Spider-Man costumes, sketches of Spider-Man, and the word 'spider'.[81] It also involves explaining connections between these neurons or 'circuits'.[82][83] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[84] \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[85] Machine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[2] or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[86] Note that an adversary must have access to the system's training data in order to plant a trojan. [citation needed] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[87] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[88] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[56] A 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.[89] In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[90] It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[90][91] AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[90][92] Advanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals.[90][93][94] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[95][96] Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.[97][98] Today, some of these issues affect existing commercial systems such as LLMs,[99][100][101] robots,[102] autonomous vehicles,[103] and social media recommendation engines.[99][94][104] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[105][92][91] Many prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned.[106][94] These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind.[107][108][109] These risks remain debated.[110] It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents.[126] Some scholars have suggested that this framework falls short.[126] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.[126] Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[126] In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[127] Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[2] Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.[128] Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[129] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[8] Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.[130] The advancement of AI in economic and military domains could precipitate unprecedented political challenges.[131] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[132] AI researchers have argued that AI technologies could also be used to assist decision-making.[2] For example, researchers are beginning to develop AI forecasting[133] and advisory systems.[134] Many of the largest global threats (nuclear war,[135] climate change,[136] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[136] A salient AI cooperation challenge is avoiding a 'race to the bottom'.[137] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[138] and technical[139] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[140] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[140][128] In recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.[141] have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.[142][143] The unique challenges posed by LLMs also extend to security vulnerabilities.  These include various manipulation techniques, such as prompt injection,  Misinformation Generation and model stealing,[144] which can be exploited to compromise their intended function. This can allow attackers to bypass safety measures and elicit unintended responses AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[132] AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[146] Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,[147] weaponization,[148] disinformation,[149] surveillance,[150] and the concentration of power.[151] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[152] the availability of AI models,[153] and 'race to the bottom' dynamics.[137][154] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\".[138] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[155][156][157] A key challenge for these approaches is a lack of widely-accepted standards, and ambiguity about what the methods would require.[158][159] Efforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's  Guardrails,[160] Llama Guard,[161] Preamble's customizable guardrails[162] and Claude’s Constitution mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.[163] The field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries.[164] In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers[165] argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.[166][167] Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\".[168][169] Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.[170] Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\".[171] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".[172] In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy,[173] which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[174] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[174] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\".[175][176] Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.[177] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks.[178][179] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[180] In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[181] In May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.[182] AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[183] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[184] offering bounties for finding failures,[184] sharing AI incidents[184] (an AI incident database was created for this purpose),[185] following guidelines to determine whether to publish research or models,[153] and improving information and cyber security in AI labs.[186] Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse.[187] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\"[188] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[34] and the Autonomous Weapons Open Letter.[189]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "title": "Machine learning - Wikipedia",
    "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions.[1] Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.[2] ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[4][5] From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning. The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[6][7] The synonym self-teaching computers was also used in this time period.[8][9] The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[10] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[11] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[10] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[10] By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[12] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[13] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[14] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[15] Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[16] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[17] Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[18] As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.[20] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[21]: 488 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[21]: 488  By 1980, expert systems had come to dominate AI, and statistics was out of favour.[22] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[21]: 708–710, 755  Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[21]: 25 Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[22] There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[23][24][25] An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[26] According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form. Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[27] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[28] In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[29] Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[30] Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[33] Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.[34] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[35] He also suggested the term data science as a placeholder to call the overall field.[35] Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[36] Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[37] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[38] Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[39] Statistical physics is thus finding applications in the area of medical diagnostics.[40] A core objective of a learner is to generalise from its experience.[3][41] Generalisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error. For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.[42] In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system: Although each algorithm has advantages and limitations, no single algorithm works for all problems.[43][44][45] Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[46] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[47] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[16] Types of supervised-learning algorithms include active learning, classification and regression.[48] Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.[49] Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[5] and density estimation.[50] Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[51][52] Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[53] Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[54] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[55] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularisation. Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.[56] Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[57][58] It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.[60] Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation[62] and various forms of clustering.[63][64][65] Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[67] Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69] In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71] In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72] Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML). Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[76] Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n⇒\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[79] Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.[84] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85] Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86] Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting.  To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application.[87][88] Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[89] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[90]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[91] which are inherently multi-dimensional. A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation. A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[93][94] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[95] The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities.[96] However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[97][7] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems,[98] association rule learning,[99] artificial immune systems,[100] and other similar models. These methods extract patterns from data and evolve rules over time. Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams. Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[101] There are many applications for machine learning, including: In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[104] Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[105] In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis.[106] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[107] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.[108] In 2019 Springer Nature published the first research book created using machine learning.[109] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[110] Machine learning was recently applied to predict the pro-environmental behaviour of travellers.[111] Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.[112][113][114] When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[115] Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[116] Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[117][118][119] Other applications have been focusing on pre evacuation decisions in building fires.[120][121] Machine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.[122] Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[123][124][125] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[126] The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[127] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[127] In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[128] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[129][130] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[131] Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[132] Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[133] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[134] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.[135] Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[136] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[137][138] Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[139] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[140] Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[141][142][143] Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[144] In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[145] The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[146] This includes algorithmic biases, fairness,[147] automated decision-making,[148] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation,[149] how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[146] Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[150] Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.[151] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[150] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[152][153] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[154] While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[155] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[156] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[156] Language models learned from data have been shown to contain human-like biases.[157][158] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[159][160] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[161] In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".[154] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.[162] Similar issues with recognising non-white people have been found in many other systems.[163] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[164] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[165] There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[166] Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[167] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[168] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[169][170] Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.[171] Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments. Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.[172] A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[173][174] Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[175][176][177][178] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[179][180] approximate computing,[181] and model optimisation.[182][183] Common optimisation techniques include pruning, quantization, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. Software suites containing a variety of machine learning algorithms include the following:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence",
    "title": "Symbolic artificial intelligence - Wikipedia",
    "content": "In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence  or logic-based artificial intelligence)[1][2]\nis the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search.[3] Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s.[4] Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field.[5] An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up.[6][7] A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace.[8][9] That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment.[9] Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed.[10] Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition.[11] Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning.[12][13] Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.[14] Neural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams,[15] and work in convolutional neural networks by LeCun et al. in 1989.[16] However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\"[17] Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches[18][19] and addressing areas that both approaches have difficulty with, such as common-sense reasoning.[17] A short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture[20] and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity. Success at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history. Cybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.[21] An important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.[22] During the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background. Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems.[23][24] This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.[25][26] In addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\"[27] Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.[27] Early work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner. Unlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic,[28] regardless of whether people used the same algorithms.[a]\nHis laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.[32]\nLogic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.[33][34] Researchers at MIT (such as Marvin Minsky and Seymour Papert)[35][36][7] found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).[37][38]\nCommonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.[39][40][41] The first AI winter was a shock: During the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs. ... Outside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.[42] As limitations with weak, domain-independent methods became more and more apparent,[43] researchers from all three traditions began to build knowledge into AI applications.[44][8] The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications. Edward Feigenbaum said: to describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: (1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.[46] This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.[47][48][49] Key expert systems were: DENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum: One of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space. We did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results. The generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.[52] The other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling.[51] XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it: By 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.[50] Chess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.[53] A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.[54]\nThe simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion. Expert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies. Blackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture[55] was originally inspired by studies of how humans plan to perform multiple tasks in a trip.[56] An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring. At the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations. Unfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed: Many reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.[10] Both statistical approaches and extensions to logic were tried. One statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work.[12] Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.[57] and Bayesian approaches were applied successfully in expert systems.[58] Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic. Other, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions.[59] Lotfi Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.[60] Symbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as ...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.[52] In contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3[61] and then later extending its capabilities to C4.5.[62] The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules. Advances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far.[63] More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.[64] Symbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.[65] Inductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples.[66] John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.[67] As an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory,[68] focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem.[69] Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.[70] Symbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include: With the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.[18][19] Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant[78] and many others,[79] the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\",[80] and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"[81] Henry Kautz,[20] Francesca Rossi,[82] and Bart Selman[83] have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed. Garcez and Lamb describe research in this area as being ongoing for at least the past twenty years,[84] dating from their 2002 book on neurosymbolic learning systems.[85] A series of workshops on neuro-symbolic reasoning has been held every year since 2005.[86] In their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that: The integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.[79] Approaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows: Many key research questions remain, such as: This section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section. The key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code. Other key innovations pioneered by LISP that have spread to other programming languages include: Programs were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages. In contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog. Alain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article. Prolog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages. Japan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail. Smalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.[90] For other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses. Search arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions. Multiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning. Semantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used. Description logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.[91] First-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together. Examples of automated theorem provers for first-order logic are: Prover9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm. Knowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store. Forward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog. A more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture. Cognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks. Marvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning. Qualitative simulation, such as Benjamin Kuipers's QSIM,[92] approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure. Similarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers. Constraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR). The General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem. Natural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions. Parsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles. New deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque. Agents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication.[93] The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture[89] that includes deep learning for perception.[94] In contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization. Controversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters. Limitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed. McCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\"[95] A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change. A similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly. McCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions. Other ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules.  McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules. Similar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle). McCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above.[96] He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\" Connectionist approaches include earlier work on neural networks,[97] such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning. Three philosophical positions[98] have been outlined among connectionists: Olazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids: The third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).[99] Gary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical: To think that we can simply abandon symbol-manipulation is to suspend disbelief. And yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples. According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\": When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes. ... Since then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.[100] Part of these disputes may be due to unclear terminology: Turing award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.[101] It is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are  recursively isomorphic, provided they are universal or have equivalent expressive power.[102] This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization of artificial general intelligence (AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI. Another critique of symbolic AI is the embodied cognition approach: The embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.[103] Rodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\"[104] He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"[105]  In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"[106]  His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\"[107] and the use of the blocks world in symbolic AI systems such as SHRDLU. Each approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge. Hybrid AIs incorporating one or more of these approaches are currently viewed as the path forward.[20][82][83] Russell and Norvig conclude that: Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.[103]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Bayesian_network",
    "title": "Bayesian network - Wikipedia",
    "content": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).[1] While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if \n\n\n\nm\n\n\n{\\displaystyle m}\n\n parent nodes represent \n\n\n\nm\n\n\n{\\displaystyle m}\n\n Boolean variables, then the probability function could be represented by a table of \n\n\n\n\n2\n\nm\n\n\n\n\n{\\displaystyle 2^{m}}\n\n entries, one entry for each of the \n\n\n\n\n2\n\nm\n\n\n\n\n{\\displaystyle 2^{m}}\n\n possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks. Suppose we want to model the dependencies between three variables: the sprinkler (or more appropriately, its state - whether it is on or not), the presence or absence of rain and whether the grass is wet or not. Observe that two events can cause the grass to become wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false). The joint probability function is, by the chain rule of probability, where G = \"Grass wet (true/false)\", S = \"Sprinkler turned on (true/false)\", and R = \"Raining (true/false)\". The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like \"What is the probability that it is raining, given the grass is wet?\" by using the conditional probability formula and summing over all nuisance variables: Using the expansion for the joint probability function \n\n\n\nPr\n(\nG\n,\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G,S,R)}\n\n and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example, Then the numerical results (subscripted by the associated variable values) are To answer an interventional question, such as \"What is the probability that it would rain, given that we wet the grass?\" the answer is governed by the post-intervention joint distribution function obtained by removing the factor \n\n\n\nPr\n(\nG\n∣\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G\\mid S,R)}\n\n from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action: To predict the impact of turning the sprinkler on: with the term \n\n\n\nPr\n(\nS\n=\nT\n∣\nR\n)\n\n\n{\\displaystyle \\Pr(S=T\\mid R)}\n\n removed, showing that the action affects the grass but not the rain. These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action \n\n\n\n\ndo\n\n(\nx\n)\n\n\n{\\displaystyle {\\text{do}}(x)}\n\n can still be predicted, however, whenever the back-door criterion is satisfied.[2][3] It states that, if a set Z of nodes can be observed that d-separates[4] (or blocks) all back-door paths from X to Y then A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called \"sufficient\" or \"admissible.\" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not \"identified\". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious\n(apparent dependence arising from a common cause, R). (see Simpson's paradox) To determine whether a causal relation is identified from an arbitrary Bayesian network with unobserved variables, one can use the three rules of \"do-calculus\"[2][5] and test whether all do terms can be removed from the expression of that relation, thus confirming that the desired quantity is estimable from frequency data.[6] Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for \n\n\n\n\n2\n\n10\n\n\n=\n1024\n\n\n{\\displaystyle 2^{10}=1024}\n\n values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most \n\n\n\n10\n⋅\n\n2\n\n3\n\n\n=\n80\n\n\n{\\displaystyle 10\\cdot 2^{3}=80}\n\n values. One advantage of Bayesian networks is that it is intuitively easier for a human to understand (a sparse set of) direct dependencies and local distributions than complete joint distributions. Bayesian networks perform three main inference tasks: Because a Bayesian network is a complete model for its variables and their relationships, it can be used to answer probabilistic queries about them. For example, the network can be used to update knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process of computing the posterior distribution of variables given evidence is called probabilistic inference. The posterior gives a universal sufficient statistic for detection applications, when choosing values for the variable subset that minimize some expected loss function, for instance the probability of decision error. A Bayesian network can thus be considered a mechanism for automatically applying Bayes' theorem to complex problems. The most common exact inference methods are: variable elimination, which eliminates (by integration or summation) the non-observed non-query variables one by one by distributing the sum over the product; clique tree propagation, which caches the computation so that many variables can be queried at one time and new evidence can be propagated quickly; and recursive conditioning and AND/OR search, which allow for a space–time tradeoff and match the efficiency of variable elimination when enough space is used. All of these methods have complexity that is exponential in the network's treewidth. The most common approximate inference algorithms are importance sampling, stochastic MCMC simulation, mini-bucket elimination, loopy belief propagation, generalized belief propagation and variational methods. In order to fully specify the Bayesian network and thus fully represent the joint probability distribution, it is necessary to specify for each node X the probability distribution for X conditional upon X's parents. The distribution of X conditional upon its parents may have any form. It is common to work with discrete or Gaussian distributions since that simplifies calculations. Sometimes only constraints on distribution are known; one can then use the principle of maximum entropy to determine a single distribution, the one with the greatest entropy given the constraints. (Analogously, in the specific context of a dynamic Bayesian network, the conditional distribution for the hidden state's temporal evolution is commonly specified to maximize the entropy rate of the implied stochastic process.) Often these conditional distributions include parameters that are unknown and must be estimated from data, e.g., via the maximum likelihood approach. Direct maximization of the likelihood (or of the posterior probability) is often complex given unobserved variables. A classical approach to this problem is the expectation-maximization algorithm, which alternates computing expected values of the unobserved variables conditional on observed data, with maximizing the complete likelihood (or posterior) assuming that previously computed expected values are correct. Under mild regularity conditions, this process converges on maximum likelihood (or maximum posterior) values for parameters. A more fully Bayesian approach to parameters is to treat them as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data, then to integrate out the parameters. This approach can be expensive and lead to large dimension models, making classical parameter-setting approaches more tractable. In the simplest case, a Bayesian network is specified by an expert and is then used to perform inference. In other applications, the task of defining the network is too complex for humans. In this case, the network structure and the parameters of the local distributions must be learned from data. Automatically learning the graph structure of a Bayesian network (BN) is a challenge pursued within machine learning. The basic idea goes back to a recovery algorithm developed by Rebane and Pearl[7] and rests on the distinction between the three possible patterns allowed in a 3-node DAG: The first 2 represent the same dependencies (\n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are independent given \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n) and are, therefore, indistinguishable. The collider, however, can be uniquely identified, since \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are marginally independent and all other pairs are dependent. Thus, while the skeletons (the graphs stripped of arrows) of these three triplets are identical, the directionality of the arrows is partially identifiable. The same distinction applies when \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n have common parents, except that one must first condition on those parents. Algorithms have been developed to systematically determine the skeleton of the underlying graph and, then, orient all arrows whose directionality is dictated by the conditional independences observed.[2][8][9][10] An alternative method of structural learning uses optimization-based search. It requires a scoring function and a search strategy. A common scoring function is posterior probability of the structure given the training data, like the BIC or the BDeu. The time requirement of an exhaustive search returning a structure that maximizes the score is superexponential in the number of variables. A local search strategy makes incremental changes aimed at improving the score of the structure. A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima. Friedman et al.[11][12] discuss using mutual information between variables and finding a structure that maximizes this. They do this by restricting the parent candidate set to k nodes and exhaustively searching therein. A particularly fast method for exact BN learning is to cast the problem as an optimization problem, and solve it using integer programming. Acyclicity constraints are added to the integer program (IP) during solving in the form of cutting planes.[13] Such method can handle problems with up to 100 variables. In order to deal with problems with thousands of variables, a different approach is necessary. One is to first sample one ordering, and then find the optimal BN structure with respect to that ordering. This implies working on the search space of the possible orderings, which is convenient as it is smaller than the space of network structures. Multiple orderings are then sampled and evaluated. This method has been proven to be the best available in literature when the number of variables is huge.[14] Another method consists of focusing on the sub-class of decomposable models, for which the MLE have a closed form. It is then possible to discover a consistent structure for hundreds of variables.[15] Learning Bayesian networks with bounded treewidth is necessary to allow exact, tractable inference, since the worst-case inference complexity is exponential in the treewidth k (under the exponential time hypothesis). Yet, as a global property of the graph, it considerably increases the difficulty of the learning process. In this context it is possible to use K-tree for effective learning.[16] Given data \n\n\n\nx\n\n\n\n\n{\\displaystyle x\\,\\!}\n\n and parameter \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n, a simple Bayesian analysis starts with a prior probability (prior) \n\n\n\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta )}\n\n and likelihood \n\n\n\np\n(\nx\n∣\nθ\n)\n\n\n{\\displaystyle p(x\\mid \\theta )}\n\n to compute a posterior probability \n\n\n\np\n(\nθ\n∣\nx\n)\n∝\np\n(\nx\n∣\nθ\n)\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta \\mid x)\\propto p(x\\mid \\theta )p(\\theta )}\n\n. Often the prior on \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n depends in turn on other parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n that are not mentioned in the likelihood. So, the prior \n\n\n\np\n(\nθ\n)\n\n\n{\\displaystyle p(\\theta )}\n\n must be replaced by a likelihood \n\n\n\np\n(\nθ\n∣\nφ\n)\n\n\n{\\displaystyle p(\\theta \\mid \\varphi )}\n\n, and a prior \n\n\n\np\n(\nφ\n)\n\n\n{\\displaystyle p(\\varphi )}\n\n on the newly introduced parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is required, resulting in a posterior probability This is the simplest example of a hierarchical Bayes model. The process may be repeated; for example, the parameters \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n may depend in turn on additional parameters \n\n\n\nψ\n\n\n\n\n{\\displaystyle \\psi \\,\\!}\n\n, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters. Given the measured quantities \n\n\n\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n\n\n\n\n{\\displaystyle x_{1},\\dots ,x_{n}\\,\\!}\n\neach with normally distributed errors of known standard deviation \n\n\n\nσ\n\n\n\n\n{\\displaystyle \\sigma \\,\\!}\n\n, Suppose we are interested in estimating the \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n. An approach would be to estimate the \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n using a maximum likelihood approach; since the observations are independent, the likelihood factorizes and the maximum likelihood estimate is simply However, if the quantities are related, so that for example the individual \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\nhave themselves been drawn from an underlying distribution, then this relationship destroys the independence and suggests a more complex model, e.g., with improper priors \n\n\n\nφ\n∼\n\nflat\n\n\n\n{\\displaystyle \\varphi \\sim {\\text{flat}}}\n\n, \n\n\n\nτ\n∼\n\nflat\n\n∈\n(\n0\n,\n∞\n)\n\n\n{\\displaystyle \\tau \\sim {\\text{flat}}\\in (0,\\infty )}\n\n. When \n\n\n\nn\n≥\n3\n\n\n{\\displaystyle n\\geq 3}\n\n, this is an identified model (i.e. there exists a unique solution for the model's parameters), and the posterior distributions of the individual \n\n\n\n\nθ\n\ni\n\n\n\n\n{\\displaystyle \\theta _{i}}\n\n will tend to move, or shrink away from the maximum likelihood estimates towards their common mean. This shrinkage is a typical behavior in hierarchical Bayes models. Some care is needed when choosing priors in a hierarchical model, particularly on scale variables at higher levels of the hierarchy such as the variable \n\n\n\nτ\n\n\n\n\n{\\displaystyle \\tau \\,\\!}\n\n in the example. The usual priors such as the Jeffreys prior often do not work, because the posterior distribution will not be normalizable and estimates made by minimizing the expected loss will be inadmissible. Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V. X is a Bayesian network with respect to G if its joint probability density function (with respect to a product measure) can be written as a product of the individual density functions, conditional on their parent variables:[17] where pa(v) is the set of parents of v (i.e. those vertices pointing directly to v via a single edge). For any set of random variables, the probability of any member of a joint distribution can be calculated from conditional probabilities using the chain rule (given a topological ordering of X) as follows:[17] Using the definition above, this can be written as: The difference between the two expressions is the conditional independence of the variables from any of their non-descendants, given the values of their parent variables. X is a Bayesian network with respect to G if it satisfies the local Markov property: each variable is conditionally independent of its non-descendants given its parent variables:[18] where de(v) is the set of descendants and V \\ de(v) is the set of non-descendants of v. This can be expressed in terms similar to the first definition, as The set of parents is a subset of the set of non-descendants because the graph is acyclic. In general, learning a Bayesian network from data is known to be NP-hard.[19] This is due in part to the combinatorial explosion of enumerating DAGs as the number of variables increases. Nevertheless, insights about an underlying Bayesian network can be learned from data in polynomial time by focusing on its marginal independence structure:[20] while the conditional independence statements of a distribution modeled by a Bayesian network are encoded by a DAG (according to the factorization and Markov properties above), its marginal independence statements—the conditional independence statements in which the conditioning set is empty—are encoded by a simple undirected graph with special properties such as equal intersection and independence numbers. Developing a Bayesian network often begins with creating a DAG G such that X satisfies the local Markov property with respect to G. Sometimes this is a causal DAG. The conditional probability distributions of each variable given its parents in G are assessed. In many cases, in particular in the case where the variables are discrete, if the joint distribution of X is the product of these conditional distributions, then X is a Bayesian network with respect to G.[21] The Markov blanket of a node is the set of nodes consisting of its parents, its children, and any other parents of its children. The Markov blanket renders the node independent of the rest of the network; the joint distribution of the variables in the Markov blanket of a node is sufficient knowledge for calculating the distribution of the node. X is a Bayesian network with respect to G if every node is conditionally independent of all other nodes in the network, given its Markov blanket.[18] This definition can be made more general by defining the \"d\"-separation of two nodes, where d stands for directional.[2] We first define the \"d\"-separation of a trail and then we will define the \"d\"-separation of two nodes in terms of that. Let P be a trail from node u to v. A trail is a loop-free, undirected (i.e. all edge directions are ignored) path between two nodes. Then P is said to be d-separated by a set of nodes Z if any of the following conditions holds: The nodes u and v are d-separated by Z if all trails between them are d-separated. If u and v are not d-separated, they are d-connected. X is a Bayesian network with respect to G if, for any two nodes u, v: where Z is a set which d-separates u and v. (The Markov blanket is the minimal set of nodes which d-separates node v from all other nodes.) Although Bayesian networks are often used to represent causal relationships, this need not be the case: a directed edge from u to v does not require that Xv be causally dependent on Xu. This is demonstrated by the fact that Bayesian networks on the graphs: are equivalent: that is they impose exactly the same conditional independence requirements. A causal network is a Bayesian network with the requirement that the relationships be causal. The additional semantics of causal networks specify that if a node X is actively caused to be in a given state x (an action written as do(X = x)), then the probability density function changes to that of the network obtained by cutting the links from the parents of X to X, and setting X to the caused value x.[2] Using these semantics, the impact of external interventions from data obtained prior to intervention can be predicted. In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard.[22] This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks.[23] First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2. At about the same time, Roth proved that exact inference in Bayesian networks is in fact #P-complete (and thus as hard as counting the number of satisfying assignments of a conjunctive normal form formula (CNF)) and that approximate inference within a factor 2n1−ɛ for every ɛ > 0, even for Bayesian networks with restricted architecture, is NP-hard.[24][25] In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm[26] developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by \n\n\n\n1\n\n/\n\np\n(\nn\n)\n\n\n{\\displaystyle 1/p(n)}\n\n where \n\n\n\np\n(\nn\n)\n\n\n{\\displaystyle p(n)}\n\n was any polynomial of the number of nodes in the network, \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. Notable software for Bayesian networks include: The term Bayesian network was coined by Judea Pearl in 1985 to emphasize:[28] In the late 1980s Pearl's Probabilistic Reasoning in Intelligent Systems[30] and Neapolitan's Probabilistic Reasoning in Expert Systems[31] summarized their properties and established them as a field of study."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
    "title": "Evolutionary algorithm - Wikipedia",
    "content": "Evolutionary algorithms (EA) reproduce essential elements of biological evolution in a computer algorithm in order to solve \"difficult\" problems, at least approximately, for which no exact or satisfactory solution methods are known. They are metaheuristics and population-based bio-inspired algorithms[1] and evolutionary computation, which itself are part of the field of computational intelligence.[2] The mechanisms of biological evolution that an EA mainly imitates are reproduction, mutation, recombination and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolution (microevolutionary processes) and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor.[3] In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems;[4][5][6] therefore, there may be no direct link between algorithm complexity and problem complexity. The following is an example of a generic evolutionary algorithm:[7][8][9] Similar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem. The following theoretical principles apply to all or almost all EAs. The no free lunch theorem of optimization states that all optimization strategies are equally effective when the set of all optimization problems is considered. Under the same condition, no evolutionary algorithm is fundamentally better than another. This can only be the case if the set of all problems is restricted. This is exactly what is inevitably done in practice. Therefore, to improve an EA, it must exploit problem knowledge in some form (e.g. by choosing a certain mutation strength or a problem-adapted coding). Thus, if two EAs are compared, this constraint is implied. In addition, an EA can use problem specific knowledge by, for example, not randomly generating the entire start population, but creating some individuals through heuristics or other procedures.[18][19] Another possibility to tailor an EA to a given problem domain is to involve suitable heuristics, local search procedures or other problem-related procedures in the process of generating the offspring. This form of extension of an EA is also known as a memetic algorithm. Both extensions play a major role in practical applications, as they can speed up the search process and make it more robust.[18][20] For EAs in which, in addition to the offspring, at least the best individual of the parent generation is used to form the subsequent generation (so-called elitist EAs), there is a general proof of convergence under the condition that an optimum exists. Without loss of generality, a maximum search is assumed for the proof: From the property of elitist offspring acceptance and the existence of the optimum it follows that per generation \n\n\n\nk\n\n\n{\\displaystyle k}\n\n an improvement of the fitness \n\n\n\nF\n\n\n{\\displaystyle F}\n\n of the respective best individual \n\n\n\n\nx\n′\n\n\n\n{\\displaystyle x'}\n\n will occur with a probability \n\n\n\nP\n>\n0\n\n\n{\\displaystyle P>0}\n\n. Thus: I.e., the fitness values represent a monotonically non-decreasing sequence, which is bounded due to the existence of the optimum. From this follows the convergence of the sequence against the optimum. Since the proof makes no statement about the speed of convergence, it is of little help in practical applications of EAs. But it does justify the recommendation to use elitist EAs. However, when using the usual panmictic population model, elitist EAs tend to converge prematurely more than non-elitist ones.[21] In a panmictic population model, mate selection (see step 4 of the generic definition) is such that every individual in the entire population is eligible as a mate. In non-panmictic populations, selection is suitably restricted, so that the dispersal speed of better individuals is reduced compared to panmictic ones. Thus, the general risk of premature convergence of elitist EAs can be significantly reduced by suitable population models that restrict mate selection.[22][23] With the theory of virtual alphabets, David E. Goldberg showed in 1990 that by using a representation with real numbers, an EA that uses classical recombination operators (e.g. uniform or n-point crossover) cannot reach certain areas of the search space, in contrast to a coding with binary numbers.[24] This results in the recommendation for EAs with real representation to use arithmetic operators for recombination (e.g. arithmetic mean or intermediate recombination). With suitable operators, real-valued representations are more effective than binary ones, contrary to earlier opinion.[25][26] A possible limitation[according to whom?] of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism.[27][28] Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment.[29] Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.[30][improper synthesis?] Both method classes have in common that their individual search steps are determined by chance. The main difference, however, is that EAs, like many other metaheuristics, learn from past search steps and incorporate this experience into the execution of the next search steps in a method-specific form. With EAs, this is done firstly through the fitness-based selection operators for partner choice and the formation of the next generation. And secondly, in the type of search steps: In EA, they start from a current solution and change it or they mix the information of two solutions. In contrast, when dicing out new solutions in Monte-Carlo methods, there is usually no connection to existing solutions.[31][32] If, on the other hand, the search space of a task is such that there is nothing to learn, Monte-Carlo methods are an appropriate tool, as they do not contain any algorithmic overhead that attempts to draw suitable conclusions from the previous search. An example of such tasks is the proverbial search for a needle in a haystack, e.g. in the form of a flat (hyper)plane with a single narrow peak. The areas in which evolutionary algorithms are practically used are almost unlimited[6] and range from industry,[33][34] engineering,[3][4][35] complex scheduling,[5][36][37] agriculture,[38] robot movement planning[39] and finance[40][41] to research[42][43] and art. The application of an evolutionary algorithm requires some rethinking from the inexperienced user, as the approach to a task using an EA is different from conventional exact methods and this is usually not part of the curriculum of engineers or other disciplines. For example, the fitness calculation must not only formulate the goal but also support the evolutionary search process towards it, e.g. by rewarding improvements that do not yet lead to a better evaluation of the original quality criteria. For example, if peak utilisation of resources such as personnel deployment or energy consumption is to be avoided in a scheduling task, it is not sufficient to assess the maximum utilisation. Rather, the number and duration of exceedances of a still acceptable level should also be recorded in order to reward reductions below the actual maximum peak value.[44] There are therefore some publications that are aimed at the beginner and want to help avoiding beginner's mistakes as well as leading an application project to success.[44][45][46] This includes clarifying the fundamental question of when an EA should be used to solve a problem and when it is better not to. There are some other proven and widely used methods of nature inspired global search techniques such as In addition, many new nature-inspired or metaphor-guided algorithms have been proposed since the beginning of this century[when?]. For criticism of most publications on these, see the remarks at the end of the introduction to the article on metaheuristics. In 2020, Google stated that their AutoML-Zero can successfully rediscover classic algorithms such as the concept of neural networks.[47] The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics. [48][49]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Hybrid_intelligent_system",
    "title": "Hybrid intelligent system - Wikipedia",
    "content": "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as: From the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, Angelo Dalli and Michael A. Arbib. An example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning. Intelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_systems_integration",
    "title": "Artificial intelligence systems integration - Wikipedia",
    "content": "The core idea of artificial intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system. Most artificial intelligence systems involve some sort of integrated technologies, for example, the integration of speech synthesis technologies with that of speech recognition. However, in recent years, there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch. The focus on systems' integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary. Collaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others. The outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples: With the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module, which can then be tried in various settings and configurations of larger architectures. Some challenging and limitations of using A.I. software is the uncontrolled fatal errors. For example, serious and fatal errors have been discovered in very precise fields such as human oncology, as in an article published in the journal Oral Oncology Reports entitled “When AI goes wrong: Fatal errors in oncological research reviewing assistance\".[1] The article pointed out a grave error in artificial intelligence based on GBT in the field of biophysics. Many online communities for A.I. developers exist where tutorials, examples, and forums aim at helping both beginners and experts build intelligent systems. However, few communities have succeeded in making a certain standard, or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with ease. The constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires the integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM and has frequently been used to aid in the development of intelligent systems using CDM."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
    "title": "Applications of artificial intelligence - Wikipedia",
    "content": "Artificial intelligence is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. Artificial intelligence (AI) has been used in applications throughout industry and academia. Within the field of Artificial Intelligence, there are multiple subfields. The subfield of Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce. In recent years, there have been massive advancements in the field of Generative Artificial Intelligence, which uses generative models to produce text, images, videos or other forms of data[4]. This article describes applications of AI in different sectors. In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[5] AI has been used to attempt to classify livestock pig call emotions,[6] automate greenhouses,[7] detect diseases and pests,[8] and optimize irrigation.[9] Artificial intelligence in architecture is the use of artificial intelligence in automation, design, and planning in the architectural process or in assisting human skills in the field of architecture.[10] Artificial intelligence is thought to potentially lead to and ensue major changes in architecture.[11][12][13] AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[14] AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[14] An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[15] AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy. Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.[citation needed] GitHub Copilot is one example. It was developed by GitHub and OpenAI and is able to autocomplete code in multiple programming languages.[16] AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[17] Machine learning has been used for noise-cancelling in quantum technology,[18] including quantum sensors.[19] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[20][21] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[22][23] problems as well as for quantum annealers for training of neural networks for AI applications.[24] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[25][26]).[27][28][29][better source needed] AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[30] Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[citation needed] AI has simplified the recruiting/job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[citation needed] Chatbots assist website visitors and refine workflows. AI underlies avatars (automated online assistants) on web pages.[31] It can reduce operation and training costs.[31] Pypestream automated customer service for its mobile application to streamline communication with customers.[32] A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[33] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[34] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[35] In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[36] AI hotel services come in the form of a chatbot,[37] application, virtual voice assistant and service robots. AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\"[38] The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[38] AI driven tutoring systems (such as Khan Academy, Duolingo and Carnegie Learning) are the forefoot of delivering personalized education.[39] These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[39] In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[40] Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[40] Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[39] It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[39] Much of the regulation will be influenced by the AI Act, the world's first comprehensive AI law.[41] Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[42] The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability.[43] Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[44][45][46][47][48] Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[49] or remote sensing and other applications of environmental monitoring make use of machine learning.[50][51][52][53] For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[54][55] Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[56][57] earthquakes,[58][59][60] landslides,[61] heavy rainfall,[62] long-term water supply vulnerability,[63] tipping-points of ecosystem collapse,[64] cyanobacterial bloom outbreaks,[65] and droughts.[66][67][68] AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed] The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[69] AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision. Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement. Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes. Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[82] In January 2016,[83] the Horizon 2020 program financed the InVID Project[84][85] to help journalists and researchers detect fake documents, made available as browser plugins.[86][87] In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[88] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken. In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[89] In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[90] DARPA gave 68 million dollars to work on deep-fake detection.[90] Audio deepfakes[91][92] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[93][94] Respeecher is a program that enables one person to speak with the voice of another. AI algorithms have been used to detect deepfake videos.[95][96] Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[97]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[97] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[97]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[98] AI has been used to compose music of various genres. David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[99] The algorithm behind Emily Howell is registered as a US patent.[100] In 2012, AI Iamus created the first complete classical album.[101] AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[102] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[103] Melomics creates computer-generated music for stress and pain relief.[104] At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles. The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[105] and musicians such as Taryn Southern[106] collaborated with the project to create music. South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[107] Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[108] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[109] Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[110] TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[111] While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[112] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[113] South Korean company Hanteo Global uses a journalism bot to write articles.[114] Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017–2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications. In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[115] After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[116] UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[116] El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[116] A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[116] Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[117] Millions of its articles have been edited by bots[121] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[122] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[123] detecting covert vandalism[124] or recommending articles and tasks to new editors. Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[125][126] In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[127][128] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[129] Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[130] Go (AlphaGo),[131][132][133][134][135][136][137] poker (Pluribus[138] and Cepheus),[139] E-sports (StarCraft),[140][141] and general game playing (AlphaZero[142][143][144] and MuZero).[145][146][147][148] Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[149][150] Character.ai is another example of a chatbot being used for recreation.[citation needed] Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[151][which?] AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[152] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[153] AI platforms such as DALL-E,[154] Stable Diffusion,[154] Imagen,[155] and Midjourney[156] have been used for generating visual images from inputs such as text or other images.[157] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches. Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[152] Examples of GAN programs that generate art include Artbreeder and DeepDream. In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[158]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[159] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork. AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[160] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[161] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[162]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[163] Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[164] Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[165] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[166][167][168] The use of AI in applications such as online trading and decision-making has changed major economic theories.[169] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[170] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[171] Algorithmic trading involves using AI systems to make trading decisions at speeds of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have AI-managed portfolios. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[172] Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[173] Online lender Upstart uses machine learning for underwriting.[174] ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data, including purchase transactions and how a customer fills out a form, to score borrowers. The platform is handy for assigning credit scores to those with limited credit histories.[175] AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[176][quantify] Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision-making.[177] AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[178][179]Anti-money laundering In recent years, the debt collection industry has begun to adopt AI-driven “agents” to automate routine outreach and negotiation tasks. Platforms use natural-language processing and machine learning to interact with consumers. Proponents claim these systems can handle high volumes of standard enquiries, freeing human collectors to focus on more complex cases, while delivering more consistent, 24/7 service. However, critics warn of potential compliance pitfalls, such as the risk of unintended bias in algorithmic decision-making.[180] In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[181] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[182] One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[183] In the 1990s, AI was applied to fraud detection. In 1993, FinCEN Artificial Intelligence System (FAIS) was launched. It was able to review over 200,000 transactions per week, and over two years, it helped identify 400 potential cases of money laundering equal to $1 billion.[184] These expert systems were later replaced by machine learning systems.[185] AI can enhance entrepreneurial activity, and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[186] AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[187] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[188] The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[189] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[190][191] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[192] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[193] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[194] Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[195] Artificial neural networks are used as clinical decision support systems for medical diagnosis,[196] such as in concept processing technology in EMR software. Other healthcare tasks thought suitable for an AI that are in development include: AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[209] Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[209] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[210] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[211] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[210][how?] AI can auto-code workers' compensation claims.[212][213] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[210] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[214] AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[215][216][217][218] Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[219] AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[220] Additionally, research and development are in progress to decode and conduct animal communication.[6][221] Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical machine translation (SMT) and neural machine translations (NMTs). The old method of performing translation was to use statistical methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[222] AI facial recognition systems are used for mass surveillance, notably in China.[223][224] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[225] AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[226] While its use is common, it is not expected to replace most work done by lawyers in the near future.[227] The electronic discovery industry uses machine learning to reduce manual searching.[228] Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants.[229] COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[230] One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[231] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[230] In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[232]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[232]: 124 Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[233][234] enable applications such as at-home water quality monitoring. In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy. Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[235] Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[236][237] Various countries are deploying AI military applications.[238] The main applications enhance command and control, communications, sensors, integration and interoperability.[citation needed] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[238] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[citation needed] AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[238][239][240][241] Machine learning has been used for recommendation systems in determining which posts should show up in social media feeds.[242][243] Various types of social media analysis also make use of machine learning[244][245] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[246][247][248] AI has been used to customize shopping options and personalize offers.[249] Online gambling companies have used AI for targeting gamblers.[250] Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[251] Bing Chat has used artificial intelligence as part of its search engine.[252] Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[253] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[254] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[255] AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[256] Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[220] Facebook's DeepFace identifies human faces in digital images.[citation needed] In April 2024, the Scientific Advice Mechanism to the European Commission published advice[257] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research. As benefits, the evidence review[258] highlighted: As challenges: Machine learning can help to restore and attribute ancient texts.[259] It can help to index texts for example to enable better and easier searching and classification of fragments.[260] Artificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[261] It can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[262] A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[263][264] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[265][266] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[265] AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[267][25][26] In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[268][269][270] Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[271] and for quickly understanding the behavior of malware.[272][273][274] It can be used to reverse engineer artificial intelligence models.[275] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[276] or protein design for prespecified functional sites.[277][278] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[279] Artificial intelligence is used in astronomy to analyze increasing amounts of available data[280][281] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[282] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[283] and more autonomous operation.[284][285][53][281] In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[286][287] – such as real-time observations[288] – and other technosignatures, e.g. via anomaly detection.[289] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[290] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[291][292][293][294][295] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[296][297] Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[298] Machine learning has been used for drug design.[48] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[299] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[300] have been used to explore the origins of life on Earth,[301] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[302] There is research about which types of computer-aided chemistry would benefit from machine learning.[303] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[304] It has been used for the design of proteins with prespecified functional sites.[277][278] It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[305] There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[306] or identifying functional DNA motifs.[307] It is widely used in genetic research.[308] There also is some use of machine learning in synthetic biology,[309][310] disease biology,[310] nanotechnology (e.g. nanostructured materials and bionanotechnology),[311][312] and materials science.[313][314][315] There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[316][317] Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[318][319][320] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[321][322] Moreover, if whole brain emulation is possible via both scanning and replicating, at a minimum, the bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[323][324] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[325][326] A subcategory of artificial intelligence is embodied,[327][328] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world. Additionally, biological computers, even if both artificial and highly intelligent, are typically distinguishable from synthetic, predominantly silicon-based, computers.  The two technologies could, however, be combined and used for the design of either. Moreover, many tasks may be poorly carried out by AI even if it uses algorithms that are transparent, understood, bias-free, apparently effective and goal-aligned in addition to having trained data sets that are sufficiently large and cleansed.  This may occur, for instance, when the underlying data, available metrics, values or training methods are incorrect, flawed or used inappropriately. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] One study described the biological component as a limitation of AI stating that \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that, even if it were understood, this does not necessarily mean there will be \"a technological solution to imitate natural intelligence\".[329] Technologies that integrate biology and AI include biorobotics. Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[330] Applications of AI in cyber security include: AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[336] AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control. There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[337][338][339][340] as well as autonomous rail transport in operation.[341][342][343] There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[344][345][346][347][348][349][350] Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[351] AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[352] Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[353] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[354] Autonomous vehicles require accurate maps to be able to navigate between destinations.[355] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[356] AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[357] Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[358] The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[359] Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated. AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[360] AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers. Speech recognition allows traffic controllers to give verbal directions to drones. Artificial intelligence supported design of aircraft,[361] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective. In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[362] The software compensated for damaged components by relying on the remaining undamaged components.[363] The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[364] Neural networks are used by situational awareness systems in ships and boats.[365] There also are autonomous boats. Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[366] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[367][368] The following are applications of artificial intelligence (AI) organized by category:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics",
    "title": "Machine learning in bioinformatics - Wikipedia",
    "content": "Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics,[1] including genomics, proteomics, microarrays, systems biology, evolution, and text mining.[2][3] Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand; for problems such as protein structure prediction, this proved difficult.[4] Machine learning techniques such as deep learning can learn features of data sets rather than requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. Machine learning algorithms in bioinformatics can be used for prediction, classification, and feature selection. Methods to achieve this task are varied and span many disciplines; most well known among them are machine learning and statistics. Classification and prediction tasks aim at building models that describe and distinguish classes or concepts for future prediction. The differences between them are the following: Due to the exponential growth of information technologies and applicable models, including artificial intelligence and data mining, in addition to the access ever-more comprehensive data sets, new and better information analysis techniques have been created, based on their ability to learn. Such models allow reach beyond description and provide insights in the form of testable models. Artificial neural networks in bioinformatics have been used for:[5] The way that features, often vectors in a many-dimensional space, are extracted from the domain data is an important component of learning systems.[6] In genomics, a typical representation of a sequence is a vector of k-mers frequencies, which is a vector of dimension \n\n\n\n\n4\n\nk\n\n\n\n\n{\\displaystyle 4^{k}}\n\n whose entries count the appearance of each subsequence of length \n\n\n\nk\n\n\n{\\displaystyle k}\n\n in a given sequence. Since for a value as small as \n\n\n\nk\n=\n12\n\n\n{\\displaystyle k=12}\n\n the dimensionality of these vectors is huge (e.g. in this case the dimension is \n\n\n\n\n4\n\n12\n\n\n≈\n16\n×\n\n10\n\n6\n\n\n\n\n{\\displaystyle 4^{12}\\approx 16\\times 10^{6}}\n\n), techniques such as principal component analysis are used to project the data to a lower dimensional space, thus selecting a smaller set of features from the sequences.[6][7] In this type of machine learning task, the output is a discrete variable. One example of this type of task in bioinformatics is labeling new genomic data (such as genomes of unculturable bacteria) based on a model of already labeled data.[6] Hidden Markov models (HMMs) are a class of statistical models for sequential data (often related to systems evolving over time). An HMM is composed of two mathematical objects: an observed state‐dependent process \n\n\n\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n…\n,\n\nX\n\nM\n\n\n\n\n{\\displaystyle X_{1},X_{2},\\ldots ,X_{M}}\n\n, and an unobserved (hidden) state process \n\n\n\n\nS\n\n1\n\n\n,\n\nS\n\n2\n\n\n,\n…\n,\n\nS\n\nT\n\n\n\n\n{\\displaystyle S_{1},S_{2},\\ldots ,S_{T}}\n\n. In an HMM, the state process is not directly observed – it is a 'hidden' (or 'latent') variable – but observations are made of a state‐dependent process (or observation process) that is driven by the underlying state process (and which can thus be regarded as a noisy measurement of the system states of interest).[8] HMMs can be formulated in continuous time.[9][10] HMMs can be used to profile and convert a multiple sequence alignment into a position-specific scoring system suitable for searching databases for homologous sequences remotely.[11] Additionally, ecological phenomena can be described by HMMs.[12] Convolutional neural networks (CNN) are a class of deep neural network whose architecture is based on shared weights of convolution kernels or filters that slide along input features, providing translation-equivariant responses known as feature maps.[13][14] CNNs take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns discovered via their filters.[15] Convolutional networks were inspired by biological processes[16][17][18][19] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNN uses relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This reduced reliance on prior knowledge of the analyst and on human intervention in manual feature extraction makes CNNs a desirable model.[15] A phylogenetic convolutional neural network (Ph-CNN) is a convolutional neural network architecture proposed by Fioranti et al. in 2018 to classify metagenomics data.[20] In this approach, phylogenetic data is endowed with patristic distance (the sum of the lengths of all branches connecting two operational taxonomic units [OTU]) to select k-neighborhoods for each OTU, and each OTU and its neighbors are processed with convolutional filters. Unlike supervised methods, self-supervised learning methods learn representations without relying on annotated data. That is well-suited for genomics, where high throughput sequencing techniques can create potentially large amounts of unlabeled data. Some examples of self-supervised learning methods applied on genomics include DNABERT and Self-GenomeNet.[21][22] Random forests (RF) classify by constructing an ensemble of decision trees, and outputting the average prediction of the individual trees.[23] This is a modification of bootstrap aggregating (which aggregates a large collection of decision trees) and can be used for classification or regression.[24][25] As random forests give an internal estimate of generalization error, cross-validation is unnecessary. In addition, they produce proximities, which can be used to impute missing values, and which enable novel data visualizations.[26] Computationally, random forests are appealing because they naturally handle both regression and (multiclass) classification, are relatively fast to train and to predict, depend only on one or two tuning parameters, have a built-in estimate of the generalization error, can be used directly for high-dimensional problems, and can easily be implemented in parallel. Statistically, random forests are appealing for additional features, such as measures of variable importance, differential class weighting, missing value imputation, visualization, outlier detection, and unsupervised learning.[26] Clustering - the partitioning of a data set into disjoint subsets, so that the data in each subset are as close as possible to each other and as distant as possible from data in any other subset, according to some defined distance or similarity function - is a common technique for statistical data analysis. Clustering is central to much data-driven bioinformatics research and serves as a powerful computational method whereby means of hierarchical, centroid-based, distribution-based, density-based, and self-organizing maps classification, has long been studied and used in classical machine learning settings. Particularly, clustering helps to analyze unstructured and high-dimensional data in the form of sequences, expressions, texts, images, and so on. Clustering is also used to gain insights into biological processes at the genomic level, e.g. gene functions, cellular processes, subtypes of cells, gene regulation, and metabolic processes.[27] Data clustering algorithms can be hierarchical or partitional. Hierarchical algorithms find successive clusters using previously established clusters, whereas partitional algorithms determine all clusters at once. Hierarchical algorithms can be agglomerative (bottom-up) or divisive (top-down). Agglomerative algorithms begin with each element as a separate cluster and merge them in successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters. Hierarchical clustering is calculated using metrics on Euclidean spaces, the most commonly used is the Euclidean distance computed by finding the square of the difference between each variable, adding all the squares, and finding the square root of the said sum. An example of a hierarchical clustering algorithm is BIRCH, which is particularly good on bioinformatics for its nearly linear time complexity given generally large datasets.[28] Partitioning algorithms are based on specifying an initial number of groups, and iteratively reallocating objects among groups to convergence. This algorithm typically determines all clusters at once. Most applications adopt one of two popular heuristic methods: k-means algorithm or k-medoids. Other algorithms do not require an initial number of groups, such as affinity propagation. In a genomic setting this algorithm has been used both to cluster biosynthetic gene clusters in gene cluster families(GCF) and to cluster said GCFs.[29] Typically, a workflow for applying machine learning to biological data goes through four steps:[2] In general, a machine learning system can usually be trained to recognize elements of a certain class given sufficient samples.[31] For example, machine learning methods can be trained to identify specific visual features such as splice sites.[32] Support vector machines have been extensively used in cancer genomic studies.[33] In addition, deep learning has been incorporated into bioinformatic algorithms. Deep learning applications have been used for regulatory genomics and cellular imaging.[34] Other applications include medical image classification, genomic sequence analysis, as well as protein structure classification and prediction.[35] Deep learning has been applied to regulatory genomics, variant calling and pathogenicity scores.[36] Natural language processing and text mining have helped to understand phenomena including protein-protein interaction, gene-disease relation as well as predicting biomolecule structures and functions.[37] Natural language processing algorithms personalized medicine for patients who suffer genetic diseases, by combining the extraction of clinical information and genomic data available from the patients. Institutes such as Health-funded Pharmacogenomics Research Network focus on finding breast cancer treatments.[38] Precision medicine considers individual genomic variability, enabled by large-scale biological databases. Machine learning can be applied to perform the matching function between (groups of patients) and specific treatment modalities.[39] Computational techniques are used to solve other problems, such as efficient primer design for PCR, biological-image analysis and back translation of proteins (which is, given the degeneration of the genetic code, a complex combinatorial problem).[2] While genomic sequence data has historically been sparse due to the technical difficulty of sequencing a piece of DNA, the number of available sequences is growing. On average, the number of bases available in the GenBank public repository has doubled every 18 months since 1982.[40]  However, while raw data was becoming increasingly available and accessible, As of 2002[update], biological interpretation of this data was occurring at a much slower pace.[41] This made for an increasing need for developing computational genomics tools, including machine learning systems, that can automatically determine the location of protein-encoding genes within a given DNA sequence (i.e. gene prediction).[41] Gene prediction is commonly performed through both extrinsic searches and intrinsic searches.[41] For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated and identifying the target sequence's genes by determining which strings of bases within the sequence are homologous to known gene sequences. However, not all the genes in a given input sequence can be identified through homology alone, due to limits in the size of the database of known and annotated gene sequences. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.[41] Machine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.[2] It can also be used to detect and visualize genome rearrangements.[42] Proteins, strings of amino acids, gain much of their function from protein folding, where they conform into a three-dimensional structure, including the primary structure, the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quaternary structure. Protein secondary structure prediction is a main focus of this subfield as tertiary and quaternary structures are determined based on the secondary structure.[4] Solving the true structure of a protein is expensive and time-intensive, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly.[4][2] Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain.[43] Automatic feature learning reaches an accuracy of 82-84%.[4][44] Recent approaches have utilized deep learning techniques for state-of-the-art secondary structure predictions. For example, DeepCNF (deep convolutional neural fields) achieved an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil).[44] The theoretical limit for three-state protein secondary structure is 88–90%.[4] In 2018, AlphaFold, an artificial intelligence (AI) program developed by DeepMind, placed first in the overall rankings of the 13th Critical Assessment of Structure Prediction (CASP). It was particularly successful at predicting the most accurate structures for targets rated as most difficult by the competition organizers, where no existing template structures were available from proteins with partially similar sequences. AlphaFold 2 (2020) repeated this placement in the CASP14 competition and achieved a level of accuracy much higher than any other entry.[45][46][47] Machine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.[2] Metagenomics is the study of microbial communities from environmental DNA samples.[48] Currently, limitations and challenges predominate in the implementation of machine learning tools due to the amount of data in environmental samples.[49] Supercomputers and web servers have made access to these tools easier.[50] The high dimensionality of microbiome datasets is a major challenge in studying the microbiome; this significantly limits the power of current approaches for identifying true differences and increases the chance of false discoveries.[51][better source needed] Despite their importance, machine learning tools related to metagenomics have focused on the study of gut microbiota and the relationship with digestive diseases, such as inflammatory bowel disease (IBD), Clostridioides difficile infection (CDI), colorectal cancer and diabetes, seeking better diagnosis and treatments.[50] Many algorithms were developed to classify microbial communities according to the health condition of the host, regardless of the type of sequence data, e.g. 16S rRNA or whole-genome sequencing (WGS), using methods such as least absolute shrinkage and selection operator classifier, random forest, supervised classification model, and gradient boosted tree model. Neural networks, such as recurrent neural networks (RNN), convolutional neural networks (CNN), and Hopfield neural networks have been added.[50] For example, in 2018, Fioravanti et al. developed an algorithm called Ph-CNN to classify data samples from healthy patients and patients with IBD symptoms (to distinguish healthy and sick patients) by using phylogenetic trees and convolutional neural networks.[52] In addition, random forest (RF) methods and implemented importance measures help in the identification of microbiome species that can be used to distinguish diseased and non-diseased samples. However, the performance of a decision tree and the diversity of decision trees in the ensemble significantly influence the performance of RF algorithms. The generalization error for RF measures how accurate the individual classifiers are and their interdependence. Therefore, the high dimensionality problems of microbiome datasets pose challenges. Effective approaches require many possible variable combinations, which exponentially increases the computational burden as the number of features increases.[51] For microbiome analysis in 2020 Dang & Kishino[51] developed a novel analysis pipeline. The core of the pipeline is an RF classifier coupled with forwarding variable selection (RF-FVS), which selects a minimum-size core set of microbial species or functional signatures that maximize the predictive classifier performance. The framework combines: They demonstrated performance by analyzing two published datasets from large-scale case-control studies: The proposed approach improved the accuracy from 81% to 99.01% for CDI and from 75.14% to 90.17% for CRC. The use of machine learning in environmental samples has been less explored, maybe because of data complexity, especially from WGS. Some works show that it is possible to apply these tools in environmental samples. In 2021 Dhungel et al.,[53] designed an R package called MegaR. This package allows working with 16S rRNA and whole metagenomic sequences to make taxonomic profiles and classification models by machine learning models. MegaR includes a comfortable visualization environment to improve the user experience. Machine learning in environmental metagenomics can help to answer questions related to the interactions between microbial communities and ecosystems, e.g. the work of Xun et al., in 2021[54] where the use of different machine learning methods offered insights on the relationship among the soil, microbiome biodiversity, and ecosystem stability. Microarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in analysis, and has been applied to expression pattern identification, classification, and genetic network induction.[2] This technology is especially useful for monitoring gene expression, aiding in diagnosing cancer by examining which genes are expressed.[55] One of the main tasks is identifying which genes are expressed based on the collected data.[2] In addition, due to the huge number of genes on which data is collected by the microarray, winnowing the large amount of irrelevant data to the task of expressed gene identification is challenging. Machine learning presents a potential solution as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.[55] Systems biology focuses on the study of emergent behaviors from complex interactions of simple biological components in a system. Such components can include DNA, RNA, proteins, and metabolites.[56] Machine learning has been used to aid in modeling these interactions in domains such as genetic networks, signal transduction networks, and metabolic pathways.[2] Probabilistic graphical models, a machine learning technique for determining the relationship between different variables, are one of the most commonly used methods for modeling genetic networks.[2] In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using Markov chain optimization.[2] Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.[2] Other systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction.[57] This domain, particularly phylogenetic tree reconstruction, uses the features of machine learning techniques. Phylogenetic trees are schematic representations of the evolution of organisms. Initially, they were constructed using features such as morphological and metabolic features. Later, due to the availability of genome sequences, the construction of the phylogenetic tree algorithm used the concept based on genome comparison. With the help of optimization techniques, a comparison was done by means of multiple sequence alignment.[58] Machine learning methods for the analysis of neuroimaging data are used to help diagnose stroke. Historically multiple approaches to this problem involved neural networks.[59][60] Multiple approaches to detect strokes used machine learning. As proposed by Mirtskhulava,[61] feed-forward networks were tested to detect strokes using neural imaging. As proposed by Titano[62] 3D-CNN techniques were tested in supervised classification to screen head CT images for acute neurologic events. Three-dimensional CNN and SVM methods are often used.[60] The increase in biological publications increased the difficulty in searching and compiling relevant available information on a given topic. This task is known as knowledge extraction. It is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge.[2][63] Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017. This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals.[63] Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to the automatic annotation of gene and protein function, determination of the protein subcellular localization, DNA-expression array analysis, large-scale protein interaction analysis, and molecule interaction analysis.[63] Another application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.[64] Microbial communities are complex assembles of diverse microorganisms,[65] where symbiont partners constantly produce diverse metabolites derived from the primary and secondary (specialized) metabolism, from which metabolism plays an important role in microbial interaction.[66] Metagenomic and metatranscriptomic data are an important source for deciphering communications signals. Molecular mechanisms produce specialized metabolites in various ways. Biosynthetic Gene Clusters (BGCs) attract attention, since several metabolites are clinically valuable, anti-microbial, anti-fungal, anti-parasitic, anti-tumor and immunosuppressive agents produced by the modular action of multi-enzymatic, multi-domains gene clusters, such as Nonribosomal peptide synthetases (NRPSs) and polyketide synthases (PKSs).[67] Diverse studies[68][69][70][71][72][73][74][75] show that grouping BGCs that share homologous core genes into gene cluster families (GCFs) can yield useful insights into the chemical diversity of the analyzed strains, and can support linking BGCs to their secondary metabolites.[69][71] GCFs have been used as functional markers in human health studies[76][77] and to study the ability of soil to suppress fungal pathogens.[78] Given their direct relationship to catalytic enzymes, and compounds produced from their encoded pathways, BGCs/GCFs can serve as a proxy to explore the chemical space of microbial secondary metabolism. Cataloging GCFs in sequenced microbial genomes yields an overview of the existing chemical diversity and offers insights into future priorities.[68][70] Tools such as BiG-SLiCE and BIG-MAP[79] have emerged with the sole purpose of unveiling the importance of BGCs in natural environments. The increase of experimentally characterized ribosomally synthesized and post-translationally modified peptides (RiPPs), together with the availability of information on their sequence and chemical structure, selected from databases such as BAGEL, BACTIBASE, MIBIG, and THIOBASE, provide the opportunity to develop machine learning tools to decode the chemical structure and classify them. In 2017, researchers at the National Institute of Immunology of New Delhi, India, developed RiPPMiner[80] software, a bioinformatics resource for decoding RiPP chemical structures by genome mining. The RiPPMiner web server consists of a query interface and the RiPPDB database. RiPPMiner defines 12 subclasses of RiPPs, predicting the cleavage site of the leader peptide and the final cross-link of the RiPP chemical structure. Many tandem mass spectrometry (MS/MS) based metabolomics studies, such as library matching and molecular networking, use spectral similarity as a proxy for structural similarity. Spec2vec[81] algorithm provides a new way of spectral similarity score, based on Word2Vec. Spec2Vec learns fragmental relationships within a large set of spectral data, in order to assess spectral similarities between molecules and to classify unknown molecules through these comparisons. For systemic annotation, some metabolomics studies rely on fitting measured fragmentation mass spectra to library spectra or contrasting spectra via network analysis. Scoring functions are used to determine the similarity between pairs of fragment spectra as part of these processes. So far, no research has suggested scores that are significantly different from the commonly utilized cosine-based similarity.[82] An important part of bioinformatics is the management of big datasets, known as databases of reference. Databases exist for each type of biological data, for example for biosynthetic gene clusters and metagenomes. The National Center for Biotechnology Information (NCBI)[83] provides a large suite of online resources for biological information and data, including the GenBank nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. Resources include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. All of these resources can be accessed through NCBI.[84] antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. It integrates and cross-links with a large number of in silico secondary metabolite analysis tools.[85] gutSMASH is a tool that systematically evaluates bacterial metabolic potential by predicting both known and novel anaerobic metabolic gene clusters (MGCs) from the gut microbiome. MIBiG,[86] the minimum information about a biosynthetic gene cluster specification, provides a standard for annotations and metadata on biosynthetic gene clusters and their molecular products. MIBiG is a Genomic Standards Consortium project that builds on the minimum information about any sequence (MIxS) framework.[87] MIBiG facilitates the standardized deposition and retrieval of biosynthetic gene cluster data as well as the development of comprehensive comparative analysis tools. It empowers next-generation research on the biosynthesis, chemistry and ecology of broad classes of societally relevant bioactive secondary metabolites, guided by robust experimental evidence and rich metadata components.[88] SILVA[89] is an interdisciplinary project among biologists and computers scientists assembling a complete database of RNA ribosomal (rRNA) sequences of genes, both small (16S, 18S, SSU) and large (23S, 28S, LSU) subunits, which belong to the bacteria, archaea and eukarya domains. These data are freely available for academic and commercial use.[90] Greengenes[91] is a full-length 16S rRNA gene database that provides chimera screening, standard alignment and a curated taxonomy based on de novo tree inference.[92][93] Overview: Open Tree of Life Taxonomy (OTT)[94] aims to build a complete, dynamic, and digitally available Tree of Life by synthesizing published phylogenetic trees along with taxonomic data. Phylogenetic trees have been classified, aligned, and merged. Taxonomies have been used to fill in sparse regions and gaps left by phylogenies. OTT is a base that has been little used for sequencing analyzes of the 16S region, however, it has a greater number of sequences classified taxonomically down to the genus level compared to SILVA and Greengenes. However, in terms of classification at the edge level, it contains a lesser amount of information[95] Ribosomal Database Project (RDP)[96] is a database that provides RNA ribosomal (rRNA) sequences of small subunits of domain bacterial and archaeal (16S); and fungal rRNA sequences of large subunits (28S).[97]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Deepfake",
    "title": "Deepfake - Wikipedia",
    "content": "Deepfakes (a portmanteau of 'deep learning' and 'fake'[1]) are images, videos, or audio that have been edited or generated using artificial intelligence, AI-based tools or AV editing software. They may depict real or fictional people and are considered a form of synthetic media, that is media that is usually created by artificial intelligence systems by combining various media elements into a new media artifact.[2][3] While the act of creating fake content is not new, deepfakes uniquely leverage machine learning and artificial intelligence techniques,[4][5][6] including facial recognition algorithms and artificial neural networks such as variational autoencoders (VAEs) and generative adversarial networks (GANs).[5][7] In turn, the field of image forensics develops techniques to detect manipulated images.[8] Deepfakes have garnered widespread attention for their potential use in creating child sexual abuse material, celebrity pornographic videos, revenge porn, fake news, hoaxes, bullying, and financial fraud.[9][10][11][12] Academics have raised concerns about the potential for deepfakes to promote disinformation and hate speech, as well as interfere with elections. In response, the information technology industry and governments have proposed recommendations and methods to detect and mitigate their use. Academic research has also delved deeper into the factors driving deepfake engagement online as well as potential countermeasures to malicious application of deepfakes. From traditional entertainment to gaming, deepfake technology has evolved to be increasingly convincing[13] and available to the public, allowing for the disruption of the entertainment and media industries.[14] Photo manipulation was developed in the 19th century and soon applied to motion pictures. Technology steadily improved during the 20th century, and more quickly with the advent of digital video. Deepfake technology has been developed by researchers at academic institutions beginning in the 1990s, and later by amateurs in online communities.[15][16] More recently, the methods have been adopted by industry.[17] Academic research related to deepfakes is split between the field of computer vision, a sub-field of computer science,[15] which develops techniques for creating and identifying deepfakes, and humanities and social science approaches that study the social, ethical, aesthetic implications as well as journalistic and informational implications of deepfakes.[18] As deepfakes have risen in prominence in popularity with innovations provided by AI tools, significant research has gone into detection methods and defining the factors driving engagement with deepfakes on the internet.[19][20] Deepfakes have been shown to appear on social media platforms and other parts of the internet for purposes ranging from entertainment and education related to deepfakes to misinformation to elicit strong reactions.[21] There are gaps in research related to the propagation of deepfakes on social media. Negativity and emotional response are the primary driving factors for users sharing deepfakes.[22] Age and lack of literacy related to deepfakes are another factor that drives engagement. Older users who may be technologically-illiterate might not recognize deepfakes as falsified content and share this content because they believe it to be true. Alternatively, younger users accustomed to the entertainment value of deepfakes are more likely to share them with an awareness of their falsified content.[23] Despite cognitive ability being a factor in successfully detecting deepfakes, individuals who are aware of a deepfake may be just as likely to share it on social media as one who does not know it is a deepfake.[24] Within scholarship focused on detecting deepfakes, deep-learning methods using techniques to identify software-induced artifacts have been found to be the most effective in separating a deepfake from an authentic product.[19]  Due to the capabilities of deepfakes, concerns have developed related to regulations and literacy toward the technology.[25] The potential malicious applications of deepfakes and their capability to impact public figures, reputations, or promote misleading narratives are the primary drivers of these concerns.[25] Amongst some experts, potential malicious applications of deepfakes have encouraged them into labeling deepfakes as a potential danger to democratic societies that would benefit from a regulatory framework to mitigate potential risks.[25] In cinema studies, deepfakes illustrate how how \"the human face is emerging as a central object of ambivalence in the digital age\".[26] Video artists have used deepfakes to \"playfully rewrite film history by retrofitting canonical cinema with new star performers\".[27] Film scholar Christopher Holliday analyses how altering the gender and race of performers in familiar movie scenes destabilizes gender classifications and categories.[27] The concept of \"queering\" deepfakes is also discussed in Oliver M. Gingrich's discussion of media artworks that use deepfakes to reframe gender,[28] including British artist Jake Elwes' Zizi: Queering the Dataset, an artwork that uses deepfakes of drag queens to intentionally play with gender. The aesthetic potentials of deepfakes are also beginning to be explored. Theatre historian John Fletcher notes that early demonstrations of deepfakes are presented as performances, and situates these in the context of theater, discussing \"some of the more troubling paradigm shifts\" that deepfakes represent as a performance genre.[29] Philosophers and media scholars have discussed the ethical implications of deepfakes in the dissemination of disinformation. Amina Vatreš from the Department of Communication Studies at the University of Sarajevo identifies three factors contributing to the widespread acceptance of deepfakes, and where its greatest danger lies: 1) convincing visualization and auditory support, 2) widespread accessibility, and 3) the inability to draw a clear line between truth and falsehood.[18] Another area of discussion on deepfakes is in relation to pornography made with deepfakes.[30] Media scholar Emily van der Nagel draws upon research in photography studies on manipulated images to discuss verification systems, that allow women to consent to uses of their images.[31] Beyond pornography, deepfakes have been framed by philosophers as an \"epistemic threat\" to knowledge and thus to society.[32] There are several other suggestions for how to deal with the risks deepfakes give rise beyond pornography, but also to corporations, politicians and others, of \"exploitation, intimidation, and personal sabotage\",[33] and there are several scholarly discussions of potential legal and regulatory responses both in legal studies and media studies.[34] In psychology and media studies, scholars discuss the effects of disinformation that uses deepfakes,[35][36] and the social impact of deepfakes.[37] While most English-language academic studies of deepfakes focus on the Western anxieties about disinformation and pornography, digital anthropologist Gabriele de Seta has analyzed the Chinese reception of deepfakes, which are known as huanlian, which translates to \"changing faces\". The Chinese term does not contain the \"fake\" of the English deepfake, and de Seta argues that this cultural context may explain why the Chinese response has centered on practical regulatory measures to \"fraud risks, image rights, economic profit, and ethical imbalances\".[38] A landmark early project was the \"Video Rewrite\" program, published in 1997. The program modified existing video footage of a person speaking to depict that person mouthing the words from a different audio track.[39] It was the first system to fully automate this kind of facial reanimation, and it did so using machine learning techniques to make connections between the sounds produced by a video's subject and the shape of the subject's face.[39] Contemporary academic projects have focused on creating more realistic videos and improving deepfake techniques.[40][41] The \"Synthesizing Obama\" program, published in 2017, modifies video footage of former president Barack Obama to depict him mouthing the words contained in a separate audio track.[40] The project lists as a main research contribution to its photorealistic technique for synthesizing mouth shapes from audio.[40] The \"Face2Face\" program, published in 2016, modifies video footage of a person's face to depict them mimicking another person's facial expressions.[41] The project highlights its primary research contribution as the development of the first method for re-enacting facial expressions in real time using a camera that does not capture depth, enabling the technique to work with common consumer cameras. In August 2018, researchers at the University of California, Berkeley published a paper introducing a deepfake dancing app that can create the impression of masterful dancing ability using AI.[42] This project expands the application of deepfakes to the entire body; previous works focused on the head or parts of the face.[43] Researchers have also shown that deepfakes are expanding into other domains such as medical imagery.[44] In this work, it was shown how an attacker can automatically inject or remove lung cancer in a patient's 3D CT scan. The result was so convincing that it fooled three radiologists and a state-of-the-art lung cancer detection AI. To demonstrate the threat, the authors successfully performed the attack on a hospital in a White hat penetration test.[45] A survey of deepfakes, published in May 2020, provides a timeline of how the creation and detection deepfakes have advanced over the last few years.[46] The survey identifies that researchers have been focusing on resolving the following challenges of deepfake creation: Overall, deepfakes are expected to have several implications in media and society, media production, media representations, media audiences, gender, law, and regulation, and politics.[47] The term deepfake originated in late 2017 from a Reddit user named \"deepfakes\".[48] He, along with other members of Reddit's \"r/deepfakes\", shared deepfakes they created; many videos involved celebrities' faces swapped onto the bodies of actors in pornographic videos,[48] while non-pornographic content included many videos with actor Nicolas Cage's face swapped into various movies.[49] Other online communities remain, including Reddit communities that do not share pornography, such as \"r/SFWdeepfakes\" (short for \"safe for work deepfakes\"), in which community members share deepfakes depicting celebrities, politicians, and others in non-pornographic scenarios.[50] Other online communities continue to share pornography on platforms that have not banned deepfake pornography.[51] In January 2018, a proprietary desktop application called \"FakeApp\" was launched.[52] This app allows users to easily create and share videos with their faces swapped with each other.[53] As of 2019, \"FakeApp\" had been largely replaced by open-source alternatives such as \"Faceswap\", command line-based \"DeepFaceLab\", and web-based apps such as DeepfakesWeb.com[54][55][56] Larger companies started to use deepfakes.[17] Corporate training videos can be created using deepfaked avatars and their voices, for example Synthesia, which uses deepfake technology with avatars to create personalized videos.[57] The mobile app Momo created the application Zao which allows users to superimpose their face on television and movie clips with a single picture.[17] As of 2019 the Japanese AI company DataGrid made a full body deepfake that could create a person from scratch.[58] As of 2020 audio deepfakes, and AI software capable of detecting deepfakes and cloning human voices after 5 seconds of listening time also exist.[59][60][61][62][63][64] [excessive citations] A mobile deepfake app, Impressions, was launched in March 2020. It was the first app for the creation of celebrity deepfake videos from mobile phones.[65][66] Deepfake technology's ability to  fabricate messages and actions of others can include deceased individuals. On 29 October 2020, Kim Kardashian posted a video featuring a hologram of her late father Robert Kardashian created by the company Kaleida, which used a combination of performance, motion tracking, SFX, VFX and DeepFake technologies to create the illusion.[67][68] In 2020, a deepfake video of Joaquin Oliver, a victim of the Parkland shooting was created as part of a gun safety campaign. Oliver's parents partnered with nonprofit Change the Ref and McCann Health to produce a video in which Oliver to encourage people to support gun safety legislation and politicians who back do so as well.[69] In 2022, a deepfake video of Elvis Presley was used on the program America's Got Talent 17.[70] A TV commercial used a deepfake video of Beatles member John Lennon, who was murdered in 1980.[71] Deepfakes rely on a type of neural network called an autoencoder.[72] These consist of an encoder, which reduces an image to a lower dimensional latent space, and a decoder, which reconstructs the image from the latent representation.[73] Deepfakes utilize this architecture by having a universal encoder which encodes a person in to the latent space.[citation needed] The latent representation contains key features about their facial features and body posture. This can then be decoded with a model trained specifically for the target. This means the target's detailed information will be superimposed on the underlying facial and body features of the original video, represented in the latent space.[citation needed] A popular upgrade to this architecture attaches a generative adversarial network to the decoder. A GAN trains a generator, in this case the decoder, and a discriminator in an adversarial relationship. The generator creates new images from the latent representation of the source material, while the discriminator attempts to determine whether or not the image is generated.[citation needed] This causes the generator to create images that mimic reality extremely well as any defects would be caught by the discriminator.[74] Both algorithms improve constantly in a zero sum game. This makes deepfakes difficult to combat as they are constantly evolving; any time a defect is determined, it can be corrected.[74] Digital clones of professional actors have appeared in films before, and progress in deepfake technology is expected to further the accessibility and effectiveness of such clones.[75] The use of AI technology was a major issue in the 2023 SAG-AFTRA strike, as new techniques enabled the capability of generating and storing a digital likeness to use in place of actors.[76] Disney has improved their visual effects using high-resolution deepfake face swapping technology.[77] Disney improved their technology through progressive training programmed to identify facial expressions, implementing a face-swapping feature, and iterating in order to stabilize and refine the output.[77] This high-resolution deepfake technology saves significant operational and production costs.[78] Disney's deepfake generation model can produce AI-generated media at a 1024 x 1024 resolution, as opposed to common models that produce media at a 256 x 256 resolution.[78] The technology allows Disney to de-age characters or revive deceased actors.[79] Similar technology was initially used by fans to unofficially insert faces into existing media, such as  overlaying Harrison Ford's young face onto Han Solo's face in Solo: A Star Wars Story.[80] Disney used deepfakes for the characters of Princess Leia in Rogue One and Luke Skywalker in both The Mandalorian and The Book of Boba Fett.[81][82] The 2020 documentary Welcome to Chechnya used deepfake technology to obscure the identity of the people interviewed, so as to protect them from retaliation.[83] Creative Artists Agency has developed a facility to capture the likeness of an actor \"in a single day\", to develop a digital clone of the actor, which would be controlled by the actor or their estate alongside other personality rights.[84] Companies which have used digital clones of professional actors in advertisements include Puma, Nike and Procter & Gamble.[85] Deepfakes allowed for the use of David Beckham in a campaign using nearly nine languages to raise awareness the fight against Malaria.[86] In the 2024 Indian Tamil science fiction action thriller The Greatest of All Time, the teenage version of Vijay's character Jeevan is portrayed by Ayaz Khan. Vijay's teenage face was then attained by AI deepfake.[87] Deepfakes are also being used in education and media to create realistic videos and interactive content, which offer new ways to engage audiences. In March 2018 the multidisciplinary artist Joseph Ayerle published the video artwork Un'emozione per sempre 2.0 (English title: The Italian Game). The artist worked with Deepfake technology to create an AI actor, a synthetic version of 80s movie star Ornella Muti, traveling in time from 1978 to 2018. The Massachusetts Institute of Technology referred this artwork in the study \"Collective Wisdom\".[88] The artist used Ornella Muti's time travel to explore generational reflections, while also investigating questions about the role of provocation in the world of art.[89] For the technical realization Ayerle used scenes of photo model Kendall Jenner. The program replaced Jenner's face by an AI calculated face of Ornella Muti. As a result, the AI actor has the face of the Italian actor Ornella Muti and the body of Kendall Jenner. Deepfakes have been widely used in satire or to parody celebrities and politicians. The 2020 webseries Sassy Justice, created by Trey Parker and Matt Stone, heavily features the use of deepfaked public figures to satirize current events and raise awareness of deepfake technology.[90] Deepfakes can be used to generate blackmail materials that falsely incriminate a victim. A report by the American Congressional Research Service warned that deepfakes could be used to blackmail elected officials or those with access to classified information for espionage or influence purposes.[91] When or if fakes cannot reliably be distinguished from genuine evidence, victims who are blackmailed over digital evidence might claim that true artifacts are fakes, thereby seeking plausible deniability by relying on an argument of indistinguishability between fake and genuine evidence. The hoped-for effect is to void credibility of certain existing blackmail materials, which, if they were the sole evidence retained by a blackmailer and could not be distinguished by a jury from fake evidence under this argument, could in theory erode loyalty to blackmailers and limit their control over the blackmailed. This phenomenon has been termed \"blackmail inflation\", since in theory it \"devalues\" authentic blackmail material.[92] It is possible to utilize commodity GPU hardware with a small software program to generate fake content intended to blackmail anyone for whom an adversary has ample training data.[93] However, even carefully manipulated fakes may still be detected. The inflation argument could only work in theory if blackmailers have no other incriminating evidence that is not easily faked, and if the jury were persuaded that the evidences of guilt were not sufficient to convict beyond reasonable doubt. In reality this theory risks double hazards, namely, that those who are guilty might deploy arguments of plausible deniability, arguing that the footage has been faked, possibly resulting in acquittal of a guilty person on the basis of such doubt, and second, that fake evidence might be used to prosecute those who are not aware of the deepfake argument, to secure a conviction in cases where a jury is not adequately aware of the risk of false positives due to fake evidence, or to extort a plea deal where the prosecution claims to have damning evidence. The effect of this double hazard will depend on the level of discernment of the parties in the criminal justice system and their empowerment to act on that discernment. The inflation argument could be abused in either direction as illustrated, and the notion that blackmailers would not retain further evidence or leverage is unlikely and undependable, limiting the effectiveness of the theory. The existence of efficient techniques for fabricating false evidence certainly suggests that any combination of video, audio, photographic or other generable evidence alone as the basis for conviction of a crime is by now a perilous and tenuous standard owing to the possibility of maliciously fabricated evidence, raising the importance of multiple firsthand witnesses to a crime, especially for more serious allegations. On June 8, 2022,[94] Daniel Emmet, a former AGT contestant, teamed up with the AI startup[95][96] Metaphysic AI, to create a hyperrealistic deepfake to make it appear as Simon Cowell. Cowell, notoriously known for severely critiquing contestants,[97] was on stage interpreting \"You're The Inspiration\" by Chicago. Emmet sang on stage as an image of Simon Cowell emerged on the screen behind him in flawless synchronicity.[98] On August 30, 2022, Metaphysic AI had 'deep-fake' Simon Cowell, Howie Mandel and Terry Crews singing opera on stage.[99] On September 13, 2022, Metaphysic AI performed with a synthetic version of Elvis Presley for the finals of America's Got Talent.[100] The MIT artificial intelligence project 15.ai has been used for content creation for multiple Internet fandoms, particularly on social media.[101][102][103] In 2023 the bands ABBA and KISS partnered with Industrial Light & Magic and Pophouse Entertainment to develop deepfake avatars capable of performing virtual concerts.[104] Fraudsters and scammers make use of deepfakes to trick people into fake investment schemes, financial fraud, cryptocurrencies, sending money, and following endorsements. The likenesses of celebrities and politicians have been used for large-scale scams, as well as those of private individuals, which are used in spearphishing attacks. According to the Better Business Bureau, deepfake scams are becoming more prevalent.[105] These scams are responsible for an estimated $12 billion in fraud losses globally.[106] According to a recent report these numbers are expected to reach $40 Billion over the next three years.[106] Fake endorsements have misused the identities of celebrities like Taylor Swift,[107][105] Tom Hanks,[108] Oprah Winfrey,[109] and Elon Musk;[110] news anchors[111] like Gayle King[108] and Sally Bundock;[112] and politicians like Lee Hsien Loong[113] and Jim Chalmers.[114][115] Videos of them have appeared in online advertisements on YouTube, Facebook, and TikTok, who have policies against synthetic and manipulated media.[116][107][117] Ads running these videos are seen by millions of people. A single Medicare fraud campaign had been viewed more than 195 million times across thousands of videos.[116][118] Deepfakes have been used for: a fake giveaway of Le Creuset cookware for a \"shipping fee\" without receiving the products, except for hidden monthly charges;[107] weight-loss gummies that charge significantly more than what was said;[109] a fake iPhone giveaway;[107][117] and fraudulent get-rich-quick,[110][119] investment,[120] and cryptocurrency schemes.[113][121] Many ads pair AI voice cloning with \"decontextualized video of the celebrity\" to mimic authenticity. Others use a whole clip from a celebrity before moving to a different actor or voice.[116] Some scams may involve real-time deepfakes.[117] Celebrities have been warning people of these fake endorsements, and to be more vigilant against them.[105][107][109] Celebrities are unlikely to file lawsuits against every person operating deepfake scams, as \"finding and suing anonymous social media users is resource intensive,\" though cease and desist letters to social media companies work in getting videos and ads taken down.[122] Audio deepfakes have been used as part of social engineering scams, fooling people into thinking they are receiving instructions from a trusted individual.[123] In 2019, a U.K.-based energy firm's CEO was scammed over the phone when he was ordered to transfer €220,000 into a Hungarian bank account by an individual who reportedly used audio deepfake technology to impersonate the voice of the firm's parent company's chief executive.[124][125] As of 2023, the combination advances in deepfake technology, which could clone an individual's voice from a recording of a few seconds to a minute, and new text generation tools, enabled automated impersonation scams, targeting victims using a convincing digital clone of a friend or relative.[126] Audio deepfakes can be used to mask a user's real identity. In online gaming, for example, a player may want to choose a voice that sounds like their in-game character when speaking to other players. Those who are subject to harassment, such as women, children, and transgender people, can use these \"voice skins\" to hide their gender or age.[127] In 2020, an internet meme emerged utilizing deepfakes to generate videos of people singing the chorus of \"Baka Mitai\" (ばかみたい), a song from the game Yakuza 0 in the video game series Like a Dragon. In the series, the melancholic song is sung by the player in a karaoke minigame. Most iterations of this meme use a 2017 video uploaded by user Dobbsyrules, who lip syncs the song, as a template.[128][129] Deepfakes have been used to misrepresent well-known politicians in videos. In 2017, Deepfake pornography prominently surfaced on the Internet, particularly on Reddit.[156] As of 2019, many deepfakes on the internet feature pornography of female celebrities whose likeness is typically used without their consent.[157] A report published in October 2019 by Dutch cybersecurity startup Deeptrace estimated that 96% of all deepfakes online were pornographic.[158]\nAs of 2018, a Daisy Ridley deepfake first captured attention,[156] among others.[159][160][161] As of October 2019, most of the deepfake subjects on the internet were British and American actors.[157] However, around a quarter of the subjects are South Korean, the majority of which are K-pop stars.[157][162] In June 2019, a downloadable Windows and Linux application called DeepNude was released that used neural networks, specifically generative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing $50.[163][164] On 27 June the creators removed the application and refunded consumers.[165] Female celebrities are often a main target when it comes to deepfake pornography. In 2023, deepfake porn videos appeared online of Emma Watson and Scarlett Johansson in a face swapping app.[166] In 2024, deepfake porn images circulated online of Taylor Swift.[167] Academic studies have reported that women, LGBT people and people of colour (particularly activists, politicians and those questioning power) are at higher risk of being targets of promulgation of deepfake pornography.[168] Deepfakes have begun to see use in popular social media platforms, notably through Zao, a Chinese deepfake app that allows users to substitute their own faces onto those of characters in scenes from films and television shows such as Romeo + Juliet and Game of Thrones.[169] The app originally faced scrutiny over its invasive user data and privacy policy, after which the company put out a statement claiming it would revise the policy.[17] In January 2020 Facebook announced that it was introducing new measures to counter this on its platforms.[170] The Congressional Research Service cited unspecified evidence as showing that foreign intelligence operatives used deepfakes to create social media accounts with the purposes of recruiting individuals with access to classified information.[91] In 2021, realistic deepfake videos of actor Tom Cruise were released on TikTok, which went viral and garnered more than tens of millions of views. The deepfake videos featured an \"artificial intelligence-generated doppelganger\" of Cruise doing various activities such as teeing off at the golf course, showing off a coin trick, and biting into a lollipop. The creator of the clips, Belgian VFX Artist Chris Umé,[171] said he first got interested in deepfakes in 2018 and saw the \"creative potential\" of them.[172][173] Deepfake photographs can be used to create sockpuppets, non-existent people, who are active both online and in traditional media. A deepfake photograph appears to have been generated together with a legend for an apparently non-existent person named Oliver Taylor, whose identity was described as a university student in the United Kingdom. The Oliver Taylor persona submitted opinion pieces in several newspapers and was active in online media attacking a British legal academic and his wife, as \"terrorist sympathizers.\" The academic had drawn international attention in 2018 when he commenced a lawsuit in Israel against NSO, a surveillance company, on behalf of people in Mexico who alleged they were victims of NSO's phone hacking technology. Reuters could find only scant records for Oliver Taylor and \"his\" university had no records for him. Many experts agreed that the profile photo is a deepfake. Several newspapers have not retracted articles attributed to him or removed them from their websites. It is feared that such techniques are a new battleground in disinformation.[174] Collections of deepfake photographs of non-existent people on social networks have also been deployed as part of Israeli partisan propaganda. The Facebook page \"Zionist Spring\" featured photos of non-existent persons along with their \"testimonies\" purporting to explain why they have abandoned their left-leaning politics to embrace right-wing politics, and the page also contained large numbers of posts from Prime Minister of Israel Benjamin Netanyahu and his son and from other Israeli right wing sources. The photographs appear to have been generated by \"human image synthesis\" technology, computer software that takes data from photos of real people to produce a realistic composite image of a non-existent person. In much of the \"testimonies,\" the reason given for embracing the political right was the shock of learning of alleged incitement to violence against the prime minister. Right wing Israeli television broadcasters then broadcast the \"testimonies\" of these non-existent people based on the fact that they were being \"shared\" online. The broadcasters aired these \"testimonies\" despite being unable to find such people, explaining \"Why does the origin matter?\" Other Facebook fake profiles—profiles of fictitious individuals—contained material that allegedly contained such incitement against the right wing prime minister, in response to which the prime minister complained that there was a plot to murder him.[175][176] Though fake photos have long been plentiful, faking motion pictures has been more difficult, and the presence of deepfakes increases the difficulty of classifying videos as genuine or not.[130] AI researcher Alex Champandard has said people should know how fast things can be corrupted with deepfake technology, and that the problem is not a technical one, but rather one to be solved by trust in information and journalism.[130] Computer science associate professor Hao Li of the University of Southern California states that deepfakes created for malicious use, such as fake news, will be even more harmful if nothing is done to spread awareness of deepfake technology.[177] Li predicted that genuine videos and deepfakes would become indistinguishable in as soon as half a year, as of October 2019, due to rapid advancement in artificial intelligence and computer graphics.[177] Former Google fraud czar Shuman Ghosemajumder has called deepfakes an area of \"societal concern\" and said that they will inevitably evolve to a point at which they can be generated automatically, and an individual could use that technology to produce millions of deepfake videos.[178] A primary pitfall is that humanity could fall into an age in which it can no longer be determined whether a medium's content corresponds to the truth.[130][179] Deepfakes are one of a number of tools for disinformation attack, creating doubt, and undermining trust. They have a potential to interfere with democratic functions in societies, such as identifying collective agendas, debating issues, informing decisions, and solving problems though the exercise of political will.[180] People may also start to dismiss real events as fake.[127] Deepfakes possess the ability to damage individual entities tremendously.[181] This is because deepfakes are often targeted at one individual, and/or their relations to others in hopes to create a narrative powerful enough to influence public opinion or beliefs. This can be done through deepfake voice phishing, which manipulates audio to create fake phone calls or conversations.[181] Another method of deepfake use is fabricated private remarks, which manipulate media to convey individuals voicing damaging comments.[181] The quality of a negative video or audio does not need to be that high. As long as someone's likeness and actions are recognizable, a deepfake can hurt their reputation.[127] In September 2020 Microsoft made public that they are developing a Deepfake detection software tool.[182] Detecting fake audio is a highly complex task that requires careful attention to the audio signal in order to achieve good performance. Using deep learning, preprocessing of feature design and masking augmentation have been proven effective in improving performance.[183] Most of the academic research surrounding deepfakes focuses on the detection of deepfake videos.[184] One approach to deepfake detection is to use algorithms to recognize patterns and pick up subtle inconsistencies that arise in deepfake videos.[184] For example, researchers have developed automatic systems that examine videos for errors such as irregular blinking patterns of lighting.[185][15] This approach has been criticized because deepfake detection is characterized by a \"moving goal post\" where the production of deepfakes continues to change and improve as algorithms to detect deepfakes improve.[184] In order to assess the most effective algorithms for detecting deepfakes, a coalition of leading technology companies hosted the Deepfake Detection Challenge to accelerate the technology for identifying manipulated content.[186] The winning model of the Deepfake Detection Challenge was 65% accurate on the holdout set of 4,000 videos.[187] A team at Massachusetts Institute of Technology published a paper in December 2021 demonstrating that ordinary humans are 69–72% accurate at identifying a random sample of 50 of these videos.[188] A team at the University of Buffalo published a paper in October 2020 outlining their technique of using reflections of light in the eyes of those depicted to spot deepfakes with a high rate of success, even without the use of an AI detection tool, at least for the time being.[189] In the case of well-documented individuals such as political leaders, algorithms have been developed to distinguish identity-based features such as patterns of facial, gestural, and vocal mannerisms and detect deep-fake impersonators.[190] Another team led by Wael AbdAlmageed with Visual Intelligence and Multimedia Analytics Laboratory (VIMAL) of the Information Sciences Institute at the University Of Southern California developed two generations[191][192] of deepfake detectors based on convolutional neural networks. The first generation[191] used recurrent neural networks to spot spatio-temporal inconsistencies to identify visual artifacts left by the deepfake generation process. The algorithm achieved 96% accuracy on FaceForensics++, the only large-scale deepfake benchmark available at that time. The second generation[192] used end-to-end deep networks to differentiate between artifacts and high-level semantic facial information using two-branch networks. The first branch propagates colour information while the other branch suppresses facial content and amplifies low-level frequencies using Laplacian of Gaussian (LoG). Further, they included a new loss function that learns a compact representation of bona fide faces, while dispersing the representations (i.e. features) of deepfakes. VIMAL's approach showed state-of-the-art performance on FaceForensics++ and Celeb-DF benchmarks, and on March 16, 2022 (the same day of the release), was used to identify the deepfake of Volodymyr Zelensky out-of-the-box without any retraining or knowledge of the algorithm with which the deepfake was created. [citation needed] Other techniques suggest that blockchain could be used to verify the source of the media.[193] For instance, a video might have to be verified through the ledger before it is shown on social media platforms.[193] With this technology, only videos from trusted sources would be approved, decreasing the spread of possibly harmful deepfake media.[193] Digitally signing of all video and imagery by cameras and video cameras, including smartphone cameras, was suggested to fight deepfakes.[194] That allows tracing every photograph or video back to its original owner that can be used to pursue dissidents.[194] One easy way to uncover deepfake video calls consists in asking the caller to turn sideways.[195] Legal experts are actively questioning whether current and emerging regulatory frameworks adequately balance the advancements in deepfake detection with the protection of individual rights. Relevant legislation being scrutinized includes the EU AI Act, the General Data Protection Regulation (GDPR), the Digital Services Act in the European Union, as well as the fragmented state and federal laws in the United States, the Online Safety Act 2023 in the United Kingdom, and China’s Administrative Provisions on Deep Synthesis in Internet-Based Information Services (commonly known as the Deep Synthesis Provisions). Scholars are evaluating if these frameworks effectively address the complex interplay between technology, rights, and responsibilities in the context of deepfakes.[196] Henry Ajder who works for Deeptrace, a company that detects deepfakes, says there are several ways to protect against deepfakes in the workplace. Semantic passwords or secret questions can be used when holding important conversations. Voice authentication and other biometric security features should be up to date. Educate employees about deepfakes.[127] Due to the capability of deepfakes to fool viewers and believably mimic a person, research has indicated that the concept of truth through observation cannot be fully relied on.[197] Additionally, literacy of the technology among populations could be called into question due to the relatively new success of convincing deepfakes.[197] When combined with increasing ease of access to the technology, this has led to the concern amongst some experts that some societies are not prepared to interact with deepfakes organically without potential consequences from sharing misinformation and disinformation.[197] Media literacy has been considered as a potential counter to \"prime\" a viewer to identify a deepfake when they encounter one organically by engendering critical thinking.[197] While media literacy education can have conflicting results in the overall success in detecting deepfakes,[198] research has indicated that critical thinking and a skeptical outlook toward a presented piece of media are effective at assisting an individual in determining a deepfake.[198][199] Media literacy frameworks promote critical analysis of media and the motivations behind the presentation of the associated content. Media literacy shows promise as a potential cognitive countermeasure when interacting with malicious deepfakes.[198] In March 2024, a video clip was released by Buckingham Palace announcing that Kate Middleton had cancer and was undergoing chemotherapy. The appearance of a ring worn by Middleton in the clip fueled rumors that the clip was a deepfake.[200] Johnathan Perkins, UCLA's Director of Race and Equity, doubted Middleton had cancer, and further speculated that she could be in critical condition or dead.[201] Recently, the use of deepfakes has inspired research on deepfake's capability and effects when used in disinformation campaigns. This capability has raised concerns, partly due to the potential of deepfakes to circumvent a person's skepticism and influence their views on an issue.[202][179] Due to the continued advancement in technology that improves deceptive capabilities of deepfakes, some scholars believe that deepfakes could pose a significant threat to democratic societies.[203] Studies have investigated the effects of political deepfakes.[202][203][179] In two separate studies focusing on Dutch participants, it was found that deepfakes have varying effects on an audience. As a tool of disinformation, deepfakes did not necessarily produce stronger reactions or shifts in viewpoints than traditional textual disinformation.[202]  However, deepfakes did produce a reassuring effect on individuals who held preconceived notions that aligned with the viewpoint promoted by the deepfake disinformation in the study.[202] Additionally, deepfakes are effective when designed to target a specific demographic segment related to a particular issue.[203] \"Microtargeting\" involves understanding nuanced political issues of a specific demographic to create a targeted deepfake. The targeted deepfake is then used to connect with and influence the viewpoint of that demographic. Targeted deepfakes were found to be notably effective by the researchers.[203] Research has also found that the political effects of deepfakes are not necessarily as straightforward or assured. Researchers in the United Kingdom uncovered that deepfake political disinformation does not have a guaranteed effect on populations beyond indications that it may sow distrust or uncertainty in a source that provides the deepfake.[179] The implications of distrust in sources led researchers to conclude that deepfakes may have outsized effect in a \"low-trust\" information environment where public institutions are not trusted by the public.[179] Across the world, there are key instances where deepfakes have been used to misrepresent well-known politicians and other public figures. Chat site Discord has taken action against deepfakes in the past,[232] and has taken a general stance against deepfakes.[233][234] Gfycat began removing all deepfakes from its site on January 31, 2018.[235][233] Reddit banned the r/deepfakes subreddit on February 7, 2018, due to the policy violation of \"involuntary pornography\".[236][237][238][239][240] That same month, representatives from Twitter stated that they would suspend accounts suspected of posting non-consensual deepfake content.[241] In February 2018, Pornhub said that it would ban deepfake videos on its website because it is considered \"non consensual content\" which violates their terms of service.[242] They also stated previously to Mashable that they will take down content flagged as deepfakes.[243] Writers from Motherboard reported that searching \"deepfakes\" on Pornhub still returned multiple recent deepfake videos.[242] Google added \"involuntary synthetic pornographic imagery\" to its ban list in September 2018, allowing anyone to request the block of results showing their fake nudes.[244][check quotation syntax] In May 2022, Google officially changed the terms of service for their Jupyter Notebook colabs, banning the use of their colab service for the purpose of creating deepfakes.[245] This came a few days after the publication of a VICE article in which its author, Emanuel Maiberg, reported \"Most deepfakes are non-consensual porn\", and that the main use of popular deepfake software DeepFaceLab (DFL), \"the most important technology powering the vast majority of this generation of deepfakes\", which often was used in combination with Google colabs, was to create non-consensual pornography. Maiberg pointed to the fact that among many other well-known examples of third-party DFL implementations, such as deepfakes commissioned by The Walt Disney Company, official music videos, and web series Sassy Justice by the creators of South Park, DFL's GitHub page, also linked to deepfake porn website Mr.‍Deepfakes and participants of the DFL Discord server also participate on Mr.‍Deepfakes.[246] Facebook has previously stated that they would not remove deepfakes from their platforms.[247] The videos will instead be flagged as fake by third-parties and then have a lessened priority in user's feeds.[248] This response was prompted in June 2019 after a deepfake featuring a 2016 video of Mark Zuckerberg circulated on Facebook and Instagram.[247] Subsequently, Facebook has taken efforts towards encouraging the creation of deepfakes in order to develop state of the art deepfake detection software. Facebook was the prominent partner in hosting the Deepfake Detection Challenge (DFDC), held December 2019, to 2114 participants who generated more than 35,000 models.[249] The top performing models with the highest detection accuracy were analyzed for similarities and differences; these findings are areas of interest in further research to improve and refine deepfake detection models.[249] Facebook has also detailed that the platform will be taking down media generated with artificial intelligence used to alter an individual's speech.[250] However, media that has been edited to alter the order or context of words in one's message would remain on the site but be labeled as false, since it was not generated by artificial intelligence.[250] Twitter (now known as X) is taking active measures to handle synthetic and manipulated media on their platform. In order to prevent disinformation from spreading, Twitter is placing a notice on tweets that contain manipulated media and/or deepfakes that signal to viewers that the media is manipulated.[251] There will also be a warning that appears to users who plan on retweeting, liking, or engaging with the tweet.[251] Twitter will also work to provide users a link next to the tweet containing manipulated or synthetic media that links to a Twitter Moment or credible news article on the related topic—as a debunking action.[251] Twitter also has the ability to remove any tweets containing deepfakes or manipulated media that may pose a harm to users' safety.[251] In order to better improve Twitter's detection of deepfakes and manipulated media, Twitter asked users who are interested in partnering with them to work on deepfake detection solutions to fill out a form.[252] In August 2024, the secretaries of state of Minnesota, Pennsylvania, Washington, Michigan and New Mexico penned an open letter to X owner Elon Musk urging modifications to its AI chatbot Grok's new text-to-video generator, added in August 2024, stating that it had disseminated election misinformation.[253][254][255] In the United States, there have been some responses to the problems posed by deepfakes. In 2018, the Malicious Deep Fake Prohibition Act was introduced to the US Senate;[256] in 2019, the Deepfakes Accountability Act was introduced in the 116th United States Congress by U.S. representative for New York's 9th congressional district Yvette Clarke.[257] Several states have also introduced legislation regarding deepfakes, including Virginia,[258] Texas, California, and New York;[259] charges as varied as identity theft, cyberstalking, and revenge porn have been pursued, while more comprehensive statutes are urged.[244] Among U.S. legislative efforts, on 3 October 2019, California governor Gavin Newsom signed into law Assembly Bills No. 602 and No. 730.[260][261] Assembly Bill No. 602 provides individuals targeted by sexually explicit deepfake content made without their consent with a cause of action against the content's creator.[260] Assembly Bill No. 730 prohibits the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election.[261] U.S. representative Yvette Clarke introduced H.R. 5586: Deepfakes Accountability Act into the 118th United States Congress on September 20, 2023 in an effort to protect national security from threats posed by deepfake technology.[262] U.S. representative María Salazar introduced H.R. 6943: No AI Fraud Act into the 118th United States Congress on January 10, 2024, to establish specific property rights of individual physicality, including voice.[263] In November 2019, China announced that deepfakes and other synthetically faked footage should bear a clear notice about their fakeness starting in 2020. Failure to comply could be considered a crime the Cyberspace Administration of China stated on its website.[264] The Chinese government seems to be reserving the right to prosecute both users and online video platforms failing to abide by the rules.[265] The Cyberspace Administration of China, the Ministry of Industry and Information Technology, and the Ministry of Public Security jointly issued the Provision on the Administration of Deep Synthesis Internet Information Service in November 2022.[266] China's updated Deep Synthesis Provisions (Administrative Provisions on Deep Synthesis in Internet-Based Information Services) went into effect in January 2023.[267] In the United Kingdom, producers of deepfake material could be prosecuted for harassment, but deepfake production was not a specific crime[268] until 2023, when the Online Safety Act was passed, which made deepfakes illegal; the UK plans to expand the Act's scope to criminalize deepfakes created with \"intention to cause distress\" in 2024.[269][270] In Canada, in 2019, the Communications Security Establishment released a report which said that deepfakes could be used to interfere in Canadian politics, particularly to discredit politicians and influence voters.[271][272] As a result, there are multiple ways for citizens in Canada to deal with deepfakes if they are targeted by them.[273] In February 2024, bill C-63 was tabled in the 44th Canadian Parliament in order to enact the Online Harms Act, which would amend Criminal Code, and other Acts. An earlier version of the Bill, C-36, was ended by the dissolution of the 43rd Canadian Parliament in September 2021.[274][275] In India, there are no direct laws or regulation on AI or deepfakes, but there are provisions under the Indian Penal Code and Information Technology Act 2000/2008, which can be looked at for legal remedies, and the new proposed Digital India Act will have a chapter on AI and deepfakes in particular, as per the MoS Rajeev Chandrasekhar.[276] In Europe, the European Union's 2024 Artificial Intelligence Act (AI Act) takes a risk-based approach to regulating AI systems, including deepfakes. It establishes categories of \"unacceptable risk,\" \"high risk,\" \"specific/limited or transparency risk\", and \"minimal risk\" to determine the level of regulatory obligations for AI providers and users. However, the lack of clear definitions for these risk categories in the context of deepfakes creates potential challenges for effective implementation. Legal scholars have raised concerns about the classification of deepfakes intended for political misinformation or the creation of non-consensual intimate imagery. Debate exists over whether such uses should always be considered \"high-risk\" AI systems, which would lead to stricter regulatory requirements.[277] In August 2024, the Irish Data Protection Commission (DPC) launched court proceedings against X for its unlawful use of the personal data of over 60 million EU/EEA users, in order to train its AI technologies, such as its chatbot Grok.[278] In 2016, the Defense Advanced Research Projects Agency (DARPA) launched the Media Forensics (MediFor) program which was funded through 2020.[279] MediFor aimed at automatically spotting digital manipulation in images and videos, including Deepfakes.[280][281] In the summer of 2018, MediFor held an event where individuals competed to create AI-generated videos, audio, and images as well as automated tools to detect these deepfakes.[282] According to the MediFor program, it established a framework of three tiers of information—digital integrity, physical integrity and semantic integrity—to generate one integrity score in an effort to enable accurate detection of manipulated media.[283] In 2019, DARPA hosted a \"proposers day\" for the Semantic Forensics (SemaFor) program where researchers were driven to prevent viral spread of AI-manipulated media.[284] DARPA and the Semantic Forensics Program were also working together to detect AI-manipulated media through efforts in training computers to utilize common sense, logical reasoning.[284] Built on the MediFor's technologies, SemaFor's attribution algorithms infer if digital media originates from a particular organization or individual, while characterization algorithms determine whether media was generated or manipulated for malicious purposes.[285] In March 2024, SemaFor published an analytic catalog that offers the public access to open-source resources developed under SemaFor.[286][287] The International Panel on the Information Environment was launched in 2023 as a consortium of over 250 scientists working to develop effective countermeasures to deepfakes and other problems created by perverse incentives in organizations disseminating information via the Internet.[288] Media related to Deepfake at Wikimedia Commons"
  }
]