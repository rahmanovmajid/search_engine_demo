[
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://realpython.com/",
    "title": "Python Tutorials – Real Python",
    "content": "Learn how Python's continue statement works, when to use it, common mistakes to avoid, and what happens under the hood in CPython byte code. Aug 04, 2025\n\nbasics\npython — FREE Email Series — 🐍 Python Tricks 💌 🔒 No spam. Unsubscribe any time. Jul 30, 2025\n\nadvanced\npython Jul 29, 2025\n\nintermediate\npython Jul 28, 2025\n\nintermediate\npython Jul 23, 2025\n\nintermediate\nweb-dev Jul 22, 2025\n\nintermediate\npython Jul 21, 2025\n\nintermediate\npython Jul 16, 2025\n\nintermediate\npython Jul 15, 2025\n\nintermediate\ndata-science\neditors\npython\ntools Jul 14, 2025\n\nbasics\npython Jul 09, 2025\n\nbasics\npython Jul 08, 2025\n\nintermediate\npython Jul 07, 2025\n\ncommunity Jul 02, 2025\n\nintermediate\npython Jul 01, 2025\n\nintermediate\nbest-practices Jun 30, 2025\n\nintermediate\nmachine-learning Jun 25, 2025\n\nbasics\npython Jun 24, 2025\n\nintermediate\ndatabases\ndata-science\npython Jun 23, 2025\n\nbasics\nbest-practices"
  },
  {
    "url": "https://developer.mozilla.org/en-US/docs/Web",
    "title": "Web technology for developers | MDN",
    "content": "Web technology reference for developers Structure of content on the web Code used to describe document style General-purpose scripting language Protocol for transmitting web resources Interfaces for building web applications Developing extensions for web browsers Build web projects usable for all Web technology reference for developers Learn web development Learn web development Learn to structure web content with HTML Learn to style content using CSS Learn to run scripts in the browser Learn to make the web accessible to all A customized MDN experience Get real-time assistance and support All browser compatibility updates at a glance Learn how to use MDN Plus Frequently asked questions about MDN Plus Write, test and share your code Scan a website for free Get real-time assistance and support The open Web presents incredible opportunities for developers. To take full advantage of these technologies, you need to know how to use them. Below you'll find links to our Web technology documentation. The Web Developer Guides provide practical, how-to content to help you use Web technologies for your goals or needs. Tutorials to take you step-by-step through learning HTML, CSS, JavaScript, and Web APIs. Enabling as many people as possible to use websites, even when those people's abilities are limited in some way. Making content as available and interactive as possible, as soon as possible. Protecting users' personal data. Protecting users from data leaks and data theft, side-channel attacks, and attacks such as cross-site scripting, content injection, and click-jacking. Definitions of Web-related terms. JavaScript programming APIs you can use to build apps on the Web. HTML provides the fundamental building blocks for structuring Web documents and apps. Cascading Style Sheets are used to describe the appearance of Web documents and apps. JavaScript is the Web's native programming language. WebAssembly allows programs written in C, C++, Rust, Swift, C#, Go, and more to run on the Web. HTTP is the fundamental Internet protocol for fetching documents, stylesheets, scripts, images, videos, fonts, and other resources over the Web â and for sending data back to Web servers. Formats, codecs, protocols, APIs, and techniques for embedding and streaming video, audio, and image content in Web documents and apps. Scalable Vector Graphics lets you create images that scale smoothly to any size. MathML lets you display complex mathematical notation on the Web. Uniform Resource Identifiers are used by various technologies, including the browser itself via the address bar, to identify resources in various ways. WebDriver is a browser-automation mechanism for remotely controlling a browser by emulating the actions of a real person using the browser. It's widely used for cross-browser testing of Web apps. Web Extensions are a way for you to give users enhanced capabilities in their browsers â for doing things such as blocking ads and other content, customizing the appearance of pages, and more. Web App Manifests let you enable users to install Web apps to their device home screens, with aspects such as portrait/landscape screen orientation and display mode (e.g., full screen) pre-set. Progressive Web Apps provide a user experience similar to native mobile apps. OpenSearch allows a website to describe a search engine for itself, so that a browser or other client application can use that search engine. The Extensible Markup Language is a strict serialization of the Document Object Model. Extensible Stylesheet Language Transformations is an XML-based language used, in conjunction with specialized processing software, for the transformation of XML documents. XPath uses a non-XML syntax to provide a flexible way of addressing (pointing to) different parts of an XML document. It can also be used to test addressed nodes within a document to determine whether they match a pattern or not. EXSLT a set of extensions to XSLT. Documentation for the set of web-developer tools built into Firefox. Documentation for the set of web-developer tools built into Chrome. Documentation for the set of web-developer tools built into Safari. Documentation for the set of web-developer tools built into Edge. This page was last modified on Jul 29, 2025 by MDN contributors. Your blueprint for a better internet. Visit Mozilla Corporationâs not-for-profit parent, the Mozilla Foundation.Portions of this content are Â©1998â2025 by individual mozilla.org contributors. Content available under a Creative Commons license."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#bodyContent",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Main_Page",
    "title": "Wikipedia, the free encyclopedia",
    "content": "2020 Missouri Amendment 2, also called the Medicaid Expansion Initiative, was a ballot measure to amend the Constitution of Missouri to expand Medicaid under the Affordable Care Act. The initiative was on the August 4, 2020, primary ballot and passed with 53.27% of the vote. Following Medicaid expansion initiatives in other states, Republican lawmakers in Nebraska and Utah added work requirements to their states' expansions; supporters aimed to prevent this by proposing state constitutional amendments for future Medicaid expansion initiatives. The measure was supported most in urban areas and opposed in rural areas. After a delay due to a lack of funding from the Missouri General Assembly and resulting litigation, the initiative was slowly implemented in October 2021. Republican lawmakers attempted to roll back the program and add a work requirement through a state constitutional amendment, which failed after the United States Supreme Court prevented its implementation. (Full article...) August 4 The Swedish pop group Tages released six studio albums and 26 singles in their home country during their existence from 1963 to 1970. Their professional career began during the summer of 1964, when they won a contest awarding them a recording contract with Platina Records, an independent record label. Their debut single, \"Sleep Little Girl\", was released in October 1964 and became a large hit in Sweden. The band's debut album, Tages, was released in November 1965, reaching the top 10 of the Finnish Albums Charts. The band's fourth and fifth albums, Contrast and Studio (both 1967), were released by Parlophone, whereas their sixth and final album, The Lilac Years (1969), was released through Fontana Records. The Lilac Years and the band's final three singles were released under the name Blond, which was considered more internationally viable by their management. (Full list...) The Cheat is a 1923 American silent drama film produced by Famous Players–Lasky and distributed by Paramount Pictures. It is a remake of Cecil B. DeMille's 1915 film The Cheat, using the same script by Hector Turnbull and Jeanie MacPherson. The remake stars Pola Negri and was directed by George Fitzmaurice,  and tells the story of Carmelita De Cordoba, a beautiful young South American woman who has been betrothed by her stern father to Don Pablo, whom she despises, and then meets and falls in love with Dudley Drake, a New York City broker. With no known prints of The Cheat remaining, it is considered a lost film, although there is an extant version in novel form, written in the same year as the film by Russell Holman, a Paramount Pictures employee. This color lithograph poster was produced in 1923 by Paramount to promote The Cheat, and depicts Negri as Carmelita with Charles de Rochefort as Claude Mace, an art swindler masquerading as the East Indian prince Rao-Singh. Poster credit: Paramount Pictures; restored by Ezarate Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects: This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Contents",
    "title": "Wikipedia:Contents - Wikipedia",
    "content": "Easily explore Wikipedia using the topic links below. You can also search directly using the search bar. All section headers are clickable for quick navigation. Wikipedia's content is divided into broad subject areas: Topics Types Places, people and times Indices"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Portal:Current_events",
    "title": "Portal:Current events - Wikipedia",
    "content": "Armed conflicts and attacks Business and economy Disasters and accidents International relations Armed conflicts and attacks Disasters and accidents Law and crime Armed conflicts and attacks Disasters and accidents Law and crime Politics and elections Sports Armed conflicts and attacks Business and economy International relations Law and crime Armed conflicts and attacks Arts and culture Disasters and accidents Health and environment International relations Law and crime Politics and elections Science and technology Armed conflicts and attacks Disasters and accidents International relations Science and technology Armed conflicts and attacks Disasters and accidents International relations Law and crime Politics and elections"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:Random",
    "title": "Libbie Janse van Rensburg - Wikipedia",
    "content": "Elizabetha \"Libbie\" Magdalena Janse van Rensburg (born 28 September 1994)[1]  is a South African rugby union and sevens player.[2][3] Janse van Rensburg made her debut for the Springbok Women in 2021. In a test match against Namibia, she scored two tries and added 14 conversions for a personal contribution of 38 points – a new Test record for the Springbok Women.[4] In 2022 she was selected for the 2021 World Cup squad. In South Africa's second game against Fiji, they were awarded a penalty in the 79th minute, Janse van Rensburg kicked and South Africa went ahead 17–14. Fiji, would retain the restart and score a try to win the game.[5] In 2023, Janse van Rensburg was named as a part of the first professional women's team in South Africa, the Bulls Daisies.[6] She was the vice-captain in their 2023 Women's Premier Division win.[7][8] In April 2023, it was announced that Janse van Rensburg would join the sevens team ahead of the 2023 World Rugby Sevens Challenger Series. South Africa won the series and earned promotion to the SVNS, Janse van Rensburg scoring the game-winning try in the final against Belgium.[9][10] Janse van Rensburg played in WXV 2 for the XV team, where she was the top try scorer and top point scorer, as South Africa finished third.[11] She joined the sevens team for the 2023 Dubai Sevens, where she sustained an injury.[12] In 2024, Janse van Rensburg was named as the 2023 SA Rugby Women's player of the year.[13] Following her recovery, Janse van Rensburg joined the team for the 2024 Spain Sevens.[14] She was a member of the South African side that competed at the 2024 Summer Olympics in Paris.[15][16]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:About",
    "title": "Wikipedia:About - Wikipedia",
    "content": "Wikipedia is a free online encyclopedia that anyone can edit, and millions already have. Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information. Written collaboratively by volunteers known as Wikipedians, Wikipedia articles can be edited by anyone with Internet access, except in limited cases in which editing is restricted to prevent disruption or vandalism. Since its creation on January 15, 2001, it has grown into the world's largest reference website, attracting over a billion visitors each month. Wikipedia currently has more than sixty-five million articles in more than 300 languages, including 7,034,172 articles in English, with 107,267 active contributors in the past month. Wikipedia's fundamental principles are summarized in its five pillars. While the Wikipedia community has developed many policies and guidelines, new editors do not need to be familiar with them before they start contributing. Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies, including being verifiable by published reliable sources. Contributions based on personal opinions, beliefs, or personal experiences, unreviewed research, libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits. Wikipedia differs from printed references in important ways. Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism. Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes. For over 24 years, editors have volunteered their time and talents to create history's most comprehensive encyclopedia while providing references and other resources to researchers worldwide (see Researching with Wikipedia). In summary, Wikipedia has tested the wisdom of the crowd since 2001 and has found that it succeeds. To start editing simply click the Edit or Edit source button, or the pencil icon , at the top of any non-protected Wikipedia page or section."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Contact_us",
    "title": "Wikipedia:Contact us - Wikipedia",
    "content": "Introduction Readers\nHow to report a problem with an article, or find out more information. Article subjects\nProblems with articles about you, your company, or somebody you represent. Licensing\nHow to copy Wikipedia's information, donate your own, or report unlicensed use of your information. Donors\nFind out about the process, how to donate, and information about how your money is spent. Press and partnerships\nIf you're a member of the press looking to contact Wikipedia, or have a business proposal for us. Back to main page Thank you for your interest in contacting Wikipedia. Before proceeding, some important disclaimers: The links on the left should direct you to how to contact us or resolve problems. If you cannot find your issue listed there, you can email helpful, experienced volunteers at info-enwikimedia.org. Please refrain from emailing about disagreements with content; they will not be resolved via email."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Help:Contents",
    "title": "Help:Contents - Wikipedia",
    "content": "This page provides help with the most common questions about Wikipedia. Use the search box below, or browse the Help menu or the Help directory to search Wikipedia's help pages. For interactive assistance related to using and editing Wikipedia, see the help desk and the Teahouse. The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia. For simple searches, there is a search bar at the top of every page. Type what you are looking for in the box. Suggested matches will appear in a dropdown list. Select any page in the list to go to that page. Or, select the \"Search\" button, or press ↵ Enter, to go to a full search result. For advanced searches, see Help:Searching. There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations. For mobile access, press the mobile view link at the very bottom of every desktop view page. Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup. Be bold in improving articles! When adding facts, please provide references so others may verify them. If you are affiliated with the article subject, please see our conflict of interest guideline. The simple guide to vandalism cleanup can help you undo malicious edits. If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox. If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help. If you spot a problem with an article, you can fix it directly, by clicking on the \"Edit\" link at the top of that page. See the \"edit an article\" section of this page for more information. If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a \"Talk\" link at the beginning of every article page. You can contact us. If it's an article about you or your organization, see Contact us – Subjects. Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article. Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations). For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process. Answers to common problems can be found at frequently asked questions. Or check out where to ask questions or make comments. New users should seek help at the Teahouse if they're having problems while editing Wikipedia. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able. Or ask for help on your talk page and a volunteer will visit you there! You can get live help with editing in the help chatroom. For help with technical issues, ask at the Village pump. If searching Wikipedia has not answered your question (for example, questions like \"Which country has the world's largest fishing fleet?\"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need. Articles created without a category should be tagged with a maintenance tag. Use the {{Uncategorized}} tag to put articles in a maintenance category. Optionally add a date parameter like {{Uncategorized|date=August 2025}} to put articles in by-date maintenance categories. Such tagged articles are found at Uncategorized pages. You can try to categorize articles yourself. One useful technique is to follow links in the article to other similar articles and see how they are categorized, so you know what to copy. Search Frequently Asked Questions Search the help desk archives"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Help:Introduction",
    "title": "Help:Introduction - Wikipedia",
    "content": "Wikipedia is made by people like you. Get started\nPolicies and Guidelines Editing\nReferencing\nImages\nTables Editing\nReferencing\nImages\nTables Talk pages\nNavigating WikipediaManual of StyleConclusion View all as single page For more training information, see also:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:Community_portal",
    "title": "Wikipedia:Community portal - Wikipedia",
    "content": "This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory. For a listing of ongoing discussions and current requests, see the Dashboard. Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements. Yearly or infrequent events Monthly or continuous events Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.\nPlease include your signature when adding a listing here. Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available. Weekly highlight Updates for editors Updates for technical contributors Meetings and events Tech news prepared by Tech News writers and posted by bot • Contribute • Translate • Get help • Give feedback • Subscribe or unsubscribe. Discussions in the following areas have requested wider attention via Requests for comment: You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.) Help counter systemic bias by creating new articles on important women. Help improve popular pages, especially those of low quality. This week's article for improvement is: Housing Previous selections:\nHistory of hide materials ·\nHuman behavior ·\nEternal Sunshine of the Spotless Mind This week's backlog of the week is: Category:Wikipedia pages about a contentious topic mislabelled as protected When you create an article through Wikipedia's Articles for Creation process, \nit creates a draft in the Drafts area. The purpose of AfC process is to help new editors learn how to write better articles. If accepted, your draft can be a valuable contribution to the encyclopedia. Wikipedia is over 17 years old and has well over five million articles. The vast majority of those articles never went through AfC which is only a few years old. AfC works as a peer review process in which registered editors can either help create an article submitted or decline the article because it is unsuitable for Wikipedia. To nominate an existing draft or user sandbox for review at Articles for Creation, add the code {{subst:submit}} to the top of the draft or sandbox page. The AfC process allows others to review the draft when you are ready, and also to create the article for you, if it is suitable."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:RecentChanges",
    "title": "Recent changes - Wikipedia",
    "content": "This is a list of recent changes to Wikipedia."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard",
    "title": "Wikipedia:File upload wizard - Wikipedia",
    "content": "Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding. Uploads to Wikimedia Commons Upload a non-free file Uploads locally to the English Wikipedia; must comply with the non-free content criteria You do not have JavaScript enabled Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript. You are not currently logged in. Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again. Your account has not become confirmed yet. Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it. You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there. Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed. Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for # < > [ ] | : { } /  and ~~~. Your filename has been modified to avoid these. Please check if it is okay now. The filename you chose seems to be very short, or overly generic. Please don't use: A file of this name already exists on Commons! If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used. This should not be done, except in very rare exceptional cases. Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead. A file of this name already exists. If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to: It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. Thank you for offering to upload a free work. Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, Wikimedia Commons.\nFiles uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. Please consider uploading your file on Commons. However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here. Please note that by \"entirely self-made\" we really mean just that. Do not use this section for any of the following: Editors who falsely declare such items as their \"own work\" will be blocked from editing. Use this only if there is an explicit licensing statement in the source. The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this. If the source website doesn't say so explicitly, please do not upload the file. Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it. Please remember that you will need to demonstrate that: This file will be used in the following article: Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the \"http://en.wikipedia.org/...\" URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. Example – article okay. This article doesn't exist! The article Example could not be found. Please check the spelling, and make sure you enter the name of an existing article in which you will include this file. If this is an article you are only planning to write, please write it first and upload the file afterwards. This is not an actual encyclopedia article! The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc. Please upload this file only if it is going to be used in an actual article. If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that. This is a disambiguation page! The page Example is not a real article, but a disambiguation page pointing to a number of other pages. Please check and enter the exact title of the actual target article you meant. If neither of these two statements applies, then please do not upload this image. This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like. In view of this, please explain how the use of this file will be minimal. Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then: Please don't upload it. Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader. In particular, please don't upload: If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you. This is the data that will be submitted to upload: Your file is being uploaded. This might take a minute or two, depending on the size of the file and the speed of your internet connection. Once uploading is completed, you will find your new file at this link: File:Example.jpg Your file has been uploaded successfully and can now be found here: File:Example.jpg Please follow the link and check that the image description page has all the information you meant to include. If you want to change the description, just go to the image page, click the \"edit\" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version. To insert this file into an article, you may want to use code similar to the following: If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the \":\" after the initial brackets!): See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages. Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:SpecialPages",
    "title": "Special pages - Wikipedia",
    "content": "This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators). You can also see what message names are used on a page by adding ?uselang=qqx to the end of its URL, e.g. https://en.wikipedia.org/wiki/Special:SpecialPages?uselang=qqx will show (specialpages-summary) in place of this message, which allows you to find MediaWiki:Specialpages-summary. For an index of special pages, see Help:SpecialPages."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:Search",
    "title": "Search - Wikipedia",
    "content": ""
  },
  {
    "url": "https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en",
    "title": "Make your donation now - Wikimedia Foundation",
    "content": "Thank you for considering a donation to the Wikimedia Foundation. We invite you to reflect on the number of times you visited Wikipedia in the last year. If the knowledge you gained here was valuable, please join the 2% of readers who donate. Any amount helps: $5, $20, $50, or whatever feels right to you today. The internet we were promised—a place of free, collaborative, and accessible knowledge—is under constant threat. On Wikipedia, volunteers work together to create and verify the pages you rely on, supported by tools that undo vandalism within minutes, ensuring the information you seek is trustworthy. If Wikipedia has given you useful knowledge this year, please give back. There are no small contributions: every edit counts, every donation counts. Thank you. Technology: Servers, bandwidth, maintenance, development. Wikipedia is one of the top 10 websites in the world, and it runs on a fraction of what other top websites spend. People and Projects: The other top websites have thousands of employees. Wikimedia Foundation has about 700 staff and contractors to support a wide variety of projects, making your donation a great investment in a highly-efficient not-for-profit organization. The Wikimedia Foundation is an international non-profit organization that supports local and independent associations around the world. Our tax-exempt status varies according to the laws of each country. Donations to the Wikimedia Foundation are likely not tax-deductible outside the USA. If you have any questions about tax exemptions or reductions, we invite you to contact donate@wikimedia.org. We do not sell or trade your information to anyone. By donating, you agree to share your personal information with the Wikimedia Foundation, the nonprofit organization that hosts Wikipedia and other Wikimedia projects, and its service providers pursuant to our donor policy. Wikimedia Foundation and its service providers are located in the United States and in other countries whose privacy laws may not be equivalent to your own. For more information please read our donor policy. For recurring donors, fixed monthly payments will be debited by the Wikimedia Foundation on the monthly anniversary of the first donation, until such time as you notify us to discontinue them. Donations initiated on the 29, 30, or 31 of the month will recur on the last day of the month for shorter months, as close to the original date as possible. For questions, please contact donate@wikimedia.org."
  },
  {
    "url": "https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Search+engine",
    "title": "Create account - Wikipedia",
    "content": "edits articles recent contributors"
  },
  {
    "url": "https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Search+engine",
    "title": "Log in - Wikipedia",
    "content": "Login processing now uses our domain auth.wikimedia.org. If you are using blocking software, you will need to allow access to this domain to log in. (technical details)"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:MyContributions",
    "title": "User contributions for 37.61.116.214 - Wikipedia",
    "content": "This user or IP address is currently globally blocked.\nIf the block is marked as locally disabled, this means that it applies on other sites, but a local administrator has decided to disable it on this wiki.\nThe global block log entry is provided below for reference:"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Special:MyTalk",
    "title": "User talk:37.61.125.86 - Wikipedia",
    "content": "People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using. Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address."
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#History",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Pre-1990s",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#1990s:_Birth_of_search_engines",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#2000s–present:_Post_dot-com_bubble",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Approach",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Local_search",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Market_share",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Russia_and_East_Asia",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Search_engine_bias",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Customized_results_and_filter_bubbles",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Religious_search_engines",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Search_engine_submission",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Comparison_to_social_bookmarking",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Technology",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Archie",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Veronica",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#The_Lone_Wanderer",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Excite",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Yahoo!",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Lycos",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Types_of_web_search_engines",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#See_also",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#References",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#Further_reading",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://en.wikipedia.org/wiki/Web_search_engine#External_links",
    "title": "Search engine - Wikipedia",
    "content": "A search engine is a software system that provides hyperlinks to web pages, and other relevant information on the Web in response to a user's query. The user enters a query in a web browser or a mobile app, and the search results are typically presented as a list of hyperlinks accompanied by textual summaries and images. Users also have the option of limiting a search to specific types of results, such as images, videos, or news. For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query are based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, although some content is not accessible to crawlers. There have been many search engines since the dawn of the Web in the 1990s, however, Google Search became the dominant one in the 2000s and has remained so. As of May 2025, according to StatCounter, Google holds approximately 89–90 % of the worldwide search share, with competitors trailing far behind: Bing (~4 %), Yandex (~2.5 %), Yahoo! (~1.3 %), DuckDuckGo (~0.8 %), and Baidu (~0.7 %).[1] Notably, this marks the first time in over a decade that Google's share has fallen below the 90 % threshold. The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google. In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk, which he called a memex.[3] He described this system in an article titled \"As We May Think\" in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5] Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7] The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10] Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12] The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18] In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format. JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered. One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor. The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search. In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his RankDex technology for the Baidu search engine, which was founded by him in China and launched in 2000. In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31] Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[32][33] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[34] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000. Around 2000, Google's search engine rose to prominence.[35] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker. By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot). Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology. As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex. A search engine maintains the following processes in near real time:[36] Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[38] Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are stored in a public database and accessible through web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[37] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[37] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot. Typically, when a user enters a query into a search engine it is a few keywords.[39] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[37] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results. For example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[40] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[37] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases the user searches for. The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[37] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[41] Local search is the process that optimizes the efforts of local businesses. They focus on ensuring consistent search results. It is important because many people determine where they plan to go and what to buy based on their searches.[42] As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[43] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[44] As of late 2023 and early 2024, search engine market shares in Russia and East Asia have remained relatively stable but with some notable shifts due to geopolitical and technological developments. In Russia, Yandex continues to dominate the search engine market with a share of approximately 70.7%, while Google holds around 23.3%.[45] Yandex also remains a key player in localized services including navigation, ride-hailing, and e-commerce, strengthening its ecosystem. In China, Baidu remains the leading search engine with a market share of about 59.3% as of early 2024. Other domestic engines such as Sogou and 360 Search hold smaller shares. Google remains inaccessible in mainland China due to long-standing censorship issues, having exited the Chinese market in 2010 following disputes over censorship and cybersecurity.[46][47] Bing, Microsoft's search engine, has maintained a niche presence in China with a market share around 13.6%, making it one of the few foreign search engines operating under local regulatory constraints.[48] In South Korea, Naver continues to lead the domestic market, with a market share of 59.8%, followed by Google at 35.4% as of Q4 2023.[49] Naver’s strength lies in its localized services and integration with Korean content platforms. In Japan, Google Japan currently holds the largest market share (around 76.2%), while Yahoo! Japan, operated by Z Holdings (a SoftBank and Naver joint venture), retains about 15.8% market share.[50] In Taiwan, Google is the predominant search engine, commanding over 93% of the market, with Yahoo! Taiwan and Bing trailing far behind.[51] Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[52][53] and the underlying assumptions about the technology.[54] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[55] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[56] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[53] Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines,[57] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[58] climate change denial,[59] and conspiracy theories.[60] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[61] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behavior and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[62][63][64] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalization in search,[64] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[65][63] The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[66] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[67] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[68] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[69] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[70] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this. The first web search engine was Archie, created in 1990[71] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.[citation needed] The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: An administrator decides that they want to make files available from their computer. They set up a program on their computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, they connect to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[72] In 1993, the University of Nevada System Computing Services group developed Veronica.[71] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[72] The World Wide Web Wanderer, developed by Matthew Gray in 1993[73] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth. The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[72] Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[72] Excite was the first serious commercial search engine which launched in 1995.[74] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite[75][39] In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites. There is difference in the way various search engines work, but they all perform three basic tasks.[76] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when a user queries a search engine to locate information, they're actually searching through the index that the search engine has created —they are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[77]"
  },
  {
    "url": "https://ady.wikipedia.org/wiki/%D0%98%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82-%D0%BB%D1%8A%D1%8B%D1%85%D1%8A%D1%83%D0%BB%D1%8A%D1%8D",
    "title": "Интернет-лъыхъулъэ - Википедие",
    "content": "Интернет-лъыхъулъэмэ ащыщых:"
  },
  {
    "url": "https://ar.wikipedia.org/wiki/%D9%85%D8%AD%D8%B1%D9%83_%D8%A8%D8%AD%D8%AB",
    "title": "محرك بحث - ويكيبيديا",
    "content": "محرك البحث (بالإنجليزية: web search engine) أو الباحوث [1] هو برنامج حاسوبي مصمم للمساعدة في العثور على مستندات مخزنة على شبكات معلوماتيةالشبكة العنكبوتية العالمية (بالإنجليزية: World Wide Web)) أو على حاسوب شخصي، وتقدم نتائج البحث عادةً على شكل قائمة من النتائج يشار إليها عادةً بـ «صفحات نتائج محرك البحث»، (مختصر إنجليزي: SERPs)، قد تكون المعلومات المقدمة مزيجًا من صفحات ويب وصور وأي نوع آخر من الملفات، تنقب بعض المحركات عن البيانات المتوفرة في قواعد البيانات أو أدلة مواقع الويب، وعلى عكس أدلة المواقع التي يحافظ عليها من خلال محررين بشريين فقط، فإن محركات البحث تحافظ على المعلومات في الزمن الحقيقي من خلال تشغيل خوارزمية على زاحف الشبكة. بنيت محركات البحث الأولى اعتمادا على التقنيات المستعملة في إدارة المكتبات الكلاسيكية. حيث يتم بناء فهارس للمستندات تشكل قاعدة للبيانات تفيد في البحث عن أي معلومة. يسمح محرك البحث للمستخدم أن يطلب المحتوى الذي يقابل معايير محددة (والقاعدة فيها تلك التي تحتوي على كلمة أو عبارة ما) ويستدعي قائمةً بالمراجع توافق تلك المعايير. تستخدم محركات البحث مؤشرات/فهارس/مسارد منتظمة التحديث لتشتغل بسرعة وفعالية. تعرض النتائج على شكل قائمة بعناوين المستندات التي توافق الطلب. يرفق بالعناوين في الغالب مختصر عن المستند المشار إليه أو مقتطف منه للدلالة على موافقته للبحث. وترتب عناصر قائمة البحث وفقًا لمعايير خاصة (قد تختلف من محرك لآخر)، من أهمها مدى موافقة كل عنصر للطلب. عند الحديث عن محركات البحث فغالبا ما يقصد محركات البحث على شبكة الإنترنت ومحركات الوِيب بالخصوص. محركات البحث في الويب تبحث عن المعلومات على الشبكة العنكبوتية العالمية، ومنها ما يستعمل على نطاق ضيق يشمل البحث داخل الشبكات المحلية للمؤسسات أي إنترانت (بالإنجليزية: Intranet). أما محركات البحث الشخصية فتبحث في الحواسيب الشخصية الفردية. تنقب بعض محركات البحث أيضاً في البيانات المتاحة على المجموعات الإخبارية، وقواعد البيانات الضخمة، أو أدلة مواقع الوِب مثل دِموز دوت أورج. إن محركات بحث الإنترنت بحد ذاتها سابقة لظهور الويب في كانون الأول من عام 1991، حيث يعود تاريخ البحث إلى عام 1982 باستخدام طريقة «هوإز»[2]، كما نفذت خدمة بحث معلومات نوبوت للمستخدمين متعددي الشبكات لأول مرة عام 1989.[3] وقد كان أول محرك بحث موثّق يقوم بالبحث في ملفات المحتويات المسماة ملفات بروتوكول نقل الملفات، هو محرك بحث أرشي، حيث قُدم لأول مرة في 10 أيلول من عام 1990. وقد أنشأه طالب في جامعة ماكجيل في مونتريال عام 1990م، وكان يعرف باسم “أرشي” (بالإنجليزية: Archie) – مشتق من كلمة “الأرشيف” مع إزالة حرف “ف”- ويحمل تلقائيا قائمة بكل الملفات المتوافرة على كل موقع إنترنت وكان البحث يتم من خلال هذه القائمة بأسماء الملفات.[4][5]\nوتطورت عملية البحث فظهر محرك باسم واندكس (بالإنجليزية: Wandex)، وهو فهرس جمعه متجول وِب وهو زاحف عنكبوتي (بالإنجليزية: web crawler) طوره ماثيو جراي في معهد ماساشوستس للتكنولوجيا MIT في 1993. ويعد آليوب (بالإنجليزية: Aliweb) محرك بحث آخر مبكر جدًا وقد ظهر في 1993 ويعمل حتى اليوم. وأول محرك بحث قائم على الزاحف العنكبوتي للنصوص الكاملة كان وبكراولر (بالإنجليزية: WebCrawler)، والذي خرج للوجود في 1994. وعلى عكس سابقيه، فقد ترك المستخدمين يبحثون عن أي كلمة على أي صفحة ويب، وهو ما صار القاعدة لكل محركات البحث الكبرى منذ ذلك الوقت. كان هو أيضا الأول في معرفة الجمهور به على نطاق واسع. في 1994 كذلك جاء لايكوس (بالإنجليزية: Lycos) (الذي بدأ في جامعة كارنيغي ميلون (بالإنجليزية: Carnegie Mellon University)) وصار مشروعًا تجاريًا كبيرًا. بعد ذلك بقليل، ظهر العديد من محركات البحث وتزاحمت على الشعبية. وكان من ضمنها إكسايت (بالإنجليزية: Excite)، إنفوسيك (بالإنجليزية: Infoseek)، وإنكتومي (بالإنجليزية: Inktomi)، ونورثرن لايت (بالإنجليزية: Northern Light)، وألتافيستا (بالإنجليزية: Alta Vista). وفي بعض الحالات تنافست مع الأدلة ذات الشعبية مثل ياهو! (بالإنجليزية: !Yahoo). فيما بعد، أدمجت الأدلة أو أضافت إليها تقنية محرك البحث من أجل أداء أكبر للوظائف. عرفت محركات البحث أيضا بكونها بعض ألمع النجوم في نوبة الاستثمار في الإنترنت التي وقعت في أواخر التسعينات. دخلت عدة شركات السوق في مشهد كبير، مسجلة مكاسب قياسية خلال طرح أسهمها العام الافتتاحي. وقد سحب البعض محركاتهم البحثية العامة، وهم يسوقون نسخا للشركات فقط، مثل نورثرن لايت التي كانت من ال8 أو 9 محركات بحث المبكرة بعد أن جاء لايكوس (بالإنجليزية: Lycos). قبل مجيء الإنترنت، كانت هناك بواحيث لموافيق (بروتوكولات)أو استخدامات أخرى، مثل محرك بحث آركي لمواقع إف‌.تي.‌بي المجهولة (بالإنجليزية: anonymous FTP) ومحرك بحث ڤيرونيكا لبروتوكول جوفر. وستأتي بعض محركات البحث الأخرى منها إيه ناين.كوم a9.com (أمازون.كوم)، آسك چيڤيز/تيوما (بالإنجليزية: Ask Jeeves/Teoma)، جيجابلاست، سناپ (بالإنجليزية: Snap)، والهاللو (بالإنجليزية: Walhello)، كازاز (بالإنجليزية: Kazazz)، ووايسنَت (بالإنجليزية: WiseNut). وبعض آخر آخر محركات البحث، والتي تبحث فقط أنواعا محددة من المحتوى هي پلازو Plazoo (لمردود الخلاصات RSS feeds)، وجوهوك GoHook (لملفات پي دي إف PDF بشكل رئيسي). ومن أشهر المحركات أيضا: Yooci و ThroughSearch تعمل محركات البحث عن طريق تخزين المعلومات عن عدد كبير من صفحات الوِب، والتي تستعيدها من الشبكة العالمية وورلد وايد وب نفسها. تستعاد هذه الصفحات بواسطة زاحف وِب (يعرف أحيانا أيضا بـ ’عنكبوت‘) – وهو مستعرض وِب آلي يتبع كل رابط يراه. بعد ذلك يجري تحليل كل صفحة لتحديد كيف ينبغي فهرستها (على سبيل المثال، تستخلص الكلمات من العناوين، رؤوس الموضوعات، أو حقول خاصة تعرف ب ميتا تاجز). تخزن البيانات عن صفحات الوِب في قاعدة بيانات فهرسية للاستخدام في عمليات البحث طلبا لللمعلومات لاحقا. بعض محركات البحث، مثل جوجل، تخزن كل أو بعض الصفحة المصدر (وتشير لها ب مخبوءة) وبالمثل معلومات عن صفحات الوِب، بينما بعضها تخزن كل كلمة من كل صفحة تجدها، مثل ألتاڤيستا. هذه الصفحة المخبوءة تمسك بنص البحث الفعلي بما أنه هو الذي تمت فهرسته فعليا، لذا فقد تكون مفيدة جدا عندما يكون محتوى الصفحة الحالية قد جرى تحديثه ولم تعد ألفاظ البحث فيه. ربما تعتبر هذه المشكلة شكلا خفيفا من تعفن الروابط، وتزيد معالجة جوجل لها من إمكانية الاستخدام بإرضاء توقعات المستخدم بأن ترد ألفاظ البحث في صفحات الوِب العائدة في الرد. وهو ما يرضي ’مبدأ مفاجأة أخف من مفاجأة‘ بما أن المستخدم يتوقع بشكل طبيعي ألفاظ البحث في النتيجة العائدة له. وهذه الصلة بالبحث تجعل هذه الصفحات المخبوءة مفيدة جدا، حتى أكثر من واقع أنها قد تحتوي على بيانات ربما لم تعد متاحة في موضع آخر. عندما يتوجه مستخدم لمحرك البحث ويجري عملية بحث طلبا للمعلومات، كما هو سائد بإعطاء كلمات مفتاحية، يفتش المحرك في الفهرس ويقدم قائمة بصفحات الوِب الأفضل توافقا تبعا لمعاييره، في المعتاد مع ملخص قصير يحتوي على عنوان الوثيقة وأحيانا أجزاء من النص. معظم محركات البحث تدعم استخدام الاصطلاحات البولينية (نسبة للجبر البوليني وهو نوع من المتغيرات المنطقية): AND وOR وNOT لمزيد من تحديد طلب المعلومات. وهناك خدمة وظيفية متقدمة هي البحث بالتقارب، والتي تسمح لك بتحديد المسافة بين الكلمات المفتاحية، باستخدام ألفاظ مثل NEAR، NOT NEAR، FOLLOWED BY، NOT FOLLOWED BY، SENTENCE، FAR. يعتمد مدى فائدة محرك بحث على مدى صلة النتائج التي يرد بها. فبينما قد تكون هناك ملايين صفحات الوِب التي تحتوي على كلمة أو عبارة محددة، قد تكون بعض أوثق صلة، أو أروج، أو معتمدة أكثر من غيرها. معظم محركات البحث توظف أساليب لوضع مراتب النتائج لتقدم أفضل النتائج أولا. الكيفية التي يقرر بها محرك بحث أي الصفحات هي الأفضل توافقا، وما النظام الذي يجب أن تظهر به النتائج، تختلف بشكل شاسع من محرك لآخر. الأساليب أيضا تتغير عبر الزمن بتغير استخدام إنترنت وتكنيكات جديدة تتطور. معظم محركات البحث هي مضاربات تجارية يدعمها عائد إعلاني و، بالنتيجة، يوظف البعض الممارسة المثيرة للجدل بالسماح للمعلنين بدفع النقود ليرفعوا لهم قوائهم في مراتب نتائج البحث. الأغلبية الكاسحة من محركات البحث تديرها شركات خاصة تستخدم خوارزميات ملكها وقواعد بيانات مغلقة، وأكثرها رواجا حاليا هي جوجل وباحث إم⁬إس⁬إن وياهو. توجد تقنية محركات بحث مفتوحة المصدر مثل إتش⁬تي⁬دِج، نتش، سيناز، إيجوثور وأوبن⁬إف⁬تي⁬إس، ولكن ليس هناك خادم بحث وورلد وايد وِب مشاع يستخدم هذه التقنية. جاء تطور محرك بحث الوِب من تطور محركات البحث على شبكات الأجهزة والشبكات الداخلية. في بعض دول شرق آسيا وروسيا ليس محرك بحث جوجل هو الأشهر حيث ان حساباته ومعادلات المحرك للبحث (algorithm) يتم عمل تصفية إقليمية (regional filtering) لها وبالتالي تختفي معظم النواتج.[6]\n.[7] محرك البحث الروسي ياندكس يمتلك 61.9 في المئة من حصة السوق. الموضوع الأصلي يمكننا القول بأن الشبكة ومواقعها لن تكون ذات فائدة كبيرة بالنسبة لنا لو لم تكن محركات البحث على إنترنت موجودة. في البدء كانت محركات البحث عبارة عن أدلاء تقوم بفهرسة مواقع الإنترنت الجديدة. وقد كان ذلك فعالا عندما كان حجم إنترنت يقدر بملايين الصفحات. ثم تطورت إنترنت، وانضم إليها الملايين من مؤسسات الأعمال، والمؤسسات الحكومية، وبلايين الصفحات من أدلة استخدام المنتجات، والمعلومات الخاصة بالمستثمرين، وغير ذلك من المعلومات التي تقوم بتسيير عجلة اقتصاد إنترنت. ومع هذا النمو أصبح من الضروري، بل ومن الحتمي إضافة محرك بحث فعال إلى كافة مواقع إنترنت، يقوم بفهرسة وتصنيف المعلومات الموجودة ضمن هذه المواقع كي تتمكن من خدمة زوارها بشكل فعال.\nواليوم، وبعد أن أصبحت محركات البحث جزءا أساسيا في حضارتنا الإنترنتية، فإن هناك العشرات من الشركات العاملة في مجال إنتاج برمجيات، وتقنيات، وأساليب بحث جديدة موجهة نحو إنترنت وإنترانت. وبسبب الدور المتزايد الذي تلعبه التجارة والأعمال الإلكترونية في اقتصاد اليوم، فإن الحافز المادي على الأقل موجود. ولكن رغم النجاح الذي تدّعي الشركات المنتجة لتقنيات البحث تحقيقه، فإن المستخدمين لا زالوا يشكون من افتقار محركات البحث إلى الدقة المطلوبة، وتلبية النتائج التي يتم تحصيلها لمتطلبات المستخدمين إن مفتاح النجاح في الحصول على نتائج بحث جيدة، تكمن في نوعية الاستفسارات، أو الأسئلة، أو العبارات أو الكلمات المفتاحية التي نقوم بإدخالها في محركات البحث. لكن المشكلة الأساسية هنا تكمن في أن الغالبية العظمى من المستخدمين لا يقومون عادة بإدخال الاستفسارات أو الكلمات المفتاحية الصحيحة، والتي تؤدي إلى الحصول على النتائج المطلوبة، وسنستعرض فيما يلي المشاكل الشائعة في عالم البحث عن المعلومات، والطرق التي يحاول بها الباحثون معالجة هذه المشكلات. من الحقائق الغريبة التي يؤكدها خبراء المعلومات هي أن المستخدمين نادرا ما يقومون بطرح الأسئلة التي تعبر عما يريدونه فعلا. والسبب الرئيس في ذلك هو الافتقار إلى الفهم الصحيح للموضوع قيد البحث، وبالتالي عدم استخدام الكلمات المفتاحية الصحيحة، والتي تؤدي إلى تكوين استعلامات وأسئلة صحيحة. فالمشكلة الأساسية هنا إذا، هي مساعدة المستخدمين على طرح الأسئلة وتكوين الاستعلامات الصحيحة.\nومن مظاهر هذه المشكلة أيضا هي صغر حجم الاستعلامات التي يكونها المستخدم عادة للاستفسار عن موضوع معين. فإذا كان مستخدم ما يريد معلومات عن «السفر» مثلا، فإنه يبدأ بإدخال كلمة عامة في محرك البحث، ومن ثم، واعتمادا على النتائج التي يحصل عليها، يقوم بتضييق نطاق بحثه إلى أن يصل إلى ما يريده. والسبب في أن العديد من مستخدمي إنترنت يستعملون هذا الأسلوب يكمن في أنهم لا يعرفون حقا الحجم المهول للمعلومات الموجودة في قواعد البيانات الخاصة بمحركات البحث، والتي تفوق عادة ما يمكن لأي إنسان التعامل معه. تتميز معظم اللغات الطبيعية بتنوع المفردات التي تتناول نفس المعنى، فمثلاً (السحاب، الغمام، المزن، العارض.. وغيرها) تدور حول نفس المعنى سحاب، وبينما يدرك الإنسان تطابق هذه المفردات في المعنى تعجز محركات البحث التقليدية عن استيعاب كل المرادفات الممكنة، فتفتقر نتائجها إلى كثير من الوثائق ذات العلاقة بموضوع البحث لكنها لم تستخدم نفس مفردات المستخدم، وتسمى هذه المشكلة فجوة المفردات. عند التعامل مع تقنيات البحث فلا بد من الموازنة بين الكم والنوع، أو ما يدعوه الخبراء بالدقة والقدرة على الاسترجاع. وهي علاقة عكسية تماما، فكلما تم تضييق نطاق البحث سعيا عن نتائج أكثر دقة، كلما قل مقدار البيانات الذي يمكن استرجاعه. ولذلك فإن هناك حاجة لوجود محركات بحث تقدم دقة عالية دون التضحية بمقدار النتائج «الدقيقة» التي نسترجعها. معظم الكلمات تحمل أكثر من معنى، ومعظم محركات البحث المستخدمة اليوم تقوم بمطابقة الكلمات وليس معانيها، ولذلك فإن نتائج عمليات البحث التي نحصل عليها، تحتوي غالبا على الكلمات المفتاحية الصحيحة، ولكنها ذات المعنى الخاطئ. فإذا جربت مثلا أن تبحث عن معنى كلمة «جافا» مثلا، وهي إحدى لغات البرمجة الشائعة الاستخدام، فإنك ستحصل الكثير من النتائج المتعلقة بالجفاف، أو باسم جزيرة إندونيسية تحمل الاسم نفسه، إضافة إلى معلومات عن لغة البرمجة لانه يبحث عن البنية الصرفية والشكليه للكلمة وليس معناها. وماذا عن البحث في الأسماء، أي البحث عن معلومات عن الأشخاص والأماكن، وما إلى ذلك. خصوصا وأن الكتاب يغيرون عادة الطريقة التي يقومون بها بكتابة الأسماء. وإذا ما تحدثنا مثلا عن أسماء الشركات، فإنها تتغير باستمرار نتيجة عملية الاندماج والضم، مما يجعل عملية البحث صعبة. وقد يقول البعض أنه يمكن التغلب على هذه المشكلة نوعا ما باستخدام برمجيات الفهرسة، ولكن المشكلة هي أن المعلومات في عالمنا تتغير باستمرار مما يجعل الفهرسة اليدوية صعبة. وماذا عن الفهرسة الآلية؟ الإجابة هي أنه لا توجد بعد التقنية التي يمكنها القيام بذلك بدقة، بحيث يمكن التمييز مثلا بين مقال كامل عن شخص معين، ومقال آخر يذكر اسم الشخص بشكل عابر. يجمع العاملون في مجال محركات البحث بأن الحل لكافة هذه المشاكل يكمن في تصميم محركات البحث، وبرمجيات البحث عموما، بحيث تستفيد من برمجيات المعالجة الطبيعية للغة Natural Language Processingعموما، والمعالجة الطبيعية للمعنى Natural Meaning Processing، والاستفادة من قواعد اللغة.\nالنقطة الأساسية هنا هي أن اللغة عموما مبنية على أساس قواعد ذات أساس شبيه بالمعادلات الرياضية، كما أن عبارات اللغة وكلماتها تكون مبنية حسب هيكلية معينة، فلكل جملة مبتدأ وخبر، أو فعل وفاعل ومفعول به، كما أن الكلمات تأتي عادة من جذور وأصول. وهذه الهيكلية المبنية ضمن اللغة تتناسب وطبيعة عمل برمجيات الحاسوب، والتي تحتاج إلى هيكلية معينة تنفذ على أساسها عملياتها.\nأما المعالجة الطبيعية للمعنى فهي أكثر صعوبة، فكلمة مثل «راحة» يمكن لها أن تعني عدة أشياء، كالارتياح بعد التعب، أو الراحة الأبدية والتي تعني الموت، أو راحة اليد. ولهذا فإن برمجيات البحث يجب أن تتمكن من فهم الكلمة ضمن سياق النص، ودور الكلمة في هذا السياق. ولكن حتى هذا الأسلوب لا يفلح دوما في فهم المعنى. جرب أن تقرأ ديوانا شعريا جيدا، وستجد أن هنالك الكثير من الكلمات الصعبة، والتي قد يحاول البعض فهمها من السياق، ولكننا عندما نعجز عن ذلك فإننا نتجه إلى القاموس. وهذا أسلوب يمكن لبرمجيات الحاسوب اتباعه، أي الاعتماد على قاموس أو فهرس بالكلمات والعبارات الشائعة الاستخدام، والبحث في السياق.\nوفيما يلي بعض مفاهيم البحث التي تعتمد على المعالجة الطبيعية للغة؛ تتميز الجُمل بأنها ذات هيكلية محددة وواضحة، مما يساعد في فهم المعنى بشكل سريع. ومن خلال تحديد نوع الجملة، يمكن لبرمجيات البحث أن تقوم بعملها بشكل أكثر دقة.\nولعمل ذلك فإن هذه البرمجيات يجب أن تعتمد على كميات كبيرة من المصادر والمراجع اللغوية، كالمكانز، والتي تحتوي على عبارات وجمل ذات علاقات محددة مسبقا. وباستخدام هذه المكانز يمكن لبرمجيات البحث أن تفهم بشكل أفضل طبيعة العلاقة بين الكلمات المختلفة ومواقعها الصحيحة ضمن الجمل. ورغم الفائدة العظيمة للمكانز، فإن مدى فعاليتها يعتمد على تحديثها باستمرار، وإلا فإنها تفقد فاعليتها تدريجيا. وباستخدام المكانز، وتصريف الجمل وإعرابها يمكن الحصول على نظم بحث قوية يمكنها فهم عبارات البحث ومعاني الكلمات ضمن السياق بشكل أفضل. أوضحنا في النقطة أعلاه كيف يمكن للمكانز ونظم تصريف وإعراب الجمل أن تسهم بشكل أفضل في فهم المعنى من خلال السياق. وهذه التقنية مفيدة إذا كنا نبحث ضمن نص معين، ولكنها ليست مفيدة عند إدخال الاستفسارات والاستعلامات ضمن محركات البحث، حيث تتكون هذه عادة من عدد محدود من الكلمات، وبالتالي فإن حجم النص غير كاف لتحليل معنى الكلمات. والحل لهذه المشكلة واضح إلى درجة الإحراج ويتمثل في سؤال المستخدم عن المعنى الذي يقصده؛ فعندما يُدخل المستخدم كلمة «راحة» مثلا ضمن مربع الاستعلام، فإن محرك البحث يسأله عن المعنى المقصود، أو المعنى المراد البحث عنه، قبل الشروع بعملية البحث. وتوجد اليوم العديد من محركات البحث التي تستخدم قواميس مضمنة تقوم بتقديم قوائم بالمعاني المختلفة التي تمثلها الكلمة الواحدة قبل الشروع في عملية البحث. ومن هذه المحركات هناك LexiGuide من شركة LexiQuest، وOingo، وSimplifind على موقع Simpli.com. هذا الأسلوب هو الأقدم، حيث يتم تصنيف الوثائق حسب تصنيفات وفروع معينة، ومن ثم البحث بشكل منفصل ضمن كل تصنيف عن المعلومات المطلوبة. فمثلا، عند البحث عن كلمة «نواة» فإن بحثك قد يقودك إلى تصنيف يتعلق بعلوم الزراعة، وأنواع الحبوب، وما إلى ذلك، ولكنه في الوقت نفسه قد يقودك إلى تصنيف يقع ضمن علوم الفيزياء النووية. والحل هنا يكمن في تصنيف الوثائق المتعلقة بأنوية الحبوب والمزروعات في قسم الزراعة مثلا، والوثائق المتعلقة بأنوية الذرات في قسم الفيزياء الذرية. ولكن ماذا لو كانت لدينا وثيقة تتعلق بتأثير التجارب النووية على أنوية الحبوب والمزروعات؟ هل يجب عندها وضع الوثيقة في القسمين، أم ما هو الحل؟ وهنا تنشأ لدينا مشكلة المعنى المزدوج.\nومن هنا تأتي أهمية التصنيف التلقائي، حيث يتم استخدام علوم النحو من تصريف وإعراب، واستخدام المكانز والقواميس، بحيث يتمكن النظام من «فهم» المواضيع الرئيسة في وثيقة ما. ويتم ذلك باستخدام أساليب إحصائية تقوم بدراسة تكرار الكلمات ضمن وثيقة ما، ومن ثم تحديد السياق، والذي يساعد في عملية البحث. وكمثال على ذلك لنأخذ كلمة، أو اسم مثل «فهد» أو «ليث» وهي أسماء عربية دارجة. ولنفترض أننا كتبنا موضوعا في مجلتنا عن شخص اسمه «فهد»، ولنفترض أن محرك بحث أراد تصنيف مقالنا هذا. في هذه الحال، وإذا كان محرك البحث يستخدم أسلوب التصنيف التلقائي، فإنه سيقوم من خلال دراسة النص ونوعية الكلمات الموجودة فيه وعلاقاتها وتكرارها، بتحديد أن المقال ينتمي إلى تصنيف علوم الحاسوب والإنترنت وليس الحيوانات والوحوش البرية.\nوتتبع برمجيات التصنيف التلقائي قواعد معينة يحددها المبرمجون، أو يمكن للآلة نفسها أن تتعلم ذاتيا كيف تقوم بتصنيف الكلمات. أو يمكن استخدام الأسلوبين، بحيث يتم تصنيف الوثائق التي تتبع نمطا معينا بشكل تلقائي، في حين يتم تحويل تلك التي يستحيل تصنيفها إلى عامل بشري كي يقوم بذلك. من يستعمل محركات البحث باستمرار يعلم أنه من المحتوم الحصول على مئات الألوف من النتائج على الأقل عند البحث عن موضوع معين بشكل عام. أما عند تضييق نطاق البحث، فإننا نحصل على مقدار أقل من النتائج الأكثر دقة.\nولزيادة دقة الاستعلامات، فإن بعض محركات البحث يقوم بتعديل الاستعلام، وذلك بتقديم معان مرادفة لكلمات البحث إلى المستخدم، لمساعدته على تحديد موضوع بحثه بدقة أكبر. ففي مثل هذه النظم، وإذا كان موضوع استعلامك هو «الرياضة» مثلا، فإن النظام يقدم لك عددا من البدائل التي تساعد في تحديد موضوع البحث مثل «التربية البدنية»، «اللياقة البدنية» أو «اللياقة» وغير ذلك من المرادفات التي تساعد على توجيه العملية بحيث يحصل المستخدم على أكبر كم من النتائج الدقيقة التي تعبّر عن موضوع البحث. إذا جربت اليوم أن تبحث عن العبارة «اسطوانات الليزر» (وهي العبارة العامية والقديمة للأقراص المدمجة) فإنك لن تحصل على الكثير من النتائج المفيدة من محركات البحث، حيث أن عبارة «الأقراص المدمجة» هي الكلمة الشائعة والمستخدمة في الغالبية العظمى من المطبوعات.\nولهذا فإن العديد من محركات البحث هذه الأيام تستخدم أسلوب الربط بين المفاهيم، بحيث أنك إذا قمت بكتابة العبارة «أقراص الليزر» فإنك ستحصل على معلومات عن «الأقراص المدمجة». ويعتمد هذا الأسلوب على تحديد العلاقة بين الكلمات والعبارات في قاعدة البيانات بشكل مسبق، كما أنه مفيد عند البحث في وثائق متعددة اللغات، فالبحث عن كلمة «أقراص الليزر» يمكن أن يعطي نتائج لوثائق باللغة الإنجليزية عن \"Compact Disks\" أو CDs وما إلى ذلك، وهذا أسلوب مفيد جدا في عالم إنترنتي لم تعد فيه اللغة الإنجليزية هي السائدة. انظر إلى أي موقع للتجارة الإلكترونية، وستجد أنه غير ذا فائدة تُذكر إذا لم يكن محرك البحث المستخدم به قادرا على نقلك إلى صفحة المنتج، أو المنتجات، التي تريدها خلال أسرع وقت ممكن. ولذلك نجد أن بوابات التجارة والأعمال الإلكترونية العالمية مثل Ebay وأمازون تعتبر برمجيات البحث أحد أهم موجوداتها وتسعى باستمرار إلى تحديثها. وقد قامت شركة Ebay قبل سنوات بشراء نظام بحث متقدم من شركة Fast Search & Transfer النرويجية، والتي كانت تنتج تقنية بحث جديدة تقوم بتقديم أحدث المعلومات للباحثين عن نتائج المزادات والأسعار المتداولة. كما أن أمازون ومواقع مثل Marthastewar.com تقوم بالتعامل مع شركة Google وAskJeeves بحيث يتم ربط المستخدمين بالبضائع التي يريدون شرائها من خلال كتابة سؤال اعتيادي ضمن مربع الاستعلام. وتقول الخبيرة مارثا فراي، وهي باحثة في شؤون التجارة الإلكترونية في مجموعة باتريشيا سيبولد، «يمكن القول بأن السبب الرئيس في فشل معظم مواقع التجارة الإلكترونية، يعود إلى اعتمادها لتقنيات بحث ضعيفة.» كما اكتشفت مؤسسة ميديا ميتريكس للأبحاث بأن 80% من مستخدمي إنترنت، يتوقفون عن استخدام موقع ما إذا لم تعمل وظيفة البحث المضمنة فيه بالشكل الصحيح.\nومن هنا كان السباق بين عدد من الشركات لتطوير تقنيات بحث متقدمة، يمكن للبشر الاعتياديين التعامل معها، والحصول على النتائج التي يريدونها تماما. وسنستعرض فيما يلي عددا من الشركات العالمية التي ابتكرت تقنيات يمكن لها أن تغير وجه إنترنت إلى الأبد. ايكساليد (Exalead) محرك بحث فرنسي متخصص في التقنيات الحديثه للبحث : نوعية نتائج البحث، تصنيف النتائج، صور تمهيدية لكل صفحة.(Exalead) بدأ محرك البحث هذا Google.com كمشروع لرسالة دكتوراة حول تقنيات الذكاء الاصطناعي والمعالجة الطبيعية للغة في جامعة ستانفورد في الولايات المتحدة، وتحول اليوم إلى بوابة إنترنت عالمية كبرى تخدم البحث بـ 66 لغة (منها العربية)، تقوم بمعالجة 120 مليون طلب بحث يوميا (حسب إحصائيات مؤسسة ميديا ميتركس للأبحاث)، كما أن الموقع أصبح مؤخرا ضمن أكبر 15 موقعا في الولايات المتحدة. ولا يتوقف الأمر هنا، حيث أن عوائده تصل إلى 50 مليون دولار سنويا، ويتوقع البعض أن يصل حجم هذه العوائد في المستقبل القريب إلى مليار دولار أمريكي حسب مجلة بيزنس ويك الأمريكية.\nوالمستخدم لهذا الموقع يعرف تمام المعرفة مدى دقته في تقديم النتائج المطلوبة، ومن المرة الأولى، كما أنه لا يتطلب خبرة كبرى من المستخدم في صياغة الأسئلة والاستعلامات. ويعتمد هذا الموقع تقنيات إحصائية ورياضية متقدمة تقوم بدراسة الوثائق المفهرسة، وتكرار الكلمات ضمن كل وثيقة، وبالتالي الحكم على موضوعها وعلاقتها بعبارة البحث التي يقوم المستخدم باستعمالها.\nومهما كانت الوصفة السحرية التي يستخدمها موقع غوغل فإنه يعتبر الأفضل بين كافة مواقع البحث المستخدمة اليوم. موقع Ask.com يستخدم اللغة الطبيعية في البحث ويعتمد قاعدة بيانات وتقنيات تمكّن المستخدم من توجيه سؤال البحث «باللغة الإنجليزية» بلغة سهلة (وعامية أيضاً) لتقوم قاعدة البيانات بالمطابقة بين الكلمات المفتاحية في السؤال، وبين ما هو موجود في قاعدة بياناتها. ورغم ذلك فإن محرك «آسك» يعتمد جزئيا على التدخل البشري لتصنيف المعلومات وفهرستها إذا لم تكن موجودة في قاعدة البيانات. فعند حصول حدث إخباري ما مثلا، فإن «آسك» لن يتمكن من التعامل مع أي سؤال يتعلق بهذا الحدث إلا إذا قام مدراء قواعد البيانات بتحديث النظام. وقد قامت الشركة المسئولة عن الموقع بطرح نظام مستقل، يمكن للمؤسسات العاملة في مجالات التجارة والأعمال الإلكترونية تضمينه في مواقعها بحيث يمكن لعملائها توجيه أسئلة واستفسارات باللغة الطبيعية، والحصول على أجوبة لها دون أي تدخل بشري. يوجد على إنترنت اليوم بلايين الصفحات، وحسب المصادر المتوفرة فإنه قد تم حتى اليوم فهرسة ما يزيد قليلا على البليون صفحة. وتتسابق الشركات التي تقوم بفهرسة هذه الصفحات في إتاحتها لمستخدمي إنترنت، والحفاظ على سرعة الاستجابة التي يحصل عليها المستخدم. وإضافة إلى السرعة فإن على قواعد البيانات هذه أن تثبت وجودها بتقديم أجوبة «طازجة»، ومتناسقة، وذات علاقة بما يبحث عنه المستخدم.\nكما أن عجلة الابتكار لا تتوقف في مجال البحث، فموقع www.hotlinks.com يتيح للمستخدمين إمكانية حفظ مفضلاتهم Favorites ضمن دليل على إنترنت، وذلك كي تكون هذه المفضلات متاحة للمستخدم أينما كان، ويمكن للمستخدمين أن يختاروا مشاركة مفضلاتهم مع مستخدمي إنترنت الآخرين، وتمكين زوار الموقع من البحث على إنترنت من خلال البحث في مفضلات الآخرين، وبالتالي الحصول على معلومات رأى مستخدمون آخرون أنها مفيدة لدرجة وضع المواقع التي تحتويها ضمن مفضلاتهم.\nكما أن هناك مواقع للبحث مثل www.expertcentral.com والتي تقدم للباحثين إجابات متخصصة. وإضافة إلى ذلك فهناك العديد من محركات البحث التي تعتمد تقنيات الشبكات العصبية Neural Networks، ومحركات البحث التي يمكن تثبيتها على أجهزة المستخدمين، وفهرسة محتويات أقراصهم الصلبة.\nونظرا للأهمية المتواصلة لمحركات البحث، فإن التقنيات الجديدة ستواصل ظهورها، وستواصل التقنية تطورها لتقديم نتائج أفضل للمستخدمين. وبظهور هذه التقنيات فإن بعضها سيفشل وبعضها سينجح، وستصبح التقنيات الناجحة جزءا من محركات البحث المستخدمة اليوم. إذا لم تكن تريد إنفاق الملايين في ابتكار تقنيات للذكاء الاصطناعي والمعالجة الطبيعية للغة، فإن الحل الأسهل هو استخدام الميزات التي تقدمها لغة لغة الترميز القابلة للامتداد لجعل عملية البحث أكثر دقة. فهذه اللغة كما هو معروف تعتمد على توصيف الوثائق والبيانات عند نشرها على الشبكة. فالمادة المتعلقة بالأسعار مثلا يتم توصيفها بعلامات تدل على أنها تمثل السعر، والمادة التي تصف أبعاد بضاعة معينة يتم توصيفها بهذا الشكل.\nوباستخدام XML يمكن للروبوتات التي تقوم بفهرسة مواقع إنترنت أن تفهم المحتوى الموجود ضمن الصفحات. وبالتالي فعندما تبحث عن تذكرة سفر بسعر معين مثلا، فإن الروبوتات لا تقوم فقط بالعثور على التذكرة بأفضل الأسعار، ولكنها تعثر أيضا على أفضل سعر لغرفة فندق، أو سيارة مستأجرة.\nومن التطبيقات الأخرى مثلا هي أنك إذا عرضت سيرتك الذاتية على الروبوت فإنه يقترح عليك أفضل وظيفة تناسب مؤهلاتك. أو إذا عرضت على الروبوت ملفك الصحي فإنه يقترح عليك الفيتامينات التي يجب عليك تناولها، أو النوادي الصحية التي يمكنك الانضمام إليها.\nوبالطبع فإننا لا نعتقد أن XML هي الحل السحري، ولكنها يمكن أن تؤدي إلى زيادة نجاعة عمليات البحث إذا ما اقترنت بالتقنيات الأخرى المذكورة في موضوعنا هذا."
  },
  {
    "url": "https://az.wikipedia.org/wiki/Axtar%C4%B1%C5%9F_sistemi",
    "title": "Axtarış sistemi — Vikipediya",
    "content": "Axtarış sistemi (ing. search engine) – Veb-də informasiyanı açar sözlərə, mövzulara və s. görə axtarmağa imkan verən proqram (məsələn, Bing, DuckDuckGo, Google, Rambler, Yahoo!, Yandex). Axtarış sistemləri avtomatlaşdırılmış indekslərdir və hər axtarış sisteminin öz verilənlər bazası var. Buna görə də eyni açar sözlərə görə müxtəlif axtarış sistemlərində axtarış etdikdə fərqli nəticələr alınır. Bəzən axtarış nəticələrində mətləbə dəxli olmayan informasiyalar çıxır, çünki elə bir veb-alət yoxdur ki, bütün Veb’i indeksləsin (nizamlasın).\nAxtarış sistemləri üç əsas hissədən ibarət olur:"
  },
  {
    "url": "https://bn.wikipedia.org/wiki/%E0%A6%93%E0%A6%AF%E0%A6%BC%E0%A7%87%E0%A6%AC_%E0%A6%85%E0%A6%A8%E0%A7%81%E0%A6%B8%E0%A6%A8%E0%A7%8D%E0%A6%A7%E0%A6%BE%E0%A6%A8_%E0%A6%87%E0%A6%9E%E0%A7%8D%E0%A6%9C%E0%A6%BF%E0%A6%A8",
    "title": "ওয়েব অনুসন্ধান ইঞ্জিন - উইকিপিডিয়া",
    "content": "ওয়েব সার্চ ইঞ্জিন বা আন্তর্জাল অনুসন্ধান ব্যবস্থা হল ওয়ার্ল্ড ওয়াইড ওয়েব বা  আন্তর্জালের দুনিয়াতে যেকোনো তথ্য বা ছবি খুঁজে বের করার প্রযুক্তি মাধ্যম। অনুসন্ধান ইঞ্জিনের মাধ্যমে বিভিন্ন ওয়েবসাইট থেকে তথ্য সংগ্রহ করে প্রদর্শন করা হয়ে থাকে।\nওয়েব সার্চ ইঞ্জিন ক্রোলার বট এর মাধ্যমে তথ্য সংগ্রহ করে। ইন্টারনেট অনুসন্ধান ইঞ্জিনের যাত্রা শুরু হয় ১৯৯০ সালের ডিসেম্বর মাসে। অবশ্য এর আগে ১৯৮৬ সালে হিউলেট প্যাকার্ড প্রতিষ্ঠান কর্তৃক প্রথম অনুসন্ধান ইঞ্জিন আবিষ্কৃত হয়। ১৯৯৪ সালে চালু হয় প্রথম পূর্ণ টেক্সট ওয়েব অনুসন্ধান ইঞ্জিন ওয়েবক্রলার। ওয়েব সার্চ ইঞ্জিন মূলত তিনটি ধাপে কাজ করে: সার্চ ইঞ্জিনের ওয়েব ক্রলার (bots/spiders) ইন্টারলিঙ্ক অনুসরণ করে নতুন ক ক্রলার দ্বারা সংগ্রহ করা ওয়েবপেজের তথ্য সার্চ ইঞ্জিনের ডাটাবেসে সংরক্ষণ (indexing) করাওয়েবপেজের টেক্সট, কীওয়ার্ড, ইমেজ, ভিডিও, মেটাডাটা ইত্যাদি বিশ্লেষণ করে এবং কোন কোন পৃষ্ঠাগুলো কোন কীওয়ার্ডের জন্য র‍্যাংক করতে পারে তা নির্ধারণ করে। যখন কেউ সার্চ করে, তখন সার্চ ইঞ্জিন তার ইনডেক্স থেকে সবচেয়ে প্রাসঙ্গিক ও মানসম্পন্ন পৃষ্ঠাগুলো অ্যালগরিদমের মাধ্যমে র‍্যাংক করে এবং সার্চ রেজাল্ট পৃষ্ঠায় (SERP) দেখায়। র‍্যাংকিং নির্ধারণে বিভিন্ন ফ্যাক্টর কাজ করে, যেমন:"
  },
  {
    "url": "https://bjn.wikipedia.org/wiki/Masin_panggagai_web",
    "title": "Masin panggagai web - Wikipidia Banjar, kindai pangatahuan",
    "content": "Masin panggagai web (basa Inggris: web search engine) adalah parugram kumputar nang dirancang gasan manggawi panggagaian atas barakas-barakas nang tasimpan dalam layanan www, ftp, publikasi milis, atawa jua news group dalam sabuting atawa jua sajumlah kumputar paladin dalam sabuting jaringan. Masin panggagai adalah pakakas panggagai maklumat matan dukumin-dukumin nang tasadia. Kulihan panggagaian rancaknya ditampaiakan dalam bantuk daptar nang rancak diurutakan maumpat hingkat akurasi atawa jua rasio pailang atas sabuting barakas nang disambat sawagai hits. Maklumat nang jadi sasahan panggagaian kawa tadapat dalam bamamacam janis barakas kaya tungkaran web, gambar, atawa jua janis-janis barakas lainnya. Sapalih masin panggagai dikatahui jua manggawi pangumpulan maklumat atas data nang tasimpan dalam sabuting basis data atawa jua direktori web. Sapalih ganal masin panggagai dijalanakan ulih pausahaan swasta nang mamakai algoritme kaanggitan wan basis data tatutup, di antaranya nang panyuhurnya adalah sapari Google (MSN Search, wan Yahoo!). Sudah ada sapalih upaya manciptaakan masin panggagai lawan sumbar tabuka (open source), umpamanya adalah Htdig, Nutch, Egothor, wan OpenFTS.[1]"
  },
  {
    "url": "https://zh-min-nan.wikipedia.org/wiki/Chhiau-chh%C5%8De_ia%CC%8Bn-j%C3%ADn",
    "title": "Chhiau-chhōe ia̋n-jín – Wikipedia",
    "content": "Chhiau-chhōe ia̋n-jín (Eng-gí: search engine), so͘-sîm ia̋n-jín (Tâi-oân Chiàⁿ-thé Tiong-bûn: 搜尋引擎) iah-sī so͘-sek ín-kèng (Kán-hòa-jī: 搜索引擎), sī thê-kiōng iōng-chiá chhiau tiān-náu hē-thóng chu-liāu--ê chu-sìn chhiau-chhōe (information retrieval) hē-thóng. Siōng chhut-miâ--ê sī bāng-lō͘ chhiau-chhōe ia̋n-jín. Chiàu kong-lêng thang hun-chò tô͘-phìⁿ chhiau-chhōe ia̋n-jín, iáⁿ-phìⁿ chhiau-chhōe ia̋n-jín. Chiàu beh chhōe--ê chu-liāu lâi-goân ū toh-bīn chhiau-chhōe ia̋n-jín téng."
  },
  {
    "url": "https://be.wikipedia.org/wiki/%D0%9F%D0%BE%D1%88%D1%83%D0%BA%D0%B0%D0%B2%D0%B0%D1%8F_%D1%81%D1%96%D1%81%D1%82%D1%8D%D0%BC%D0%B0",
    "title": "Пошукавая сістэма — Вікіпедыя",
    "content": "Пошукавая сістэма — праграмна-апаратны комплекс з вэб-інтэрфейсам, які дае магчымасць пошуку інфармацыі ў Інтэрнэце. Пад пошукавай сістэмай звычайна маецца на ўвазе сайт, на якім размешчаны інтэрфейс (фронт-энд) сістэмы. Праграмнай часткай пошукавай сістэмы з’яўляецца пошукавая машына (пошукавы рухавік) — комплекс праграм, які забяспечвае функцыянальнасць пошукавай сістэмы і звычайна з’яўляецца камерцыйнай тайнай кампаніі-распрацоўшчыка пошукавай сістэмы. Большасць пошукавых сістэм шукаюць інфармацыю на сайтах Сусветнага павуціння, але існуюць таксама сістэмы, здольныя шукаць файлы на FTP-серверах, тавары ў інтэрнэт-крамах, а таксама інфармацыю ў групах навін Usenet. Паляпшэнне пошуку — гэта адна з прыярытэтных задач сучаснага Інтэрнэту (гл. пра асноўныя праблемы ў працы пошукавых сістэм у артыкуле Глыбокае павуцінне). Па даных кампаніі Net Applications,[1] у лістападзе 2011 года выкарыстанне пошукавых сістэм размяркоўвалася наступным чынам:"
  },
  {
    "url": "https://be-tarask.wikipedia.org/wiki/%D0%9F%D0%BE%D1%88%D1%83%D0%BA%D0%B0%D0%B2%D0%B0%D1%8F_%D1%81%D1%8B%D1%81%D1%82%D1%8D%D0%BC%D0%B0",
    "title": "Пошукавая сыстэма — Вікіпэдыя",
    "content": "Пошукавая сыстэма — сукупнасьць запісаў іскры (устава або праграма), які падае магчымасьць пошуку інфармацыі ў Сеціве. Пад пошукавай сыстэмай звычайна маецца на ўвазе сайт, на якім разьмешчаны інтэрфэйс (фронт-энд) сыстэмы. Праграмнай часткай пошукавай сыстэмы зьяўляецца пошукавая машына (пошукавы рухавічок) — комплекс праграм, які забясьпечвае дзейнасьць пошукавай сыстэмы і звычайна зьяўляецца камэрцыйнай тайнай кампаніі-распрацоўшчыка пошукавай сыстэмы. Большасьць пошукавых сыстэм шукаюць інфармацыю на сайтах Сусьветнага павуціньня, але існуюць таксама сыстэмы, здольныя шукаць файлы на FTP-сэрвэрах, тавары ў інтэрнэт-крамах, а таксама інфармацыю ў групах навін Usenet. Паляпшэньне пошуку — гэта адна з прыярытэтных задач сучаснага Сеціва (гл. пра асноўныя праблемы ў працы пошукавых сістэм у артыкуле Глыбокае павуціньне). Па даных кампаніі Net Applications,[1] у лістападзе 2011 года выкарыстаньне пошукавых сыстэм размяркоўвалася наступным чынам:"
  },
  {
    "url": "https://bh.wikipedia.org/wiki/%E0%A4%B5%E0%A5%87%E0%A4%AC_%E0%A4%B8%E0%A4%B0%E0%A5%8D%E0%A4%9A_%E0%A4%87%E0%A4%82%E0%A4%9C%E0%A4%A8",
    "title": "वेब सर्च इंजन - विकिपीडिया",
    "content": "वेब जोह इंजन आ सरच इंजन (search engine) एगो अइसन सॉफ्टवेयर होला जे खास एह खाती डिजाइन कइल गइल होला कि वल्ड वाइड वेब से जनकारी खोजल जा सके। अइसन खोज के उर सभ के एक कतार मे परयोग करे आला के सोझा रखे ला आ एह परिनाम सभ मे वेब सामग्री, इमेज, बीडियो वगैरह नियर कई किसिम के चीज के हो सके ला। जहवाँ वेब डाइरेक्टरी सभ आदमी के हाथे खुद बनावल आ मेंटेन कइल जालीं, सरच इंजन सभ एल्गोरिदम के इस्तेमाल से वेब क्राउलर के इस्तेमाल से ऑटोमेटिक तरीका से रियल-टाइम में जानकारी इकट्ठा कऽ के ओकरा लिस्ट बना ली। अइसन सामग्री जे वल्ड वाइड वेब पर होखे लेकिन सरच इंजन के इंडेक्सिंग (लिस्ट बनावे) के पहुँच से बहरा होखे ओकरा के डीप वेब के हिस्सा कहल जाला। वेब सर्च इंजन तिन गो मुख्य प्रक्रिया में काम करेला: सर्च इंजन के वेब क्रॉलर (bots/spiders) इंटरनेट पर अलग-अलग वेबसाइट के स्कैन करके नया आ अपडेट भइल पेजन के जानकारी जमा करेला। ई एक वेबसाइट से दोसरा वेबसाइट के लिंक के पीछा करके नया कंटेंट खोजेला। क्रॉलर से जुटावल जानकारी के सर्च इंजन के डाटाबेस में सेव (indexing) कइल जाला। एह प्रक्रिया में सर्च इंजन वेबपेज के टेक्स्ट, कीवर्ड, इमेज, वीडियो, मेटाडाटा वगैरह के विश्लेषण करेला आ तय करेला कि कवन पेज कवन कीवर्ड खातिर रैंक होई। जब केहू कुछु सर्च करेला, त सर्च इंजन आपन इंडेक्स से सबसे बढ़िया आ संबंधित वेबपेज के एल्गोरिदम से रैंक करके सर्च रिजल्ट पेज (SERP) पर देखावे ला। रैंकिंग तय करे में कई गो चीज के ध्यान राखल जाला, जइसे:"
  },
  {
    "url": "https://bs.wikipedia.org/wiki/Internetski_pretra%C5%BEiva%C4%8D",
    "title": "Internetski pretraživač - Wikipedia",
    "content": "Internetski pretraživač (engleski: web search engine) jest veb-sajt koji omogućava pretragu informacija na internetu. Internetski pretraživači mogu se podijeliti na dvije grupe: meta pretraživači i \"pravi\" internetski pretraživači. Meta pretraživači su programi koji skupljaju informacije o veb-sajtovima koji se nalaze u META oznakama HTML dokumenata. Pravi internetski pretraživači jesu programi koji skupljaju sve dostupne HTML dokumente veb-sajtova i sortiraju i obrađuju iste u svojim bazama podataka. Na veb-sajtovima ovih pretraživača mogu se pogledati skupljene informacije u bazi podataka. Najpoznatiji internetski pretraživači su Google, Bing i Yahoo!.[1] Nedovršeni članak Internetski pretraživač koji govori o računarstvu treba dopuniti. Dopunite ga prema pravilima Wikipedije."
  },
  {
    "url": "https://ca.wikipedia.org/wiki/Motor_de_cerca_web",
    "title": "Motor de cerca - Viquipèdia, l'enciclopèdia lliure",
    "content": "Quota d'ús dels Motor de cerca més coneguts, segons StatCounter al Desembre del 2021[1] Un motor de cerca o de recerca o bé cercador[2][3] és un programa informàtic dissenyat per ajudar a trobar informació emmagatzemada en un sistema informàtic com ara una xarxa, Internet, un servidor o un ordinador personal. L'objectiu principal és el de trobar altres programes informàtics, pàgines web i documents, entre d'altres. A partir d'una determinada paraula o paraules o una determinada frase, l'usuari demana un contingut sota un criteri determinat, i llavors recupera una llista de referències que compleixen aquest criteri. El procés es realitza a través de les metadades,[4] elements que permeten recuperar informació als motors de cerca. Els índexs que utilitzen els cercadors sempre estan actualitzats a través d'un robot web per generar rapidesa i eficàcia en la recerca. Els directoris, en canvi, són gestionats per editors humans. La forma més pública i visible d'un motor de cerca és un motor de cerca web que cerca informació a la World Wide Web. Un rastrejador web, indexador web, o aranya web és una programa informàtic que inspecciona les pàgines de World Wide Web de forma metòdica i automatizada.[7] Un dels usos més freqüents que se'ls dona consisteix a crear una còpia de totes les pàgines web visitades per al seu processament posterior per un motor de cerca que indexa les pàgines proporcionant un sistema de recerques ràpid. Les aranyes web solen ser bots.[8] Les aranyes web comencen visitant una llista d'URL, identifica els hiperenllaços en aquestes pàgines i els afegeix a la llista d'URL a visitar de manera recurrent d'acord a determinat conjunt de regles. L'operació normal és que se li dona a el programa un grup d'adreces inicials, l'aranya descàrrega aquestes adreces, analitza les pàgines i busca enllaços a pàgines noves. Després descàrrega aquestes pàgines noves, analitza els seus enllaços, i així successivament. Entre les tasques més comunes de les aranyes del web tenim: Un directori web és un tipus de lloc web que conté un directori organitzat de dades, imatges o, més generalment, enllaços a altres llocs web. Els directoris web, contràriament als motors de cerca, es caracteritzen per estar estructurats en categories i subcategories. Habitualment, els directoris web permeten als administradors web o creadors de llocs web que informin del seu lloc perquè sigui inclòs, i després els editors autoritzats revisen aquestes sol·licituds abans d'incloure les seves enllaços per comprovar que s'adeqüen als requisits d'acceptació determinats pel directori web.[9] Entre els directoris web generalistes més coneguts es poden esmentar el Yahoo! Directory (inactiu des 2014) i DMOZ (inactiu des 2017). En l'actualitat els directoris web supervivents són petites bases de dades especialitzades en temes concrets i per això ja no són tan populars. Els grans repertoris generalistes com van ser Yahoo! Directory o DMOZ ja han estat definitivament reemplaçats pels motors de cerca, principalment el cercador de Google. Els directoris web regionals integren en un mateix lloc a comerços, serveis, empreses o participants de determinat sector, enfocant-se en un territori comercial en específic, creant d'aquesta manera una comunitat que facilita la navegació, localització i mercadeig. Aquests directoris promouen el creixement econòmic de el sector a què estan enfocats ja que posen a l'abast de l'usuari la possibilitat de descobrir proveïdors que desconeixia que existissin i amb això resoldre una necessitat de compra. Una tecnologia molt simple per gran quantitat de scripts disponibles, ja que no calen molts recursos. En canvi, cal més suport humà i mantenimient.[10] Un metacercador és un sistema que localitza informació en els motors de cerca més usats, no té base de dades pròpia pel que fa servir les d'altres cercadors i mostra una combinació de les millors pàgines que ha cada un.[11] Un cercador normal recopila la informació de les pàgines mitjançant la seva indexació, com Google o bé manté un ampli directori temàtic, com Yahoo. La definició simplista seria que un metacercador és un cercador en cercadors. «En altres paraules per al·ludir a el concepte més genèric d'un cercador, podem afirmar que un metacercador és el cercador que incorpora un conjunt de cercadors. Alguns exemples de metacercadors són: Dogpile, Aleyares [12] MetaCrawler, entre d'altres. Aquests metacercadors presenten avantatges, com ampliar l'espai de recerca i en alguns casos mostrar la posició del web ».[13] Els motors de cerca proporcionen una interfície a un grup d'elements que permet als usuaris especificar criteris sobre un article d'interès i que el motor trobi els elements coincidents. Els criteris s'anomenen consulta de cerca. En el cas dels motors de cerca de text, la consulta de cerca normalment s'expressa com un conjunt de paraules que identifiquen el concepte desitjat que un o més documents poden contenir.[14] Hi ha diversos estils de sintaxi de consulta de cerca que varien en rigor. També pot canviar de nom als motors de cerca dels llocs anteriors. Mentre que alguns motors de cerca de text requereixen que els usuaris introdueixin dues o tres paraules separades per espai en blanc, altres motors de cerca poden permetre als usuaris especificar documents sencers, imatges, sons i diverses formes de llenguatge natural. Alguns motors de cerca apliquen millores a les consultes de cerca per augmentar la probabilitat de proporcionar un conjunt d'elements de qualitat mitjançant un procés conegut com a expansió de la consulta. Els mètodes de comprensió de consultes es poden utilitzar com a llenguatge de consulta estandarditzat. La llista d'elements que compleixen els criteris especificats per la consulta normalment s'ordena o es classifica. La classificació dels elements per rellevància (de major a menor) redueix el temps necessari per trobar la informació desitjada. Els motors de cerca probabilístics classifiquen els elements basant-se en mesures de similaritat (entre cada element i la consulta, normalment en una escala d'1 a 0, l'1 és el més semblant) i de vegades la popularitat o autoritat (vegeu bibliometria) o utilitza la opinió sobre la rellevància. Els motors de cerca booleans normalment només retornen elements que coincideixen exactament sense tenir en compte l'ordre, tot i que el terme motor de cerca booleà pot referir-se simplement a l'ús de la sintaxi d'estil booleà (l'ús d'operadors AND, OR, NOT i XOR) en un context probabilístic. Per proporcionar un conjunt d'elements coincidents que s'ordenen d'acord amb alguns criteris ràpidament, un motor de cerca normalment recopilarà metadades sobre el grup d'elements que s'està considerant prèviament mitjançant un procés anomenat indexació. L'índex normalment requereix una quantitat més petita d'emmagatzematge informàtic, per la qual cosa alguns motors de cerca només emmagatzemen la informació indexada i no el contingut complet de cada element, i en canvi proporcionen un mètode per navegar als elements a la pàgina de resultats del cercador. Alternativament, el motor de cerca pot emmagatzemar una còpia de cada element en una caché perquè els usuaris puguin veure l'estat de l'element en el moment en què es va indexar o amb finalitats d'arxiu o perquè funcionin processos repetitius. de manera més eficient i ràpida. Altres tipus de cercadors no emmagatzemen un índex. Els Crawler, o motors de cerca de tipus aranya (també coneguts com motors de cerca en temps real) poden recollir i avaluar elements en el moment de la consulta de cerca, considerant dinàmicament elements addicionals basats en el contingut d'un element inicial (conegut com a una llavor, o URL de llavor en el cas d'un rastrejador d'Internet). Els motors de cerca meta no emmagatzemen ni un índex ni una memòria cau i, en canvi, simplement reutilitzen l'índex o els resultats d'un o més motors de cerca per proporcionar un conjunt final agregat de resultats. El primer cercador va ser \"Wandex\", un índex realitzat pel World Wide Web Wanderer, un robot desenvolupat per Mattew Gray al MIT, el 1993. Un altre dels primers cercadors, Aliweb, també va aparèixer en 1993 i encara està en funcionament. El primer motor de cerca de text complet va ser WebCrawler, que va aparèixer el 1994. A diferència dels seus predecessors, aquest permetia als seus usuaris una recerca per paraules en qualsevol pàgina web, el que va arribar a ser un estàndard per a la gran majoria dels cercadors . WebCrawler va ser així mateix el primer a donar-se a conèixer àmpliament entre el públic. També va aparèixer en 1994 Lycos (que va començar a la Carnegie Mellon University). Molt aviat van aparèixer molts més cercadors, com Excite, Infoseek, Inktomi, Northern Light i Altavista. D'alguna manera, competien amb directoris (o índexs temàtics) populars com Yahoo!. Més tard, els directoris es van integrar o es van afegir a la tecnologia dels cercadors per augmentar la seva funcionalitat. Abans de l'adveniment de la Web, hi havia motors de cerca per a altres protocols o usos, com el cercador Archie, per a llocs FTP anònims i el motor de cerca Verònica, per al protocol Gopher. El 1996 Larry Page i Serguei Brin van començar un projecte que portaria a l'aparició del cercador més utilitzat avui dia: Google. El projecte inicial es va cridar BackRub,[15] que era el nom de la tecnologia utilitzada per al seu desenvolupament. BackRub basava la importància dels llocs web en la quantitat d'enllaços que rebia. Presentava una interfície molt senzilla i capaç de mostrar als l'usuari els resultats més rellevants per a cadascuna de les recerques. Amb l'arribada de Google, la manera en què els motors de cerca funcionaven va canviar de forma radical, democratitzant els resultats que s'ofereixen en el seu cercador. Google va basar el funcionament del seu motor de cerca a la rellevància dels continguts de cada lloc web per als propis usuaris, és a dir, prioritzant aquells resultats que els usuaris consideraven més rellevants per a una temàtica concreta. Per a això va patentar el seu famós PageRank, un conjunt d'algoritmes que valoraven la rellevància d'un lloc web assignant-li un valor numèric de el 0 a el 10. En la majoria de països Google.com o la versió de Google per al país concret, és el cercador més utilitzat, però, això no passa en alguns països. Per exemple, a Rússia el cercador més utilitzat és Yandex[16][17] i a la Xina és Baidu.[18] La Unió Europea en 2018 li va imposar una multa de 5.000 milions d'euros per pràctiques monopolístiques, al considerar que força injustament als fabricants per a que la seva aplicació de recerca estigui a tots els telèfons que executin Android.[19] Viccionari"
  },
  {
    "url": "https://cs.wikipedia.org/wiki/Webov%C3%BD_vyhled%C3%A1va%C4%8D",
    "title": "Webový vyhledávač – Wikipedie",
    "content": "Webový vyhledávač je služba, která umožňuje na Internetu najít webové stránky, které obsahují požadované informace. Uživatel zadává do rozhraní vyhledávače klíčová slova, která charakterizují hledanou informaci, a vyhledávač obratem na základě své databáze vypisuje seznam odkazů na stránky, které hledané informace obsahují (text, obrázky nebo jiné typy multimediálních informací). Databáze je udržována převážně automaticky na rozdíl od internetových katalogů, které jsou udržovány převážně ručně. Cílem vyhledávačů je poskytnout uživateli při odpovědi na dotaz co nejrelevantnější informace, a proto různými způsoby hodnotí důležitost obsažených informací na webových stránkách (např. pomocí PageRank) a stránky s vyšším hodnocením zobrazuje uživateli jako první. Vyhledávač pracuje z větší části automaticky, k čemuž využívá desítky až statisíce počítačů. Kvalita vyhledávače je závislá na tom, jak kvalitní dává odpovědi, tj. jestli uživatel najde hledanou informaci na prvních místech odpovědi vyhledávače. Z tohoto důvodu je nutné měřit kvalitu stránek, které vyhledávač má ve své databázi (např. PageRank u Google, S-rank u Seznamu) a naopak majitelé stránek se snaží modifikací svých stránek dosáhnout na co nejvyšší pozice ve výstupu vyhledávače (SEO). Výsledkem je, že vyhledávač musí své metody neustále vylepšovat, aby vyhověl čím dál vyšším požadavkům svých návštěvníků. Obecně většina internetových vyhledávačů pracuje ve čtyřech krocích: Pro procházení webových stránek má internetový vyhledávač automatický program, tzv. vyhledávací robot (bot nebo též spider – „pavouk“), který se pomocí hypertextových odkazů snaží navštívit všechny webové stránky na Internetu (celý World Wide Web, tj. WWW). Robot pracuje tak, že dostane na začátku seznam atraktivních stránek (tj. vstupních míst, resp. seznam URL odkazů). Nejlépe je to seznam rozcestníků, jako je například katalog Seznamu[1], Yahoo! Directory[2] a podobně. Robot každou stránku stáhne na svůj pevný disk a poznamená si její URL adresu, aby ji nenavštěvoval opakovaně. V uložené stránce přečte všechny hypertextové odkazy na další webové stránky, čímž získá další místa, která stejným způsobem navštíví. Robot pracuje cyklicky, takže se po určitém čase na stránky vrací, aby zjistil jejich případné změny. Stránky, které robot uložil na pevný disk, je nutné zpracovat a vytvořit z nich databázi. V databázi jsou uvedena všechna nalezená slova a k nim adresy, na kterých se tato slova vyskytují. Databáze je tedy schopna poskytnout informaci, na kterých stránkách se hledané slovo nachází. Problémem je velikost databáze, protože její sekvenční prohledání by trvalo neúměrně dlouho. Proto následuje další krok, tzv. indexace. Indexování databáze urychluje vyhledání požadované informace. Zároveň je index vytvořen tak, aby poskytoval na prvních místech stránky s nejvyšší užitnou hodnotou (tzv. relevancí, mající nejvyšší hodnocení kvality, nejvyšší váhu). Pro výpočet relevance se používají nejrůznější algoritmy, které jsou založeny na nejrůznějších znacích stránek a různých úhlech analýzy jejich obsahu, například: Vyhledávač poskytuje svým uživatelům vstupní formulář, do kterého jsou zadávána hledaná slova (fráze atp.). Po odeslání dotazu jsou pomocí indexu získány z databáze odkazy na stránky, které hledané slovo obsahují. Podle kvality indexu jsou na prvních místech většinou odkazy na stránky, které jsou pro uživatele dostačující. Pro vyšší přehlednost se zobrazuje kromě odkazu ještě titulek stránky, okolí nalezených slov a případně i další informace (stáří informace, kvalita odkazu, …). Z principu práce vyhledávače vyplývá, že nikdy nemá úplně aktuální informace, ale prezentuje je se zpožděním. Robot navštěvuje zajímavé adresy co nejčastěji (např. zpravodajské servery) nebo se dokonce uzavírá smlouva o snadnějším zpřístupnění obsahu pro robota (místo pasivního čekání na návštěvu robota jsou nové informace robotovi přímo zaslány). Pro vyšší efektivitu se databáze aktualizuje po částech nebo průběžně nebo se co nejčastěji aktualizují alespoň nejzajímavější a nejčastěji hledané informace. Někdy je nežádoucí, aby robot indexoval některé stránky. Proto existuje možnost, jak roboty omezit pomocí souboru robots.txt, který se umisťuje do kořene webového serveru. Technika, která dokáže stránky upravit tak, aby se co nejlépe umístily ve výsledcích vyhledávání, se nazývá SEO (anglicky Search Engine Optimization) a v poslední době je velmi žádanou službou. SEO techniky se rozlišují na „povolené“ a „zakázané“ (tzv. Black Hat SEO, které vyhledávače tvrdě postihují například vyřazením ze svého indexu), avšak z hlediska vyhledávačů je jakékoliv umělé zlepšování umístění ve výsledcích vyhledávání nežádoucí (snad kromě případů, kdy robot stránce z nějakého důvodu nerozumí). Na podobném principu funguje i tzv. Google bomba, která umožňuje do výsledků vyhledávání zahrnout i stránky, které hledané slovo neobsahují. Historie internetových vyhledávačů se začala psát v roce 1990, kdy byl pro prohledávání FTP archivů vytvořen vyhledávač Archie. V roce 1998 byl představen vyhledávač Google, který pomocí unikátního řadícího algoritmu výrazně změnil pohled na vyhledávání v internetovém obsahu."
  },
  {
    "url": "https://ary.wikipedia.org/wiki/%D9%85%D9%88%D8%B7%D9%88%D8%B1_%D8%AF_%D8%A7%D9%84%D8%AA%D9%82%D9%84%D8%A7%D8%A8",
    "title": "موطور د التقلاب - ويكيپيديا",
    "content": "لموطور د التقلاب (ب نّݣليزية Search engine) هوّا صوفتوير مديور باش يقلب ف لويب، بشكل سيستيماتيكي، على معلومات محددة ب كويري د التقلاب ف لويب لي كاتكون على شكل كلمة ؤلا نص.[1] هاد لكلمة ؤلا نّص ف لعادة كيدخلهوم خدايمي ف صندوقة د التقلاب، يلا كان لموطور د التقلاب عندو أنطيرفاص. نّتيجة د التقلاب، يلا لموطور د لبحت لقا شي حاجة، كاتكون عادةً على شكل ليستة د لعناصر، بحال أدريسات د لويب، تصاور ؤلا ڤيديوات. عكس لكاطالوݣات د لويب، لي ف لعادة كيكونو مصاوبين من طاراف بنادم، لموطور د التقلاب كيجبد لمعلومات بشكل ديناميكي، حيت كيتبدّلو بشكل مستامر، بستيعمال ألݣوريتم لي كيخدّم رتيلات د لويب. أي محتوى د لويب لي لموطور د التقلاب مايقدرش يأنديكسيه كيتّعتابر جزء من دّيپ ويب ؤلا لويب لعميق. أشهر موطور د التقلاب ف لويب هوّا ݣووݣل، ولاكين كاينين موطورات خرين بحال بينݣ، يانديكس، بايدو ؤ ياهو!."
  },
  {
    "url": "https://et.wikipedia.org/wiki/Otsingumootor",
    "title": "Otsingumootor – Vikipeedia",
    "content": "Otsingumootor ehk otsimootor on arvutiprogramm (tavaliselt otsinguprogrammide-andmebaaside süsteem), mille väljundi abil saab veebis infot kiiremini leida. Otsimootor otsib kindlate tunnustega andmeid veebist ja FTP-serveritest.[1] Päringu vastused esitatakse nimekirjana, mis võib koosneda viidetest veebilehtedele, piltidele, dokumentidele, videotele jt objektidele võrgus. Mõned otsingumootorid kaevandavad andmeid võrku ühendatud andmebaasidest või avatud loenditest. Erinevalt veebiregistritest-portaalidest, mida hooldavad toimetajad, uuendavad otsingumootorid infot reaalajas, käitades veebiämblike abil algoritme.[2] Veebi algusaegadel kasutati veebiserverite loetelu, mida toimetas Tim Berners-Lee ja majutas CERN-i serveris. 1992. aastast on alles ka üks ajalooline pilt[3] Kuna internetti tekkis järjest kiiremini uusi veebiservereid, ei suutnud see loetelu enam kasvuga sammu pidada. NCSA lehel teatati uutest serveritest jaotises \"What's New!\" ('mida uut!').[4] Esimene tööriist, mida kasutati internetis otsimiseks, oli Archie. Nimi tähendas \"archive\" ('arhiiv') ilma \"v\"-ta. Selle lõid 1990. aastal Alan Emtage, Bill Heelan ja J. Peter Deutsch, arvutitehnika õpilased McGilli ülikoolist Montréalis. Programm laadis alla registri nimekirjad kõikidest failidest, mis asusid avalikes FTP võrgukohtades, luues failinimedega otsitava andmebaasi. Archie ei indekseerinud lehtede sisu, sest andmemahud olid piiratud, samas võis andmeid lihtsalt leida. Gopheri loomine 1991. aastal Mark McCahilli poolt avas tee kahele uuele otsinguprogrammile: Veronica ja Jughead. Sarnaselt Archiega otsisid nad failinimesid ja pealkirju, mis olid salvestatud Gopheri indeksisüsteemidesse. Veronica võimaldas märksõnaotsingut enamikule Gopheri menüü pealkirjadele terves Gopheri nimekirjas. Jughead oli tööriist, mille abil võis leida menüü infot kindlatest Gopheri serveritest. Kuigi otsingumootori Archie nimi polnud viide Archie koomiksisarjale, said Veronica ja Jughead nime selle sarja tegelaste järgi ja viitasid niimoodi oma eelkäijale. 1993. aasta suvel ei olnud veebi jaoks ühtegi otsingumootorit, mitmeid spetsiaalseid katalooge hallati käsitsi. Oscar Nierstrasz Geneva Ülikoolist kirjutas seeria Perli skripte, mis perioodiliselt peegeldasid neid lehti ja nad kirjutasid need ümber standardvormingusse, mis moodustas W3Catalogi aluse. See oli esimene primitiivne veebis kasutatav otsingumootor ning anti välja 2. septembril 1993.[5] 1993. aasta juunis lõi Matthew Gray arvatavasti esimese veebiroboti, Perli baasil loodud World Wide Web Wandereri ja kasutas seda Wandexi-nimelise indeksi loomiseks. Wandereri eesmärgiks oli mõõta veebi suurust, mida see tegi kuni 1995. aastate lõpuni. Veebi teine otsingumootor ALiweb ilmus novembris 1993. Aliweb ei kasutanud veebirobotit, vaid sõltus veebilehtede adminide teavitustest iga lehe olemasolu kohta. JumpStation ('hüppejaam') kasutas veebirobotit veebilehtede leidmiseks ja neist registri ehitamiseks ning kasutas veebiankeeti kasutajaliidesena oma päringu programmina. See oli seega esimene WWW ressursi avastamise vahend, milles olid liidetud kolm põhilist otsingumootori omadust (roomamine, indekseerimine ja otsimine). Kuna platvormil, millel see jooksis, olid piiratud ressursid, piirdus selle indekseerimine ja seega ka otsimine pealkirjadega lehtedega, mida ämblik oli külastanud. Üks esimesi täistekstiämbliku baasil toimivaid otsingumootoreid oli WebCrawler ('veebiämblik'), mis tuli välja 1994. aastal. Erinevalt eelkäijatest lasi see oma kasutajatel otsida iga sõna igal veebilehel, mis on sellest ajast saadik muutunud standardiks kõigile otsingumootoritele. See oli ka üks esimesi otsingumootoreid, mis sai tuntuks laiema avalikkuse ees. Peagi loodi veel mitu otsingumootorit, mis konkureerisid omavahel populaarsuse pärast. Nende seas olid Magellan, Excite, Infoseek, Inktomi, Northern Light ja AltaVista. Yahoo! oli üks populaarsemaid viise inimesi huvitavate veebilehtede leidmiseks, kuid selle otsingufunktsioon toimis enda veebiregistril täistekstlehtede koopiate asemel. Info otsijad said vaadata ka registrit otsingusõnal põhineva otsingu asemel. 1996. aastal tahtis Netscape anda ühele otsingumootorile eksklusiivse lepingu, millega see muutuks Netscape'i brauseriga kaasasolevaks otsingumootoriks. Huvi selle vastu oli nii suur, et Netscape tegi tehingu viie suurema otsingumootoriga. 5 miljoni dollari eest aastas olid Netscape'i otsingulehel ringluses viis otsimootorit: Yahoo!, Magellan, Lycos, Infoseek ja Excite.[6] Otsingumootoreid peeti ka eredamateks tähtedeks 1990. aastate lõpu internetti investeerimise hulluses.[7] Mitmel firmal läks turule sisenemine suurepäraselt, nad said avalikel pakkumistel rekordilisi tulusid. Mõned võtsid maha oma avalikud otsingumootorid ja turustasid ainult ettevõtetele mõeldud versioone nagu Northern Light. 2000. aasta paiku tõusis tippu Google'i otsingumootor. Firma saavutas paremaid tulemusi innovatsiooniga PageRank. See korduv algoritm hindab veebilehti PageRank-numbri alusel. Eeldatakse, et headele ja nõutud lehtedele lingitakse teistelt veebisaitidelt rohkem kui teistele. Google säilitas oma otsingumootoris ka minimalistliku kasutajaliidese, vastandina mitmetele konkurentidele, kelle otsingumootor oli veebiportaali sisse ehitatud.[viide?] 2000. aastal hakkas Yahoo! kasutama otsingutulemuste saamiseks Google'it. Yahoo! hankis endale Inktomi 2002. ja Overture'i 2003. aastal. 2004 tuli ta välja oma otsingumootoriga, milles olid ühendatud Yahoo!-le kuuluvate rakenduste tehnoloogiad.[viide?] Microsoft lasi MSN Searchi välja 1998. aasta sügisel, kasutades Inktomi otsingutulemusi. 1999. aasta alguses hakkas leht näitama loetelusid Looksmartist, mis olid kokku segatud tulemustega Inktomist.[viide?] 2004 hakkas Microsoft oma otsingutehnoloogiale üle minema.[viide?] Microsofti taasmärgistatud otsingumootor Bing avati kasutajatele 1. juunil 2009.[viide?]\n29. juulil lõpetasid Yahoo! ja Microsoft lepingu, mille kohaselt hakkab Yahoo! Search toimima Microsoft Bingi tehnoloogia baasil.[viide?] Otsingumootor toimib sellises järjekorras: Otsingumootorid toimivad salvestades infot mitmete veebilehtede kohta, mille nad otsivad välja HTML-ist endast. Need lehed leitakse veebiämbliku abil – see on automatiseeritud veebibrauser, mis järgib igat linki lehel. Erandeid saab teha robots.txt abil. Seejärel analüüsitakse iga lehe sisu, misjärel otsustatakse, kuidas seda indekseerida. Andmeid veebilehtede kohta hoitakse indeksi andmebaasides, et kasutada hilisemates päringutes, mis võib olla ka ühe sõna pikkune. Indeksi eesmärk on lubada info võimalikult kiiret leidmist. Mõned otsingumootorid, näiteks Google, salvestavad kas kõik või osa allika lehest ja ka infot veebilehtede kohta. Teised, nagu AltaVista, salvestavad iga sõna igalt lehelt, mis nad leiavad. Nii salvestatud leht omab tegelikku otsingumootori teksti, sest see oli see, mis tegelikult indekseeriti. Sellest on kasu, kui lehte on uuendatud ja otsingusõnu pole seal enam näha. Lehtede salvestamine võimaldab otsingu suurt täpsust, sest need võivad sisaldada andmeid, mida enam kusagil mujal ei leidu. Kui kasutaja sisestab otsingumootorisse päringu, siis uurib mootor oma indekseid ja tagastab nimekirja parima sobivusega veebilehtedest vastavalt oma sisule, tavaliselt lühikese kokkuvõttega dokumendi pealkirjast ja mõnikord lõiguga tekstist. Indeks ehitatakse üles informatsioonist, mis salvestatakse koos andmetega vastavalt info indekseerimismeetodile. Seni pole aga ühtegi avalikku otsingumootorit, mis lubaks faile otsida kuupäeva alusel. Enamik otsingumootoreid toetab konnektorite JAH, VÕI ja EI kasutamist, et võimaldada täpsema päringu esitamist. Konnektorid lubavad kasutajal muuta ja laiendada otsingutingimusi. Mootorid otsivad sõnu või fraase täpselt nii, nagu need sisestati. Mõned otsingumootorid pakuvad arenenud võimalust, mis lubab kasutajal määrata võtmesõnade vahelist kaugust. On ka ideelisi otsinguid, kus uurimine sisaldab statistilise analüüsi kasutamist lehtedel, mis sisaldavad sõnu või fraase, mida otsitakse. Loomuliku keele päringud lubavad kasutajal sisestada küsimuse nõnda, nagu seda küsitaks teiselt inimeselt, üks selline sait on näiteks ask.com. Otsingumootori tõhusus sõltub otsingutulemuste asjakohasusest. Kuigi teatud sõna või fraasi sisaldavaid lehekülgi on miljoneid, on mõned neist asjakohasemad, populaarsemad või usaldusväärsemad kui teised. Enamik otsingumootoreid kasutab meetmeid tulemuste järjestamiseks, et tuua \"parimad\" tulemused ettepoole. See, kuidas mootorid otsustavad, millised vasted on parimad ja millises järjekorras neid näidata, sõltub mootorist endast. Meetodid muutuvad aja jooksul samamoodi, nagu muutub interneti kasutamine ja tekivad uued tehnikad. Põhiliselt on olemas kahte tüüpi otsingumootoreid: üks on süsteem kindlaksmääratud ja hierarhiliselt järjestatud otsingusõnadega, mida on laialdaselt programmeeritud. Teine süsteem loob tagurpidi indeksi, analüüsides leitavat teksti ning toetub tugevamalt arvutile, mis teeb ära suurema osa tööst. Enamik otsingumootoreid on äriprojektid, mis teenivad tulu reklaami müügiga. Reklaamiandjad maksavad, et enda lehekülge otsingutulemuste seas kõrgemale tõsta. Otsingumootorid, mis tulemusi raha eest ei järjesta, teenivad, näidates oma tavatulemuste kõrval otsingutulemustega seotud reklaame. Otsingumootor teenib raha iga kord, kui keegi avab ühe sellise reklaami. Google'i otsingumootori ülemaailmne populaarsus jõudis tippu aprillis 2010, kui saadi kätte 86,3% turuosa.[8] Otsingumootorid nagu Yahoo! ja Bing olid populaarsemad Ameerikas kui Euroopas. Hiina Rahvavabariigis oli 2009. aasta juulis 61,6% turust Baidu käes.[9] NET MARKETSHARE kohaselt olid veebruaris 2021 otsimootorite turuosad maailmas järgmised: Google 72,68%, Bing 11,94%, Baido 11,72%, Yahoo! 1,81% ja ülejäänud jäid 2% sisse.[10] Kuigi otsingumootorid on programmeeritud reastama veebilehti populaarsuse ja asjakohasuse järgi, on kogemustel põhinevast uurimistööst näha, et neis leidub poliitilisi, majanduslikke ja sotsiaalseid eelarvamusi[11].[12] Nende eelarvamuste põhjuseks võivad olla majanduslikud, ärilised (nt firmad, mis reklaamivad end otsingumootorite abil võivad muutuda populaarsemaks loomulikes otsingutulemustes) ja poliitilised (nt otsingu tulemuste kustutamine, et olla vastavuses kohalike seadustega) protsessid.[13] Üheks näiteks, kus otsingutulemusi üritatakse mõjutada poliitilistel, sotsiaalsetel või ärilistel põhjustel, on \"Google Bombing\". Veebiämblik on robotprogramm, mis otsib veebis kindla ja korrapärase meetodiga uusi veebidokumente ja lisab leitud tulemused andmebaasidesse. Nimetuse on programm saanud selle järgi, et see ronib veebis ringi, nii nagu ämblik oma võrgul. Paljud otsingumootorite saidid kasutavad veebiämblikke, et tagada uusima info näitamine otsingutulemustes.\nKiirete otsingutulemuste saamiseks kasutatakse veebiämblikke, mis teevad külastatud lehtedest koopia, mida saab hiljem töödelda.\nÄmblikke võib kasutada veebilehtedel automaatseteks hooldustöödeks nagu linkide kontrollimine või HTML-koodi kinnitamine.\nLisaks võib neid kasutada ka veebilehtedelt kindla info leidmiseks, näiteks kogutakse e-posti aadresse rämpsposti saatmise eesmärgil. Veebiämblikel on alguses nimekiri URL-idest, mida nad peavad külastama. Iga kord, kui ta külastab ühte URL-i, tuvastab see kõik lehel olevad hüperlingid ja lisab need oma URL-ide nimekirja. Kõiki linke külastatakse kindlate reeglite järgi ja veebiämbliku käitumine oleneb nende reeglite kooskõlast:[14]"
  },
  {
    "url": "https://es.wikipedia.org/wiki/Motor_de_b%C3%BAsqueda",
    "title": "Motor de búsqueda - Wikipedia, la enciclopedia libre",
    "content": "Un motor de búsqueda o buscador es un sistema informático que busca archivos almacenados en servidores web gracias a su araña web.[1]​ Un ejemplo son los buscadores de Internet (algunos buscan únicamente en la web, pero otros lo hacen además en noticias, servicios como Gopher, FTP, etc.) cuando se pide información sobre algún tema. Las búsquedas se hacen con palabras clave o con árboles jerárquicos por temas; el resultado de la búsqueda «Página de resultados del buscador» es un listado de direcciones web en los que se mencionan temas relacionados con las palabras clave buscadas. Como operan de forma automática, los motores de búsqueda contienen generalmente más información que los directorios web. Sin embargo, estos últimos también han de construirse a partir de búsquedas (no automatizadas) o bien a partir de avisos dados por los creadores de páginas. Se pueden clasificar en tres tipos: Un rastreador web, indexador web, indizador web o araña web es un programa informático que inspecciona las páginas del World Wide Web de forma metódica y automatizada.[2]​ Uno de los usos más frecuentes que se les da consiste en crear una copia de todas las páginas web visitadas para su procesado posterior por un motor de búsqueda que indexa las páginas proporcionando un sistema de búsquedas rápido. Las arañas web suelen ser bots.[3]​ Las arañas web comienzan visitando una lista de URL, identifica los hiperenlaces en dichas páginas y los añade a la lista de URL a visitar de manera recurrente de acuerdo a determinado conjunto de reglas. La operación normal es que se le da al programa un grupo de direcciones iniciales, la araña descarga estas direcciones, analiza las páginas y busca enlaces a páginas nuevas. Luego descarga estas páginas nuevas, analiza sus enlaces, y así sucesivamente. Entre las tareas más comunes de las arañas de la web tenemos: Una tecnología muy simple por gran cantidad de scripts disponibles, ya que no se requieren muchos recursos. En cambio, se requiere más soporte humano y mantenimiento.[5]​ Un metabuscador es un sistema que localiza información en los motores de búsqueda más usados, carece de base de datos propia, por lo que usa las de otros buscadores y muestra una combinación de las mejores páginas que ha devuelto cada uno,[6]​ de una sola vez y desde un solo punto.[7]​ «En otras palabras para aludir al concepto más genérico de un buscador, podemos afirmar que un metabuscador es el buscador que incorpora un conjunto de buscadores. Algunos ejemplos de metabuscadores son: Dogpile, Aleyares[9]​[10]​ MetaCrawler, entre otros. Estos metabuscadores presentan ventajas, como ampliar el espacio de búsqueda y en algunos casos mostrar la posición de la web».[11]​ En 1945, Vannevar Bush , quien escribió un artículo en  The Atlantic Monthly titulado As We May Think[13]​ en el que imaginó bibliotecas de investigación con anotaciones conectadas no muy diferentes a los hiperenlaces modernos.[14]​ El análisis de enlaces finalmente se convertiría en un componente crucial de los motores de búsqueda a través de algoritmos como Hyper Search y PageRank.[15]​[16]​ Los primeros motores de búsqueda de Internet son anteriores al debut de la Web en diciembre de 1990: la búsqueda de usuarios de WHOIS se remonta a 1982,[17]​ y la búsqueda de usuarios de redes múltiples del Knowbot Information Service se implementó por primera vez en 1989.[18]​ La primera búsqueda bien documentada El motor que buscaba archivos de contenido, a saber, archivos FTP, era Archie, que debutó el 10 de septiembre de 1990.[19]​ Antes de septiembre de 1993, la World Wide Web se indexaba completamente a mano. Había una lista de servidores web editada por Tim Berners-Lee y alojada en el servidor web del CERN. Queda una instantánea de la lista en 1992,[20]​ pero a medida que más y más servidores web se pusieron en línea, la lista central ya no pudo mantenerse al día. En el sitio de la NCSA, se anunciaron nuevos servidores bajo el título What's New!.[21]​ La primera herramienta utilizada para buscar contenido (a diferencia de usuarios) en Internet fue Archie.[22]​ El nombre significa \"archivo\" sin la \"v\",[23]​ Fue creado por Alan Emtage[23]​[24]​[25]​[26]​ estudiante de informática en la Universidad McGill en Montreal, Quebec, Canadá . El programa descargó las listas de directorios de todos los archivos ubicados en sitios públicos anónimos de FTP (Protocolo de transferencia de archivos), creando una base de datos de búsqueda de nombres de archivos; sin embargo, Archie Search Engineno indexó el contenido de estos sitios ya que la cantidad de datos era tan limitada que se podía buscar fácilmente de forma manual. El auge de Gopher (creado en 1991 por Mark McCahill en la Universidad de Minnesota) dio lugar a dos nuevos programas de búsqueda, Veronica y Jughead. Al igual que Archie, buscaron los nombres y títulos de los archivos almacenados en los sistemas de índice Gopher. Veronica (Very Easy Rodent - Oriented Net-wide Index to Computerized Archives) proporcionó una búsqueda de palabras clave de la mayoría de los títulos de menú de Gopher en todos los listados de Gopher. Jughead (Jonzy 's Universal Gopher Hierarchy Excavation And Display) era una herramienta para obtener información de menú de servidores Gopher específicos. Si bien el nombre del motor de búsqueda \"Archie Search Engine\" no era una referencia a la serie de cómics de Archie, Veronica y Jughead son personajes de la serie, haciendo así referencia a su predecesor. En el verano de 1993 no existía ningún motor de búsqueda para la web, aunque se mantenían a mano numerosos catálogos especializados. Oscar Nierstrasz de la Universidad de Ginebra escribió una serie de secuencias de comandos de Perl que reflejaban periódicamente estas páginas y las reescribían en un formato estándar. Esto formó la base de W3Catalog , el primer motor de búsqueda primitivo de la web, lanzado el 2 de septiembre de 1993.[27]​ En junio de 1993, Matthew Gray,[28]​ entonces en el MIT, produjo lo que probablemente fue el primer robot web, el World Wide Web Wanderer basado en Perl , y lo usó para generar un índice llamado Wandex. El propósito de Wanderer era medir el tamaño de la World Wide Web, lo que hizo hasta fines de 1995. El segundo motor de búsqueda de la web, Aliweb, apareció en noviembre de 1993. Aliweb no usaba un robot web sino que dependía de ser notificado por administradores del sitio web de la existencia en cada sitio de un archivo índice en un formato particular. JumpStation (creada en diciembre de 1993[29]​ por Jonathon Fletcher) usó un robot web para encontrar páginas web y construir su índice, y usó un formulario web como interfaz para su programa de consulta. Por lo tanto, fue la primera herramienta de descubrimiento de recursos WWW que combinó las tres características esenciales de un motor de búsqueda web (rastreo, indexación y búsqueda) como se describe a continuación. Debido a los recursos limitados disponibles en la plataforma en la que se ejecutaba, su indexación y, por lo tanto, las búsquedas se limitaban a los títulos y encabezados que se encontraban en las páginas web que encontraba el rastreador. El primer buscador fue Wandex, un índice realizado por el World Wide Web Wanderer, un robot desarrollado por Mattew Gray en el MIT, en 1993. Otro de los primeros buscadores, Aliweb, también apareció en 1993 y todavía está en funcionamiento. El primer motor de búsqueda de texto completo fue WebCrawler, que apareció en 1994. A diferencia de sus predecesores, este permitía a sus usuarios una búsqueda por palabras en cualquier página web, lo que llegó a ser un estándar para la gran mayoría de los buscadores. WebCrawler fue asimismo el primero en darse a conocer ampliamente entre el público. También apareció en 1994 Lycos (que comenzó en la Carnegie Mellon University). Muy pronto aparecieron muchos más buscadores, como Excite, Infoseek, Inktomi, Northern Light y Altavista. De algún modo, competían con directorios (o índices temáticos) populares tales como Yahoo!. Más tarde, los directorios se integraron o se añadieron a la tecnología de los buscadores para aumentar su funcionalidad. Antes del advenimiento de la Web, había motores de búsqueda para otros protocolos o usos, como el buscador Archie, para sitios FTP anónimos y el motor de búsqueda Verónica, para el protocolo Gopher. En 1996 Larry Page y Sergey Brin comenzaron un proyecto que llevaría a la aparición del buscador más utilizado hoy en día: Google. El proyecto inicial se llamó BackRub,[30]​ que era el nombre de la tecnología utilizada para su desarrollo. BackRub basaba la importancia de los sitios web en la cantidad de enlaces que recibía. Presentaba una interfaz muy sencilla y capaz de mostrar al usuario los resultados más relevantes para cada una de las búsquedas. Con la llegada de Google, el modo en que los motores de búsqueda funcionaban cambió de forma radical, democratizando los resultados que se ofrecen en su buscador. Google basó el funcionamiento de su motor de búsqueda en la relevancia de los contenidos de cada sitio web para los propios usuarios, es decir, priorizando aquellos resultados que los usuarios consideraban más relevantes para una temática concreta. Para ello patentó su famoso PageRank, un conjunto de algoritmos que valoraban la relevancia de un sitio web asignándole un valor numérico del 0 al 10. En la mayoría de países Google.com o la versión de Google para el país concreto, es el buscador más utilizado, sin embargo, esto no ocurre en algunos países. Por ejemplo, en Rusia el buscador más utilizado es Yandex.ru[31]​[32]​ y en China es Baidu.[33]​ Conforme ha ido pasando el tiempo, miles de buscadores han ido naciendo y muriendo, entre los que podemos mencionar: Ver más información sobre esto en el Anexo:Motores de búsqueda Alrededor de 2000, el motor de búsqueda de  Google saltó a la fama.[34]​ La empresa logró mejores resultados para muchas búsquedas con un algoritmo llamado PageRank, como se explicó en el artículo Anatomía de un motor de búsqueda escrito por Sergey Brin y Larry Page , los fundadores posteriores de Google.[16]​ Este algoritmo iterativo clasifica las páginas web según el número y el PageRank de otros sitios web y páginas que enlazan allí, con la premisa de que las páginas buenas o deseables están enlazadas a más que otras. La patente de Larry Page para PageRank cita la patente anterior de RankDex de Robin Li como una influencia. Google también mantuvo una interfaz minimalista para su motor de búsqueda. En cambio, muchos de sus competidores incrustaron un motor de búsqueda en un portal web. De hecho, el motor de búsqueda de Google se hizo tan popular que surgieron motores falsos como Mystery Seeker. Para el año 2000, Yahoo! proporcionaba servicios de búsqueda basados en el motor de búsqueda de Inktomi. Yahoo! adquirió Inktomi en 2002 y Overture (propietaria de AlltheWeb y AltaVista) en 2003. Yahoo! cambió al motor de búsqueda de Google hasta 2004, cuando lanzó su propio motor de búsqueda basado en las tecnologías combinadas de sus adquisiciones. Microsoft lanzó por primera vez MSN Search en el otoño de 1998 utilizando los resultados de búsqueda de Inktomi. A principios de 1999, el sitio comenzó a mostrar listados de Looksmart, combinados con los resultados de Inktomi. Durante un breve período de tiempo en 1999, MSN Search utilizó en su lugar los resultados de AltaVista. En 2004, Microsoft inició una transición hacia su propia tecnología de búsqueda, impulsada por su propio rastreador web (llamado msnbot). El motor de búsqueda renombrado de Microsoft, Bing, se lanzó el 1 de junio de 2009. El 29 de julio de 2009, Yahoo! y Microsoft cerraron un trato en el que Yahoo! la búsqueda estaría impulsada por la tecnología Microsoft Bing. A partir de 2019, los rastreadores de motores de búsqueda activos incluyen los de Google, Petal, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo y Yandex. Aunque los motores de búsqueda están programados para clasificar sitios web en función de una combinación de su popularidad y relevancia, los estudios empíricos indican varios sesgos políticos, económicos y sociales en la información que proporcionan[35]​[36]​ y las suposiciones subyacentes sobre la tecnología.[37]​ Estos sesgos pueden ser el resultado directo de procesos económicos y comerciales (p. ej., las empresas que anuncian con un motor de búsqueda también pueden volverse más populares en sus resultados de búsqueda orgánicos) y procesos políticos (p. ej., la eliminación de resultados de búsqueda para cumplir con las leyes locales).[38]​ Por ejemplo, Google no mostrará ciertos sitios web neonazis en Francia y Alemania, donde la negación del Holocausto es ilegal. Los sesgos también pueden ser el resultado de procesos sociales, ya que los algoritmos de los motores de búsqueda suelen estar diseñados para excluir puntos de vista no normativos en favor de resultados más populares.[39]​ Los algoritmos de indexación de los principales motores de búsqueda se inclinan hacia la cobertura de sitios basados en los EE. UU., en lugar de sitios web de países fuera de los EE. UU.[36]​ Google Bombing es un ejemplo de un intento de manipular los resultados de búsqueda por motivos políticos, sociales o comerciales. Varios académicos han estudiado los cambios culturales desencadenados por los motores de búsqueda,[40]​ y la representación de ciertos temas controvertidos en sus resultados, como el terrorismo en Irlanda,[41]​ la negación del cambio climático,[42]​ y las  teorías de la conspiración.[43]​ Muchos motores de búsqueda como Google y Bing brindan resultados personalizados basados en el historial de actividad del usuario. Esto conduce a un efecto que se ha denominado filtro burbuja. El término describe un fenómeno en el que los sitios web usan algoritmos para adivinar selectivamente qué información le gustaría ver a un usuario, en función de la información sobre el usuario (como la ubicación, el comportamiento de clics anterior y el historial de búsqueda). Como resultado, los sitios web tienden a mostrar solo información que concuerda con el punto de vista anterior del usuario. Esto pone al usuario en un estado de aislamiento intelectual sin información contraria. Los principales ejemplos son los resultados de búsqueda personalizados de Google y el flujo de noticias personalizado de Facebook. Según Eli Pariser, quien acuñó el término, los usuarios están menos expuestos a puntos de vista conflictivos y están aislados intelectualmente en su propia burbuja informativa. Pariser relató un ejemplo en el que un usuario buscó en Google \"BP\" y obtuvo noticias de inversión sobre British Petroleum , mientras que otro buscador obtuvo información sobre el derrame de petróleo de Deepwater Horizon y que las dos páginas de resultados de búsqueda eran \"sorprendentemente diferentes\".[44]​[45]​[46]​ El efecto burbuja puede tener implicaciones negativas para el discurso cívico, según Pariser.[47]​ Desde que se identificó este problema, han surgido motores de búsqueda de la competencia que buscan evitar este problema al no rastrear o \"burbujear\" a los usuarios, como DuckDuckGo. Otros académicos no comparten el punto de vista de Pariser y consideran que la evidencia en apoyo de su tesis no es convincente.[48]​ El envío de un motor de búsqueda web es un proceso en el que un webmaster envía un sitio web directamente a un motor de búsqueda. Si bien el envío a los motores de búsqueda a veces se presenta como una forma de promocionar un sitio web, generalmente no es necesario porque los principales motores de búsqueda utilizan rastreadores web que finalmente encontrarán la mayoría de los sitios web en Internet sin ayuda. Pueden enviar una página web a la vez o pueden enviar todo el sitio usando un mapa del sitio , pero normalmente solo es necesario enviar la página de inicio.de un sitio web ya que los motores de búsqueda pueden rastrear un sitio web bien diseñado. Quedan dos razones para enviar un sitio web o una página web a un motor de búsqueda: agregar un sitio web completamente nuevo sin esperar a que un motor de búsqueda lo descubra y actualizar el registro de un sitio web después de un rediseño sustancial. Algunos programas de envío de motores de búsqueda no solo envían sitios web a múltiples motores de búsqueda, sino que también agregan enlaces a sitios web desde sus propias páginas. Esto podría parecer útil para aumentar la clasificación de un sitio web,[49]​ ya que los enlaces externos son uno de los factores más importantes que determinan la clasificación de un sitio web. Sin embargo, John Mueller de Google ha declarado que esto \"puede dar lugar a una gran cantidad de enlaces no naturales para su sitio\" con un impacto negativo en la clasificación del sitio.[50]​"
  },
  {
    "url": "https://eo.wikipedia.org/wiki/Interreta_ser%C4%89ilo",
    "title": "Serĉilo - Vikipedio",
    "content": "Serĉilo estas programo, kiun oni uzas por serĉi informojn en komputila sistemo kiel la Interreto. Kun serĉilo, oni povas trovi informon pri laŭvola temo. Oni tajpas la temon en formo de vortoj aŭ frazeto. La serĉilo respondos kun listo de dokumentoj, kiuj (normale) koncernas la temon. La plej ofta serĉo per serĉilo estas por TTT-paĝoj. Tamen, serĉoj en serĉilo ne estas limigita al nuraj TTT-paĝoj. Oni povas serĉi bildojn, novaĵgrupajn artikolojn, telefonnumerojn, mapojn, kaj aliajn dokumentojn. Serĉilo malsimilas al interreta katalogo per tio, ke katalogo estas ĝisdatigata per homaj redaktistoj, sed serĉilo estas ĝisdatigata per komputila algoritmo. Sten Johansson en sia eseo Uzi interreton kiel tekstaron por lingvaj esploroj [2005][1] klarigas kiel li uzis interretajn rimedojn, precipe serĉilojn kaj la disponeblon de la Tekstaro de Esperanto (versioj kaj de antaŭ kaj de post 1940) por studi kiuj vortoj, kombinoj kaj gramatikaj formoj estas uzataj, kaj laŭ kiu ofteco. La eseisto komparas la rezultojn de serĉiloj en 2002 kaj 2004. Antaŭ ĉio la aŭtoro avertas pri dek limigoj kiuj relativigas la trovitajn rezultojn, inter kiuj: Krom precizaj ekzemploj, li dediĉas partan atenton al klasikaj diskutoj. Ekzemple pri la nomitaj mal-vortoj, li konkludas ke inter \"venkintaj\" kazoj estus humida kontraŭ malseketa, stulta kontraŭ malsaĝa kaj ĉefe malsprita, kvereli kontraŭ malpaci, strikta kontraŭ malvasta, dum inter nesukcesintaj estus povra super kiu ankoraŭ hegemonias malriĉa aŭ eĉ kompatinda, leĝera super kiu ankoraŭ hegemonias malpeza, frida super kiu ankoraŭ hegemonias malvarma, aŭ olda super kiu ankoraŭ hegemonias maljuna kaj malnova. Pri la polemiko inter landnomoj, la komparo inter diversaj epokoj konfirmas la venkon de landonomoj kun finaĵo -io super finaĵo -ujo (ĉiam en la interreta etoso kaj ne nepre en aliaj), la partikularan venkon de Barato super Hindio, kaj la retenon de la finaĵo -ujo ĉe Esperantujo kaj ĉe antikvaj landoj kiaj Egiptujo antaŭ Egipto por la nuna ŝtato, aŭ Ĉinujo antaŭ Ĉinio. Pri la litero ĥ la komparoj faritaj en 2004 konstatas, ke ĝi pluhegemonias nur en kelkaj vortoj kiaj Ĥarkovo, monaĥejo, ĥimero aŭ jaĥto kontraŭ la respektivaj korespondoj kun litero k, dum tiuj klare venkas en kirurgo, kameleono, mekanika, kemio, arkaika, anarkio, tekniko kaj ĉefe arkitekturo kun 90 %; same ankaŭ en la Tekstaro   hegemoniis monaĥejo kaj ĥaoso, dum ankaŭ tekniko. Pri la elekto inter kiel kaj kiom por indiki gradon, oni konstatas ke ĉe \"adjektivoj kaj adverboj mezureblaj oni pli ofte uzas tiom. Ĉe la vortoj multe kaj malmulte oni eĉ preferas tiom. Ĉe nemezureblaj ecoj kiel bela, bona kaj feliĉa oni plej ofte preferas esprimi gradon per tiel.\"[2] El la falsaj korelativoj formitaj el radiko ali- oni konstatas ke nur estas iomete uzata alies, kiu relative malkreskas. Oni konstatas malgrandan uzon de vorto gepatro en singularo, de finaĵo -ika anstataŭ -a precipe ĉe ekzotika aŭ erotika kaj nedisputeble ĉe mekanika, de finaĵo -acio anstataŭ -o ĉefe en civilizacio aŭ konversacio, sed ne en inaŭguro. Oni konfirmas ankaŭ la finan venkon de modernaj vortoj kiaj komputilo, aidoso, retpoŝto, poŝtelefono, hamburgero, fritoj, kolao kaj ĝinzo."
  },
  {
    "url": "https://eu.wikipedia.org/wiki/Web_bilaketa_motore",
    "title": "Web bilaketa motore - Wikipedia, entziklopedia askea.",
    "content": "Bilaketa-tresna edo bilatzailea bere web armiarmari esker web zerbitzarietan gordetako fitxategiak bilatzen dituen sistema informatikoa da.  Adibide bat Interneteko bilatzaileak dira (batzuek sarean bakarrik bilatzen dute, baina beste batzuek albisteak, Gopher bezalako zerbitzuak, FTP, etab.) gai bati buruzko informazioa eskatzen dugunean. Bilaketak gako-hitzekin edo zuhaitz hierarkikoekin egiten dira gaika; bilaketaren emaitza «Bilaketa-emaitzen orria» web-helbideen zerrenda bat da, non bilatutako gako-hitzekin lotutako gaiak aipatzen diren. Automatikoki funtzionatzen dutenez, bilatzaileek, oro har, web direktorioek baino informazio gehiago dute. Hala ere, azken horiek bilaketetatik (ez automatizatuak) edo orrialde sortzaileek emandako oharretatik ere eraiki behar dira."
  },
  {
    "url": "https://fa.wikipedia.org/wiki/%D9%85%D9%88%D8%AA%D9%88%D8%B1_%D8%AC%D8%B3%D8%AA%D8%AC%D9%88%DB%8C_%D9%88%D8%A8",
    "title": "موتور جستجوی وب - ویکی‌پدیا، دانشنامهٔ آزاد",
    "content": "موتور جستجوی وب یا جویشگر (به انگلیسی: Web search engine) موتور جستجو و ابزاری است که به منظور جُستجو در وب برای به‌دست آوردن اطلاعات درخواست شده، به کار می‌رود. نتایج یافته شده به‌طور معمول در صفحه‌ای با عنوان صفحهٔ نتایج جستجو فهرست می‌شوند.[۳] با استفاده از کلمهٔ کلیدی (کلیدواژه) که در واقع توضیحی است کوتاه دربارهٔ آنچه لازم است در اینترنت پیدا شود، کلمه کلیدی باید تا آنجا که ممکن است کوتاه، جزئی، قابل فهم و دقیق باشد. به غیر از وارد کردن کلمهٔ مستقیم، می‌توان با استفاده از عملگرهایی عمل جستجو را دقیق‌تر و منظم‌تر انجام داد.[۴][۵][۶] موتورهای جستجوی وب برای آنکه بتوانند به درخواست‌ها و جستجوهای کاربران پاسخ مناسبی بدهند باید خزیدن در وب‌سایت‌ها و ایندکس کردن صفحات را انجام دهند.[۷]\nوظیفه اصلی موتورهای جستجو این است که بهترین نتیجه را به جستجوی کاربر ارائه دهند. هنگامی که کاربر یک پرس و جو را در موتور جستجو انجام می‌دهد، صفحه نتایج موتور جستجو (SERP) که حاوی نتایج است به کاربر نمایش داده می‌شود. نتایج یافت شده بر اساس ارتباط با پرس و جوی کاربر رتبه‌بندی می‌شوند. نحوه انجام رتبه‌بندی در موتورهای جستجو متفاوت است. موتورهای جستجو مثل گوگل، مدام الگوریتم‌های خود را تغییر می‌دهند و بروز می‌کنند تا تجربه کاربر از جستجو را بهبود بخشند. هدف آنها این است که ابتدا درک کنند چگونه کاربران جستجو می‌کنند و چه چیزی را جستجو می‌کنند و سپس به آنها بهترین پاسخ‌ها را نمایش دهند. [۸] موتورهای جستجو با استفاده از خزنده‌های خود می‌توانند تمامی صفحات و فایل‌های موجود در وب را ایندکس کنند. این خزنده‌ها با ورود به هر صفحه به دنبال لینک‌ها هستند تا با دنبال کردن آن‌ها وارد صفحات جدید شوند. با این روش، موتورهای جستجو می‌توانند تمامی صفحات موجود در وب را ایندکس کنند.[۹][۱۰] اصلی‌ترین وظیفه موتورهای جستجوی وب، ارائه بهترین و مرتبط‌ترین نتایج به کاربران است. موتورهای جستجو برای آنکه بتوانند بهترین نتایج را هنگام جستجوی یک عبارت خاص به کاربران نمایش دهند، قوانین و استانداردهایی برای وب سایت‌ها تعریف کرده‌اند تا بتوانند علاوه بر دسترسی ساده‌تر به صفحات وب سایت‌ها، محتوای موجود در صفحات را بهتر درک کنند. با این کار، موتورهای جستجو می‌توانند وب سایت‌های که دارای بیشترین ارتباط معنایی با عبارت جستجو شده توسط کاربر هستند در رتبه‌های بالاتری در نتایج جستجو قرار دهند.[۱۱][۱۲][۱۳] موتورهای جستجو برای ایجاد نتیجه یک جستجو، تعدادی فعالیت انجام می‌دهند: سئو مخفف عبارت Search Engine Optimization به معنی «بهینه‌سازی موتورهای جستجو» است و به مجموعه اقدامات برنامه‌ریزی شده و دقیقی گفته می‌شود که رنکینگ سایت شما را در رتبه‌بندی موتورهای جستجو بهبود می‌بخشد. سئو سایت باید توسط متخصصان سئو صورت گیرد. سئو را برخی اوقات نیز SEO Copywriting نیز می‌گویند زیرا بیشتر تکنیک‌هایی که در آن استفاده می‌شود به محتوا مربوط می‌شود. تمام حرف سئو بهینه‌سازی سایت برای موتورهای جستجو است و به صورت کلی فرایندی است که باعث می‌شود یک وب سایت رتبه خوبی در موتورهای جستجو به دست آورد. سئو زیر مجموعه ای از بازاریابی موتورهای جستجو است. بازاریابی سئو یعنی آگاهی از نحوه کار الگوریتم‌های جستجو و شناخت اینکه کاربران اینترنت چه چیزهایی را جستجو می‌کنند.[۱۴][۱۵][۱۶] اما در سئو خارجی موارد مورد بحث و بررسی متفاوت است که مهم‌ترین آنها لینک سازی می‌باشد؛ لینک سازی را می‌توان در جاهای مختلفی انجام داد که چند عدد از بهترین محل‌ها برای لینک سازی را در ادامه معرفی می‌شود: تا تاریخ ژوئن ۲۰۲۱[بروزرسانی]،[۲۷] موتورجستجو گوگل بیشترین سهم با ۹۲٫۴۹٪جستجوهای انجام شده در دنیا را دارد. بقیهٔ موتورهای پراستفاده این‌ها بودند:"
  },
  {
    "url": "https://fr.wikipedia.org/wiki/Moteur_de_recherche_web",
    "title": "Moteur de recherche — Wikipédia",
    "content": "Cet article ne cite pas suffisamment ses sources (septembre 2010). Si vous disposez d'ouvrages ou d'articles de référence ou si vous connaissez des sites web de qualité traitant du thème abordé ici, merci de compléter l'article en donnant les références utiles à sa vérifiabilité et en les liant à la section « Notes et références ». Cet article contient une ou plusieurs listes (décembre 2024). modifier - modifier le code - modifier Wikidata Un moteur de recherche est une application permettant à un utilisateur d'effectuer une recherche locale ou en ligne, c'est-à-dire de trouver des ressources à partir d'une requête composée de termes. Les ressources peuvent notamment être des pages web, des articles de forums Usenet, des images, des vidéos, des fichiers, des ouvrages, des sites, pédagogiques, des applications, des logiciels open source. Des intelligences artificielles comme Perplexity commencent à entrer en concurrence avec les moteurs de recherche « classiques ». Un moteur de recherche fonctionne généralement selon le principe suivant : Il commence par indexer du contenu dans une ou plusieurs bases de données qui lui appartiennent. Cette indexation est effectuée au préalable de toute recherche effectuée par un utilisateur. Lorsqu'une requête est soumise, le moteur utilise ses paramètres et algorithmes de classement pour générer une liste de résultats. Certains sites web offrent un moteur de recherche comme principale fonctionnalité ; on appelle alors « moteur de recherche » le site lui-même. Contrairement aux annuaires, les moteurs de recherche ne nécessitent pas d'intervention humaine. Ils sont basés sur des « robots », appelés « bots », « spiders «, « crawlers » ou « agents », qui parcourent automatiquement les sites à intervalles réguliers afin de découvrir de nouvelles adresses (URL). Ils suivent les liens hypertexte qui relient les pages les unes aux autres, les uns après les autres. Chaque page identifiée est alors indexée dans une base de données, accessible ensuite par les internautes à partir de mots-clés. C'est par abus de langage qu'on appelle également « moteurs de recherche » des sites web proposant des annuaires de sites web : dans ce cas, ce sont des instruments de recherche élaborés par des personnes qui répertorient et classifient des sites web jugés dignes d'intérêt, et non des robots d'indexation. Les moteurs de recherche ne s'appliquent pas uniquement à Internet : certains moteurs sont des logiciels installés sur un ordinateur personnel. Ce sont des moteurs dits « de bureau » qui combinent la recherche parmi les fichiers stockés sur le PC et la recherche parmi les sites web. On peut citer par exemple Copernic Desktop Search, Windex Server, etc. On trouve également des métamoteurs, c'est-à-dire des sites web où une même recherche est lancée simultanément sur plusieurs moteurs de recherche, les résultats étant ensuite fusionnés pour être présentés à l'internaute. Les moteurs de recherche Internet précèdent les débuts du Web à la fin des années 1990 : Les moteurs de recherche sont inspirés des outils de recherche documentaire (à base de fichiers inversés, alias fichiers d'index) utilisés sur les mainframes depuis les années 1970, comme le logiciel STAIRS sur IBM. Le mode de remplissage de leurs bases de données est cependant différent, car orienté réseau. Par ailleurs la distinction entre données formatées (« Champs ») et texte libre n'y existe plus, bien que commençant depuis 2010 à se réintroduire par le biais du web sémantique. Des moteurs historiques ont été Yahoo! (1994), Lycos (1994), AltaVista (1995, premier moteur 64 bits) et Backrub (1997), ancêtre de Google (1998). Google apporte un changement important : il stocke dans ses serveurs les pages qu'il indexe, ce que ne faisaient pas les autres moteurs. Et pour répondre aux besoins des étudiants, des universitaires, chercheurs et ingénieurs, des moteurs spécialisés destinés aux sujets scientifiques et techniques, comme Google Scholar sont apparus. Dans les années 2020, des systèmes d'intelligence artificielle émergent face aux moteurs de recherche classiques. Par exemple, Perplexity AI et son robot d'exploration web (PerplexityBot), pourrait à moyen terme bouleverser la recherche en ligne, à ce moment dominé par des acteurs comme Google ou Bing[1]. Ces nouveaux agents conversationnels, au lieu de présenter une longue liste de liens parmi lesquels l'internaute peut choisir, fournissent des réponses synthétiques, contextualisées et sourcées[2],[3], à partir de données qu'ils consultent en temps réel. Chaque réponse est accompagnée de citations cliquables, renforçant la transparence et la vérifiabilité des contenus. Une autre différence est que les IA comme Perplexity comprennent bien mieux le langage courant, mémorisent le contexte des échanges et proposent des suggestions pertinentes pour approfondir les recherches[4]. Les géants du web réagissent à cette montée en puissance : Apple a confirmé en 2025 des discussions avec Perplexity, OpenAI et Anthropic pour intégrer leurs moteurs IA dans Safari, en réponse au procès antitrust contre Google. Mozilla teste aussi l'intégration de Perplexity dans Firefox ; pendant que Perplexity a, de son côté, lancé son propre navigateur, Comet, conçu autour de son moteur IA et d'un assistant embarqué capable de résumer des pages web, interagir avec des contenus et automatiser des tâches simples. Cette concurrence soulève des enjeux majeurs : Le fonctionnement d'un moteur de recherche comme tout instrument de recherche se décompose en trois processus principaux : Des modules complémentaires sont souvent utilisés en association avec les trois briques de bases du moteur de recherche. Les plus connus sont les suivants : Afin d'optimiser les moteurs de recherche, les webmestres insèrent des métaéléments (métatags) dans les pages web, dans l'en-tête HTML (head). Ces informations permettent d'optimiser les recherches d'information sur les sites web. Les sites dont la recherche est le principal service se financent par la vente de technologie et de publicité. Le financement par la publicité consiste à présenter des publicités correspondant aux mots recherchés par le visiteur. L'annonceur achète des mots-clés : par exemple une agence de voyages peut acheter des mots-clés comme « vacances », « hôtel » et « plage » ou « Cannes », « Antibes » et « Nice » si elle est spécialisée dans cette région. Cet achat permet d'obtenir un référencement dit « référencement payant » à distinguer du référencement dit « référencement naturel ». Le moteur de recherche peut afficher la publicité de deux manières : en encart séparé ou en l'intégrant aux résultats de la recherche. Pour le visiteur, l'encart séparé se présente comme une publicité classique. L'intégration aux résultats se fait en revanche au détriment de la pertinence des résultats et peut avoir des retombées négatives sur la qualité perçue du moteur. De ce fait, tous les moteurs ne vendent pas de placement dans les résultats. Les moteurs de recherche constituent un enjeu économique. La valeur boursière du holding Alphabet propriétaire de Google, principal moteur de recherche, était de 831 milliards USD en avril 2020[5]. L'importance des enjeux économiques a généré des techniques de détournement malhonnêtes des moteurs de recherche pour obtenir des référencements « naturels », le spamdexing (référencement abusif en français). Les techniques les plus pratiquées de spamdexing sont : Les techniques de référencement abusif sont pourchassées par les éditeurs de moteurs de recherches, qui constituent des listes noires, provisoires ou définitives. On distingue le spamdexing, détournement malhonnête, du « SEO », Search Engine Optimization (optimisation pour les moteurs de recherche en français). Les techniques de SEO sont commercialisées par des sociétés spécialisées. Les grandes organisations (entreprises, administrations) disposent généralement de très nombreuses ressources informatiques dans un vaste intranet. Leurs ressources n'étant pas accessibles depuis Internet, elles ne sont pas couvertes par les moteurs de recherche du web. Elles doivent donc installer leur propre moteur si elles veulent mener des recherches dans leurs ressources. Elles constituent donc un marché pour les développeurs de moteurs de recherche. On parle alors de moteur de recherche pour entreprise (voir plus bas). Il arrive également que des sites web publics utilisent les services d'un moteur de recherche pour étoffer leur offre. On parle alors de « SiteSearch ». Ces logiciels permettent la recherche de contenus dans un ou plusieurs groupes de sites. Ces technologies sont particulièrement exploitées sur les sites de contenus et les sites de vente en ligne. La particularité de ces outils est souvent la complexité de mise en œuvre et les ressources techniques nécessaires disponibles. Il arrive aussi que de grands portails exploitent la technologie des moteurs de recherche. Ainsi Yahoo!, spécialiste de l'annuaire web, a utilisé pendant quelques années la technologie de Google pour la recherche jusqu'à ce qu'elle lance son propre moteur de recherche Yahoo Search Technology en 2004 dont les fondations proviennent de Altavista, Inktomi et Overture, sociétés fondatrices des moteurs de recherche et rachetées par Yahoo!. De plus en plus de producteurs de contenu, à la suite des recommandations du W3C sur le web sémantique, indexent leurs bases avec des métadonnées ou des taxinomies (ontologies), en vue de permettre aux moteurs de recherche de s'adapter aux analyses sémantiques. Ces formes de recherches et d'analyses de corpus d'informations par voie informatique ne sont encore que des potentialités. Par comparaison avec des recherches plein texte, des recherches réalisées sur le web sémantique doivent être plus conviviales pour l'utilisateur : Il n'existe pas encore à proprement parler de moteur de recherche sémantique qui permette de comprendre une question en langue naturelle et d'adapter une réponse en fonction des résultats trouvés. Quelques tentatives existent néanmoins pour chercher à répondre par des formes intermédiaires à cette problématique du sens dans la recherche d'information : L'abandon progressif des annuaires imprimés conduit les usagers à effectuer les mêmes recherches sur l'internet « profession+localité ». Google a donc acquis en 2010 un fichier d'entreprises (pour la France et un certain nombre de pays), pour effectuer un mixage des données web et annuaire lorsque les requêtes correspondent a une activité localisée. Cette nouvelle tendance se vérifie chez les principaux moteurs de recherche et de nouveaux « outils mixte » voient le jour. Yandex et Baidu n'ont pas encore adopté ce modèle de mixage. Selon une étude réalisée par McKinsey&Co[6], seulement 65 % des PME françaises disposaient d'une présence sur Internet en 2013. Les moteurs de recherche qui par définition collectent uniquement des données issues de l'internet, ont donc été obligés d'acquérir et de proposer ces adresses d'annuaire en complément pour satisfaire la recherche d'adresses des internautes. Google a baptisé ces adresses « Google Adresses », puis d'office basculées vers « Google + », actuellement « Google My Business ». Les moteurs de recherche Bing et Google ne communiquent pas l'origine de ces fichiers d'entreprises intégrés, hormis Yahoo! qui est en partenariat avec Pages Jaunes [7]. Les métamoteurs sont des outils de recherche qui interrogent plusieurs moteurs de recherche simultanément et affichent à l'internaute une synthèse pertinente. Exemples : Startpage, Searx, Seeks, Lilo, Framabee, Kagi... On désigne par « multi-moteurs (en) » (ou plus rarement, « super moteur »[9]) une page web proposant un ou plusieurs formulaires permettant d'interroger plusieurs moteurs. Il peut également (mais plus rarement) s'agir d'un logiciel, d'une fonction ou d'une extension de navigateur web, ou d'une barre d'outils… Le choix d'un des moteurs peut se faire par bouton, bouton radio, onglet, liste déroulante ou autre. Les premières pages de ce type recopiaient le code des formulaires de plusieurs moteurs. Avec l'apparition du JavaScript il est devenu possible de n'avoir plus qu'un seul formulaire.\nOn peut citer par exemple Creative Commons Search[10], Ecosia, Disconnect, le moteur de recherche de Maxthon, HooSeek (fermé en 2012), searchall.net, etc. Le moteur de recherche le plus connu et le plus utilisé concernant la littérature scientifique et technique est Google Scholar, dont l'algorithme indexe un grand nombre de bases de données et de métadonnées structurées de littérature scientifique et technique et de brevets, mais il existe d'autres moteurs, plus ou moins spécialisés : On désigne par « moteur de recherche solidaire », un moteur qui reverse une partie de ses revenus à des causes écologiques, sociales ou humanitaires. Ces moteurs sont nés du constat que les revenus annuels générés par la publicité sur les moteurs de recherche sont assez importants (environ 45 $ par an par utilisateur pour Google en 2014[12]). Les moteurs de recherches solidaires se distinguent notamment dans la façon de distribuer les revenus générés. Certains moteurs comme Ecosia reversent alors une partie des revenus à une seule et unique cause, alors que des moteurs comme Lilo et YouCare permettent aux internautes de choisir les projets à financer. Certains moteurs ont également adopté une politique de neutralité carbone, tels que Google, DuckDuckGo et Ecosia. Google affirme qu'il sera neutre en carbone d'ici 2030, en partie en achetant des énergies renouvelables et dès 2017, l'entreprise en rachète autant qu'elle en consomme mais il ne faut pas oublier que l'utilisation des énergies renouvelables produit souvent des gaz à effet de serre. Au début des années 2020 Google est le plus gros acheteur privé au monde de ce type d'énergie. Les énergies renouvelables étant essentiellement intermittentes, Google ne peut les utiliser directement ou de façon permanente : une entreprise qui se déclare neutre en carbone utilise dans les faits des « garanties d'origines renouvelables »(ce qui n'est pas forcément vrai, car certaines entreprises achètent des garanties d'origine renouvelables, mais fournissent en réalité l'électricité du réseau) qui permettent de s'assurer que l'énergie carbonée qu'elles consomment sera compensée par une production équivalente d'énergie renouvelable. Or, selon l'association The Shift Project, le modèle économique de Google nécessite toujours plus de puissance de calcul, de renouveler et d'augmenter son infrastructure, ses réseaux et ses équipements dont la production est une source importante de gaz à effet de serre. Pour l'association négaWatt, la communication de Google se focalise sur l'usage, mais éclipse les problèmes environnementaux liés à l'extraction des ressources, du transport et du recyclage[13],[14]. Le métamoteur de recherche Ecosia utilise 80 % de ses revenus publicitaires pour des projets de reforestation aux quatre coins du monde[15]. On désigne par « moteurs verticaux » une page web ou un service multimédia qui propose une recherche spécialisée dans un domaine professionnel ou qui est particulièrement ciblée. Cet outil de recherche est spécialisé dans un secteur particulier, tel que les télécommunications, le droit, la biotechnologie, la finance (assurance) ou encore l'immobilier. Son fonctionnement général est basé sur une base de données constituée à partir des bases de tous les sites spécialisés de l'activité ciblée. Certains de ces moteurs sont utilisé par des professionnels pour cibler des consommateurs, avec le plus souvent une finalité économique qui dérive sur la géolocalisation. On retrouve ainsi pour le grand public des annuaires, des comparateurs. Il en existe maintenant pour toutes les activités : immobilier, tourisme, recherche d'emploi, recrutement, automobile, loisirs, jeux. L'explosion du nombre de contenus de formats divers (données, informations non structurées, images, vidéos…) disponibles dans les entreprises les poussent à s'équiper de moteurs de recherche en interne. Selon une étude menée par MARKESS International en février 2008, 49 % des organisations ont déjà recours à un moteur de recherche d'entreprise, et 18 % envisagent son utilisation d'ici à 2010. Ces moteurs de recherche sont en majeure partie intégrés aux postes de travail ou aux outils de gestion électronique des documents, mais ils sont dans un nombre grandissant d'entreprises capables de couvrir à la fois les contenus internes et externes de l'entreprise, ou encore intégrés aux outils de gestion de contenu ou aux solutions décisionnelles. Parmi les acteurs proposant des moteurs de recherche d'entreprise figurent Google, Exalead, PolySpot ou OpenSearchServer. Les technologies d'analyse du langage, telles que la lemmatisation, l'extraction d'entités nommées, la classification et le clustering permettent d'améliorer grandement le fonctionnement des moteurs de recherche. Ces technologies permettent à la fois d'améliorer la pertinence des résultats et d'engager l'internaute dans un processus de recherche plus performant, comme c'est le cas avec la recherche à facettes [précision nécessaire]. Selon l'étude de l'ADEME « Internet, courriels, réduire les impacts » publiée en février 2014, aller directement à l'adresse d'un site, soit en tapant son adresse dans son navigateur, soit en l'ayant enregistré comme « favori » (plutôt que de rechercher ce site via un moteur de recherche) divise par 4 les émissions de gaz à effet de serre, ce qui dépend en réalité du type de moteur de recherche, les recherches dans les moteurs comme Ecosia ayant une empreinte moyenne négative[16]. Sur les autres projets Wikimedia :"
  },
  {
    "url": "https://ga.wikipedia.org/wiki/Inneall_cuardaigh",
    "title": "Inneall cuardaigh - Vicipéid",
    "content": "Is e atá i gceist le inneall cuardaigh ná acmhainn ar an ngréasán domhanda atá insroichte le brabhsálaí Gréasáin, a chabhraíonn leis an úsáideoir ionaid is eolas a aimsiú. Bíonn na hinnill chuardaigh (Yahoo, Lycos, Google, Ask Jeeves) ag cuardach tríd an ngréasán an t-am ar fad, ag tógáil innéacsanna ábhar éagsúla — mar shampla, ag aimsiú teidil, fotheidil, eochairfhocail is céadlínte cáipéisí. Uaidh sin, is féidir cuid mhaith cáipéisí éagsúla ar ábhar ar leith a aisghabháil. Déanann an cuardach leanúnach cinnte de go bhfuil na hinnéacsanna suas chun dáta. Mar sin féin, aisghabhann na hinnill an-chuid cháipéisí nach mbaineann le hábhar, agus tá an-iarracht ar siúl an t-am ar fad iad a fheabhsú. Comhlíonann an t-inneall cuardaigh na riachtanais seo: Ó Fheabhra 2023, is é Google an t-inneall cuardaigh is mó a úsáidtear ar domhan, le sciar den mhargadh de 93.37%. B’iad na hinnill chuardaigh ba mhó eile ná Bing, Yahoo!, Baidu, Yandex agus DuckDuckGo[1].[2]"
  },
  {
    "url": "https://ko.wikipedia.org/wiki/%EA%B2%80%EC%83%89_%EC%97%94%EC%A7%84",
    "title": "검색 엔진 - 위키백과, 우리 모두의 백과사전",
    "content": "검색 엔진(search engine)은 사용자의 쿼리에 응답하여 웹의 웹 페이지 및 기타 관련 정보에 대한 하이퍼링크를 제공하는 소프트웨어 시스템이다. 사용자는 웹 브라우저 또는 모바일 앱에 쿼리를 입력하며, 검색 결과는 일반적으로 텍스트 요약 및 이미지와 함께 하이퍼링크 목록으로 표시된다. 사용자는 검색을 이미지, 비디오 또는 뉴스 등 특정 유형의 결과로 제한할 수도 있다. 검색 제공자에게는 엔진이 전 세계의 많은 데이터 센터를 포함할 수 있는 분산 컴퓨팅 시스템의 일부이다. 엔진의 쿼리에 대한 응답 속도와 정확성은 자동화된 웹 크롤러에 의해 지속적으로 업데이트되는 복잡한 색인 시스템에 기반한다. 여기에는 웹 서버에 저장된 컴퓨터 파일 및 데이터베이스의 데이터 마이닝이 포함될 수 있지만, 일부 콘텐츠는 크롤러가 접근할 수 없는 심층 웹에 있다. 1990년대 웹이 등장한 이래로 많은 검색 엔진이 있었지만, 구글 검색은 2000년대에 지배적인 위치를 차지했으며 지금까지 유지하고 있다. 2025년 5월 현재, StatCounter에 따르면 구글은 전 세계 검색 시장 점유율의 약 89–90%를 차지하며, 경쟁업체들은 한참 뒤처져 있다: 빙 (~4%), 얀덱스 (~2.5%), 야후! (~1.3%), 덕덕고 (~0.8%), 그리고 바이두 (~0.7%).[1] 특히, 이는 구글의 점유율이 90% 아래로 떨어진 것이 10여 년 만에 처음이다. 따라서 웹사이트가 마케팅 및 최적화로 알려진 검색 결과에서 가시성을 향상시키는 사업은 주로 구글에 집중되었다. 1945년 버니바 부시는 사용자가 하나의 책상에서 방대한 정보에 접근할 수 있게 해주는 정보 검색 시스템을 설명했으며, 이를 메멕스라고 불렀다.[2] 그는 이 시스템을 《디 애틀랜틱 먼슬리》에 실린 \"As We May Think\"라는 제목의 기사에서 설명했다.[3] 메멕스는 계속해서 성장하는 과학 연구의 중앙 집중식 색인에서 정보를 찾는 것이 점점 더 어려워지는 문제를 사용자가 극복할 수 있도록 고안되었다. 버니바 부시는 현대의 하이퍼링크와 유사한 연결된 주석이 있는 연구 라이브러리를 구상했다.[4] 링크 분석은 결국 하이퍼 서치와 페이지랭크와 같은 알고리즘을 통해 검색 엔진의 중요한 구성 요소가 되었다.[5][6] 최초의 인터넷 검색 엔진은 1990년 12월 웹이 등장하기 전부터 존재했다. WHOIS 사용자 검색은 1982년으로 거슬러 올라가고,[7] Knowbot 정보 서비스 다중 네트워크 사용자 검색은 1989년에 처음 구현되었다.[8] 콘텐츠 파일, 즉 FTP 파일을 검색한 최초의 잘 문서화된 검색 엔진은 1990년 9월 10일에 출시된 아키였다.[9] 1993년 9월 이전에는 월드 와이드 웹이 전적으로 수동으로 색인화되었다. 팀 버너스리가 편집하고 CERN 웹 서버에서 호스팅한 웹 서버 목록이 있었다. 1992년의 목록 스냅샷이 하나 남아 있지만,[10] 점점 더 많은 웹 서버가 온라인 상태가 되면서 중앙 목록은 더 이상 따라갈 수 없었다. 국립 슈퍼컴퓨팅 응용 센터 사이트에서는 \"What's New!\"라는 제목으로 새로운 서버가 발표되었다.[11] 인터넷에서 콘텐츠(사용자와는 반대로)를 검색하는 데 사용된 최초의 도구는 아키였다.[12] 이름은 \"v\"가 없는 \"아카이브\"를 의미한다.[13] 캐나다 몬트리올의 맥길 대학교 컴퓨터 과학 학생이었던 앨런 엠티지가 만들었다.[13][14][15][16] 이 프로그램은 공용 익명 FTP(파일 전송 프로토콜) 사이트에 있는 모든 파일의 디렉터리 목록을 다운로드하여 파일 이름의 검색 가능한 데이터베이스를 생성했다. 그러나 아키 검색 엔진은 이 사이트의 내용을 색인화하지 않았다. 데이터 양이 너무 제한적이어서 수동으로 쉽게 검색할 수 있었기 때문이다. 1991년 미네소타 대학교의 마크 맥캐힐이 만든 고퍼의 등장은 두 가지 새로운 검색 프로그램인 베로니카와 저그헤드로 이어졌다. 아키와 마찬가지로 이들은 고퍼 색인 시스템에 저장된 파일 이름과 제목을 검색했다. 베로니카(Very Easy Rodent-Oriented Net-wide Index to Computerized Archives)는 전체 고퍼 목록에서 대부분의 고퍼 메뉴 제목에 대한 키워드 검색을 제공했다. 저그헤드(Jonzy's Universal Gopher Hierarchy Excavation And Display)는 특정 고퍼 서버에서 메뉴 정보를 얻기 위한 도구였다. 검색 엔진 \"아키 검색 엔진\"의 이름이 아치 만화책 시리즈를 참조한 것은 아니지만, \"베로니카\"와 \"저그헤드\"는 이 시리즈의 등장인물로, 전작을 참조하고 있다. 1993년 여름에는 웹을 위한 검색 엔진이 존재하지 않았지만, 수많은 전문화된 카탈로그가 수동으로 관리되고 있었다. 제네바 대학교의 오스카 니어스트라즈는 이러한 페이지를 주기적으로 미러링하고 표준 형식으로 다시 작성하는 일련의 펄 스크립트를 작성했다. 이것은 웹의 최초의 원시적인 검색 엔진인 W3Catalog의 기반이 되었고, 1993년 9월 2일에 출시되었다.[17] 1993년 6월, 당시 MIT에 재학 중이던 매튜 그레이는 아마도 최초의 웹 로봇인 펄 기반의 월드 와이드 웹 원더러를 만들었고, 이를 사용하여 \"Wandex\"라는 색인을 생성했다. 원더러의 목적은 월드 와이드 웹의 크기를 측정하는 것이었으며, 1995년 말까지 이 작업을 수행했다. 웹의 두 번째 검색 엔진 Aliweb은 1993년 11월에 등장했다. Aliweb은 웹 로봇을 사용하지 않고, 웹사이트 관리자가 특정 형식의 색인 파일이 각 사이트에 존재한다는 것을 통보해 주는 방식에 의존했다. JumpStation (1993년 12월 조너선 플레처가 만듦[18])은 웹 로봇을 사용하여 웹 페이지를 찾고 색인을 구축했으며, 웹 폼을 쿼리 프로그램의 인터페이스로 사용했다. 따라서 이는 아래에 설명된 웹 검색 엔진의 세 가지 필수 기능(크롤링, 색인화, 검색)을 결합한 최초의 WWW 리소스 검색 도구였다. 실행되는 플랫폼의 제한된 리소스 때문에 색인화 및 검색은 크롤러가 발견한 웹 페이지의 제목과 헤딩으로 제한되었다. 최초의 \"모든 텍스트\" 크롤러 기반 검색 엔진 중 하나는 1994년에 출시된 웹크롤러였다. 이전 엔진들과 달리, 웹크롤러는 사용자가 모든 웹 페이지에서 어떤 단어든 검색할 수 있도록 허용했으며, 이는 이후 모든 주요 검색 엔진의 표준이 되었다. 또한 일반 대중에게 널리 알려진 검색 엔진이었다. 또한 1994년에는 카네기 멜런 대학교에서 시작된 라이코스가 출시되어 주요 상업적 사업이 되었다. 웹에서 최초로 인기 있는 검색 엔진은 야후! 검색이었다.[19] 제리 양과 데이비드 파일로가 1994년 1월에 설립한 야후!의 첫 번째 제품은 웹 디렉터리인 Yahoo! Directory였다. 1995년에는 검색 기능이 추가되어 사용자가 Yahoo! Directory를 검색할 수 있게 되었다.[20][21] 이는 사람들이 관심 있는 웹 페이지를 찾는 가장 인기 있는 방법 중 하나가 되었지만, 그 검색 기능은 웹 페이지의 전체 텍스트 사본이 아닌 웹 디렉터리에서 작동했다. 곧이어 많은 검색 엔진이 등장하여 인기를 다투었다. 여기에는 매젤란, 익사이트, 인포시크, 잉크토미, 노던 라이트, 알타비스타 등이 포함되었다. 정보 검색자들은 키워드 기반 검색 대신 디렉터리를 탐색할 수도 있었다. 1996년 리옌훙은 검색 엔진 결과 페이지 순위 지정을 위한 RankDex 사이트 스코어링 알고리즘을 개발했고,[22][23][24] 이 기술로 미국 특허를 받았다.[25] 이는 하이퍼링크를 사용하여 색인화하는 웹사이트의 품질을 측정하는 최초의 검색 엔진이었으며,[26] 2년 후인 1998년에 구글이 출원한 매우 유사한 알고리즘 특허보다 앞선 것이었다.[27] 래리 페이지는 자신의 페이지랭크 미국 특허 중 일부에서 리의 작업을 언급했다.[28] 리는 나중에 자신의 RankDex 기술을 사용하여 2000년에 중국에서 설립한 바이두 검색 엔진에 적용했다. 1996년, 넷스케이프는 넷스케이프의 웹 브라우저에서 주요 검색 엔진으로 단일 검색 엔진에게 독점 계약을 제공하려 했다. 관심이 너무 많아서 대신 넷스케이프는 주요 검색 엔진 5곳과 계약을 맺었다. 연간 5백만 달러를 받고 각 검색 엔진은 넷스케이프 검색 엔진 페이지에서 번갈아 가며 나타나게 되었다. 이 5개 엔진은 야후!, 매젤란, 라이코스, 인포시크, 익사이트였다.[29][30] 구글은 1998년 goto.com이라는 작은 검색 엔진 회사로부터 검색어 판매 아이디어를 채택했다. 이 움직임은 검색 엔진 사업에 상당한 영향을 미쳤는데, 고전하던 사업이 인터넷에서 가장 수익성 높은 사업 중 하나로 변모했다.[31][32] 검색 엔진은 1990년대 후반에 발생한 인터넷 투자 광풍에서 가장 빛나는 별 중 일부로도 알려져 있었다.[33] 여러 회사가 화려하게 시장에 진입하여 기업공개 시 기록적인 이익을 얻었다. 일부는 공개 검색 엔진을 폐쇄하고 노던 라이트처럼 기업 전용 버전을 판매하고 있다. 많은 검색 엔진 회사는 2000년 3월에 정점을 찍은 투기 주도 시장 호황인 닷컴버블에 휩쓸렸다. 2000년경 구글의 검색 엔진이 유명해졌다.[34] 이 회사는 구글의 창립자인 세르게이 브린과 래리 페이지가 작성한 \"검색 엔진의 해부학\"이라는 논문에 설명된 페이지랭크라는 알고리즘을 사용하여 많은 검색에서 더 나은 결과를 얻었다.[6] 이 반복 알고리즘은 좋거나 바람직한 페이지가 다른 페이지보다 더 많이 링크된다는 전제하에 다른 웹사이트와 페이지의 링크 수와 페이지랭크를 기반으로 웹 페이지의 순위를 매긴다. 래리 페이지의 페이지랭크 특허는 리옌훙의 이전 RankDex 특허를 영향을 준 것으로 인용한다.[28][24] 구글은 또한 검색 엔진에 대한 미니멀리스트 인터페이스를 유지했다. 대조적으로, 많은 경쟁업체는 검색 엔진을 웹 포털에 내장했다. 사실, 구글 검색 엔진은 너무 인기가 많아 Mystery Seeker와 같은 스푸프 엔진이 등장하기도 했다. 2000년까지 야후!는 잉크토미의 검색 엔진을 기반으로 검색 서비스를 제공하고 있었다. 야후!는 2002년에 잉크토미를, 2003년에는 오버추어(AlltheWeb과 알타비스타를 소유)를 인수했다. 야후!는 2004년까지 구글의 검색 엔진으로 전환하다가 인수한 기술들을 결합하여 자체 검색 엔진을 출시했다. 마이크로소프트는 1998년 가을 잉크토미의 검색 결과를 사용하여 MSN 검색을 처음 출시했다. 1999년 초에는 룩스마트의 목록을 잉크토미의 결과와 혼합하여 표시하기 시작했다. 1999년 잠시 동안 MSN 검색은 대신 알타비스타의 결과를 사용했다. 2004년, 마이크로소프트는 자체 웹 크롤러(msnbot이라고 함)로 구동되는 자체 검색 기술로 전환하기 시작했다. 마이크로소프트의 새로운 검색 엔진인 빙은 2009년 6월 1일에 출시되었다. 2009년 7월 29일, 야후!와 마이크로소프트는 야후! 검색이 마이크로소프트 빙 기술로 구동될 것이라는 계약을 최종 확정했다. 2019년 기준[update] 현재 활동 중인 검색 엔진 크롤러에는 구글, Sogou, 바이두, 빙, Gigablast, Mojeek, 덕덕고 및 얀덱스의 크롤러가 있다. 검색 엔진은 다음 프로세스를 거의 실시간으로 유지한다.[35] 웹 검색 엔진은 웹 크롤링을 통해 사이트에서 사이트로 정보를 얻는다. \"스파이더\"는 자신에게 할당된 표준 파일 이름인 robots.txt를 확인한다. robots.txt 파일에는 검색 스파이더에 어떤 페이지를 크롤링하고 어떤 페이지를 크롤링하지 않을지 지시하는 지시문이 포함되어 있다. robots.txt를 확인하고 파일을 찾거나 찾지 못한 후, 스파이더는 제목, 페이지 내용, 자바스크립트, 캐스케이딩 스타일 시트(CSS), 헤딩 또는 HTML 메타 태그의 메타데이터와 같은 여러 요인에 따라 특정 정보를 다시 색인하도록 보낸다. 특정 수의 페이지를 크롤링하거나, 색인화된 데이터 양이 많거나, 웹사이트에서 보낸 시간이 지나면 스파이더는 크롤링을 중단하고 다음으로 넘어간다. \"어떤 웹 크롤러도 실제로 접근 가능한 웹 전체를 크롤링할 수는 없다. 무한한 웹사이트, 스파이더 트랩, 스팸 및 실제 웹의 다른 비상사태로 인해 크롤러는 대신 크롤링 정책을 적용하여 사이트 크롤링이 충분하다고 판단될 시기를 결정한다. 일부 웹사이트는 철저히 크롤링되지만, 다른 웹사이트는 부분적으로만 크롤링된다.\"[37] 색인화란 웹 페이지에서 발견된 단어와 기타 정의 가능한 토큰을 해당 도메인 이름 및 HTML 기반 필드와 연결하는 것을 의미한다. 이러한 연결은 공개 데이터베이스에 저장되며 웹 검색 쿼리를 통해 접근할 수 있다. 사용자로부터의 쿼리는 단일 단어, 여러 단어 또는 문장일 수 있다. 색인은 쿼리와 관련된 정보를 가능한 한 빨리 찾는 데 도움이 된다.[36] 색인화 및 캐싱 기술 중 일부는 영업 비밀인 반면, 웹 크롤링은 모든 사이트를 체계적으로 방문하는 간단한 과정이다. 스파이더가 방문하는 동안, 검색 엔진 작업 메모리에 저장된 페이지의 캐시된 버전(페이지를 렌더링하는 데 필요한 콘텐츠의 일부 또는 전부)이 요청자에게 빠르게 전송된다. 방문 기한이 지난 경우, 검색 엔진은 대신 웹 프록시 역할을 할 수 있다. 이 경우 페이지는 색인화된 검색어와 다를 수 있다.[36] 캐시된 페이지는 이전에 단어가 색인화되었던 버전의 모양을 유지하므로 실제 페이지가 손실되었을 때 웹사이트에 캐시된 버전의 페이지가 유용할 수 있지만, 이 문제 또한 가벼운 형태의 링크 깨짐으로 간주된다. 일반적으로 사용자가 검색 엔진에 쿼리를 입력할 때에는 몇 가지 키워드를 사용한다.[38] 역색인에는 이미 키워드를 포함하는 사이트의 이름이 있으며, 이들은 색인에서 즉시 얻어진다. 실제 처리 부하는 검색 결과 목록인 웹 페이지를 생성하는 데 있다. 전체 목록의 모든 페이지는 색인의 정보에 따라 가중치가 부여되어야 한다.[36] 그런 다음 상위 검색 결과 항목은 일치하는 키워드의 컨텍스트를 보여주는 스니펫을 조회, 재구성 및 마크업해야 한다. 이것들은 각 검색 결과 웹 페이지가 요구하는 처리의 일부에 불과하며, 추가 페이지(상위 다음)는 이러한 후처리 작업을 더 많이 요구한다. 간단한 키워드 조회 외에도 검색 엔진은 검색 결과를 구체화하기 위해 자체적인 GUI 또는 명령 기반 연산자 및 검색 매개변수를 제공한다. 이러한 기능은 첫 검색 결과의 초기 페이지를 기반으로 사용자가 검색 결과를 필터링하고 가중치를 부여하면서 생성하는 피드백 루프에 참여하는 데 필요한 제어를 제공한다. 예를 들어, 2007년부터 Google.com 검색 엔진은 초기 검색 결과 페이지의 가장 왼쪽 열에서 \"검색 도구 표시\"를 클릭한 다음 원하는 날짜 범위를 선택하여 날짜별로 필터링할 수 있도록 허용했다.[39] 각 페이지에는 수정 시간이 있으므로 날짜별로 가중치를 부여하는 것도 가능하다. 대부분의 검색 엔진은 최종 사용자가 검색 쿼리를 구체화하는 데 도움이 되도록 Boolean 연산자 AND, OR 및 NOT의 사용을 지원한다. 불리언 연산자는 사용자가 검색 용어를 구체화하고 확장할 수 있도록 하는 리터럴 검색에 사용된다. 엔진은 입력된 단어 또는 구문을 정확히 찾는다. 일부 검색 엔진은 사용자가 키워드 간의 거리를 정의할 수 있는 근접 검색이라는 고급 기능을 제공한다.[36] 또한 사용자가 검색하는 단어 또는 구문을 포함하는 페이지에 대한 통계 분석을 사용하는 개념 검색도 있다. 검색 엔진의 유용성은 반환하는 결과 세트의 적합성에 달려 있다. 특정 단어나 구문을 포함하는 수백만 개의 웹 페이지가 있을 수 있지만, 일부 페이지는 다른 페이지보다 더 관련성이 높거나, 인기가 많거나, 권위가 있을 수 있다. 대부분의 검색 엔진은 \"최고의\" 결과를 먼저 제공하기 위해 결과를 순위 매기는 방법을 사용한다. 검색 엔진이 어떤 페이지가 가장 일치하는지, 그리고 어떤 순서로 결과를 표시해야 하는지를 결정하는 방법은 엔진마다 크게 다르다.[36] 이러한 방법은 인터넷 사용이 변하고 새로운 기술이 발전함에 따라 시간과 함께 변한다. 진화한 검색 엔진의 주요 유형은 두 가지다. 하나는 인간이 광범위하게 프로그래밍한 미리 정의되고 계층적으로 정렬된 키워드 시스템이다. 다른 하나는 찾은 텍스트를 분석하여 \"역색인\"을 생성하는 시스템이다. 첫 번째 형태는 컴퓨터 자체에 훨씬 더 많이 의존하여 대부분의 작업을 수행한다. 대부분의 웹 검색 엔진은 광고 수익으로 운영되는 상업적 사업이므로, 일부는 광고주가 유료로 검색 결과에서 자신의 목록 순위를 높일 수 있도록 허용한다. 검색 결과에 대해 돈을 받지 않는 검색 엔진은 일반 검색 엔진 결과와 함께 검색 관련 광고를 게재하여 돈을 번다. 검색 엔진은 누군가 이러한 광고를 클릭할 때마다 돈을 번다.[40] 지역 검색은 지역 비즈니스의 노력을 최적화하는 과정이다. 이들은 일관된 검색 결과를 보장하는 데 중점을 둔다. 많은 사람들이 검색을 기반으로 어디로 갈지, 무엇을 살지 결정하기 때문에 중요하다.[41] 2022년 01월 기준[update] 구글은 전 세계에서 가장 많이 사용되는 검색 엔진으로 시장 점유율 90%를 차지하고 있으며, 세계에서 두 번째로 많이 사용되는 검색 엔진은 빙이 4%, 얀덱스가 2%, 야후!가 1%를 차지했다. 목록에 없는 다른 검색 엔진은 시장 점유율이 3% 미만이다.[42] 2024년, 구글의 지배력은 미국 법무부가 제기한 소송에서 불법 독점으로 판결되었다.[43] 러시아에서 얀덱스는 62.6%의 시장 점유율을 가지고 있으며, 구글은 28.3%이다. 얀덱스는 아시아와 유럽에서 스마트폰에서 두 번째로 많이 사용되는 검색 엔진이다.[44] 중국에서는 바이두가 가장 인기 있는 검색 엔진이다.[45] 한국 기반 검색 포털인 네이버는 국내 온라인 검색의 62.8%를 차지한다.[46] Yahoo! Japan과 Yahoo! Taiwan은 각각 일본과 대만에서 인터넷 검색에 가장 인기 있는 선택지이다.[47] 중국은 구글이 웹 검색 엔진 시장 점유율 상위 3위에 들지 못하는 몇 안 되는 국가 중 하나이다. 구글은 이전에 중국에서 더 인기가 있었지만, 검열과 사이버 공격에 대한 정부와의 불화로 인해 크게 철수했다. 그러나 빙은 14.95%의 시장 점유율로 웹 검색 엔진 상위 3위에 속한다. 바이두는 49.1%의 시장 점유율로 선두를 달리고 있다.[48] 유럽 연합 대부분의 국가 시장은 구글이 지배하고 있으며, 체코에서는 Seznam이 강력한 경쟁자이다.[49] 검색 엔진 Qwant는 프랑스 파리에 본사를 두고 있으며, 대부분의 월간 5천만 명의 등록 사용자를 이곳에서 유치한다. 검색 엔진은 인기도와 관련성 조합을 기반으로 웹사이트 순위를 매기도록 프로그램되어 있지만, 실증 연구에 따르면 제공하는 정보와[50][51] 기술의 기본 가정에[52] 다양한 정치적, 경제적, 사회적 편향이 존재함을 나타낸다. 이러한 편향은 경제적 및 상업적 과정(예: 검색 엔진에 광고하는 회사가 자연 검색 결과에서 더 인기를 얻을 수 있음)과 정치적 과정(예: 현지 법률 준수를 위한 검색 결과 삭제)의 직접적인 결과일 수 있다.[53] 예를 들어, 구글은 홀로코스트 부정이 불법인 프랑스와 독일에서는 특정 네오나치 웹사이트를 검색 결과에 표시하지 않는다. 편향은 또한 사회적 과정의 결과일 수도 있는데, 검색 엔진 알고리즘은 종종 비정규적인 관점을 배제하고 더 \"인기 있는\" 결과를 선호하도록 설계되기 때문이다.[54] 주요 검색 엔진의 색인화 알고리즘은 비미국 국가의 웹사이트보다 미국 기반 사이트의 적용 범위에 편향되어 있다.[51] 구글 폭탄은 정치적, 사회적 또는 상업적 목적으로 검색 결과를 조작하려는 시도의 한 예이다. 여러 학자들이 검색 엔진이 촉발한 문화적 변화,[55] 그리고 아일랜드에서의 테러리즘,[56] 기후 변화 부정,[57] 및 음모론과 같은 논란이 많은 주제의 검색 결과 표현을 연구했다.[58] 구글이나 빙과 같은 검색 엔진이 사용자의 활동 기록에 기반하여 맞춤 결과를 제공하여, 일라이 파리저가 2011년에 에코 챔버 또는 필터 버블이라고 부른 현상으로 이어진다는 우려가 제기되었다.[59] 주장은 검색 엔진과 소셜 미디어 플랫폼이 사용자 정보(위치, 과거 클릭 행동 및 검색 기록 등)를 기반으로 사용자가 보고 싶어할 정보를 선택적으로 추측하기 위해 알고리즘을 사용한다는 것이다. 결과적으로 웹사이트는 사용자의 과거 관점과 일치하는 정보만을 보여주는 경향이 있다. 일라이 파리저에 따르면, 사용자들은 상충되는 관점에 덜 노출되고 자신의 정보 거품 속에 지적으로 고립된다. 이 문제가 확인된 이후, 덕덕고와 같이 사용자를 추적하거나 \"거품을 만들지\" 않아 이 문제를 피하려는 경쟁 검색 엔진들이 등장했다. 그러나 많은 학자들은 파리저의 견해에 의문을 제기하며, 필터 버블에 대한 증거가 거의 없음을 발견했다.[60][61][62] 오히려 필터 버블의 존재를 확인하려는 여러 연구는 검색에서 사소한 수준의 개인화만을 발견했으며,[62] 대부분의 사람들이 온라인에서 다양한 관점을 접하고, 구글 뉴스는 주류 언론을 홍보하는 경향이 있음을 발견했다.[63][61] 지난 10년간 아랍 및 이슬람 세계에서 인터넷과 전자 미디어의 전 세계적인 성장은 중동 및 아시아 아대륙의 이슬람 신도들에게 자체 검색 엔진, 즉 사용자가 안전 검색을 수행할 수 있도록 하는 자체 필터링된 검색 포털을 시도하도록 장려했다. 일반적인 안전 검색 필터보다 더 많은 이슬람 웹 포털은 샤리아 법 해석에 따라 웹사이트를 \"할랄\" 또는 \"하람\"으로 분류한다. ImHalal은 2011년 9월에 온라인으로 출시되었다. Halalgoogling은 2013년 7월에 온라인으로 출시되었다. 이들은 구글과 빙(및 기타)에서 수집된 내용에 하람 필터를 사용한다.[64] 투자 부족과 이슬람 세계 기술 발전의 더딘 속도가 이슬람 검색 엔진의 발전과 성공을 방해했지만, 주요 소비층인 이슬람 신도를 대상으로 하는 Muxlim(무슬림 라이프스타일 사이트)과 같은 프로젝트는 Rite Internet Ventures와 같은 투자자로부터 수백만 달러를 받았지만 역시 실패했다. 다른 종교 지향 검색 엔진으로는 구글의 유대인 버전인 Jewogle과[65] 기독교 검색 엔진인 SeekFind.org가 있다. SeekFind는 신앙을 공격하거나 훼손하는 사이트를 필터링한다.[66] 웹 검색 엔진 제출은 웹마스터가 웹사이트를 검색 엔진에 직접 제출하는 과정이다. 검색 엔진 제출은 때때로 웹사이트를 홍보하는 방법으로 제시되지만, 일반적으로는 필요하지 않다. 주요 검색 엔진은 웹 크롤러를 사용하여 인터넷상의 대부분의 웹사이트를 도움 없이도 결국 찾아내기 때문이다. 웹마스터는 한 번에 한 웹 페이지를 제출하거나, 사이트맵을 사용하여 전체 사이트를 제출할 수 있지만, 검색 엔진이 잘 설계된 웹사이트를 크롤링할 수 있으므로 일반적으로 웹사이트의 홈페이지만 제출하면 된다. 웹사이트 또는 웹 페이지를 검색 엔진에 제출해야 하는 두 가지 남은 이유는 다음과 같다. 검색 엔진이 새로운 웹사이트를 발견할 때까지 기다리지 않고 완전히 새로운 웹사이트를 추가하는 경우, 그리고 웹사이트가 크게 재설계된 후 웹사이트 기록을 업데이트하는 경우이다. 일부 검색 엔진 제출 소프트웨어는 여러 검색 엔진에 웹사이트를 제출할 뿐만 아니라, 자체 페이지에서 웹사이트로의 링크를 추가한다. 이는 웹사이트의 랭킹을 높이는 데 도움이 될 수 있는데, 외부 링크가 웹사이트 랭킹을 결정하는 가장 중요한 요소 중 하나이기 때문이다. 그러나 구글의 존 뮬러는 이것이 \"사이트에 엄청난 수의 부자연스러운 링크를 초래할 수 있으며\" 사이트 랭킹에 부정적인 영향을 미칠 수 있다고 밝혔다.[67] 최초의 웹 검색 엔진은 1990년[68] 몬트리올의 맥길 대학교 학생이었던 앨런 엠티지가 만든 아키였다. 저자는 원래 이 프로그램을 \"아카이브\"라고 부르고 싶었지만, grep, cat, troff, sed, awk, perl 등과 같이 프로그램과 파일에 짧고 알기 어려운 이름을 할당하는 유닉스 세계 표준을 준수하기 위해 줄여야 했다. 파일을 저장하고 검색하는 주된 방법은 파일 전송 프로토콜(FTP)을 통하는 것이었다. 이것은 컴퓨터가 인터넷을 통해 파일을 교환하는 일반적인 방법을 지정하는 시스템이었다(그리고 지금도 그렇다). 작동 방식은 다음과 같다. 어떤 관리자가 자신의 컴퓨터에서 파일을 사용할 수 있도록 만들기로 결정한다. 그는 자신의 컴퓨터에 FTP 서버라는 프로그램을 설정한다. 인터넷의 누군가가 이 컴퓨터에서 파일을 검색하려면 FTP 클라이언트라는 다른 프로그램을 통해 컴퓨터에 연결한다. 클라이언트와 서버 프로그램이 모두 FTP 프로토콜에 명시된 사양을 완전히 준수하는 한, 어떤 FTP 클라이언트 프로그램이든 어떤 FTP 서버 프로그램과도 연결할 수 있다. 처음에는 파일을 공유하고 싶은 사람은 파일을 다른 사람이 사용할 수 있도록 FTP 서버를 설정해야 했다. 나중에는 \"익명\" FTP 사이트가 파일 저장소가 되어 모든 사용자가 파일을 게시하고 검색할 수 있게 되었다. 아카이브 사이트가 있더라도 많은 중요한 파일은 여전히 작은 FTP 서버에 흩어져 있었다. 이러한 파일은 인터넷의 입소문과 동등한 방식으로만 찾을 수 있었다. 누군가가 메시지 목록이나 토론 포럼에 이메일을 게시하여 파일의 가용성을 알리는 식이었다. 아키는 이 모든 것을 바꿨다. 아키는 익명 FTP 파일의 사이트 목록을 가져오는 스크립트 기반 데이터 수집기와 사용자 쿼리와 일치하는 파일 이름을 검색하는 정규 표현식 매처를 결합했다. (4) 다시 말해, 아키의 수집기는 인터넷을 통해 FTP 사이트를 샅샅이 뒤져서 발견한 모든 파일을 색인화했다. 정규 표현식 매처는 사용자에게 데이터베이스에 대한 접근 권한을 제공했다.[69] 1993년에 네바다 대학교 시스템 컴퓨팅 서비스 그룹은 베로니카를 개발했다.[68] 이는 아키와 유사하지만 고퍼 파일용으로 만들어진 검색 장치였다. 얼마 후 저그헤드라는 또 다른 고퍼 검색 서비스가 등장했는데, 아마도 만화 삼총사를 완성하기 위한 유일한 목적으로 보였다. 저그헤드는 Jonzy's Universal Gopher Hierarchy Excavation and Display의 약자이지만, 베로니카처럼 창작자가 약자를 나중에 끼워 맞췄을 것으로 추정하는 것이 안전하다. 저그헤드의 기능은 베로니카와 거의 동일했지만, 좀 더 거칠고 미완성된 느낌이었다.[69] 1993년 매튜 그레이가 개발한 월드 와이드 웹 원더러는 웹 성장을 추적하기 위해 설계된 웹 최초의 로봇이었다.[70] 초기에는 웹 서버만 세었지만, 곧이어 URL도 수집하기 시작했다. 수집된 URL 데이터베이스는 최초의 웹 데이터베이스인 Wandex가 되었다. 매튜 그레이의 원더러는 당시 상당한 논란을 일으켰는데, 부분적으로는 초기 버전의 소프트웨어가 넷 전체를 난잡하게 돌아다니며 눈에 띄는 네트워크 성능 저하를 초래했기 때문이다. 이 저하는 원더러가 하루에 같은 페이지에 수백 번 접근했기 때문에 발생했다. 원더러는 곧 개선되었지만, 로봇이 인터넷에 좋은지 나쁜지에 대한 논란은 계속되었다. 원더러에 대한 응답으로 마르타인 코스터는 1993년 10월 Archie-Like Indexing of the Web, 즉 ALIWEB을 만들었다. 이름에서 알 수 있듯이 ALIWEB은 아키의 HTTP 버전이었으며, 이 때문에 여러 면에서 여전히 독특하다. ALIWEB에는 웹 검색 로봇이 없다. 대신, 참여 사이트의 웹마스터는 각 목록에 올리고 싶은 페이지에 대한 자체 색인 정보를 게시한다. 이 방법의 장점은 사용자가 자신의 사이트를 설명할 수 있고, 로봇이 네트워크 대역폭을 잡아먹지 않는다는 것이다. ALIWEB의 단점은 오늘날 더 큰 문제이다. 주요 단점은 특별한 색인 파일을 제출해야 한다는 것이다. 대부분의 사용자는 이러한 파일을 만드는 방법을 이해하지 못하므로 페이지를 제출하지 않는다. 이는 상대적으로 작은 데이터베이스로 이어지며, 이는 사용자가 대규모 봇 기반 사이트보다 ALIWEB를 검색할 가능성이 낮다는 것을 의미한다. 이 캐치-22는 다른 데이터베이스를 ALIWEB 검색에 통합함으로써 어느 정도 상쇄되었지만, 여전히 야후!나 라이코스와 같은 검색 엔진의 대중적인 매력은 없다.[69] 처음에는 아키텍스트(Architext)라고 불렸던 익사이트는 1993년 2월 스탠퍼드 대학교 6명의 학부생이 시작했다. 그들의 아이디어는 인터넷에 있는 방대한 정보 속에서 더 효율적인 검색을 제공하기 위해 단어 관계에 대한 통계 분석을 사용하는 것이었다.\n그들의 프로젝트는 1993년 중반까지 전액 자금 지원을 받았다. 자금이 확보되자마자 그들은 웹마스터들이 자신의 웹사이트에서 사용할 수 있는 검색 소프트웨어 버전을 출시했다. 당시 이 소프트웨어는 아키텍스트라고 불렸지만, 지금은 웹 서버용 익사이트라는 이름으로 사용된다.[69] 익사이트는 1995년에 출시된 최초의 진지한 상업용 검색 엔진이었다.[71] 스탠퍼드에서 개발되었고 @Home에 65억 달러에 인수되었다. 2001년에 익사이트와 @Home은 파산했고 InfoSpace는 익사이트를 1천만 달러에 인수했다. 웹 검색에 대한 최초의 분석 중 일부는 익사이트의 검색 로그에서 수행되었다.[72][38] 1994년 4월, 스탠퍼드 대학교 박사 과정 학생인 데이비드 파일로와 제리 양은 상당히 인기 있는 페이지들을 만들었다. 그들은 이 페이지 모음을 야후!라고 불렀다. 이름 선택에 대한 공식적인 설명은 그들 스스로를 어리숙한 야후라고 여겼기 때문이라는 것이었다. 링크 수가 늘어나고 페이지가 하루에 수천 건의 조회를 받기 시작하면서 팀은 데이터를 더 잘 정리하는 방법을 만들었다. 데이터 검색을 돕기 위해 야후!(www.yahoo.com)는 검색 가능한 디렉터리가 되었다. 검색 기능은 간단한 데이터베이스 검색 엔진이었다. 야후! 항목은 수동으로 입력되고 분류되었기 때문에 야후!는 실제로 검색 엔진으로 분류되지 않았다. 대신, 일반적으로 검색 가능한 디렉터리로 간주되었다. 야후!는 이후 수집 및 분류 과정의 일부 측면을 자동화하여 엔진과 디렉터리 간의 구분을 모호하게 만들었다. 원더러는 URL만 수집했기 때문에 URL로 명시적으로 설명되지 않은 것을 찾기가 어려웠다. URL은 처음부터 다소 암호 같았기 때문에 일반 사용자에게는 도움이 되지 않았다. 야후!나 갤럭시를 검색하는 것이 훨씬 더 효과적이었는데, 그들은 색인화된 사이트에 대한 추가 설명 정보를 포함하고 있었기 때문이다. 1994년 7월, 카네기 멜런 대학교에서 마이클 모들린은 라이코스 검색 엔진을 개발했다. 웹 검색 엔진은 다른 사이트에 저장된 콘텐츠를 검색하는 기능을 갖춘 사이트이다. 다양한 검색 엔진이 작동하는 방식에는 차이가 있지만, 모두 세 가지 기본 작업을 수행한다.[73] 이 과정은 사용자가 제공된 인터페이스를 통해 시스템에 쿼리 문을 입력할 때 시작된다. 기본적으로 세 가지 유형의 검색 엔진이 있다. 로봇(크롤러; 개미 또는 스파이더)에 의해 구동되는 것과 인간 제출에 의해 구동되는 것, 그리고 이 두 가지의 하이브리드이다. 크롤러 기반 검색 엔진은 자동화된 소프트웨어 에이전트(크롤러라고 불림)를 사용하는 검색 엔진으로, 웹사이트를 방문하고 실제 사이트의 정보를 읽으며, 사이트의 메타 태그를 읽고, 또한 사이트가 연결되는 링크를 따라가 모든 연결된 웹사이트에 대해서도 색인화를 수행한다. 크롤러는 이 모든 정보를 중앙 저장소로 다시 보내고, 거기서 데이터가 색인화된다. 크롤러는 주기적으로 사이트를 다시 방문하여 변경된 정보가 있는지 확인한다. 이 작업이 발생하는 빈도는 검색 엔진 관리자가 결정한다. 인간 기반 검색 엔진은 인간이 정보를 제출하는 방식에 의존하며, 제출된 정보는 이후 색인화되고 분류된다. 제출된 정보만 색인에 포함된다. 두 경우 모두, 사용자가 정보를 찾기 위해 검색 엔진에 쿼리를 보내면, 실제로는 검색 엔진이 생성한 색인을 검색하는 것이지 웹 자체를 검색하는 것이 아니다. 이러한 색인은 수집 및 저장된 정보의 거대한 데이터베이스이며 이후 검색된다. 이것이 야후!나 구글과 같은 상업용 검색 엔진에서 검색했을 때 때때로 더 이상 유효하지 않은 링크가 반환되는 이유를 설명한다. 검색 결과는 색인을 기반으로 하므로, 웹 페이지가 유효하지 않게 된 이후 색인이 업데이트되지 않았다면 검색 엔진은 해당 페이지를 여전히 활성 링크로 간주한다. 이는 색인이 업데이트될 때까지 계속된다. 그렇다면 왜 다른 검색 엔진에서 동일한 검색이 다른 결과를 생성할까? 그 질문에 대한 답의 일부는 모든 색인이 정확히 같지는 않기 때문이다. 그것은 스파이더가 무엇을 찾는지 또는 인간이 무엇을 제출했는지에 달려 있다. 하지만 더 중요한 것은 모든 검색 엔진이 색인을 검색하는 데 동일한 알고리즘을 사용하지 않는다는 점이다. 알고리즘은 검색 엔진이 색인에 있는 정보가 사용자가 검색하는 내용과 얼마나 관련성이 높은지를 결정하는 데 사용하는 것이다. 검색 엔진 알고리즘이 스캔하는 요소 중 하나는 웹 페이지에 있는 키워드의 빈도와 위치이다. 빈도가 높은 키워드는 일반적으로 더 관련성이 높은 것으로 간주된다. 그러나 검색 엔진 기술은 키워드 스터핑 또는 스팸덱싱으로 알려진 것을 억제하기 위해 점점 더 정교해지고 있다. 알고리즘이 분석하는 또 다른 일반적인 요소는 페이지가 웹의 다른 페이지에 링크되는 방식이다. 페이지가 서로 어떻게 링크되는지 분석함으로써 엔진은 페이지가 무엇에 관한 것인지(링크된 페이지의 키워드가 원본 페이지의 키워드와 유사한 경우) 그리고 해당 페이지가 \"중요\"하다고 간주되어 순위 상승에 도움이 되는지 판단할 수 있다. 기술이 키워드 스터핑을 무시하기 위해 점점 더 정교해지는 것처럼, 인위적인 순위를 높이기 위해 사이트에 인위적인 링크를 구축하는 웹마스터에게도 점점 더 현명해지고 있다. 현대 웹 검색 엔진은 수년에 걸쳐 진화한 기술을 사용하는 고도로 복잡한 소프트웨어 시스템이다. 특정 '탐색' 요구에 개별적으로 적용 가능한 여러 하위 범주의 검색 엔진 소프트웨어가 있다. 여기에는 웹 검색 엔진(예: 구글), 데이터베이스 또는 구조화된 데이터 검색 엔진(예: Dieselpoint), 그리고 혼합 검색 엔진 또는 엔터프라이즈 검색이 포함된다. 구글과 야후!와 같은 더 널리 사용되는 검색 엔진은 수십만 대의 컴퓨터를 사용하여 수조 개의 웹 페이지를 처리하여 상당히 정확한 결과를 반환한다. 이러한 방대한 쿼리 양과 텍스트 처리로 인해 소프트웨어는 높은 중복성을 가진 고도로 분산된 환경에서 실행되어야 한다. 또 다른 범주의 검색 엔진은 과학 검색 엔진이다. 이들은 과학 문헌을 검색하는 검색 엔진이다. 가장 잘 알려진 예는 구글 스칼라이다. 연구자들은 이론적 구성이나 핵심 연구 결과를 추출하는 등 기사의 콘텐츠 요소를 이해하도록 검색 엔진 기술을 개선하는 작업을 하고 있다.[74]"
  },
  {
    "url": "https://hy.wikipedia.org/wiki/%D5%88%D6%80%D5%B8%D5%B6%D5%B8%D5%B2%D5%A1%D5%AF%D5%A1%D5%B6_%D5%B0%D5%A1%D5%B4%D5%A1%D5%AF%D5%A1%D6%80%D5%A3",
    "title": "Որոնողական համակարգ - Վիքիպեդիա",
    "content": "Որոնողական համակարգը գործիք է, որը նախատեսված է համապատասխան բառերով Համաշխարհային ցանցում որոնումներ կատարելու համար։ Ստեղծված է համացանցում և FTP սերվերներում ինֆորմացիա փնտրելու համար։ Փնտրված արդյունքները ընդհանրապես ներկայացվում են արդյունքների ցանկում և սովորաբար կոչվում են նպատակակակետ, հիթ։ Ինֆորմացիան կարող է բաղկացած լինել վեբ էջերից, նկարներից, ինֆորմացիաներից և այլ տիպի ֆայլերից ու տվյալներից։ Այն կարող է օգտագործվել տարբեր տեսակի տեղեկատվություն որոնելու համար, ներառյալ՝ կայքեր, ֆորումներ, նկարներ, վիդեոներ, ֆայլեր և այլն։ Որոշ կայքեր արդեն իրենցից ներկայացնում են ինչ-որ որոնողական համակարգ, օրինակ՝ Dailymotion, YouTube և Google Videos ինտերնետում տեղադրված տեսահոլովակների որոնողական կայքեր են։ Որոնողական կայքը բաղկացած է \"ռոբոտներից\", որոնց անվանում են նաև bot, spider, crawler, որոնք ավտոմատ կերպով, առանց մարդկային միջամտության պարբերաբար հետազոտում են կայքերը։ Որոնողական կայքերը հետևում են հղումներին, որոնք կապված լինելով իրար հետ ինդեքսավորում է յուրաքանչյուր էջ տվյալների բազայում՝ հետագայում բանալի բառերի օգնությամբ դառնալով հասանելի ինտերնետից օգտվողների համար։ Սխալմամբ, որոնողական կայքեր են անվանում նաև այն կայքերը, որոնք իրենցից ներկայացնում են կայքային տեղեկատուներ։ Այս կայքերում ուշադրության արժանի կայքերը ցուցակագրվում և դասակարգվում են մարդկային ռեսուրսների շնորհիվ, այլ ոչ թե բոտերի կամ ռոբետների միջոցով։ Այդ կայքերից կարելի է նշել օրինակ՝ Yahoo!։ Yahoo!-ի որոնողական կայքը գտնվում է այստեղ։ Բոլոր որոնողական համակարգերը նախատեսված են ինտերնետում որոնում իրականացնելու համար, սակայն կան որոշ որոնողական համակարգերի տարատեսակներ, որոնք համակարգչային ծրագրեր են և հետևաբար տեղակայվում են համակարգչի մեջ։ Այս համակարգերը կոչվում են desktop։ Վերջիներս հնարավորություն են տալիս որոնելու թե համակարգչի մեջ կուտակված ֆայլեը, թե կայքերում տեղադրված ռեսուրսները։ Այդ ծրագրերից ամենահայտնիներն են՝ Exalead Desktop, Copernic Desktop Search Արխիվացված 2009-02-17  Wayback Machine Գոյություն ունեն նաև մետա-որոնողական համակարգեր, այսինքն կայքեր, որ նույն որոնումը կատարում են միաժամանակ տարբեր որոնողական կայքերի միջնորդությամբ։ Որոնման արդյունքները հետո դասակարգվում են որպեսզի ներկայացվեն օգտագործողին։ Մետա-որոնողական համակարգերի շարքից կարելի է թվարկել օրինակ՝ Mamma և Kartoo Արխիվացված 2010-01-20  Wayback Machine։ Որոնողական համակարգը հատուկ վեբ հանգույց է, որը նախատեսված է տեղեկատվության որոնման համար։ Որոնման համակարգերի ամենատարածված կիրառություններից մեկը համացանցում տեքստային կամ գրաֆիկական տեղեկատվության որոնման վեբ ծառայությունն է։ Կան նաեւ համակարգեր, որոնք կարող են FTP սերվերներում փնտրել ֆայլեր, առցանց խանութներում արտադրող ապրանքներ, Usenet- ի լրատվական խմբերի տեղեկություններ[1]։ Որոնողական համակարգերի միջոցով որոնելու համար օգտագործողը ձեւավորում է որոնման հարցումը ։ Որոնողական համակարգի աշխատանքը, օգտագործողի խնդրանքով, փաստաթղթերի որոնումն է, որոնք պարունակում են նշված բառերը։ Այս դեպքում որոնողական համակարգը ստեղծում է որոնման արդյունքների էջ։ Նման որոնման արդյունքները կարող են պարունակել տարբեր տեսակի արդյունքներ, օրինակ `վեբ էջեր, պատկերներ, աուդիո ֆայլեր։ Որոշ որոնման համակարգեր նաեւ տեղեկացնում են համացանցում համապատասխան տվյալների բազաների եւ առկայության մասին։ 2018 թվականի տվյալներով Google- ը աշխարհում ամենատարածված որոնողական համակարգն է[2]։ Ըստ որոնման մեթոդների եւ սպասարկման ձևերի, որոնողական համակարգերը բաժանված են չորս տեսակի `որոնողական ռոբոտներ, մարդկանց կողմից կառավարվող համակարգեր, հիբրիդային համակարգեր եւ մետա-համակարգեր։ Համացանցի զարգացման վաղ փուլում, Թիմ Բերներս Լին ՑԵՌՆ- ի կայքում հսկում էր վեբ սերվերների ցանկը։ Ավելի ու ավելի շատ կայքերի առաջացումից հետո  այդպիսի ցուցակի ձեռքով հսկելը ավելի ու ավելի դժվար էր դարձել։ NCSA- ի ինտերնետային կայքում կար «Ինչ նորություն» հատուկ բաժինը, որտեղ նրանք նոր կայքերոի հղումներն էին հրապարակում։\nԻնտերնետում որոնելու առաջին համակարգչային ծրագիրը Archie-ն էր։ Այն ստեղծվել է 1990 թ.-ին Մոնրեալում McGill համալսարանի համակարգչային գիտության ուսանողների կողմից[3]։ Ծրագիրը ներբեռնել է բոլոր ֆայլերի ցանկը բոլոր մատչելի անանուն FTP սերվերներից եւ կառուցել տվյալների բազա, որտեղ հնարավորություն է եղել որոնում կատարել ֆայլի անունով։ Այնուամենայնիվ, Archie-ն չի ցուցաբերել այդ ֆայլերի բովանդակությունը, քանի որ տվյալների քանակը այնքան փոքր էր, որ ամեն ինչ կարելի է հեշտությամբ ստանալ առանց համակարգի օգնությամբ։ Google որոնողական համակարգը հայտնի է 2000-ականների սկզբից։ Ընկերությունը հասել է բարձր դիրքի `լավ որոնման արդյունքների շնորհիվ, օգտագործելով PageRank ալգորիթմը։ Ալգորիթմը հանրությանը ներկայացվեց «The Anatomy of Search Engine» հոդվածում[4], որը գրվել է Google- ի հիմնադիրներ Սերգեյ Բրինի եւ Լարի Փեյջի կողմից։ Այս ալգորիթմը վեբ էջեր է, որը հիմնված է վեբ էջի հիպերհոլերի քանակի գնահատման վրա։ Google- ի ինտերֆեյսը շատ մատչելի է, որտեղ չկա ավելորդ բան, ի տարբերություն իր մրցակիցներից շատերի, որոնք որոնողական համակարգը վերածել են վեբ-պորտալի։ Microsoft- ը առաջին անգամ կիրառել է Microsoft Network Search[5]-ը (MSN Search) 1998 թ. աշնանը, օգտագործելով Inktomi-ի որոնման արդյունքները։ 2004-ին Microsoft- ը սկսեց անցում կատարել սեփական որոնման տեխնոլոգիայի, օգտագործելով սեփական որոնողական ռոբոտը։ Microsoft- ի վերաձևավորումից հետո, 2009 թ. Հունիսի 1-ին, մեկնարկեց Bing[6] որոնիչը։ Հուլիս 29, 2009 Yahoo!-ն  եւ Microsoft- ը ստորագրել են համաձայնագիր, համաձայն որի Yahoo!-ն աշխատում է Microsoft Bing տեխնոլոգիայի հիման վրա։ Գոյություն ունի որոնողական համակարգերի չորս տեսակներ՝ Ծրագրի նպատակն է գնահատել որոնման արդյունքները։ Շնորհիվ այն հանգամանքի, որ այս մեխանիզմում որոնման ռոբոտը մշտապես ուսումնասիրում է ցանցը, տեղեկատվությունը ավելի կարեւոր է։ Ժամանակակից որոնման համակարգերի մեծ մասը այս տեսակի համակարգերն են։ Այս որոնողական համակարգերը ստանում են վեբ էջերի ցուցակները:Ցուցակը պարունակում է հասցե, վերնագիր և կայքի կարճ նկարագրություն։ Ռեսուրսների կատալոգը որոնում է արդյունքներ միայն վեբ վարպետների կողմից ներկայացված էջի նկարագրություններից ելնելով։ Ցուցակների առավելությունն այն է, որ բոլոր ռեսուրսները ստուգվում են ձեռքով, հետեւաբար, բովանդակության որակը կլինի ավելի լավ, քան համակարգչային ավտոմատ ստացված արդյունքները։ Բայց կա մի թերություն `այդ ցուցակների թարմացումը կատարվում է ձեռքով և կարող է զգալիորեն հետ մնալ իրական վիճակից։ Նման համակարգերի օրինակներ ներառում է Yahoo- ի կատալոգը։ Որոնման համակարգերը,  ինչպիսիք են Yahoo- ն, Google- ը, MSN- ը, համատեղում են որոնողական ռոբոտների կիրառությունը և մարդու կողմից կառավարելու գործոնը։ Մետա-համակարգերը համատեղում և դասավորում են մի քանի որոնման արդյունքներ։ Այս որոնիչները օգտակար էին, երբ յուրաքանչյուր որոնիչ ուներ յուրահատուկ ինդեքս եւ որոնիչները այդքան էլ «խելացի» չէին։ Քանի որ որոնումը հիմա բարելավվել է, նրանց անհրաժեշտությունը նվազել է։ Օրինակներ `MetaCrawler և MSN Որոնում։ Համաշխարհային սարդոստայն Համաձայն Comscore Արխիվացված 2012-07-29(Timestamp length) archive.today-ի 2007 թվականի օգոստոսին կատարված ուսումնասիրությունների, հիմնական որոնողական կայքերն են՝"
  },
  {
    "url": "https://hi.wikipedia.org/wiki/%E0%A4%B5%E0%A5%87%E0%A4%AC_%E0%A4%96%E0%A5%8B%E0%A4%9C%E0%A5%80_%E0%A4%87%E0%A4%82%E0%A4%9C%E0%A4%A8",
    "title": "वेब खोजी इंजन - विकिपीडिया",
    "content": "वेब खोजी इंजन (web search engine) वह सॉफ्टवेयर है जो विश्वव्यापी जाल (World Wide Web) पर संग्रहित सूचनाओं को खोजने के काम आता है।[उद्धरण चाहिए] खोज के परिणामस्वरूप ये खोजी इंजन वांछित सूचना से सम्बन्धित वेब पेज, छबियाँ, तथा अन्य प्रकार की फाइलें प्रस्तुत करते हैं। कुछ वेब खोजी इंजन डेटाबेस तथा खुली डायरेक्टरी में उपलब्ध आँकडे भी प्रस्तुत करते हैं।[उद्धरण चाहिए] कुछ प्रमुख वेब खोजी इंजन:[उद्धरण चाहिए]"
  },
  {
    "url": "https://hr.wikipedia.org/wiki/Internetska_tra%C5%BEilica",
    "title": "Internetska tražilica – Wikipedija",
    "content": "Internetski pretraživač (tražilica) je specijalizirano mrežno mjesto čija je glavna funkcija pomoć u pronalaženju informacija pohranjenih na drugim mrežnim mjestima (domenama). Internetski pretraživač je pretraživač namijenjen pretraživanju informacija na World Wide Web-u. Informacije mogu biti Web stranice, slike i ostale vrste datoteka. Neki pretraživači, također, pretražuju podatke dostupne u tematskim grupama, bazama podataka ili u otvorenim imenicima. Za razliku od Web imenika koje održavaju uređivači teksta (ljudi), internetski pretraživači djeluju po algoritmu ili su kombinacija algoritma i ljudskog upisa. Najpoznatiji internetski pretraživač u svijetu je Google. U najjačim tražilicama postoji mogućnost usmjerenog i specijalnog pretraživanja, gdje je dovoljno upisati neku rečenicu ili više ključnih riječi, odnosno natuknicu, pa se nakon razmjerno kratkog vremena (ovisno o brzini veze) dobiju brojne poveznice na traženi pojam. Internetski sadržaj koji se ne može pronaći pretraživanjem internetskom tražilicom općenito nazivamo duboki Web. Prvi alat upotrijebljen za pretraživanje na Internetu bio je Archie, stvoren 1990. godine. Pretraživao je imena datoteka i naslove, no nije indeksirao sadržaj tih datoteka. Prvi internetski pretraživač bio je Wandex, sada izumrli indeks sabran od strane World Wide Web Wanderer-a, programa za prikupljanje podataka s Web stranica razvijenog 1993. godine. Jedan od prvih internetskih pretraživača čitavog teksta bio je WebCrawler, koji je izašao 1994. godine. Za razliku od svojih prethodnika, dopuštao je korisnicima pretraživanje bilo koje riječi s bilo koje stranice, što je od tada postalo pravilo za sve značajne pretraživače. Također, bio je to prvi pretraživač poznat javnosti u velikoj mjeri. Ubrzo nakon toga, pojavili su se mnogi pretraživači nadmećući se za popularnost. Neki od njih bili su: Excite, Infoseek, Inktomi, Northern Light, AltaVista, Yahoo!, MSN Search (danas Live Search). Oko 2000. godine Googleov pretraživač uzdigao se na vrh. Poduzeće je postiglo bolje rezultate za mnoge pretrage uz pomoć novine zvane PageRank. Ovaj učestali algoritam raspoređuje Web stranice temeljem broja i PageRank-a ostalih Web mjesta i stranica koje se na njih spajaju, s pretpostavkom da se na bolje i poželjnije stranice češće spaja. Isto tako, Google je održao minimalističko sučelje svog pretraživača. Suprotno tome, mnogi od njegovih konkurenata umetnuli su pretraživač na Web portal. Od kraja 2007. godine, Google je daleko najpopularniji internetski pretraživač širom svijeta. Internetski pretraživači skupljaju različite informacije o internetskim stranicama, uključuju ih u svoju bazu te nude svakom korisniku, koji traži određeni pojam, uslugu, proizvod ili bilo što drugo. Internetski pretraživač ima tri dijela: 1. program za prikupljanje podataka s Web stranica koje posjećuje (eng. crawler, spider), 2. indeks koji sadrži pojmove koje je program za prikupljanje prikupio tijekom svojih posjeta Web stranicama (eng. index), 3. pretraživač (postojećeg indeksa) (eng. search engine). 1. Internetski pretraživači rade prikupljajući informacije o mnogim Web stranicama, koje pronađu na samom World Wide Web-u. Te stranice je prikupio Web crawler, spider (hrv. pauk) – automatizirani Web preglednik koji slijedi svaki link koji vidi. 2. Sadržaj svake stranice se potom analizira, s ciljem ustanovljavanja načina indeksiranja. Podaci o Web stranicama su spremljeni u bazu podataka indeksa za upotrebu u sljedećim upitima. 3. Kada korisnik postavi upit pretraživaču (obično koristeći ključne riječi) pretraživač pregledava svoj indeks i osigurava listu najbolje usklađenih Web stranica s kriterijima, u pravilu sa sažetkom koji sadržava naslov dokumenta, a ponekad i dijelove teksta. Korisnost pretraživača ovisi o relevantnosti skupa rezultata koje on daje. Iako mogu postojati milijuni Web stranica koje uključuju određenu riječ ili izraz, neke stranice mogu biti relevantnije, popularnije ili pouzdanije od drugih. Većina pretraživača primjenjuje metode nizanja rezultata s ciljem pribavljanja prvo „najboljih“ rezultata – način rada razlikuje se od jednog do drugog pretraživača. Postoje 1. opći internetski pretraživači koji pretražuju različita brojna područja ljudskog znanja i djelovanja (npr. Google) te 2. specijalizirani internetski pretraživači koji pretražuju jedno uže područje ljudskog znanja i djelovanja (npr. eBay). Google je danas na zapadu svakako najpoznatija i razmjerno najbolja tražilica za regionalne teme na germanskim i romanskim jezicima latiničnog pisma iz zapadne Europe, te obje Amerike i Australije, dok je znatno slabiji za istočnu Europu, Afriku i Aziju (gdje uglavnom pokriva Izrael, Taiwan i Japan). Bing je tražilica u vlasništvu kompanije Microsoft. To je jedna od najstarijih tražilica, koja radi na principu tražilice Google. Nakon pojave Googlea na zapadu se počela manje upotrijebljavati. Osnovana je već 1995., a od 2003. je u vlasništvu kompanije Yahoo!. Ugašena je 8. srpnja 2013. godine. Wolfram Alpha, pravog naziva Wolfram|Alpha je znanstvena tražilica, da je tako nazovemo. Razvijena je od strane kompanije Wolfram Research. Svekolikoj javnosti prikazana je 15.5.2009. Podloga tražilice je program Mathematica, oko 6 milijuna linija programskog koda, a pogoni je preko 10 000 procesora. Naravno, ta se brojka stalno mijenja zbog nadogradnje. Alpha za sada nema snagu Mathematice, ali nas Wolfram Research uvjerava da da će razvijati Alphu upravo u tom smjeru. Čovjek koji stoji iza cijelog projekta je Stephen Wolfram, autor Mathematice. DuckDuckGo je tražilica koja za razliku od ostalih većih pretraživača ne prikuplja osobne podatke korisnika niti ih prosljeđuje oglašivačima (trećoj strani). Od početka rada u siječnju 2017. bilježi stalan rast pretraživanja te se prometnula u glavnu alternativnu tražilicu, i sama se namećući kao zamjena monopolu velikih tražilica, posebice Googlea. Meta-pretraživači su vrsta internetskog pretraživača uz pomoć čijeg sučelja je moguće pretraživati nekoliko drugih izvora informacija (najčešće pretraživača) odjednom. Korisnici upit unose samo jednom, a on se potom distribuira prema više drugih pretraživača i/ili baza podataka, uz zajednički prikaz rezultata sa svakog od njih. Postoje 1. Web meta-pretraživači koji objedinjuju i rangiraju rezultate na jednoj Web stranici (npr. Metacrawler) i 2. samostalni programi za pretraživanje s vlastitim sučeljem (npr. Copernic Agent, FirstStop Websearch)."
  },
  {
    "url": "https://id.wikipedia.org/wiki/Mesin_pencari",
    "title": "Mesin pencari - Wikipedia bahasa Indonesia, ensiklopedia bebas",
    "content": "Mesin pencari web atau mesin telusur web (bahasa Inggris: web search engine) adalah program komputer yang dirancang untuk melakukan pencarian atas berkas-berkas yang tersimpan dalam layanan www, ftp, publikasi milis, ataupun news group dalam sebuah ataupun sejumlah komputer peladen dalam suatu jaringan. Mesin pencari merupakan perangkat penelusur informasi dari dokumen-dokumen yang tersedia. Hasil pencarian umumnya ditampilkan dalam bentuk daftar yang sering kali diurutkan menurut tingkat akurasi ataupun rasio pengunjung atas suatu berkas yang disebut sebagai hits. Informasi yang menjadi target pencarian bisa terdapat dalam berbagai macam jenis berkas seperti halaman situs web, gambar, ataupun berkas lainnya. Beberapa mesin pencari juga diketahui melakukan pengumpulan informasi atas data yang tersimpan dalam suatu basis data ataupun direktori laman/situs (web).\nSebagian besar mesin pencari dijalankan oleh perusahaan swasta yang menggunakan algoritme kepemilikan dan basis data tertutup, diantaranya yang paling populer adalah safari Google (MSN Search dan Yahoo!). Telah ada beberapa upaya menciptakan mesin pencari dengan sumber terbuka (open source), contohnya adalah Htdig, Nutch, Egothor dan OpenFTS.[1] Saat awal perkembangan internet, Tim Berners-Lee membuat sebuah situs web yang berisikan daftar situs web yang ada di internet melalui peladen web CERN. Sejarah yang mencatat sejak tahun 1992 masih ada hingga kini.[2] Dengan semakin banyaknya situs web yang aktif membuat daftar ini tidak lagi memungkinkan untuk dikelola oleh manusia. Utilitas pencari yang pertama kali digunakan untuk melakukan pencarian di internet adalah Archie yang berasal dari kata \"archive\" tanpa menggunakan huruf \"v\".[3] Archie dibuat tahun 1990 oleh Alan Emtage, Bill Heelan dan J. Peter Deutsch, saat itu adalah mahasiswa ilmu komputer Universitas McGill, Amerika Serikat. Cara kerja program tersebut adalah mengunduh daftar direktori serta berkas yang terdapat pada layanan ftp publik (anonim) kemudian memuatnya ke dalam basis data yang memungkinkan pencarian. Mesin pencari lainnya seperti Aliweb, muncul di 1993 dan masih berjalan hingga saat ini. Salah satu mesin pencari pertama yang sekarang berkembang menjadi usaha komersial yang cukup besar adalah Lycos, yang dimulai di Carnegie Mellon University sebagai proyek riset pada tahun 1994. Segera setelah itu, banyak mesin pencari yang bermunculan dan bersaing memperebutkan popularitas. Termasuk di antaranya adalah Safari (sebuah mesin pencarian) yang aman dan untuk publik. Masing-masing bersaing dengan menambahkan layakan-layanan tambahan seperti yang dilakukan oleh Yahoo. Tahun 2002 Yahoo! mengakuisisi Inktomi, setahun kemudian mengakuisisi AlltheWeb dan Altavista kemudian meluncurkan mesin pencari sendiri yang didasarkan pada teknologi gabungan dari mesin-mesin pencari yang telah diakuisisinya serta memberikan layanan yang mengutamakan situs pencarian  daripada layanan-layanan lainnya. Di bulan desember 2003, Orase menerbitkan versi pertama dari teknologi situs atau laman pencarian. Mesin ini memiliki banyak fungsi baru dan tingkat unjuk kerja yang jauh lebih baik. Mesin pencari juga dikenal sebagai target investasi internet yang terjadi pada akhir tahun 1990-an. Beberapa perusahaan mesin pencari yang masuk ke dalam pasar saham diketahui mencatat keuntungan besar. Sebagian lagi sama sekali menonaktifkan layanan mesin pencari, dan hanya memasarkannya pada edisi perusahaan (entreprise) saja, contoh Northern Light, sebelumnya diketahui merupakan salah satu perintis layanan mesin pencari di internet. Buku Osmar R. Zaïane From Resource Discovery to Knowledge Discovery on the Internet menjelaskan secara rinci sejarah teknologi mesin pencari sebelum munculnya Google. Mesin-mesin pencari lainnya mencakup a9.com, AlltheWeb, Ask Jeeves, Clusty, Gigablast, Teoma, Wisenut, GoHook, Kartoo, dan Vivisimo. Google muncul pada akhir tahun 1997, di mana Google memasuki pasar yang telah diisi oleh para pesaing lain dalam penyediaan layanan mesin pencari, seperti Yahoo, Altavista, HotBot, Excite, InfoSeek dan Lycos, di mana perusahaan-perusahaan tersebut mengklaim sebagai perusahaan yang bergerak dalam bidang layanan pencarian di internet. Hingga akhirnya Google mampu menjadi sebagai penyedia mesin pencari yang cukup diperhitungkan di dunia. Saat tingginya persaingan antar mesin pencari yang ada, namun mesin pencari lain tidak mampu menghentikan kesuksesan Google. Setelah Yahoo mampu pada posisi puncak di sekitar tahun 2000, Google mampu menerobos liga besar tersebut. sehingga Google dipandang sebagai mesin pencari yang utama seperti yang kita ketahui pada hari ini. Yahoo! raja direktori di internet, di samping para pengguna internet melihat DMOZ serta LookSmart berusaha menurunkan nya dari posisi puncak tersebut. Akhir-akhir ini, telah tumbuh secara cepat dalam ukurannya, mereka pun sudah memiliki harga sehingga mudah untuk memasukinya, dengan demikian, mendapatkan sebuah daftar pada direktori Yahoo memang memiliki nilai yang tinggi. pada tahun 2001, mesin pencari Google berkembang besar. Keberhasilan ini didasarkan pada bagian konsep dasar dari link popularity dan PageRank. Setiap halaman diurutkan berdasarkan seberapa banyak situs yang terkait, dari sebuah premis bahwa situs yang diinginkan pasti lebih banyak terhubung daripada yang lain. Rangking situs (The PageRank) dari sebuah link halaman dan jumlah link dari halaman-halaman tersebut merupakan masukan bagi Rangking situs yang bersangkutan. Hal ini memungkinkan bagi Google untuk mengurutkan hasilnya berdasarkan seberapa banyak halaman situs yang menuju ke halaman yang ditemukannya. User interface Google sangat disukai oleh pengguna, dan hal ini berkembang ke para pesaingnya. Mesin pencari web bekerja dengan cara menyimpan informasi tentang banyak halaman web, yang diambil langsung dari WWW. Halaman-halaman ini diambil dengan web crawler — browser web otomatis yang mengikuti setiap pranala/link yang dilihatnya. Isi setiap halaman lalu dianalisis untuk menentukan cara indeks-nya (misalnya, kata-kata diambil dari judul, subjudul, atau field khusus yang disebut meta tag). Data tentang halaman web disimpan dalam sebuah indeks basis data untuk digunakan dalam pencarian selanjutnya. Sebagian mesin pencari, seperti Google, menyimpan seluruh atau sebagian halaman sumber (yang disebut tembolok/cache) maupun informasi tentang halaman situs/laman itu sendiri. Selain halaman situs (web), Mesin pencari juga menyimpan dan memberikan informasi hasil pencarian berupa pranala yang merujuk pada file, seperti dokumen/file audio, dokumen/file video, gambar, foto dan sebagainya, serta informasi tentang seseorang, suatu produk, layanan, dan informasi beragam lainnya yang semakin terus berkembang sesuai dengan perkembangan teknologi informasi. Ketika seseorang mengunjungi mesin pencari dan memasukkan query, biasanya dengan memasukkan kata kunci, mesin mencari indeks dan memberikan daftar halaman web yang paling sesuai dengan kriterianya, biasanya disertai ringkasan singkat mengenai judul dokumen dan kadang-kadang sebagian teksnya. Ada jenis mesin pencari lain: mesin pencari real-time. Mesin seperti ini tidak menggunakan indeks. Informasi yang diperlukan mesin tersebut hanya dikumpulkan jika ada pencarian baru. Jika dibandingkan dengan sistem berbasis indeks yang digunakan mesin-mesin seperti Google, sistem real-time ini unggul dalam beberapa hal: informasi selalu mutakhir, (hampir) tak ada pranala mati, dan lebih sedikit sumber daya sistem yang diperlukan. (Google menggunakan hampir 100.000 komputer, Orase hanya satu.) Tetapi, ada juga kelemahannya: pencarian lebih lama rampungnya. Manfaat mesin pencari bergantung pada relevansi hasil-hasil yang diberikannya. Meskipun mungkin ada jutaan halaman web yang mengandung suatu kata atau frasa, sebagian halaman mungkin lebih relevan, populer, atau autoritatif daripada yang lain. Kebanyakan mesin pencari menggunakan berbagai metode untuk menentukan peringkat hasil pencarian agar mampu memberikan hasil \"terbaik\" lebih dahulu. Cara mesin menentukan halaman mana yang paling sesuai, dan urutan halaman-halaman itu diperlihatkan, sangat bervariasi. Metode-metode nya juga berubah seiring waktu dengan berubahnya penggunaan internet dan berevolusinya teknik-teknik baru. Sebagian besar mesin pencari web adalah usaha komersial yang didukung pemasukan iklan dan karenanya sebagian menjalankan praktik kontroversial, yaitu membolehkan pengiklan membayar agar halaman mereka diberi peringkat lebih tinggi dalam hasil pencarian. Melakukan pencarian dokumen yang dimuat pada suatu situs bisa begitu mudah dan kelihatannya mungkin sulit juga. apalagi mengingat begitu menyebarnya informasi di mana-mana, bahkan University of California menyebutkan saat ini telah terdapat lebih dari 50 miliar halaman web di internet, meskipun tidak ada ada satupun yang benar-benar tahu jumlah persisnya. Kesulitan yang mungkin terjadi adalah karena WWW tersebut tidak terdata dalam bentuk yang terstandardisasi isinya. Tidak sama halnya dengan katalog yang ada di perpustakaan, yang memiliki standardisasi secara mendunia berdasarkan subjek dari judul buku, meskipun jumlahnya juga tidak sedikit. Dalam pencarian di web, pengguna selalu memperkirakan kata apa yang kira-kira terdapat pada halaman yang ingin di temukan. Atau kira-kira apa subjek yang dipilih oleh seseorang untuk mengelola halaman situs yang mereka kelola, topik apa saja kira-kira yang di bahas. Jika pengguna melakukan apa yang dikenal dengan pencarian pada halaman web, sebenarnya tidaklah melakukan pencarian. Tidak mungkin melakukan pencarian di WWW secara langsung. Pada web benar-benar terdiri dari banyak sekali halaman web yang disimpan dari berbagai server diseluruh dunia. Komputer pengguna tidak langsung melakukan pencarian kepada seluruh komputer tersebut secara langsung. Apa yang mungkin pengguna lakukan hanyalah melalui komputer untuk mengakses satu atau lebih perantara yang disebut dengan alat bantu pencarian yang ada saat ini. Melakukan pencarian pada alat bantu itu tadi ke database yang dimiliki. Database tersebut mengkoleksi situs-situs yang ditemukan dan simpan. Alat bantu pencarian ini menyediakan hasil pencarian dalam bentuk hypertext link dengan URL menuju halaman lainnya. Saat diklik, dan menuju ke alamat tersebut maka dokumen, gambar, suara dan banyak lagi bentuk lainnya yang ada pada server tersebut disediakan, sesuai dengan informasi yang terdapat di dalamnya. Layanan ini bisa menjangkau ke manapun di seluruh dunia. Tidak mungkin seseorang melakukan pencarian ke seluruh komputer yang terhubung ke internet, atau bahkan alat bantu pencarian yang mengklaim bahwa melakukannya, tidak benar. Saat ini, tiga bentuk dari alat bantu pencarian ini. Menggunakan strategi yang berbeda untuk memanfaatkan kemampuan potensial dari masing-masing nya, yaitu Karakteristik: Karakteristik: Sistem kinerja mesin ini ada beberapa hal yang perlu di perhatikan terutama keterkaitannya dengan masalah arsitektur dan mekanismenya. Merupakan program yang men-download halaman-halaman yang mereka temukan, mirip dengan browser. Perbedaannya adalah bahwa browser menampilkan secara langsung informasi yang ada (baik tekas, gambar, dll). Untuk kepentingan manusia yang menggunakannya pada saat itu, sedangkan spider tidak melakukan untuk menampilkan dalam bentuk yang terlihat seperti itu, karena kepentingannya adalah untuk mesin, bukan untuk manusia, spider pun dijalankan oleh mesin secara otomatis. Kepentingannya adalah untuk mengambil halaman-halaman yang dikunjunginya untuk disimpan kedalam database yang dimiliki oleh search engine. Merupakan program yang dimiliki mesin pencari untuk melacak dan menemukan link yang terdapat dari setiap halaman yang ditemuinya. Tugasnya adalah untuk menentukan spider harus pergi ke mana dan mengevaluasi link berdasarkan alamat yang ditentukan dari awal. Crawler mengikuti link dan mencoba menemukan dokumen yang belum dikenal oleh mesin pencari. Komponen ini melakukan aktivitas untuk menguraikan masing-masing halaman dan meneliti berbagai unsur, seperti teks, headers, struktur atau fitur dari gaya penulisan, tag HTML khusus, dll. Merupakan tempat standar untuk menyimpan data-data dari halaman yang telah dikunjungi, di-download dan sudah dianalisis. Kadang kala disebut juga dengan indeks dari suatu mesin pencari. Mesin yang melakukan penggolongan dan penentuan peringkat dari hasil pencarian pada mesin pencari. Mesin ini menentukan halaman mana yang menemui kriteria terbaik dari hasil pencarian berdasarkan permintaan penggunanya, dan bagaimana bentuk penampilan yang akan ditampilkan. Proses ini dilaksanakan berdasarkan algoritme perangkingan yang dimiliki oleh mesin pencari tersebut, mengikuti kaidah perangkingan halaman yang dipergunakan oleh mereka adalah hak mereka, para peneliti mempelajari sifat-sifat yang mereka gunakan, terutama untuk meningkatkan pencarian yang dihasilkan oleh mesin pencari tersebut. Merupakan komponen yang melayani permintaan dan memberikan respon balik dari permintaan tersebut. Web Server ini biasanya menghasilkan informasi atau dokumen dalam format HTML. Pada halaman tersebut tersedia layanan untuk mengisikan kata kunci pencarian yang diinginkan oleh usernya. Web Server ini juga bertanggung jawab dalam menyampaikan hasil pencarian yang dikirimkan kepada komputer yang meminta informasi. Berikut ini adalah beberapa mesin pencari populer hingga saat ini: Templat:Pencarian Internet"
  },
  {
    "url": "https://is.wikipedia.org/wiki/Leitarv%C3%A9l",
    "title": "Leitarvél - Wikipedia, frjálsa alfræðiritið",
    "content": "Leitarvél getur ýmist vísað til sérstakra vefja eða virkni á vefsíðu er hefur þá virkni að gera notandanum kleift að setja inn leitarorð eða -frasa og finna þær tilteknu síður sem innihalda það sem sóst er eftir. Þróaðri leitarvélar reyna að greina samhengi orða og orðasambanda í þeim tilgangi að birta eingöngu niðurstöður tengdar því sem notandinn var í raun að leita eftir, til að mynda með því að útiloka ótengdar merkingar orða."
  },
  {
    "url": "https://he.wikipedia.org/wiki/%D7%9E%D7%A0%D7%95%D7%A2_%D7%97%D7%99%D7%A4%D7%95%D7%A9_(%D7%90%D7%99%D7%A0%D7%98%D7%A8%D7%A0%D7%98)",
    "title": "מנוע חיפוש (אינטרנט) – ויקיפדיה",
    "content": "מנוע חיפוש היא תוכנה שמוצאת דפי אינטרנט התואמים לחיפוש באינטרנט. המנוע מחפש ברשת העולמית בצורה שיטתית אחר מידע מסוים שצוין בשאילתת החיפוש הטקסטואלית. תוצאות החיפוש מוצגות בדרך כלל בשורה של תוצאות, המכונה לרוב דפי תוצאות של מנועי חיפוש (SERP). המידע עשוי להיות שילוב של היפר-קישורים לדפי אינטרנט, תמונות, סרטונים, אינפוגרפיקה, מאמרים וסוגים אחרים של קבצים. מנועי חיפוש מסוימים גם כורים נתונים הזמינים במסדי נתונים או בספריות פתוחות. שלא כמו ספריות אינטרנט שמתוחזקים על ידי עורכים אנושיים, מנועי החיפוש גם שומרים על מידע בזמן אמת על ידי הפעלת אלגוריתם שסורק את האינטרנט. כל תוכן מבוסס אינטרנט שלא ניתן לאינדקס ולחפש במנוע חיפוש אינטרנטי נופל לקטגוריה של \"אינטרנט עמוק\". מערכת לאיתור מידע מפורסם שנועדה להתגבר על הקושי ההולך וגובר לאתר מידע באינדקסים מרכזיים הולכים וגדלים של עבודה מדעית תוארה ב-1945 על ידי ואנבר בוש, שכתב מאמר באטלנטיק מאנתלי בשם \"כפי שאנו עשויים לחשוב\" (אנ')[1] שבו חזה ספריות מחקר עם הערות מקושרות לא בניגוד לקישורי היפרטקסט מודרניים. ניתוח קישורים לבסוף יהפוך לרכיב מכריע במנועי חיפוש באמצעות אלגוריתמים כמו Hyper Search ו-PageRank. מנועי החיפוש הראשונים באינטרנט קדמו להופעת הרשת בדצמבר 1990: חיפוש משתמשי WHOIS(אנ') ושירות מידע Knowbot לחיפוש משתמשים רב-רשתי יושם לראשונה ב-1989. מנוע החיפוש המתועד היטב הראשון שחיפש קובצי תוכן, ובפרט קובצי FTP, היה ארצ'י, שהופיע לראשונה ב-10 בספטמבר 1990. לפני ספטמבר 1993, הרשת העולמית (World Wide Web) אונדקסה כולה באופן ידני. הייתה רשימה של שרתי רשת עולמית שערך טים ברנרס-לי ואירח בשרת CERN. נותר תיעוד אחד של הרשימה מ-1992[2]. ככל שיותר ויותר שרתי רשת החלו לפעול מקוונת, הרשימה המרכזית לא יכלה עוד לעקוב אחר המצב. באתר NCSA הוכרזו שרתים חדשים תחת הכותרת \"מה חדש!\" הכלי הראשון ששימש לחיפוש תוכן (לעומת משתמשים) באינטרנט היה ארצ'י. השם מייצג את המילה האנגלית ל\"ארכיון\" ללא ה-\"ון\". הכלי נוצר על ידי אלן אמטאג', סטודנט למדעי המחשב באוניברסיטת מקגיל במונטריאול, קוויבק, קנדה. התוכנה הורידה את רשימות התיקיות של כל הקבצים הממוקמים באתרי FTP ציבוריים אנונימיים, ויצרה בסיס נתונים ניתן לחיפוש של שמות קבצים; עם זאת, מנוע חיפוש ארצ'י לא אינדקס את תוכן האתרים האלה מכיוון שכמות הנתונים הייתה כה מוגבלת שניתן היה לחפש בה ידנית בקלות. עליית פרוטוקול התקשורת גופר (שנוצרה ב-1991 על ידי מארק מק'קאהיל מאוניברסיטת מינסוטה) הובילה לשני מנועי חיפוש חדשים, ורוניקה וג'אגהד. כמו ארצ'י, הם חיפשו בשמות הקבצים והכותרות המאוחסנים במערכות האינדקס של גופר. ורוניקה (אינדקס רשתי קל מאוד להפעלה של ארכיונים ממוחשבים) סיפקה חיפוש מילות מפתח של רוב כותרות התפריט של גופר בכל רשימות הגופר. ג'אגהד (חפירה היררכית אוניברסלית של ג'ונזי ותצוגה שלהם) היה כלי להשגת מידע תפריטי משרתי גופר ספציפיים. בקיץ 1993, לא היה קיים מנוע חיפוש ברשת, למרות שקטלוגים מקצועיים רבים תחזקו ידנית. אוסקר ניירשטראז מהאוניברסיטה של ז'נבה כתב סדרה של תסריטי Perl שהעתיקו מדי פעם דפים אלה וכתבו אותם מחדש בפורמט אחיד. זה עיצב את הבסיס עבור W3Catalog, מנוע החיפוש הפרימיטיבי הראשון של הרשת, שיצא לאור ב-2 בספטמבר 1993. ביוני 1993, מתיו גריי, אז ב-MIT, הפיק כנראה את \"רובוט הרשת\" הראשון, הנודד ברחבי הרשת העולמית בפרל, והשתמש בו כדי לייצר אינדקס בשם \"Wandex\". מטרת הרובוט הייתה למדוד את גודל הרשת העולמית, מה שעשה עד סוף 1995. מנוע החיפוש השני ברשת, אליווב, הופיע בנובמבר 1993. אליווב לא השתמש ברובוט רשת, אלא הסתמך על הודעה ממנהלי אתרי אינטרנט על קיומו של קובץ אינדקס בפורמט מסוים בכל אתר. JumpStation (נוצר בדצמבר 1993 על ידי ג'ונתון פלטצ'ר) הוא השתמש ברובוט רשת כדי למצוא דפי רשת ולבנות את האינדקס שלו, והשתמש בטופס רשת כממשק לתוכנת השאילתה שלו. לכן, זו הייתה הכלי הראשון לגילוי משאבי WWW ששילב את שלושת המאפיינים החיוניים של מנוע חיפוש רשת (זחילה, אינדוקציה וחיפוש) כמתואר להלן. בגלל המשאבים המוגבלים שהיו זמינים על הפלטפורמה שפעלה עליה החיפוש שלה הוגבל לכותרות וכותרות שנמצאו בדפי הרשת שמצא הרובוט. אחד ממנועי החיפוש הראשונים \"כל הטקסט\" על בסיס רובוט רשת היה WebCrawler, שיצא ב-1994. בניגוד לקודמיו, הוא איפשר למשתמשים לחפש כל מילה בכל דף רשת, מה שהפך לסטנדרט עבור כל מנועי החיפוש הגדולים מאז. זה גם היה מנוע החיפוש שהיה מוכר היטב לציבור. כמו כן, ב-1994, Lycos (שהחל באוניברסיטת קרנגי מלון) הושק והפך למאמץ מסחרי גדול. מנוע החיפוש הפופולרי הראשון ברשת היה Yahoo! חיפוש. המוצר הראשון מ-Yahoo!, שנוסד על ידי ג'רי יאנג ודייוויד פילו בינואר 1994, היה ספריית רשת בשם ספריית Yahoo! ב-1995 נוספה פונקציית חיפוש, שאיפשרה למשתמשים לחפש בספריית Yahoo! זה הפך לאחת הדרכים הפופולריות ביותר עבור אנשים למצוא דפי רשת מעניינים, אבל פונקציית החיפוש שלו פעלה על ספריית הרשת שלה, ולא על העתקי הטקסט המלא של דפי הרשת. זמן קצר לאחר מכן, הופיעו מספר רב של מנועי חיפוש והתחרו על פופולריות. אלה כללו מגלן, אקסייט, אינפוסיק, אינקטומי, Northern Light ו-AltaVista. מחפשי מידע יכלו גם לעיין בספרייה במקום לבצע חיפוש מבוסס מילות מפתח. ב-1996, רובין לי פיתח את אלגוריתם דירוג האתרים RankDex עבור דירוג תוצאות דף מנועי חיפוש וקיבל פטנט אמריקאי על הטכנולוגיה. זה היה מנוע החיפוש הראשון שהשתמש בקישורי היפר-טקסט כדי למדוד את איכות האתרים באינדקס, פטנט דומה מאוד הוגש על ידי Google שנתיים מאוחר יותר ב-1998. לארי פייג' התייחס לעבודתו של לי בחלק מהפטנטים האמריקאיים שלו עבור PageRank. מאוחר יותר לי השתמש בטכנולוגיית Rankdex שלו עבור מנוע החיפוש Baidu, שאותו ייסד בסין והשיק בשנת 2000. ב-1996, נטסקייפ חיפשה לתת עסקה בלעדית למנוע חיפוש אחד כמנוע החיפוש המוצג בדפדפן הרשת של נטסקייפ. היה כל כך הרבה עניין עד שבמקום זאת, נטסקייפ חתמה עסקאות עם חמישה מנועי חיפוש גדולים: תמורת 5 מיליון דולר בשנה, כל מנוע חיפוש יהיה ברוטציה בדף מנוע החיפוש. סביב שנת 2000, מנוע החיפוש של גוגל עלה לתודעה. החברה השיגה תוצאות טובות יותר עבור חיפושים רבים באמצעות אלגוריתם בשם PageRank, כפי שהוסבר במאמר \"אנטומיה של מנוע חיפוש\" שנכתב על ידי סרגיי ברין ולארי פייג', המייסדים העתידיים של גוגל. אלגוריתם איטרטיבי זה מדרג דפי אינטרנט בהתבסס על מספר ו-PageRank של אתרי אינטרנט ודפים אחרים המקשרים אליהם, על ההנחה שדפים טובים או רצויים מקושרים יותר מאחרים. בקשת הפטנט של לארי פייג' על PageRank מציינת את בקשת הפטנט המוקדמת יותר של רובין לי על RankDex כהשפעה. גוגל גם שמרה על ממשק מינימליסטי למנוע החיפוש שלה. לעומת זאת, רבים ממתחריה שילבו מנוע חיפוש בפורטל אינטרנט. למעשה, מנוע החיפוש של גוגל הפך כל כך פופולרי עד שהופיעו מנועי חיפוש מזויפים כמו Mystery Seeker. עד שנת 2000 סיפקה Yahoo! שירותי חיפוש מבוססים על מנוע החיפוש של אינקטומי. Yahoo! רכשה את אינקטומי ב-2002, ואת אוברצ'ר (שבבעלותה AlltheWeb ו-AltaVista) ב-2003. Yahoo! עברה למנוע החיפוש של גוגל עד 2004, עת שחררה את מנוע החיפוש שלה שהתבסס על הטכנולוגיות המשולבות של רכישותיה. מיקרוסופט שחררה לראשונה את MSN Search בסתיו 1998 תוך שימוש בתוצאות חיפוש מאינקטומי. בתחילת 1999, האתר החל להציג רשימות מלוקסמארט, מעורבבות עם תוצאות מאינקטומי. לפרק זמן קצר ב-1999, MSN Search השתמשה בתוצאות מ-AltaVista במקום. ב-2004, מיקרוסופט החלה במעבר לטכנולוגיית חיפוש משלה, המונעת מזחלן אינטרנט משלה (הנקרא msnbot). מנוע החיפוש הממותג מחדש של מיקרוסופט, Bing, הושק ב-1 ביוני 2009. ב-29 ביולי 2009, Yahoo! ומיקרוסופט סיכמו על עסקה שבה חיפוש Yahoo! יונע על ידי טכנולוגיית Bing של מיקרוסופט."
  },
  {
    "url": "https://kn.wikipedia.org/wiki/%E0%B2%B8%E0%B2%B0%E0%B3%8D%E0%B2%9A%E0%B3%8D_%E0%B2%8E%E0%B2%82%E0%B2%9C%E0%B2%BF%E0%B2%A8%E0%B3%8D",
    "title": "ಸರ್ಚ್ ಎಂಜಿನ್ - ವಿಕಿಪೀಡಿಯ",
    "content": "ವಿಶ್ವವ್ಯಾಪಿ ಜಾಲದಲ್ಲಿರುವ ಮಾಹಿತಿಯನ್ನು ನಮ್ಮ ಅಗತ್ಯಕ್ಕೆ ತಕ್ಕಂತೆ ಹುಡುಕಲು ಸರ್ಚ್ ಎಂಜಿನ್ ಅಥವಾ ಶೋಧನ ಚಾಲಕ ತಂತ್ರಾಂಶಗಳು ಸಹಾಯಮಾಡುತ್ತವೆ. ಗೂಗಲ್, ಬಿಂಗ್ ಇವೆಲ್ಲ ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳಿಗೆ ಉದಾಹರಣೆಗಳು[೧]. ಯಾವುದೇ ವಿಷಯದ ಕುರಿತು ವಿಶ್ವವ್ಯಾಪಿ ಜಾಲದಲ್ಲಿ ಇರಬಹುದಾದ ಮಾಹಿತಿಯನ್ನು ಅತ್ಯಂತ ಸುಲಭವಾಗಿ ಹುಡುಕಿಕೊಡುವ ಈ ತಂತ್ರಾಂಶಗಳು ವಿಶ್ವದಾದ್ಯಂತ ಇರುವ ಅಂತರಜಾಲ ಬಳಕೆದಾರರಲ್ಲಿ ಅತ್ಯಂತ ಜನಪ್ರಿಯವಾಗಿವೆ. ವಿವಿಧ ವಿಷಯಗಳಿಗೆ ಸಂಬಂಧಪಟ್ಟ ಪಠ್ಯರೂಪದ ಮಾಹಿತಿಯಷ್ಟೇ ಅಲ್ಲದೆ ಚಿತ್ರಗಳು, ಸುದ್ದಿಗಳು, ಇ-ಪುಸ್ತಕಗಳು, ಭೂಪಟಗಳು ಮುಂತಾದ ಅನೇಕ ರೂಪಗಳಲ್ಲಿರುವ ಮಾಹಿತಿಯನ್ನು ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳು ಹುಡುಕಿಕೊಡುತ್ತವೆ. ನಿಮ್ಮ ಇಚ್ಛೆಯ ವಿಷಯಗಳಿಗಾಗಿ ವಿಶ್ವವ್ಯಾಪಿ ಜಾಲದ ಪುಟಗಳ ನಡುವೆ ಹುಡುಕಾಟ ನಡೆಸುವುದು ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳ ಕೆಲಸ. ನಾವು ಹುಡುಕುತ್ತಿರುವ ವಿಷಯಕ್ಕೆ ಸಂಬಂಧಿಸಿದ ಮುಖ್ಯ ಪದಗಳನ್ನು (ಕೀ ವರ್ಡ್ಸ್) ನಿರ್ದಿಷ್ಟರೂಪದಲ್ಲಿ ಬೆರಳಚ್ಚಿಸಿ, ಹುಡುಕುವ ಪ್ರಕ್ರಿಯೆಯನ್ನು ಪ್ರಾರಂಭಗೊಳಿಸಿದರೆ, ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳು ನಮಗೆ ಬೇಕಾದ ಮಾಹಿತಿ ವಿಶ್ವವ್ಯಾಪಿ ಜಾಲದಲ್ಲಿ ಎಲ್ಲೆಲ್ಲಿ ಲಭ್ಯವಿದೆ ಎಂಬುದನ್ನು ಹುಡುಕಿಕೊಡುತ್ತವೆ; ಕಡತಗಳು, ಚಿತ್ರಗಳು, ಸುದ್ದಿಗಳು, ವಿಡಿಯೋಗಳು - ಹೀಗೆ ನಮಗೆ ಬೇಕಾದ ವಿಷಯಕ್ಕೆ ಸಂಬಂಧಪಟ್ಟ, ಯಾವುದೇ ರೂಪದಲ್ಲಿರುವ ಮಾಹಿತಿಯನ್ನು ಪತ್ತೆಮಾಡುತ್ತವೆ.\nಹೀಗೆ ಹುಡುಕಿದ ಮಾಹಿತಿಯನ್ನು ಅದರ ಪ್ರಾಮುಖ್ಯತೆಗೆ ಅನುಗುಣವಾಗಿ ಜೋಡಿಸಿ ಅವು ನಿಮ್ಮ ಮುಂದೆ ಪ್ರದರ್ಶಿಸುತ್ತವೆ. ಇಂತಹ ಪ್ರತಿಯೊಂದು ಮಾಹಿತಿಯೂ ಒಂದೊಂದು ತಂತು ಅಥವಾ 'ಲಿಂಕ್'ನ ರೂಪದಲ್ಲಿರುವುದರಿಂದ ಅವುಗಳ ಮೇಲೆ ಕ್ಲಿಕ್ ಮಾಡಿ ನಿಮಗೆ ಬೇಕಾದ ಮಾಹಿತಿ ಪಡೆದುಕೊಳ್ಳುವುದು ಸಾಧ್ಯವಾಗುತ್ತದೆ. ಈ ಮಾಹಿತಿ ಹುಡುಕಾಟ ನಡೆಸಲು ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳು ವೆಬ್ ಸ್ಪೈಡರ್‌ಗಳೆಂಬ ತಂತ್ರಾಂಶಗಳನ್ನು ಬಳಸುತ್ತವೆ. ವಿಶ್ವವ್ಯಾಪಿ ಜಾಲದಲ್ಲಿರುವ ಲಕ್ಷಾಂತರ ಜಾಲತಾಣಗಳನ್ನು (ವೆಬ್‌ಸೈಟ್) ಹಾಗೂ ಅವುಗಳಲ್ಲಿರುವ ಪುಟಗಳನ್ನು ಅವುಗಳ ಜನಪ್ರಿಯತೆಗೆ ಅನುಗುಣವಾಗಿ ವರ್ಗೀಕರಿಸಿ, ಆ ಪುಟಗಳಲ್ಲಿರುವ ಮಾಹಿತಿ ಯಾವ ವಿಷಯಗಳಿಗೆ ಸಂಬಂಧಿಸಿದ್ದು ಎಂಬ ಅಂಶವನ್ನು ದಾಖಲಿಸಿಕೊಳ್ಳುವುದು ಇವುಗಳ ಕೆಲಸ. ಜಾಲತಾಣಗಳನ್ನು ರೂಪಿಸುವವರು ಇಂತಹ ಸ್ಪೈಡರ್‌ಗಳಿಗೆ ಸಹಾಯವಾಗಲೆಂದೇ ತಮ್ಮ ತಾಣದ ಪುಟಗಳ ಬಗ್ಗೆ ಪ್ರಮುಖ ಮಾಹಿತಿಗಳನ್ನು 'ಮೆಟಾ ಟ್ಯಾಗ್'ಗಳ ರೂಪದಲ್ಲಿ ಸಂಕ್ಷಿಪ್ತವಾಗಿ ಶೇಖರಿಸಿಟ್ಟಿರುತ್ತಾರೆ. ಸ್ಪೈಡರ್‌ಗಳ ಹುಡುಕಾಟ ಇದೇ ಮೆಟಾ ಟ್ಯಾಗ್‌ಗಳನ್ನು ಆಧರಿಸಿರುತ್ತದೆ. ಸ್ಪೈಡರ್‌ಗಳು ಸಾಮಾನ್ಯವಾಗಿ ತಮ್ಮ ಹುಡುಕಾಟವನ್ನು ಅತ್ಯಂತ ಪ್ರಸಿದ್ಧ ಜಾಲತಾಣಗಳು ಹಾಗೂ ಅತಿಹೆಚ್ಚು ಬಳಕೆದಾರರನ್ನು ಹೊಂದಿರುವ ಸರ್ವರ್‌ಗಳಿಂದ ಪ್ರಾರಂಭಿಸುತ್ತವೆ. ಅಲ್ಲಿಂದ ಮುಂದಕ್ಕೆ ಆ ಜಾಲತಾಣ ಹಾಗೂ ಆ ಜಾಲತಾಣವನ್ನು ಹೊಂದಿರುವ ಸರ್ವರ್‌ನಲ್ಲಿರುವ ಎಲ್ಲ ಪುಟಗಳ ಮೇಲೂ ಒಮ್ಮೆ ಕಣ್ಣಾಡಿಸಿ ಅವುಗಳಲ್ಲಿರುವ ಮಾಹಿತಿಯನ್ನು ವರ್ಗೀಕರಿಸಿಟ್ಟುಕೊಳ್ಳುವ ಕೆಲಸ ಪ್ರಾರಂಭವಾಗುತ್ತದೆ. ಒಂದು ಜಾಲತಾಣದಲ್ಲಿರುವ ಎಲ್ಲ ತಂತುಗಳನ್ನೂ ಈ ಜೇಡಗಳು ಹಿಂಬಾಲಿಸುವುದರಿಂದ ಅವುಗಳ ನಿಲುಕಿಗೆ ಸಿಗುವ ಪುಟಗಳ ಸಂಖ್ಯೆ ಬೆಳೆಯುತ್ತಲೇ ಹೋಗುತ್ತದೆ. ಸ್ಪೈಡರ್‌ಗಳು ಸಂಗ್ರಹಿಸುವ ಮಾಹಿತಿಯ ಅಧಾರದ ಮೇಲೆ ಒಂದು ಅಕಾರಾದಿಯನ್ನು (ಇಂಡೆಕ್ಸ್) ತಯಾರಿಸಿಕೊಳ್ಳುವ ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳು ನಮಗೆ ಬೇಕಾದ ಮಾಹಿತಿಯನ್ನು ಅತ್ಯಂತ ವೇಗವಾಗಿ ಹುಡುಕಿಕೊಳ್ಳಲು ಸಹಾಯ ಮಾಡುತ್ತವೆ. ಸರ್ಚ್ ತಂತ್ರಜ್ಞಾನ ಬೆಳೆದಂತೆ ಎಲ್ಲಬಗೆಯ ಮಾಹಿತಿಯನ್ನೂ ಹುಡುಕುಕೊಡುವ ಗೂಗಲ್‌ನಂತಹ ಸರ್ಚ್‌ಎಂಜಿನ್‌ಗಳ ಜೊತೆಗೆ ನಿರ್ದಿಷ್ಟ ವಿಷಯಗಳಿಗೆ (ಉದ್ಯೋಗಾವಕಾಶಗಳು, ವೈಜ್ಞಾನಿಕ ಮಾಹಿತಿ, ಪ್ರವಾಸ ಇತ್ಯಾದಿ) ಮಾತ್ರವೇ ಸೀಮಿತವಾದ ವರ್ಟಿಕಲ್ ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳೂ ಹುಟ್ಟಿಕೊಂಡಿವೆ. ಹೆಚ್ಚು ಪ್ರಚಲಿತದಲ್ಲಿರುವ ಪಠ್ಯಾಧಾರಿತ ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳ ಜೊತೆಗೆ ಚಿತ್ರ ಹಾಗೂ ಧ್ವನಿಯ ನೆರವಿನಿಂದ ಹುಡುಕಾಟವನ್ನು ಸಾಧ್ಯವಾಗಿಸುವ ಸರ್ಚ್ ಎಂಜಿನ್‌ಗಳೂ ಇವೆ."
  },
  {
    "url": "https://la.wikipedia.org/wiki/Machina_quaesitoria",
    "title": "Machina quaesitoria - Vicipaedia",
    "content": "Machina quaesitoria,[1] sive quaesitorium,[2]  est ratio informationis recuperandae, quae ad informationem in systemate computatrali inveniendam designatur. Exitus investigationis, plerumque in indicibus exhibiti, ictus usitate appellantur. Machinae quaerendi tempus informationis inveniendae summamque informationis consuluendae minuere adiuvant, aliarum artium ad nimium informationis curandum similes. Maxime apertum et aspectabile genus machinae quaerendi est machina quaerendi interretialis, quae informationem intra telam totius terrae quaerit."
  },
  {
    "url": "https://lt.wikipedia.org/wiki/Internetin%C4%97s_paie%C5%A1kos_sistema",
    "title": "Internetinės paieškos sistema – Vikipedija",
    "content": "Internetinės paieškos sistema – sistema, leidžianti ieškoti informacijos saityne. Pastaroji sistema veikia kompiuterinių programų (interneto robotai, „tinklalapių voriukai“) pagrindu, kurios nuskaito visas internetu pasiekiamas rinkmenas ir įtraukia juos į internetinę rodyklę, atsižvelgdama į tam tikrus raktažodžius rinkmenose. Be to, kiekviena paieškos sistema naudoja savus, viešai neatskiedžiamus algoritmus sudarinėjant internetines rodykles.[1] Kai kurie paieškos sistemų robotai paiso robots.txt (Vikipedijos robots.txt byla) nurodomų apribojimų. 2017 m. duomenimis „Google“ yra populiariausia paieškos sistema tiek pasaulyje, tiek JAV ir Europoje, atsiriekusi 78 % rinkos dalį. „Bing“ ir „Yahoo! Search“ tenka atitinkamai 8 % ir 5 % pasaulinės rinkos dalies.[2] Rusijoje itin populiari „Yandex“ paieškos sistema, o Kinijoje viešpatauja „Baidu“ paieškos sistema (ten JAV bendrovių paieškos sistemomis uždrausta naudotis). „Yahoo! Search“ populiariausia Japonijoje."
  },
  {
    "url": "https://nia.wikipedia.org/wiki/Mesin_Wangalui",
    "title": "Mesin Wangalui - Wikipedia",
    "content": "Mesin Wangalui (li Indonesia: mesin pencari, li Inggris: search engine) ya'ia da'ö no sambua nahia heza tola la'alui ngawalö zinura ba Internet niha. Mesin Wangalui fondrege ebua ba gulidanö ya'ia Google, me 83.49% moroi fefu niha sangalui hadia ba internet me ndröfi 2023, la'alui ia ba google.com. Furinia so Bing, ni'okhögö Microsoft, ba zi 9.19%.[1] Mesin Wangalui no komputer sangirö'ö khai-khai ba fefu nisura niha ba gu'ö Internet. Khai-khai andrö to'ölö latötöi pranala ba li Indonesia ma link ba li Inggris. Eluahania lö i'irö'ö zinura andrö mesin wangalui, ha i'irö'ö khai-khainia. Gofu hadia zinura si so ba internet no te'irö'ö ia ba zi sambua komputer. Ero komputer andre so nomoronia nifotöi nomoro IP, same'e irege tola tesöndra ia. Duma-dumania nomoro IP mbolokha gu'ö kabarnias.com ya'ia 117.53.44.147. Ena'ö aoha khö niha wanötöi ya'ia ba aoha khöda wananö ba dödö, andrö labe'e döinia simane kabarnias.com. Na lö khönia nomoro IP andrö, ba lö tola tesöndra kabarnias.com da'ö. Ba duma-duma andre, ero zinura nifairö yaŵa ba mbolokha gu'ö kabarnias.com, so khai-khainia. Duma-dumania khai-khai zinura \"Ejaan Bahasa Nias yang Perlu Pembaharuan\" so ba https://kabarnias.com/sudut-pandang/ejaan-bahasa-nias-yang-perlu-pembaharuan-10503 Archived 2024-06-09 at the Wayback Machine Faoma khai-khai andrö tola tabokai ba tabaso zinura andrö yaŵa, gofu hadia wuka gu'ö ni'oguna'öda, Google Chrome ma Microsoft Edge ma zui mesin wangalui tanöbö'ö. Na tabe'e bakha ba nahia wangalui khai-khai da'ö ba tola tesöndra ba tebokai ia. Nifalua mesin wangalui ya'ia da'ö ba wangowuloi khai-khai fefu zinura si so ba gulidanö. Moroi ba mesin wangalui Google tola tasöndra khai-khai zinura andrö no mege yaŵa, heŵa'ae sinura andrö samösa te'irö'ö ia ba komputer (nifotöi komputer server) kabarnias.com. Na lö mesin wangalui, lö lala khöda wanöndra ngawalö zura nisura ba internet. Ha awai khöda lala wanöndra sambua zinura, na möi ita ba nomoro IP komputer server khönia. Bahiza haniha zi tola manöngöni fefu döi server si so ba gulidanö? Eluahania mesin wangalui andre hulö ono meza, heza ta'irö'ö fefu mbalö garate nisurada bakha ngawalö nomoro telepon, ma ngawalö nisurada somasi ita ta'irö'ö. Na möi ita tabokai google.com, fakhili ira na tabokai nono meza andrö, ba ta'alui gofu hadia ia zi no mu'irö'ö bakha ba da'ö."
  },
  {
    "url": "https://hu.wikipedia.org/wiki/Keres%C5%91rendszer_(informatika)",
    "title": "Keresőrendszer (informatika) – Wikipédia",
    "content": "Ez a lap egy ellenőrzött változata Pontosságellenőrzött Keresőrendszer alatt az informatikában olyan webes felületű vagy szoftveres szolgáltatást értenek, ami multimédiás tartalmak vagy adatbázisok rendszeres vagy egyéni kérésre történő rendezését és/vagy nyomon követését és/vagy kivonatolását, és a tartalomnak a felhasználó, és általában a szélesebb nyilvánosság részére történő rendelkezésére bocsátását nyújtja. A megjelenítendő tartalom neve, ami szó vagy (szavakat, logikai operátorokat és egyéb attribútumokat tartalmazó) összetett nyelvi kifejezés lehet, a keresőszó, tehát ezt kell megadni a keresőrendszernek, és az megkeresi és kijelzi mindazt, amit ezzel kapcsolatban tud. Általában kulcsszavas keresés történik, vagyis a rendszer bekéri a keresendő szót, a felhasználó betáplálja (például begépeli egy űrlapmezőbe), majd a rendszer megjeleníti azokat a tartalmi egységeket, találatokat, amelyek a rendszer szerint a kulcsszóhoz kapcsolódnak. A kulcsszavas keresők segítségével adott szavak vagy kifejezések előfordulására kereshetünk. Ilyen kereső például a Google (www.google.com). Ha valaki például szeretne többet tudni Britney Spearsről (a statisztikák szerint az ő neve volt 2000-ben az egyik leggyakrabban megadott keresőszó a legtöbb keresőrendszerben, lásd itt), akkor a webböngészőjével elnavigál valamelyik keresőrendszer weboldalára (például Google), ott a megfelelő mezőbe beírja, hogy \"Britney Spears\", majd megjelenik több száz weboldal címe, rövid leírással, ami esetleg Spearsszel foglalkozik. Persze lehet, hogy semmit sem talál: például a „Britney Spears nyekereg” keresőkifejezésre a Google rendszer 2004. július 26-án semmit sem talált (ám most már talál, mivel annak a szócikknek az oldalát, amit most olvasol, már meg fogja jeleníteni, mert szerepel rajta az a kifejezés, hogy „Britney Spears nyekereg”: illusztráció…). Az egyik legismertebb webes címszókereső rendszer a Google, ami a World Wide Web tartalmát próbálja feldolgozni és szolgáltatni. A webes keresők mellett a keresőrendszerek legfontosabb példái az adatbáziskezelő programok keresőrendszerei."
  },
  {
    "url": "https://mr.wikipedia.org/wiki/%E0%A4%B6%E0%A5%8B%E0%A4%A7%E0%A4%AF%E0%A4%82%E0%A4%A4%E0%A5%8D%E0%A4%B0",
    "title": "शोधयंत्र - विकिपीडिया",
    "content": "इंटरनेटावर/महाजालात माहितीचा शोध घेण्यास मदत करणाऱ्या संकेतस्थळांना 'शोध यंत्र' (इंग्लिश:Search Engine) असे म्हणतात. एखादा कोणताही विशिष्ट शब्द किंवा शब्दसमूह  ज्या-ज्या वेबपानांवरील मजकुरात असेल, अशी सर्व वेबपाने दाखवण्याचे काम शोधयंत्र करते. बहुतांश आधुनिक लोकप्रिय शोधयंत्रे ही केवळ मजकूरच नव्हे तर, एखाद्या शब्दासंबंधित चित्रे, चलचित्रे व इतर माहिती शोधण्यास मदत करतात. सर्वसाधारणपणे 'शोधयंत्र' ही संज्ञा फक्त इंटरनेटावरील माहिती शोधण्यासंदर्भात वापरली जाते; संगणकावर साठवलेल्या माहितीमध्ये शोध घेण्यासाठी वापरल्या जाणाऱ्या सॉफ्टवेअरांना 'शोधयंत्र' म्हणले जात नाही. महाजालात विविध प्रकारांची शोधयंत्रे वापरली जातात. अधिकाधिक लोक आपल्या वेबपानावर येणे हे शोधयंत्राने दिलेल्या स्थानांकनावर अवलंबून आहे; तसेच ते वेबपानाच्या शोधयंत्र मैत्रीपूर्णतेवरही अप्रत्यक्षपणे अवलंबून आहे. गूगल शोधयंत्राचे महत्त्व आणि वापर २००१ सालापासून वाढला. गूगल शोधयंत्राचे यश दुव्याच्या उपयोगाचे प्रमाण व त्या-त्या वेबपानाच्या स्थानांकनाच्या संकल्पनेत आहे. एखाद्या वेबपानाचा दुवा इतर किती आणि किती महत्त्वाच्या वेबपानांनी दिला आहे यावर त्या वेबपानाचे स्थानांकन निश्चित होते. याकरिता संबंधित वेबपानावर योग्य आणि जास्तीत जास्त लोकांकडून शोधले जाणारे संदर्भशब्द असणे खूप उपयुक्त ठरते. मराठी विकिपीडियाचे संदर्भ  गूगल शोधयंत्राच्या मराठी आवृत्तीत सहज मिळतात; पण मराठी विकिपीडियाचे संदर्भ गूगल शोधयंत्राच्या इंग्रजी आवृत्तीत उशिरा मिळतात. मराठी विकिपीडियातील सुयोग्य शब्दांचे मराठीबरोबरच रोमन लिपीतील पर्यायही उपलब्ध झाल्यास मराठी विकिपीडिया शोध यंत्र मैत्रीपूर्ण होण्यास मदत होईल. याहू इंडिया 'याहू बझ इंडेक्स'चा वापर करून सर्वांत जास्त शोधल्या जाणाऱ्या शब्दांची माहिती देते. अशा शब्दांचा योग्य वापर वेबपानाची शोधयंत्र मैत्रीपूर्णता वाढवण्यास मदत करतो. शोधयंत्र डायनॅमिक फाँट वापरणाऱ्या वेबपानांचे स्थानांकन करू शकत नाही. फक्त युनिकोड चालते. उदाहरणार्थ महाराष्ट्र टाइम्स शोधयंत्रांमार्फत शोधता येतो; पण ई-सकाळ शोधता येत नाही. . (first original results)"
  },
  {
    "url": "https://ms.wikipedia.org/wiki/Enjin_gelintar_web",
    "title": "Enjin gelintar web - Wikipedia Bahasa Melayu, ensiklopedia bebas",
    "content": "Enjin carian sesawang ialah sebuah enjin carian direka khas untuk mencari maklumat dalam Jaringan Sejagat yang turut merangkumi laman sesawang, imej, dan pelbagai jenis fail lain. Sesetengah enjin carian juga mengorek data yang boleh didapati dalam newsbook, pangkalan data, atau direktori terbuka. Tidak seperti direktori web yang diselenggara oleh manusia, enjin carian dikelola secara algoritma atau campuran algoritma dan masukan manusia.[1]"
  },
  {
    "url": "https://min.wikipedia.org/wiki/Masin_paruntun_web",
    "title": "Masin paruntun web - Wikipedia baso Minang",
    "content": "Mesin paruntun web (Baso Inggirih: search engine) iolah sabuah program komputer nan bapaliek-gan untuak mambuek paruntunan ka aleh file nan taasek bakeh layanan www, ftp, pambasuikan pangiriman surek, sarato news group bakeh sabuah/kabara server komputer bakeh sabuah tangkokan (network). Search engine marupoan alaik paruntun maklumaik daripado data-data nan dionyok-gan alah. Acoknyo pulo, hasil paruntunan nangko dipaliek-gan bakeh bantuak lirikan nan kodoknyo balirik-gan basangkalan kabakeh darajaik kaaluihan, buliah pulo basangkalan rasio urang-urang nan manjalang ka aleh sabuah file nan disabuik hits. Maklumaik nan jadi pusek paruntunan dapek disuo bakeh jinih-jinih file nan balainan saroman laman web, gambar, sarato jinih-jinih file lainnyo. Kabara mesin paruntun dikana pulo sadang manyauak maklumaik untuak data nan taasek bakeh sabuah data nan batampaik-gan sarato pulo lirikan-lirikan web. Sabagian gadang mesin paruntun bapalabuah-gan dek sarikaik basurang-surang nan mampaguno-gan algoritma kapunyo-an sarato tampaik data nan talimpok, di antaro etan sadoalahnyo nan amaik tamusahua adolah Google (MSN Search sarato Yahoo!). Ado kabara alah caro-caro untuak mampacipto-gan mesin paruntun dek sumber-sumber nan taungkai (open source) co Htdig, Nutch, Egothor sarato OpenFTS."
  },
  {
    "url": "https://my.wikipedia.org/wiki/%E1%80%9D%E1%80%98%E1%80%BA%E1%80%9B%E1%80%BE%E1%80%AC%E1%80%96%E1%80%BD%E1%80%B1%E1%80%9B%E1%80%B1%E1%80%B8%E1%80%85%E1%80%80%E1%80%BA%E1%80%85%E1%80%94%E1%80%85%E1%80%BA",
    "title": "ဝဘ်ရှာဖွေရေးစက်စနစ် - ဝီကီပီးဒီးယား",
    "content": "ဝဘ်ရှာဖွေရေးစက် (အင်္ဂလိပ်: web search engine) သည် ဆေ စ်ခုဖြစ်သည်။ ၎င်းဆော့ဖ်ဝဲကို အင်တာနက် World Wide Web ပေါ်ရှိ အချက်အလက်များကို ရှာဖွေရာတွင် အသုံးပြုသည်။ ရှာဖွေရရှိလာသော ရလဒ်များကို စာမျက်နှာပေါ်တွင် တစ်ကြောင်ချင်းစီ ပြသထားသည်။ ရလဒ်များတွင် ဝဘ်စာမျက်နှာများ၊ ရုပ်ပုံများ၊ အခြားသော ဖိုင်ပုံစံများစွာလည်း ပါဝင်သည်။ အခြားသော ရှာဖွေရေးစက်စနစ်များသည် အချက်အလက်များကို ခွဲခြမ်းစိတ်ဖြာသည့် စနစ်ကို အသုံးပြုကြသည်။ ရှာဖွေရေးစက်စနစ်များသည် အချိန်နှင့်တစ်ပြေးညီ အချက်အလက်များကို Algorithm များကို အသုံးပြုကာ ထိန်းသိမ်းမွမ်းမံမှုများ ပြုလုပ်ကြသည်။ ရှာဖွေရေးစက်စနစ်သည် ဝဘ် Directories များနှင့်မတူပေ။ ဝဘ် Directories များကိုမူ ပြုပြင်မွမ်းမံရန် လူကိုယ်တိုင်သာလျှင် ပြုလုပ်မှ ရနိုင်သည်။ ၂၀၀၀ ခုနှစ်ပတ်လည်လောက်တွင် ဂူဂဲလ်၏ ရှာဖွေရေးစက်စနစ်သည် ထင်ရှားလူသိများကာ အသုံးပြုမှု မြင့်တက်လာခဲ့သည်။[၁] ကုမ္ပဏီသည် ကောင်းမွန်သည့် ရှာဖွေမှုရလဒ်ပေါင်းများစွာကို ထောက်ပံ့ကာ ရှာပေးနိုင်သည်။ PageRank ဟုခေါ်သည့် တီထွင်မှုနည်းပညာဖြင့် ရှာဖွေပေးခြင်းဖြစ်သည်။[၂] ၂၀၀၀ ခုနှစ်တွင် Yahoo! သည် Inktomi ၏ ရှာဖွေရေးစက်စနစ်အပေါ် အခြေခံထားသည့် ရှာဖွေရေးဝန်ဆောင်မှုများကို ထောက်ပံ့ပေးခဲ့သည်။ Yahoo! သည် Inktomi ကို ၂၀၀၂ ခုနှစ်တွင် ရရှိခဲ့သည်။ Overture ကို ၂၀၀၃ ခုနှစ်တွင် ရရှိခဲ့သည်။ Overture သည် AlltheWeb နှင့် AltaVista တို့က ပိုင်ဆို်င်ကြသည်။ Yahoo! သည် ရရှိထားသည် နည်းပညာများကို ပေါင်းစပ်ကာ ကိုယ်ပိုင်ရှာဖွေရေးစက်စနစ်ကို စတင်ခဲ့သည်။ မိုက်ခရိုဆော့ဖ်သည် Bing ဟုခေါ်သည့် ရှာဖွေရေးစက်စနစ်ကို ၂၀၀၉ ခုနှစ်၊ ဇွန်လ ၁ ရက်နေ့တွင် စတင်ခဲ့သည်။ ဂူဂဲလ်သည် ကမ္ဘာပေါ်တွင် လူသုံးအများဆုံး ရှာဖွေရေးစက်စနစ် တစ်ခု ဖြစ်သည်။ ၂၀၁၇ ခုနှစ်၊ မတ်လထိ ဈေးကွ၏က်၏ ၈၀.၉၂ ရာခိုင်နှုန်းကို ပိုင်ဆိုင်ထားသည်။[၃] ကမ္ဘာပေါ်တွင် လူသုံးများသော ရှာဖွေရေးစက်စနစ်များ (ဈေးကွက်ဝေစု ၁ ရာခိုင်နှုန်းထက်ပိုသော) မှာ အရှေ့အာရှ နိုင်ငံအချို့ နှင့် ရုရှနိုင်ငံတို့တွင် ဂူဂဲလ်သည် လူသိများကာ အသုံးအများဆုံး ရှာဖွေရေး စက်စနစ် တစ်ခု ဖြစ်သည်။ ရုရှနိုင်ငံတွင် Yandex သည် ဈေးကွက်၏ ၆၁.၉ ရာခိုင်နှုန်း ရရှိထားပြီး ဂူဂဲလ်မှာမူ ၂၈.၃ ရာခိုင်နှုန်းသာ ရရှိသည်။[၄] တရုတ်နိုင်ငံတွင် Baidu သည် လူသုံးအများဆုံး ဖြစ်သည်။[၅] တောင်ကိုရီးယားနိုင်ငံတွင် အွန်လိုင်းရှာဖွေမှုများပြုလုပ်ရာတွင် Naver ကို ၇၀ ရာခိုင်နှုန်း အသုံးပြုကြသည်။[၆] Yahoo! ဂျပန် နှင့်Yahoo! ထိုင်ဝမ် တို့သည် ဂျပန် နှင့် ထိုင်ဝမ်နိုင်ငံတို့တွင် လူသုံးများကြသည်။[၇] ချက်သမ္မတနိုင်ငံမှတပါး အနောက်ဥရောပနိုင်ငံအများစုတွင် ဂူဂဲကို အသုံးများကြသည်။ ချက်သမ္မတနိုင်ငံတွင် Seznam သည် ကောင်းမွန်သည့် ပြိုင်ဘက်တစ်ခုလည်း ဖြစ်သည်။[၈]"
  },
  {
    "url": "https://ne.wikipedia.org/wiki/%E0%A4%B5%E0%A5%87%E0%A4%AC_%E0%A4%96%E0%A5%8B%E0%A4%9C_%E0%A4%B8%E0%A4%82%E0%A4%AF%E0%A4%A8%E0%A5%8D%E0%A4%A4%E0%A5%8D%E0%A4%B0",
    "title": "वेब खोज संयन्त्र - विकिपिडिया",
    "content": "वेब खोज संयन्त्र वा इन्टरनेट खोज संयन्त्र वेब खोज वा इन्टरनेटमा खोज गर्नका लागि सृजना गरिएको एक सफ्टवेयर प्रणाली हो जसको अर्थ विश्वव्यापी वेबलाई एक मुलपाठ वेब खोज प्रश्नमा निर्दिष्टित, विशेष जानकारीको लागि व्यवस्थित तरीकाले खोजी गर्नु हो।[१]\nवा अर्को शब्दमा भन्नु पर्दा वेब खोज संयन्त्र एक वेब अनुप्रयोग हो जसको प्रयोगबाट प्रयोगकर्ताले शब्दहरूको रूपमा  भण्डारबाट स्रोतहरू फेला पार्न सक्दछन्। यसका संसाधनमा वेब पृष्ठ, इन्टरनेट मञ्च लेख, तस्वीर, श्रव्य दृश्य, फाइल, पुस्तक, शैक्षिक वेबसाइट, अनुप्रयोग, खुला स्रोत सफ्टवेयर, आदि हुन्छन्।[२] केही वेबसाइटहरूले खोज संयन्त्रलाई उनीहरूको मुख्य सुविधाको रूपमा प्रस्ताव गर्दछन् जसलाई खोज संयन्त्र वेबसाइट भनिएको छ जुन मानव हस्तक्षेप बिना वेबमा अनुसन्धान गर्ने उपकरणहरू पनि हुन्। सामान्यतया वेब खोज संयन्त्रहरू \"इन्टरनेट सरीसृप\"मा आधारित हुन्छन् जसले नियमित अन्तरालमा साइटहरूलाई जाँच गर्ने र नयाँ ठेगानाहरू पत्ता लगाउन स्वचालित रूपमा मद्दत गर्दछ। सेवाहरूले सूत्र पछ्याउँछन् जसले पृष्ठहरूलाई एक पछि अर्को गर्दै सूत्रबद्ध गर्दछ। प्रत्येक पहिचान गरिएको पृष्ठ त्यसपछि भण्डारमा अनुक्रमित हुन्छन् र पछि इन्टरनेट प्रयोगकर्ताहरूले शब्दहरू प्रयोग गरेर पहुँच गर्न सक्दछन्। खोज संयन्त्रहरू इन्टरनेटमा मात्र लागू हुँदैन। केही संयन्त्रहरूले सफ्टवेयरको रूपमा कार्य गर्दछन् जुन व्यक्तिगत सुखाङ्ख्यमा स्थापित अवस्थामा रहेका हुन्छन्। यी तथाकथित सुशाङ्ख्य संयन्त्रहरू हुन् जसले सुशाङ्ख्यमा भण्डार गरिएका फाइलहरूको बीचमा खोजी र वेबसाइटहरूको बीचमा जोडेर खोजी गर्दछ। इन्टरनेट मञ्चमा विस्तारित खोज संयन्त्रहरू पनि रहेका हुन्छन् जसले विस्तारित खोजी गर्ने काम गर्दछन्।[३] इन्टरनेट खोज संयन्त्रले डिसेम्बर १९९० मा, विश्वव्यापी वेबमा प्रदापण गरेको थियो। [४] १० सेप्टेम्बर १९९० मा, पहिलो खोज संयन्त्र आर्चीले इन्टरनेटमा कदम राखेको थियो। आर्ची, इन्टरनेटमा सामग्रीहरूको खोजीका लागि प्रयोग गरिएको पहिलो उपकरण थियो।[५] यसलाई क्यानडाको क्युबेकको मोन्ट्रियलको म्याकगिल विश्वविद्यालयका कम्प्युटर विज्ञान विद्यार्थी एलन इम्जेट, बिल हेलन र जे पिटर दोएच्सले संयुक्त रूपमा निर्माण गरेका थिए। उपकरणले सार्वजनिक फाइल नामहरू खोजीयोग्य भण्डारमा सृजना गर्दै फाइल आदनप्रदान सौजन्यविधि साइटहरूमा अवस्थित सबै फाइलहरूको निर्देशिकाहरूलाई सूचीबद्ध रूपमा अभिभारण गरेको थियो।[६] आर्ची खोज संयन्त्रको आँकडा सीमित भएको र यसलाई सजिलैसँग गैर स्वचालित रूपमा खोजी गर्न सकिने भएकाले यसले यी वेबसाइटहरूको सामग्रीलाई अनुक्रमणिका गरेको थिएन।[७] वेबमा पहिलो लोकप्रिय खोज संयन्त्र भनेको याहुको याहु! खोज थियो जसलाई जेरि याङ र डेभिड फिलोले जनवरी १९९४ मा, एक वेब निर्देशिकाको रूपमा सृजना गरेका थिए।[८] य याहुको पहिलो वेब सेवा पनि हो। सन् १९९५ मा, यसमा खोज विशेषता थपिएको थियो जसले प्रयोगकर्ताहरूलाई यसको निर्देशिकामा खोजी गर्न अभिप्रेरित गरेको थियो।[९] यो व्यक्तिहरूको रूचि अनुसार वेब पृष्ठहरू फेला पार्न सबैभन्दा लोकप्रिय तरिकाहरू मध्ये एक बनेको थियो र पछि याहु निर्देशिकालाई याहु! खोजमा पुनर्निर्देशन गरिएको थियो।[१०][११] केही वर्ष पछि, इन्टरनेटमा विभिन्न वेब खोज संयन्त्रहरू देखा परेका थिए जसमा एक्साइट, इन्फोसिक, इङ्कटोमी, नर्थन लाइट, अल्टा भिस्टा आदी समावेश छ जसको मद्दतले सूचना खोज्नेहरूले प्रयोगकर्ताहरूले कुञ्जी शब्दमा आधारित खोजी गर्नुको सट्टा निर्देशिकामा खोजी गर्न सक्षम थिए। सन् १९९६ मा, रोबिन लिले रेङ्कडेक्स नामक सेवाको विकास गरेका थिए जसले पछि प्रविधिको क्षेत्रमा अमेरिकामा एकश्व प्राप्त गरेको थियो।[१२][१३] सूत्र प्रयोग गरी वेबसाइटको गुणस्तर जाँच गर्ने यो एक पहिलो खोज संयन्त्र थियो। यसको समान कलन विधि भएको एकश्वलाई पछि गुगलले सन् १९९८ मा, दर्ता गराएको थियो।[१४] ल्यारी पेजले अमेरिकाको केही एकश्वहरूमा पृष्ठ वरियताको लागि लिको कार्यलाई सान्दर्भित गरेका थिए।[१५] पछि रोबिन लिले आफ्नो रेङ्कडेक्स प्रविधि बायदु खोज संयन्त्रका लागि प्रयोग गरेका थिए जसलाई चीनमा रोबिन लिले स्थापना गरेका थिए भने यसलाई सन् २००० मा सञ्चालनमा ल्याइएको थियो। [१६][१७] सन् १९९६ मा, नेटस्केपले आफ्नो वेब ब्राउजरमा खोज संयन्त्र प्रयोगमा ल्याउनका लागि गृहकार्य थालेको थियो।[१८][१९] गुगलले सन् १९९८ मा गेटो डटकम नामको एक सानो खोज संयन्त्र कम्पनीबाट खोज सर्तहरू बेच्ने विचार अपनाएको थियो।[२०] सन् १९९६ मा, स्थापित भएको रूसी बहुराष्ट्रिय इन्टरनेट निगम यान्दिक्सले सन् १९९७ मा, यान्दिक्स खोजको स्थापना गरेको थियो जसले मुख्यतया रूस र स्वतन्त्र राज्य राष्ट्रसङ्घमा पूर्णरूपमा सेवा प्रदान गर्दछ।[२१][२२] यो रूसको सर्वाधिक लोकप्रिय खोज संयन्त्र बन्न सफल भएको छ।[२३][२४] सन् २००० देखि, याहु!ले इङ्कटोमी खोजी संयन्त्रमा आधारित खोजी सेवाहरू प्रदान गर्ने गरेको थियो। याहु!ले सन् २००२ मा इङ्कटोमी र सन् २००३ मा, ओभर्चको अधिग्रहण गरेको थियो जुन अल द वेब र अल्टा भिस्टाको मालिक थियो। याहु!ले सन् २००४ सम्म गुगलको खोज संयन्त्रको प्रयोग गरेको थियो भने पछि यसले आफ्नो स्वतन्त्र खोज संयन्त्रको सुरु गरेको थियो। माइक्रोसफ्टले सन् १९९८ को अन्त्य तिर, इङ्कटोमीको खोज संयन्त्रको प्रयोग गर्दै एमएसएन खोजको प्रक्षेपण गरेको थियो। सन् १९९९ मा, छोटो समयको लागि यसले अल्टा भिस्टाको प्रयोग गरेको थियो। यसले पछि आफ्नो स्वतन्त्र खोज संयन्त्रको विकास गरेको थियो र माइक्रोसफ्टले १ जुन २००९ मा, यसको स्वतन्त्र खोज संयन्त्रलाई बिङमा पुनर्गठन गरेको थियो। २९ जुलाई २००९ मा, माइक्रोसफ्ट र याहु! एक सम्झौतामा हस्ताक्षर गरेका थिए जसमा माइक्रोसफ्टको खोज संयन्त्र बिङले याहु! खोजलाई थप शक्तिशाली बनाएको थियो।[२५] सन् २०२० का अति लोकप्रिय खोज संयन्त्रहरू, यान्दिक्स, गुगल, बिङ, बायदु, सोसो, सोगौ, डकडकगो, याहु! आदि हुन्। इन्टरनेट खोज संयन्त्रहरूले वेब पृष्ठहरूको बारेमा विभिन्न जानकारी सङ्कलन गर्दै तिनीहरूलाई आफ्नो भण्डारणमा समावेश गर्दछ र प्रत्येक प्रयोगकर्ताहरूका माझ प्रस्तुत गर्ने गर्दछ।संसारको बारेमा सबै जानकारी खोज इन्जिन मार्फत प्राप्त गरिन्छ।[२६] इन्टरनेट खोज संयन्त्रको तीन भागहरू हुन्छन्। कुनै पनि संयन्त्रले भ्रमण गर्ने वेबसाइटहरूबाट आँकडा सङ्कलन गर्दछ र यसको सङ्ग्रह उपकरणले वेबसाइटहरूमा आफ्नो भ्रमणको समयमा सङ्कलन गरेको सर्तहरू समावेश गर्ने सूचकाङ्क यतावत र कार्यशील राखेको हुन्छ।[२७] इन्टरनेट खोज संयन्त्रहरूले धेरै वेबसाइटहरूको बारेमा जानकारी सङ्कलन गरेर काम गर्दछ जसलाई सेवाले विश्वव्यापी वेबमा फेला पार्दछ। यी पृष्ठहरूलाई एक स्वचालित उपकरणद्वारा सङ्कलन गरिएको हुन्छ र आवश्यक परेको खण्डमा प्रदर्शन गर्दछ। प्रत्येक पृष्ठको सामग्रीहरूलाई अनुक्रमणिका विधि पहिचान गर्न विश्लेषण गरिन्छ।[२८][२९] वेब पृष्ठहरूका बारेमा तथ्याङ्कहरू आँकडा अनुक्रमणिकामा भण्डारण गरिन्छ।प्रयोगकर्ताले सेवाको उपयोग गरी खोज गर्दा (सामान्यतया कुञ्जी शब्दहरूको प्रयोग गरेर), खोजी संयन्त्रले यसको अनुक्रमणिकामा छानबिन गर्दछ र मापदण्डको साथ उत्तम नजिताहरू साथ वेब पृष्ठहरूको सूची प्रदान गर्दछ जसमा सामान्यतया सामग्री, तस्वीर, नक्सा, प्रश्नोत्तर, कागजात, श्रव्य दृश्य आदि समावेश हुन्छ। [३०] खोज संयन्त्र उपयोगिता परिणामको प्रासङ्गिकतामा निर्भर गर्दछ। संयन्त्रहरूले उपयुक्त तथा उत्तम परिणाम देखाउने गर्दछन् र कुनै कुनै खोज संयन्त्रहरूले अश्लील सामग्रीहरूलाई समर्थन गर्दैनन्।[३१] सामान्य इन्टरनेट खोज संयन्त्रहरूले मानव ज्ञान र गतिविधिमा जोड दिने गरेका छन्। सेप्टेम्बर २०१९ मा, गुगल विश्वको सर्वाधिक लोकप्रिय खोज संयन्त्र थियो जसले कुल रूपमा ९२.९६% बजार साझेदारी गरेको थियो।[३२] रूसमा यान्दिक्सको लोकप्रिय बढ्दो रूपमा रहेको छ। यसले हाल कुल बजार साझेदारीको ६१.९% नियन्त्रण गरेको छ भने प्रतिस्पर्धी गुगलले जम्मा २८.३% नियन्त्रण गरेको छ।[३३] चीनको कुरा गर्दा, बायदु त्यहाँको सर्वाधिक लोकप्रिय खोज संयन्त्र हो जसले ७२% नियन्त्रण गरेको छ।[३४][३५] दक्षिण कोरियाको भने नाभरको प्रयोग बढ्दो छ जसले वेब खोज संयन्त्रको ७०% नियन्त्रण गर्दछ।[३६] जापान र ताइवानमा क्रमश: याहु! जापान र याहु! ताइवन लोकप्रिय छन्। चीन र रूस यस्ता देशहरू हुन् जसमा शीर्ष ३ भित्र गुगल खोज संयन्त्र रहेको छैन।\n[३७] युरोपका विभिन्न देशहरूमा गुगलको लोकप्रिय रहेता पनि चेक गणतन्त्रमा सेज्नाम यसको कडा प्रतिस्पर्धी हो। [३८] हालको अवस्थामा प्रयोगकर्ताको गोपनियतालाई मध्यनजर गर्ने र यसलाई सुरक्षा गर्ने खोज संयन्त्रको रूपमा डकडकगो[४०], मोजिक[४१] र स्टार्टपेजलाई[४२][४३][४४] लिन सकिन्छ। वेब खोज संयन्त्र कर्लीमा"
  },
  {
    "url": "https://ja.wikipedia.org/wiki/%E6%A4%9C%E7%B4%A2%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3",
    "title": "検索エンジン - Wikipedia",
    "content": "検索エンジン（けんさくエンジン、英: search engine）は、狭義にはインターネットに存在する情報（ウェブページ、ウェブサイト、画像ファイル、ネットニュースなど）を検索する機能およびそのプログラム。インターネットの普及初期には、検索としての機能のみを提供していたウェブサイトそのものを検索エンジンと呼んだが、現在では様々なサービスが加わったポータルサイト化が進んだため、検索をサービスの一つとして提供するウェブサイトを単に検索サイトと呼ぶことはなくなっている。広義には、インターネットに限定せず情報を検索するシステム全般を含む。 狭義の検索エンジンは、ロボット型検索エンジン、ディレクトリ型検索エンジン、メタ検索エンジンなどに分類される。広義の検索エンジンとしては、ある特定のウェブサイト内に登録されているテキスト情報の全文検索機能を備えたソフトウェア（全文検索システム）等がある。 検索エンジンは、検索窓と呼ばれるテキストボックスにキーワードを入力して検索をかけるもので、全文検索が可能なものと不可能なものとがある。検索サイトを一般に「検索エンジン」と呼ぶことはあるが、厳密には検索サイト自体は検索エンジンでない。 与えられた検索式に従って、ウェブページ等を検索するサーバ、システムのこと。検索式は、最も単純な場合はキーワードとなる文字列のみであるが、複数のキーワードにAND（「かつ」、論理積）やOR（「または」、論理和）等の論理条件を組み合わせて指定することができるものが多い。 ロボット型検索エンジンの大きな特徴の一つとして、クローラ（ロボット・スパイダー）を用いることが挙げられる。このことにより、WWW上にある多数の情報を効率よく収集（日本の著作権法では複製）することができる。大規模な検索エンジンでは、80億ページ以上のページから検索が可能になっている。 収集したページの情報は、前もって解析し、索引情報（インデックス）を作成する（日本の著作権法では編集）。日本語などの言語では、自然言語処理機能が生成される索引の質に影響する。このため、多言語対応した検索エンジンの方が精度の高い検索が可能となる。 検索結果の表示順は、検索エンジンの質が最も問われる部分である。ユーザーが期待したページを検索結果の上位に表示することができなければ、ユーザーが離れてしまうからである。そのため、多くの検索エンジンが、表示順を決定するアルゴリズムを非公開にし、その性能を競っている。検索エンジン最適化業者の存在も、アルゴリズムを公開しない要因になっている。Googleは、そのアルゴリズムの一部であるページランクを公開してきたが、やはり、多くの部分が非公開になっている。Googleの場合、創設初期におけるアルゴリズムについては、創設者自身がウェブ上で公表している論文でその一端を知ることができる。\n参照 英語原文[1] 日本語の解説[2] ウェブページの更新時刻の情報を用いて、新しい情報に限定して検索できるものや、検索結果をカテゴリ化して表示するものなど、特長のある機能を搭載したり、検索結果をユーザーへ最適化していく動きもある。 従来のウェブページを検索するだけの検索エンジンにとどまらず、最近ではインターネットショッピング専用の検索エンジンなど、特定の分野に特化した検索エンジンの開発も散見される。商品検索では、価格比較サービス日本最大手の価格.comや、ベンチャー企業が開発するQOOPIEなどある。また、職業検索エンジンとしてはCraigslistなどがある。\nGoogle、Yahoo!、千里眼（終了）、インフォシーク、テクノラティ、MARSFLAG、Altavista、ムーター、AlltheWeb、Teoma（英語版）（終了）、WiseNut（英語版）、Inktomi（終了）、SAGOOL、Yahoo! JAPAN (2005.10〜2010.11[要出典]) など。 人手で構築したウェブディレクトリ内を検索するサーバ、システムのこと。 人手で構築しているため、質の高いウェブサイトを検索可能。概要を人手で記入しているため、検索結果の一覧から目的のサイトを探しやすい、サイトのカテゴリ分けがされていることから、特定分野や地区などに限定したサイトを探しやすいという特長がある。 しかし、検索対象となるサイトは人手で入力するため、検索対象となるサイト数が多くできないという欠点がある。 インターネットが一般に使われるようになった初期（1990年代）のころには、ディレクトリ型が主体であったが、WWWの爆発的な拡大によって、あらゆるウェブサイトを即時にディレクトリに反映させることが事実上不可能になり、現在では主流ではなくなっている[いつ?]。\nこのため、ディレクトリ型検索エンジンでは、検索にヒットするサイトが無かった場合、ロボット型検索エンジンを用いて結果を表示するような、併用型のものが多い[いつ?]。 日立国際ビジネスのHole-in-One（ - 2004年11月）、Yahoo!JAPANのYahoo!カテゴリ（ - 2018年3月[3]）、LookSmart Japan（ - 2006年6月[4]）、gooのgooカテゴリー検索（ - 2019年8月[5]）、Open Directory ProjectことDMOZ（ - 2017年3月）など。 P2P通信によってウェブコンテンツのインデックスを多数のピアに分散させ、P2Pネットワーク全体で各ピアの持つインデックスを共有する検索システムのこと。 ウェブのクロールは各ピアが独自に行い、インデクサーはRWI(Reverse Word Index)を作成する。作成されたインデックスの一部はDHT(分散ハッシュテーブル、Distributed Hash Table)として他のピアに分配される。 検索は自分のピアの端末からP2Pネットワーク上にある他のピアにリクエストを送信することにより行うことができる。 分散型検索エンジンの例としてはYaCyがある。YaCyは「人民による人民のためのウェブ検索」を標榜し、分散型であることにより検閲を防ぐことができるとしている。[6] ひとつの検索ワードを複数の検索エンジンで検索することをメタ検索という（横断検索エンジンと呼ぶこともある）。\n詳細は「メタ検索エンジン」を参照のこと。 与えられた文書群から、検索式（キーワードなど）による全文検索機能を提供するソフトウェア、システムの総称で、ウェブサーバに組み込んで利用されることが多い。スタンドアローン環境で用いられる個人用途のものもあり、そういったものは特に「デスクトップ検索」と呼ばれている。企業内のファイルサーバーや企業内ポータルを対象とするものは「エンタープライズサーチ」と呼ばれる。 検索エンジンのはしりは1994年にスタンフォード大学のジェリー・ヤンとデビッド・ファイロが開発したYahoo!である[7]。Yahoo!はディレクトリ型の検索エンジンでインターネットの普及に大きな役割を果たした[7]。 その後、ウェブ上の情報を自動的に探索して情報を索引として整理するロボットまたはクローラと呼ばれるプログラムが開発された[7]。 ロボット型検索エンジンの中でもラリー・ペイジとセルゲイ・ブリンが開発したGoogle検索は検索結果のランキングと高速検索に優れていたため検索エンジンのトップに躍り出た[7]。Googleが1998年に稼動させたGoogle検索は、従来の検索エンジンがポータルサイト化へと進む流れに逆行し、独創的な検索技術に特化し、バナー広告等を排除したシンプルな画面だった。 Googleは2000年には米Yahoo!のロボット型検索エンジンに採用されたが、Google躍進に危機感を募らせた米Yahoo!は、2004年にロボット型検索エンジンを独自技術Yahoo! Search Technology (YST)（Yahoo!が買収したInktomiと、Overtureが買収したAltaVista、Alltheweb等の技術を統合した）に切り替えた。 2009年にはマイクロソフトが新たな検索エンジンとしてBingを発表した[7]。 検索という行為が一般化するにつれて、各種目的別に多様化した検索エンジンが現れるようになった。ブログの情報に特化した検索TechnoratiやblogWatcher、商品情報の検索に特化した商品検索サイト、サイトの見た目で検索するMARSFLAG、音楽検索、動画検索、ファイル検索、アップローダ検索ほか、次々と新しい検索エンジンが生まれている。 日本のインターネット普及初期から存在した検索エンジンには以下のようなものがある。黎明期には、豊橋技術科学大学の学生が作成したYahho[8] や、東京大学の学生が作成したODiN、早稲田大学の学生が作成した千里眼など、個人の学生が作成したものが商用に対して先行していた（いずれも1995年に作成、日本電信電話株式会社（現・NTT株式会社）のNTT DIRCECTORY[9]、サイバースペースジャパン（現・ウェブインパクト）のCSJインデックスは1994年に作成）[10]。これらは単に実験用に公開されていただけでなく、多くの人に用いられていたものであり、黎明期のユーザにとっては知名度、実用度ともに高いものであった。またMondouなどのように研究室（京都大学）で作成したものもあった。 1995年12月にソフトバンクがアメリカ合衆国Yahoo!株を一部買い取り、翌年4月から日本版にローカライズしたYahoo! JAPANをサービス開始した。同年7月の展示会Interopでは机2つぶん並べる程度の小規模ブースで出展する程度の力の入れ具合で、ソフトバンクの一部署として開始する程度だったものが、もともとの米国Yahoo!の知名度、90年代後半のインターネット利用者人口の増加、ディレクトリ型だけだった検索をロボット型も追加、サイト登録した一部のウェブサイトの紹介をするYahoo! Internet Guide（ソフトバンククリエイティブ出版）との連携、日本Yahoo!株高騰のニュースでインターネットを利用しない人にも名前が知れ渡るなど、様々なプラス要因と経営戦略が見事に当たり、検索サイト首位の座を固めた。そして、検索サイトの集客力を武器にニュース、オークションなど、検索サービス以外のサービスを含めたポータルサイトとしての独走を始めた。 1997年頃から、WWWの爆発的な拡大に伴って、ディレクトリ型のみであったYahoo!のウェブディレクトリの陳腐化が急速に進んだ。2000年代には、日本でもGoogleに代表されるロボット型検索エンジンが人気を集め始め、国産ではinfoseekやgooが登場（Yahoo! JAPANがロボット型検索エンジンにgooを採用）、2004年にはGoogleやYahoo!のエンジンに匹敵すると謳うTeomaを利用した検索エンジン、Ask Jeeves（現・Ask.com）が「Ask.jp」として日本上陸、2005年にはオーストラリアで誕生したMooterが日本上陸など、群雄割拠の時代になった。検索エンジンを利用すること＝「ググる」というネットスラングも生まれた。 また、検索エンジンでは判断できない抽象的な条件などでの検索を人手に求めた、OKWaveや人力検索はてななどの「人力検索」「ナレッジコミュニティ」と呼ばれるサービスも登場した。 モバイル検索の分野は長らく公式サイトと呼ばれる世界がユーザーの囲い込みを行っていたため、脚光を浴びることが少なかった。次第にパソコンだけでなくフィーチャーフォンや携帯型ゲーム機からもウェブサイトが検索される傾向が高くなり、GoogleやYahoo!をはじめとする携帯向けのモバイル検索サイトが登場した。ソフトバンク・Yahoo! JAPANがボーダフォンを買収し、KDDIがGoogleと提携するなどした。 2010年、Yahoo! JAPANがGoogleの検索エンジンを採用し、日本でも事実上Googleが圧倒的なシェアを保有するに至った[11]。 Googleなどのウェブ検索エンジンでは、データベースの検索結果など多くの動的ページが検索対象になっていない。このような動的ページは「深層ウェブ」「見えないウェブ」「隠されたウェブ」などと呼ばれている。静的ページの500倍の量が存在し、多くは無料だといわれる。深層ウェブは、一般の検索エンジンなどからデータベースなどを見つけ出すか、直接アクセスした上で、それぞれの検索機能から再度検索しなければならない。また、ダークウェブを探索する際に使われる検索エンジンAhmiaも存在している。 ロボット型検索エンジンは、その原理上インターネット上のコンテンツを複製の上で、検索を目的とした蓄積に適した形態で保存する他、場合によってはキャッシュとして提供できるような形態でも保存する場合がある。著作権をたてに、ウェブサイトの閲覧利用規約等と称して、一切のいかなる複製も禁ずるとするサイト等があり、どういったものかと古くより話題になっていた[12]。 また、2006年11月には、日本の知的財産戦略本部コンテンツ専門調査会第3回企画WGにおいて、検索エンジンに関して「著作権法上、複製、編集には権利者の許諾が必要であり、Yahoo!、Googleなど大手検索システムのサーバーは海外に置かれているのが現状。」[13] と報告され、これをうけて経済産業省が日本国内でも合法的に検索エンジンサービスが行えるように著作権法の改正や検索エンジンの開発に取り組むと発表し[要出典]、2010年1月の改正で複製が合法とされた。 2006年頃から日本ではURL（アドレス）を表示せず、社名や商品名などの検索キーワードを表示し、検索エンジンで検索させるように仕向けるテレビコマーシャルなどの広告表現が急増している。大抵はキーワードが書かれた状態の検索フォームとボタンを表示し、マウスクリックを促す演出がなされている。このような変化が生じた理由は不明であるが、各メディアの広告掲載基準の変更や、コマーシャルでURLを表示するのに比べてアクセス数を獲得しやすいことが増加の要因である。しかし検索結果に企業にとって不都合な情報が現れる場合があるため、グーグル八分のような検索結果の操作が行われるケースも考えられる。 現在、主流となっている広告手法として、ユーザーの検索結果後に広告を露出させる検索連動型広告と、サイトの中を分析し、そのサイトに合った広告を配信するコンテンツ連動型広告が主流である。 英語圏でも2013年ごろから「#wikipedia」のような番号記号を使った広告活動をおこなっている。 いわゆる「使用言語からみたインターネット人口の割合」は Internet Archive を用いて Euro Marketing と Global Reach から過去の月次資料を整理すると次のような推移を辿っている。 ※2005年2月2日の時点で、WWW検索エンジンの代表格であるGoogleでは80億を越す8,058,044,651ウェブページが登録されていた。 1995年以前のInternet Societyによればインターネットで用いられている言語のうち英語が占める割合は85%とされていたが、その後のITの進歩や各国のインターネットの普及により多言語化が進み、上表に見られるように2000年の年末には英語と非英語の言語人口が逆転し、その傾向は継続していった。このため検索エンジン各社は多言語対応に苦慮することとなった。 検索エンジンは、利便性がある一方、危険性も存在する事やその被害例について参考文献や資料が存在する。検索エンジンの安全性に関する調査報告については、ウイルス対策ソフトなどを提供するセキュリティベンダーの米マカフィーが、2007年6月4日「検索エンジンの安全性に関する調査報告」を発表し「検索エンジンは危険であり、検索エンジンにキーワードを入力して上位に現れるサイトの危険度を調べたら、広告として表示されるサイトは、そうでないサイトの2.4倍も危険率が高い」としている[14][15]。 また2006年05月12日に公表された調査報告書によると検索エンジンのキーワード検索結果には危険なリンクがあり、検索エンジンが自分を守ってくれると思ってはいけない。それどころか検索結果ランキングがサイトの安全性を反映していないことも多く、特に検索エンジン広告を訪れる場合、ユーザーは高いリスクにさらされると警鐘を鳴らしている[16]。\nさらに、検索エンジンの提供サイトの危険度についての調査報告では、同マカフィーが「検索エンジンの安全度調査」を発表し「最も危険な結果が多いのは米ヤフー」としている[17]。 SEO対策の技術が進んだこともあり、検索上位が大手サイトやまとめサイトばかりになるなど、表示されるサイトに偏りが生じている[18][19][20]。昔のインターネットのほうが精度が高かったという感想も多い[21]。 ウェブサイトの人気と関連性のいくつかの組み合わせにもとづいてそれらを番付するよう検索エンジンはプログラムされているけれども、それらが与える情報のなかのさまざまな政治的、経済的、社会的な、バイアスを'経験的な'（英: empirical）研究はしめす[22]。 など。"
  },
  {
    "url": "https://no.wikipedia.org/wiki/Webs%C3%B8kemotor",
    "title": "Websøkemotor – Wikipedia",
    "content": "En websøkemotor er et programvaresystem som er utformet for å søke etter informasjon på World Wide Web. Søkeresultatene presenteres generelt på sider som ofte refereres til som search engine results pages (SERPs). Informasjonen kan være en miks av websider, bilder og andre typer filer. Noen søkemotorer minerer også data tilgjengelig i databaser eller åpne kataloger. Til forskjell fra webkataloger, som vedlikeholdes bare av menneskelige redaktører, opprettholder søkemotorer også informasjon i sanntid ved å kjøre en algoritme på en søkerobot."
  },
  {
    "url": "https://mhr.wikipedia.org/wiki/%D0%9A%D1%8B%D1%87%D0%B0%D0%BB%D1%88%D0%B5_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B5",
    "title": "Кычалше системе — Википедий",
    "content": "Кычалше системе (рушла поиско́вая систе́ма) — посна вотсайт, тушто пайдаланыше шке йодмашыжлан келшыше сайтушыкым/сайткылверым муын кертеш."
  },
  {
    "url": "https://uz.wikipedia.org/wiki/Veb_qidiruv_tizimi",
    "title": "Veb qidiruv tizimi - Vikipediya",
    "content": "Veb qidiruv tizimi - foydalanuvchi soʻroviga javoban veb-sahifalarga giperhavolalar va Internetdagi boshqa tegishli maʼlumotlarni taqdim qiluvchi dasturiy tizimdir. Foydalanuvchi veb-brauzer yoki mobil ilova ichida soʻrovni kiritadi va qidiruv natijalari koʻpincha matnli xulosalar va rasmlar bilan birga giperhavolalar roʻyxati boʻladi. Foydalanuvchilar, qidiruvni rasmlar, videolar yoki yangiliklar kabi maʼlum turdagi natijalar bilan cheklash imkoniyatiga ega. Qidiruv provayderi uchun uning mexanizmi butun dunyo boʻylab koʻplab maʼlumotlar markazlarini qamrab oladigan taqsimlangan hisoblash tizimining bir qismidir. Mexanizmning soʻrovga javob tezligi va aniqligi avtomatlashtirilgan veb-brauzerlar tomonidan doimiy ravishda yangilanadigan murakkab indekslash tizimiga asoslanadi. Bunga veb-serverlarda saqlangan fayllar va maʼlumotlar bazalari maʼlumotlarini ishlab chiqarish kiradi. 1990-yillarda Internet paydo boʻlganidan beri koʻplab qidiruv tizimlari ishlab chiqildi, lekin Google Search 2000-yillarda dominant boʻldi va shunday boʻlib qoldi. Hozirda u 91% global bozor ulushiga ega[1][2]. Baʼzi qidiruv tizimlari yangiliklar guruhlari, maʼlumotlar bazalari yoki ochiq kataloglarda mavjud boʻlgan maʼlumotlarni ham ishlab chiqaradi. Inson muharrirlari tomonidan yuritiladigan veb-kataloglardan farqli oʻlaroq, qidiruv tizimlari algoritmik ishlaydi yoki algoritmik va inson maʼlumotlarining aralashmasi sifatida ishlaydi. 1945-yilda Vannevar Bush foydalanuvchiga bir stolda katta hajmdagi maʼlumotlarga kirish imkonini beradigan axborot qidirish tizimini tasvirlab berdi[3] va buni memeks deb nomladi. Bush The Atlantic Monthly jurnalida chop etilgan „Biz oʻylagandek“ sarlavhali maqolasida tizimni tasvirlab bergan[4]. Memeks foydalanuvchiga ilmiy ishlarning doimiy oʻsib borayotgan markazlashtirilgan indekslarida maʼlumotni topishning tobora ortib borayotgan qiyinchiliklarini yengish qobiliyatini berish uchun moʻljallangan edi. Vannevar Bush zamonaviy giperhavolalarga oʻxshash bogʻlangan izohli tadqiqot kutubxonalarini nazarda tutgan[5]. Havola tahlili oxir-oqibat Hyper Search va PageRank kabi algoritmlar orqali qidiruv tizimlarining muhim tarkibiy qismiga aylandi[6][7]. Birinchi internet qidiruv tizimlari 1990-yil dekabrida Internet debyutidan oldin paydo boʻlgan: WHOIS foydalanuvchi qidiruvi 1982-yilga borib taqaladi[8] va Knowbot Information Service koʻp tarmoqli foydalanuvchi qidiruvi birinchi marta 1989-yilda amalga oshirilgan[9]. Kontent fayllarini, yaʼni FTP fayllarini qidiradigan birinchi yaxshi hujjatlashtirilgan qidiruv tizimi Archie boʻlib, u 1990-yil 10-sentyabrda debyut qilgan[10]."
  },
  {
    "url": "https://pms.wikipedia.org/wiki/Motor_d%27arserca_web",
    "title": "Motor d'arserca web - Wikipedia an piemontèis, l'enciclopedìa lìbera e a gràtis",
    "content": "Un motor d'arserca web (an anglèis web search engine) a l'é un sistema software progetà për serché e trové d'anformassion an sla ragnà. A analisa miliard ëd document, figure, video, e àutri contnù, organisandje an n'ìndes përmëttend a j'utent ëd trové lòn ch'a l'han da manca an anserend ëd paròle-ciav. Esempi famos a son Google, Bing, DuckDuckGo, e Yahoo! Search. 1. Crawler (ragnà): Scansion-a la Web për trové contnù neuv o modificà.  \n2. Ìndes: Archivi ch'a mapa paròle-ciav a document rilevant.  \n3. Algoritm ëd ranking: Valuta la rilevansa dij contnù (es. Google PageRank).  \n4. Antërfacia utent: Pàgina d'arserca andova j'utent a inserisso ëd query. Tecnologie avansà:"
  },
  {
    "url": "https://pl.wikipedia.org/wiki/Wyszukiwarka_internetowa",
    "title": "Wyszukiwarka internetowa – Wikipedia, wolna encyklopedia",
    "content": "Wyszukiwarka internetowa – program komputerowy lub strona internetowa odnajdująca w internecie informacje według podanych przez użytkownika słów kluczowych lub wyrażeń sformułowanych w języku naturalnym[1]. Umożliwia użytkownikom wyszukiwanie – co do zasady – wszystkich stron internetowych lub stron internetowych w danym języku za pomocą zapytania na jakikolwiek temat przez podanie słowa kluczowego, wyrażenia lub innej wartości wejściowej. W wyniku przedstawia odnośniki, pod którymi można znaleźć informacje związane z zadanym zapytaniem[2]. Określenie „wyszukiwarka” stosowane jest w odniesieniu do: Wyszukiwarki gromadzą w sposób automatyczny informacje o dokumentach tekstowych oraz plikach zgromadzonych w sieci (z obszaru wyznaczonego do indeksowania). Ponieważ Internet rośnie znacznie szybciej niż jakakolwiek grupa ludzi może go katalogować oraz z powodu wad katalogów (np. pod danym hasłem może znajdować się tysiące stron), powstały wyszukiwarki, które przeszukują Internet, analizując zawartość stron.\nKiedy użytkownik poda wyszukiwarce zapytanie, ona odpowie mu łączami do stron, które uzna, w zależności od użytego algorytmu, za najbardziej odpowiednie. Wyszukiwarki oparte na tej zasadzie mogą objąć znacznie większą część sieci niż katalogi. Niestety są one bardzo podatne na nadużycia, przez co użytkownik zamiast użytecznych informacji dostaje linki na strony niemające nic wspólnego z jego zapytaniem. Szczególnie wyspecjalizowały się w tym strony pornograficzne. Żeby przeciwdziałać temu, stosuje się wyszukiwarki, w których na szczycie list pojawiają się strony, do których odnosi się najwięcej stron dotyczących danego zapytania. Tak więc stronę uważa się za odpowiadającą zapytaniu „britney spears”, jeśli wiele stron na temat „britney spears” do niej linkuje. Strona porno z nagimi zdjęciami Britney, niezależnie od własnej treści i niezależnie od całkowitej liczby linków (głównie z innych stron porno) na nią, nie będzie w ten sposób uznana za związaną z tematem. Natomiast jeśli zada się zapytanie „britney spears nude”, strona ta zostanie uznana za istotną, ponieważ linkuje na nią wiele stron o tematyce „nude”. Początkowa istotność na podstawie prostej heurystyki, po czym zwykle używa się algorytmu losowego skakania po linkach. Pierwszą wyszukiwarką, która zastosowała zaawansowane algorytmy analizy topologii sieci był Google. Wyszukiwarki oparte na analizie topologicznej są często uważane za bardzo odporne na nadużycia. W rzeczywistości stosunkowo częstym atakiem są spam-systemy automatycznej wymiany linków. Inną formą ataku jest stworzenie dużej ilości gęsto linkowanych stron, z czego wszystkie na ten sam temat. Jest to jednak zadanie trudne i wymagające dużego nakładu pracy, a modyfikując heurystykę wartości początkowych, można znacznie ograniczyć ten proceder, którego skala na razie jest minimalna. Osobnym pomysłem jest wprowadzony przez Overture system, gdzie strony płacą wyszukiwarce kilka centów za każde kliknięcie, przy czym miejsca są licytowane – strona, która daje więcej za kliknięcie znajdzie się wyżej na liście rezultatów. Pozycje płatne są oznaczone jako takie, razem z ceną. System ten jest korzystny dla właścicieli stron – płacą oni tylko za wejścia nie za wyświetlenia. Twórcy twierdzą, że jest on również korzystny dla użytkownika, gdyż tylko strony, które oferują coś użytecznego z danej dziedziny mogą sobie pozwolić na taką reklamę. Z drugiej jednak strony wiele użytecznych stron jest niekomercyjnych, a nawet przy stronach komercyjnych wyniki będą często nieoptymalne – np. na taką reklamę nie mogą sobie pozwolić strony, które mają niskie marże i oferują produkty po niskich cenach, a jedynie te, które mają wysokie marże i oferują produkty drożej. Wyszukiwarki stanowią wymarzony cel reklamodawców, ponieważ mają oni praktycznie pełną gwarancję, że osoba wyszukująca dane hasło jest nim zainteresowana. Tak więc większość wyszukiwarek oferuje reklamy zależne od treści zapytań (np. Google Ads oferowany przez Google). Nie zawsze są one właściwie oddzielone od wyników poszukiwań, co stało się źródłem protestów grup ochrony praw konsumentów oraz kilku do dziś nierozstrzygniętych spraw sądowych. Ze względu na szeroką krytykę procederu nieoddzielania reklam od wyników, większość wyszukiwarek z niego zrezygnowała i wyraźnie zaznacza teraz reklamy. Oprogramowanie wyszukiwarek to zestaw programów, modułów, z których każdy ma oddzielne zadanie.\nW skład zestawu wchodzą takie elementy jak: Oraz dochodzą do tego: Współczesne oprogramowanie wyszukiwarek jest wysoce skomplikowanym systemem rozproszonym uruchamianym zwykle w wielu oddzielnych etapach na tysiącach oddzielnych komputerów – zarówno ze względu na rozmiar i skalę przeszukiwanej sieci, jak i ze względów na poprawienie dostępności usługi w wypadku awarii poszczególnych komponentów. Algorytmy oceny istotności (tzw. relewancji) dokumentu względem szukanej frazy – algorytmy oceny zawartości strony"
  },
  {
    "url": "https://qu.wikipedia.org/wiki/Mask%27ay_kuyuchina",
    "title": "Mask'ay kuyuchina - Wikipidiya",
    "content": "Mask'ay kuyuchiq icha Mask'a purichiqqa huk llamp'ukaq llikacham, web mask'aykunata ruwanapaq ch'antasqa. Paykunaqa World Wide Web kaqpi huk sistimatiku kaqpi mask'anku huk willakuy web mask'ana tapuypi willasqa partikular willayta. Mask'ana ruwasqakuna tukuypaq huk chiru ruwasqakunapi rikuchikunku, sapa kuti maskanapaq ruwasqa p'anqakuna (Inlish simipi: search engine results pages) nispa sutichasqa. Mayk'aq huk llamk'achiq huk tapuyta maskanaman yaykuchin, llamk'anaqa web p'anqakunap urquyninta qhawan, llamk'aqpa tapuyninwan tupaqkunata tarinanpaq. Chaymanta ruwasqakuna yachasqa kaqmanhina rankikunku chaymanta ruwaqman rikuchikunku. Willakuyqa web p'anqakunaman, siq'ikunaman, widiyukunaman, yachamanta qillqa, qillqakunaman, mask'ana qillqakunaman, waq laya willañiqikunaman t'inkikuna chaqrusqa kanman. Wakin mask'anakunapis willakuypa waqaychanankunapi utaq kichasqa directoriokunapi tarikuq willakuykunatam hurqunku. Mana web kamachikuna hinachu chaymanta rima markatur sitiyukuna hina, mayqinkunachus runa allichaqkunawan waqaychasqa kanku, maskanakuna chiqa pacha willayta waqaychankutaq huk allquritmu web watiqaq kaqpi purichispa. Ima intirnitpi ruwasqa willakuypis mana web mask'anawan urqusqa chaymanta mask'ay atikuq, ukhu web katikuriya kaqman urmaykun. Huk mask'ay kuyuchinaqa kay ruwaykunata yaqa chiqa pachapi waqaychan:"
  },
  {
    "url": "https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0",
    "title": "Поисковая система — Википедия",
    "content": "Поиско́вая систе́ма (жарг. поискови́к) или поиско́вый движо́к (англ. search engine) — алгоритмы и реализующая их совокупность компьютерных программ (в широком смысле этого понятия, включая аналоговые системы автоматизированной обработки информации первого поколения), предоставляющая пользователю возможность быстрого доступа к необходимой ему информации при помощи поиска в обширной коллекции доступных данных[1]. Одно из наиболее известных применений поисковых систем — веб-сервисы для поиска текстовой или графической информации во Всемирной паутине. Существуют также системы, способные искать файлы на FTP-серверах, товары в интернет-магазинах, информацию в группах новостей Usenet. Для поиска информации с помощью поисковой системы пользователь формулирует поисковый запрос[2]. Работа поисковой системы заключается в том, чтобы по запросу пользователя найти документы, содержащие либо указанные ключевые слова, либо слова, как-либо связанные с ключевыми словами[3]. При этом поисковая система генерирует страницу результатов поиска. Такая поисковая выдача может содержать различные типы результатов, например: веб-страницы, изображения, аудиофайлы. Некоторые поисковые системы также извлекают информацию из подходящих баз данных и каталогов ресурсов в Интернете.\nДля поиска нужных сведений удобнее всего воспользоваться современными поисковыми машинами, которые позволяют быстро обнаружить необходимые сведения и обеспечивают точность и полноту поиска. При работе с этими машинами достаточно задать ключевые слова, наиболее точно отражающие искомую информацию, или составить более сложный запрос из ключевых слов для уточнения области поиска. После ввода запроса на поиск вы получите список ссылок на документы в Интернете, обычно называемые web-страницами или просто страницами, в которых содержатся указанные ключевые слова. Обычно ссылки дополняются фрагментами текста из обнаруженного документа, которые часто помогают сразу определить тематику найденной страницы. Щёлкнув мышью на ссылке, можно перейти к выбранному документу. Поисковая система тем лучше, чем больше документов, релевантных запросу пользователя, она будет возвращать. Результаты поиска могут становиться менее релевантными из-за особенностей алгоритмов или вследствие человеческого фактора➤. По состоянию на 2020 год самой популярной поисковой системой в мире и, в частности, России является Google[источник не указан 1514 дней]. По методам поиска и обслуживания разделяют четыре типа поисковых систем: системы, использующие поисковых роботов, системы, управляемые человеком, гибридные системы и мета-системы➤. В архитектуру поисковой системы обычно входят: На раннем этапе развития сети Интернет Тим Бернерс-Ли поддерживал список веб-серверов, размещённый на сайте ЦЕРН[4]. Сайтов становилось всё больше, и поддерживать вручную такой список становилось всё сложнее. На сайте NCSA был специальный раздел «Что нового!» (англ. What's New!)[5], где публиковали ссылки на новые сайты. Первой компьютерной программой для поиска в Интернете была программа Арчи[англ.] (англ. archie — архив без буквы «в»). Она была создана в 1990 году Аланом Эмтэджем (Alan Emtage), Биллом Хиланом (Bill Heelan) и Дж. Питером Дойчем (J. Peter Deutsch), студентами, изучающими информатику в университете Макгилла в Монреале. Программа скачивала списки всех файлов со всех доступных анонимных FTP-серверов и строила базу данных, в которой можно было выполнять поиск по именам файлов. Однако, программа Арчи не индексировала содержимое этих файлов, так как объём данных был настолько мал, что всё можно было легко найти вручную. Развитие и распространение сетевого протокола Gopher, придуманного в 1991 году Марком Маккэхилом (Mark McCahill) в университете Миннесоты, привело к созданию двух новых поисковых программ, Veronica[англ.] и Jughead[англ.]. Как и Арчи, они искали имена файлов и заголовки, сохранённые в индексных системах Gopher. Veronica (англ. Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) позволяла выполнять поиск по ключевым словам большинства заголовков меню Gopher во всех списках Gopher. Программа Jughead (англ. Jonzy's Universal Gopher Hierarchy Excavation And Display) извлекала информацию о меню от определённых Gopher-серверов. Хотя название поисковика Арчи не имело отношения к циклу комиксов «Арчи»[англ.], тем не менее Veronica и Jughead — персонажи этих комиксов. К лету 1993 года ещё не было ни одной системы для поиска в вебе, хотя вручную поддерживались многочисленные специализированные каталоги. Оскар Нирштрасс (Oscar Nierstrasz) в Женевском университете написал ряд сценариев на Perl, которые периодически копировали эти страницы и переписывали их в стандартный формат. Это стало основой для W3Catalog, первой примитивной поисковой системы сети, запущенной 2 сентября 1993 года[6]. Вероятно, первым поисковым роботом, написанным на языке Perl, был «World Wide Web Wanderer» — бот Мэтью Грэя (Matthew Gray) из Массачусетского технологического института в июне 1993 года. Этот робот создавал поисковый индекс «Wandex». Цель робота Wanderer состояла в том, чтобы измерить размер всемирной паутины и найти все веб-страницы, содержащие слова из запроса. В 1993 году появилась и вторая поисковая система «Aliweb». Aliweb не использовала поискового робота, но вместо этого ожидала уведомлений от администраторов веб-сайтов о наличии на их сайтах индексного файла в определённом формате. JumpStation[англ.], [7] созданный в декабре 1993 года Джонатаном Флетчером, искал веб-страницы и строил их индексы с помощью поискового робота, и использовал веб-форму в качестве интерфейса для формулирования поисковых запросов. Это был первый инструмент поиска в Интернете, который сочетал три важнейших функции поисковой системы (проверка, индексация и собственно поиск). Из-за ограниченности ресурсов компьютеров того времени индексация и, следовательно, поиск были ограничены только названиями и заголовками веб-страниц, найденных поисковым роботом. Первой полнотекстовой индексирующей ресурсы при помощи робота («craweler-based») поисковой системой, стала система «WebCrawler»[англ.], запущенная в 1994 году. В отличие от своих предшественниц, она позволяла пользователям искать по любым словам, расположенным на любой веб-странице — с тех пор это стало стандартом для большинства поисковых систем. Кроме того, это был первый поисковик, получивший широкое распространение. В 1994 году была запущена система «Lycos», разработанная в Университете Карнеги — Меллон и ставшая серьёзным коммерческим предприятием. Вскоре появилось множество других конкурирующих поисковых машин, таких как: «Magellan»[англ.], «Excite», «Infoseek»[англ.], «Inktomi»[англ.], «Northern Light»[англ.] и «AltaVista». В некотором смысле они конкурировали с популярными интернет-каталогами, такими как «Yahoo!». Но поисковые возможности каталогов ограничивались поиском по самим каталогам, а не по текстам веб-страниц. Позже каталоги объединялись или снабжались поисковыми роботами с целью улучшения поиска. В 1996 году компания Netscape хотела заключить эксклюзивную сделку с одной из поисковых систем, сделав её поисковой системой по умолчанию на веб-браузере Netscape. Это вызвало настолько большой интерес, что Netscape заключила контракт сразу с пятью крупнейшими поисковыми системами (Yahoo!, Magellan, Lycos, Infoseek и Excite). За 5 млн долларов США в год они предлагались по очереди на поисковой странице Netscape[8][9]. Поисковые системы участвовали в «Пузыре доткомов» конца 1990-х[10]. Несколько компаний эффектно вышли на рынок, получив рекордную прибыль во время их первичного публичного предложения. Некоторые отказались от рынка общедоступных поисковых движков и стали работать только с корпоративным сектором, например, Northern Light[англ.]. Google взял на вооружение идею продажи ключевых слов в 1998 году, тогда это была маленькая компания, обеспечивавшая работу поисковой системы по адресу goto.com[англ.]. Этот шаг ознаменовал для поисковых систем переход от соревнований друг с другом к одному из самых выгодных коммерческих предприятий в Интернете[11]. Поисковые системы стали продавать первые места в результатах поиска отдельным компаниям. Поисковая система Google занимает видное положение с начала 2000-х[12]. Компания добилась высокого положения благодаря хорошим результатам поиска с помощью алгоритма PageRank. Алгоритм был представлен общественности в статье «The Anatomy of Search Engine», написанной Сергеем Брином и Ларри Пейджем, основателями Google[13]. Этот итеративный алгоритм ранжирует веб-страницы, основываясь на оценке количества гиперссылок на веб-страницу в предположении, что на «хорошие» и «важные» страницы ссылаются больше, чем на другие. Интерфейс Google выдержан в спартанском стиле, где нет ничего лишнего, в отличие от многих своих конкурентов, которые встраивали поисковую систему в веб-портал. Поисковая система Google стала настолько популярной, что появились подражающие ей системы, например, Mystery Seeker[англ.](тайный поисковик). К 2000 году Yahoo! осуществлял поиск на основе системы Inktomi. Yahoo! в 2002 году купил Inktomi, а в 2003 году купил Overture, которому принадлежали AlltheWeb[англ.] и AltaVista. Затем Yahoo! работал на основе поисковой системы Google вплоть до 2004 года, пока не запустил, наконец, свой собственный поисковик на основе всех купленных ранее технологий. Фирма Microsoft впервые запустила поисковую систему Microsoft Network Search (MSN Search) осенью 1998 года, используя результаты поиска от Inktomi. Совсем скоро в начале 1999 года сайт начал отображать выдачу Looksmart[англ.], смешанную с результатами Inktomi. Недолго (в 1999 году) MSN search использовал результаты поиска от AltaVista. В 2004 году фирма Microsoft начала переход к собственной поисковой технологии с использованием собственного поискового робота — msnbot[англ.]. После проведения ребрендинга компанией Microsoft 1 июня 2009 года была запущена поисковая система Bing. 29 июля 2009 Yahoo! и Microsoft подписали соглашение, согласно которому Yahoo! Search работал на основе технологии Microsoft Bing. На момент 2015 года союз Bing и Yahoo! дал первые настоящие плоды. Теперь Bing занимает 20,1 % рынка, а Yahoo! 12,7 %, что в общем занимает 32,60 % от общего рынка поисковых систем в США по данным из разных источников. В 1996 году был реализован поиск с учётом русской морфологии на поисковой машине Altavista и запущены оригинальные российские поисковые машины Рамблер и Апорт. 23 сентября 1997 года была открыта поисковая машина Яндекс. В 2014 году компания Ростелеком открыла национальную поисковую машину Спутник, которая была закрыта в 2017 году, поскольку весь проект был признан неудачным, существовавшим только благодаря государственной поддержке. Большую популярность получили методы кластерного анализа и поиска по метаданным. Из международных машин такого плана наибольшую известность получила «Clusty»[англ.] компании Vivisimo[англ.]. В 2005 году в России при поддержке МГУ запущен поисковик «Нигма», поддерживающий автоматическую кластеризацию. В 2006 году открылась российская метамашина Quintura, предлагающая визуальную кластеризацию в виде облака тегов. «Нигма» тоже экспериментировала[14] с визуальной кластеризацией. Основные составляющие поисковой системы: поисковый робот, индексатор, поисковик[15]. Как правило, системы работают поэтапно. Сначала поисковый робот получает контент, затем индексатор генерирует доступный для поиска индекс, и наконец, поисковик обеспечивает функциональность для поиска индексируемых данных. Чтобы обновить поисковую систему, этот цикл индексации выполняется повторно[15]. Поисковые системы работают, храня информацию о многих веб-страницах, которые они получают из HTML-страниц. Поисковый робот (англ. Crawler) — программа, которая автоматически проходит по всем ссылкам, найденным на странице, и выделяет их. Поисковый робот, основываясь на ссылках или исходя из заранее заданного списка адресов, осуществляет поиск новых документов, ещё не известных поисковой системе. Владелец сайта может исключить определённые страницы при помощи robots.txt, используя который можно запретить индексацию файлов, страниц или каталогов сайта. Поисковая система анализирует содержание каждой страницы для дальнейшего индексирования. Слова могут быть извлечены из заголовков, текста страницы или специальных полей — метатегов. Индексатор — это модуль, который анализирует страницу, предварительно разбив её на части, применяя собственные лексические и морфологические алгоритмы. Все элементы веб-страницы вычленяются и анализируются отдельно. Данные о веб-страницах хранятся в индексной базе данных для использования в последующих запросах. Индекс позволяет быстро находить информацию по запросу пользователя[16]. Ряд поисковых систем, подобных Google, хранят исходную страницу целиком или её часть, так называемый кэш, а также различную информацию о веб-странице. Другие системы, подобные системе AltaVista, хранят каждое слово каждой найденной страницы. Использование кэша помогает ускорить извлечение информации с уже посещённых страниц[16]. Кэшированные страницы всегда содержат тот текст, который пользователь задал в поисковом запросе. Это может быть полезно в том случае, когда веб-страница обновилась, то есть уже не содержит текст запроса пользователя, а страница в кэше ещё старая[16]. Эта ситуация связана с потерей ссылок (англ. linkrot) и дружественным по отношению к пользователю (юзабилити) подходом Google. Это предполагает выдачу из кэша коротких фрагментов текста, содержащих текст запроса. Действует принцип наименьшего удивления, пользователь обычно ожидает увидеть искомые слова в текстах полученных страниц (User expectations[англ.]). Кроме того, что использование кэшированных страниц ускоряет поиск, страницы в кэше могут содержать такую информацию, которая уже нигде более не доступна. Поисковик работает с выходными файлами, полученными от индексатора. Поисковик принимает пользовательские запросы, обрабатывает их при помощи индекса и возвращает результаты поиска[15]. Когда пользователь вводит запрос в поисковую систему (обычно при помощи ключевых слов), система проверяет свой индекс и выдаёт список наиболее подходящих веб-страниц (отсортированный по какому-либо критерию), обычно с краткой аннотацией, содержащей заголовок документа и иногда части текста[16]. Поисковый индекс строится по специальной методике на основе информации, извлечённой из веб-страниц[12]. С 2007 года поисковик Google позволяет искать с учётом времени создания искомых документов (вызов меню «Инструменты поиска» и указание временного диапазона). Большинство поисковых систем поддерживает использование в запросах булевых операторов И, ИЛИ, НЕ, что позволяет уточнить или расширить список искомых ключевых слов. При этом система будет искать слова или фразы точно так, как было введено. В некоторых поисковых системах есть возможность приближённого поиска[англ.], в этом случае пользователи расширяют область поиска, указывая расстояние до ключевых слов[16]. Есть также концептуальный поиск[англ.], при котором используется статистический анализ употребления искомых слов и фраз в текстах веб-страниц. Эти системы позволяют составлять запросы на естественном языке. Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть миллионы веб-страниц и могут включать некое слово или фразу, но одни из них могут быть более релевантны, популярны или авторитетны, чем другие. Большинство поисковых систем использует методы ранжирования, чтобы вывести в начало списка «лучшие» результаты. Поисковые системы решают, какие страницы более релевантны, и в каком порядке должны быть показаны результаты, по-разному[16]. Методы поиска, как и сам Интернет со временем меняются. Так появились два основных типа поисковых систем: системы предопределённых и иерархически упорядоченных ключевых слов и системы, в которых генерируется инвертированный индекс на основе анализа текста. Большинство поисковых систем являются коммерческими предприятиями, которые получают прибыль за счёт рекламы, в некоторых поисковиках можно купить за отдельную плату первые места в выдаче для заданных ключевых слов. Те поисковые системы, которые не берут денег за порядок выдачи результатов, зарабатывают на контекстной рекламе, при этом рекламные сообщения соответствуют запросу пользователя. Такая реклама выводится на странице со списком результатов поиска, и поисковики зарабатывают при каждом клике пользователя на рекламные сообщения. Существует четыре типа поисковых систем: с поисковыми роботами, управляемые человеком, гибридные и мета-системы[17]. Google — самая популярная поисковая система в мире с долей на рынке 92,16 %. Bing занимает вторую позицию, его доля 2,88 %[18]. Самые популярные поисковые системы в мире[19]: В восточноазиатских странах и в России Google — не самая популярная поисковая система. В Китае, например, более популярна поисковая система Soso. В Южной Корее поисковым порталом собственной разработки Naver пользуется около 70 % жителей[22] Yahoo! Japan и Yahoo! Taiwan — самые популярные системы для поиска в Японии и Тайване соответственно[23]. Поисковой системой Google пользуются 50,3 % пользователей в России, Яндексом — 47,9 %[24]. Согласно данным LiveInternet в декабре 2017 года об охвате русскоязычных поисковых запросов[25]: Некоторые из поисковых систем используют внешние алгоритмы поиска. Число пользователей Интернета и поисковых систем и требований пользователей к этим системам постоянно растёт. Для увеличений скорости поиска нужной информации крупные поисковые системы содержат большое количество серверов. Сервера обычно группируют в серверные центры (дата-центры). У популярных поисковых систем серверные центры разбросаны по всему миру. В октябре 2012 года Google запустила проект «Где живёт Интернет», где пользователям предоставляется возможность познакомиться с центрами обработки данных этой компании[26]. О работе дата-центров поисковой системе Google известно следующее[27]: Размер всемирной паутины, проиндексированной Google на декабрь 2014 года, составляет примерно 4,36 миллиарда страниц[28]. Глобальное распространение Интернета и увеличение популярности электронных устройств в арабском и мусульманском мире, в частности, в странах Ближнего Востока и Индийского субконтинента, способствовало развитию локальных поисковых систем, учитывающих исламские традиции. Такие поисковые системы содержат специальные фильтры, которые помогают пользователям не попадать на запрещённые сайты, например, сайты с порнографией, и позволяют им пользоваться только теми сайтами, содержимое которых не противоречит исламской вере. Незадолго до мусульманского месяца Рамадан, в июле 2013 года, миру был представлен Halalgoogling[англ.] — система, выдающая пользователям только халяльные «правильные» ссылки[29], фильтруя результаты поиска, полученные от других поисковых систем, таких как Google и Bing. Двумя годами ранее, в сентябре 2011 года, был запущен поисковый движок I’mHalal, предназначенный для обслуживания пользователей Ближнего Востока. Однако этот поисковый сервис пришлось вскоре закрыть, по сообщению владельца, из-за отсутствия финансирования[30]. Отсутствие инвестиций и медленный темп распространения технологий в мусульманском мире препятствовали прогрессу и мешали успеху серьёзного исламского поисковика. Очевиден провал огромных инвестиций в веб-проекты мусульманского образа жизни, одним из которых был Muxlim[англ.]. Он получил миллионы долларов от инвесторов, таких как Rite Internet Ventures, и теперь — в соответствии с последним сообщением от I’mHalal перед его закрытием — выступает с сомнительной идеей о том, что «следующий Facebook или Google могут появиться только в странах Ближнего Востока, если вы поддержите нашу блестящую молодёжь»[источник не указан 1716 дней]. Тем не менее исламские эксперты в области Интернета в течение многих лет занимаются определением того, что соответствует или не соответствует шариату, и классифицируют веб-сайты как «халяль» или «харам». Все бывшие и настоящие исламские поисковые системы представляют собой просто специальным образом проиндексированный набор данных либо это главные поисковые системы, такие как Google, Yahoo и Bing, с определённой системой фильтрации, использующейся для того, чтобы пользователи не могли получить доступ к харам-сайтам, таким как сайты о наготе, ЛГБТ, азартных играх и каким-либо другим, тематика которых считается антиисламской[источник не указан 1716 дней]. Среди других религиозно-ориентированных поисковых систем распространёнными являются Jewogle — еврейская версия Google и SeekFind.org — христианский сайт, включающий в себя фильтры, оберегающие пользователей от контента, который может подорвать или ослабить их веру[31]. Многие поисковые системы, такие как Google и Bing, используют алгоритмы выборочного угадывания того, какую информацию пользователь хотел бы увидеть, основываясь на его прошлых действиях в системе. В результате, веб-сайты показывают только ту информацию, которая согласуется с прошлыми интересами пользователя. Этот эффект получил название «пузырь фильтров»[32]. Всё это ведёт к тому, что пользователи получают намного меньше противоречащей своей точке зрения информации и становятся интеллектуально изолированными в своём собственном «информационном пузыре». Таким образом, «эффект пузыря» может иметь негативные последствия для формирования гражданского мнения[33]. В начале 2024 года Google внедрили технологии искусственного интеллекта в свою поисковую систему[34]. Через несколько месяцев, Яндекс запустили в своём сервисе \"Поиска\" функционал \"Нейро\"[35], который позволяет объединить в один ответ информацию из нескольких источников и основан на результатах работы Yandex GPT Несмотря на то, что поисковые системы запрограммированы, чтобы оценивать веб-сайты на основе некоторой комбинации их популярности и релевантности, в реальности экспериментальные исследования указывают на то, что различные политические, экономические и социальные факторы оказывают влияние на поисковую выдачу[36][37]. Такая предвзятость может быть прямым результатом экономических и коммерческих процессов: компании, которые рекламируются в поисковой системе, могут стать более популярными в результатах обычного поиска в ней. Удаление результатов поиска, не соответствующих местным законам, является примером влияния политических процессов. Например, Google не будет отображать некоторые неонацистские веб-сайты во Франции и Германии, где отрицание Холокоста незаконно[38]. Предвзятость может также быть следствием социальных процессов, поскольку алгоритмы поисковых систем часто разрабатываются, чтобы исключить неформатные точки зрения в пользу более «популярных» результатов[39]. Алгоритмы индексации главных поисковых систем отдают приоритет американским сайтам[37]. Поисковая бомба — один из примеров попытки управления результатами поиска по политическим, социальным или коммерческим причинам."
  },
  {
    "url": "https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%99%E0%B6%B6%E0%B7%8A_%E0%B7%83%E0%B7%99%E0%B7%80%E0%B7%94%E0%B6%B8%E0%B7%8A_%E0%B6%BA%E0%B6%B1%E0%B7%8A%E0%B6%AD%E0%B7%8A%E2%80%8D%E0%B6%BB",
    "title": "වෙබ් සෙවුම් යන්ත්‍ර - විකිපීඩියා",
    "content": "වෙබ් සෙවුම් යන්ත්‍රයක් යනු  විශ්ව විසිර වියමනෙහි තොරතුරු ගවේෂණය කිරීම සඳහා නිපදවන ලද මෘදුකාංග පද්ධතියකි. ගූගල්, යාහූ සහ බිං, එම්.එස්.එන්(msn), ආස්ක් ජීවස්(ask jeeves) ජනප්‍රිය වෙබ් සෙවුම් යන්ත්‍රවලට උදාහරණ වෙයි. සෙවුම් යන්ත්‍රයක් මගින් වෙබ් පිටුවලට අමතරව පිංතූර, වීඩියෝ සහ වෙනත් ගොනු වර්ග ද සොයාගැනීමේ හැකියාව ඇත."
  },
  {
    "url": "https://simple.wikipedia.org/wiki/Search_engine",
    "title": "Search engine - Simple English Wikipedia, the free encyclopedia",
    "content": "A search engine is a website that allows users to look up information on the World Wide Web (WWW), part of the Internet. The search engine will achieve this by looking at many web pages to find matches to the user's search inputs. It will return results ranked by relevancy and popularity by the search engine. The most popular search-engines are Google Search and Bing. Older services include Webcrawler are,  Yahoo! Search, Ask.com, Lycos, and Alta Vista.[1] Examples of specialized engines are Ecosia (supports ecological goals) or Tenor (picture engine). To use a search engine you must enter at least one keyword into the search box. Usually, an on-screen button must be clicked on to submit the search. The search engine looks for matches between the keyword(s) entered and its database of websites and words. After the user inputs their search or query into the search bar, a list of results will appear on the screen known as the search engine results page (SERP). This list of webpages contains matches related to the user's query in a particular order determined by a ranking system. Most search engines will remove \"spam\" pages from the list of results to provide a better list of results. The user can then click on any of the links to go to that webpage. Search engines are some of the most advanced websites on the web. They use special computer code to sort the web pages on SERPs. The most popular or highest-quality web pages will be near the top of the list. When a user types words into the search engine, it looks for web pages with those words. There could be thousands, or even millions, of web pages with those words. So, the search engine helps users by putting the web pages it thinks the user wants first. Search engines are very useful to find information about anything quickly and easily. Using more keywords or different keywords improves the results of searches. A search service may also include a portal with news, games, and more information besides a search engine.  Yahoo! has a popular portal, while Google has a simple design on its front page.  Search services usually work without charging money for finding sites, and are often supported with text or banner advertisements. Search engines use robots to ‘crawl’ online content. The process of crawling is the first measure that search engines take before indexing content in virtually any form–videos, text, images, webpages, etc. The content may constitute newly uploaded content to the internet or content that features updates or changes to its material. These robots, also known as crawlers or bots, record the information along with its links. Once the material has been crawled, it can be stored in a massive URL database. It’s this database that generates internet search results. After the bots crawl content, it can be indexed in the database and arranged in terms of its relevance. If internet content has not been crawled or indexed, it is unlikely to appear in the search results when someone makes a query no matter how relevant that content may be. After the content has been crawled, each of its words is indexed. The search engines also pinpoint where words are located on the crawled pages. During the indexing process, the search engine compares the content to other content with similar ‘words’ and decides how to organize it within its index. Ranking is a complex process that is dependent on search engine algorithms. When a searcher makes a query on Google looking for anything from 19th-century British landscape painters to New York City plumbers, the search engine will generate a list of good matches to that query. How these matches appear in the list relates to their rank. The search engine lists what it ‘thinks’ are the best answers to the query early in its search results.[2] Google and other search engines rely on algorithms to interpret the searcher’s query, identify the websites and pages in its index that are related to the request, and it then ranks them in terms of relevance in its presented search results list. What’s important to search engines is to provide searchers with the most relevant matches to their queries possible. Website operators, in turn, use search engine optimization to give their pages a higher rank."
  },
  {
    "url": "https://sd.wikipedia.org/wiki/%D9%88%D9%8A%D8%A8_%DA%B3%D9%88%D9%84%D8%A7_%D8%A7%D9%86%D8%AC%DA%BB",
    "title": "ويب ڳولا انجڻ - وڪيپيڊيا",
    "content": "ويب ڳولا انجڻ هڪ سافٽويئر سسٽم آهي، جيڪو عالمگير ويب مان ڄاڻ جي ڳولا لاءِ جوڙيو ويو آهي. ڳولا جا نتيجا عمومي طور قطار ۾ ڏيکاريا وڃن ٿا، جن کي ڳولا انجڻ نتيجن وارو صفحا چيو ويندو آهي. ڳولا انجڻ پاران ڳوليل نتيجن ۾ ويب صفحا، تصويرون ۽ ٻين قسمن جا فائيل شامل هوندا آهن. ڪجهه ڳولا انجڻ ميسر سرنامن جي ڪتاب مان پڻ ڄاڻ ڳولي ڏيندا آهن. ويب سرنامن جو ڪتاب جنهن ۾ ڄاڻ کي ماڻهو شامل ڪندا آهن، پر ويب ڳولا انجڻ الخوارزمي (انگريزي: Algorithm) جي مدد سان عالمگير ويب مان ڄاڻ تلاش ڪندا آهن. انٽرنيٽ ۾ اها ڄاڻ جن تائين ڳولا انجڻ کي پهچ حاصل نه هجي ان کي اونهو ويب (انگريزي: Deep Web) چيو ويندو آهي. انٽرنيٽ اندر ڳولا انجڻ جو بنياد 1990 ڌاري پيو. ڪير آهي (انگريزي: Whois) ۾ واپرائيندڙ جي ڳولا جي شروعات 1982ع ۾ ٿي[1] ۽ نونبوٽ انفارميشن سروس (انگريزي: Knowbot information Service) جي شروعات 1989ع ۾ ٿي.[2] پهريون قلمبند ڪيل ڳولا انجڻ 10 سيپٽمبر 1990ع ۾ شروع ٿي جنهن جي زريعي فائيل وغيره ڳولي سگھبا هئا.[3]\n1993ع کان اڳ سڄي عالمگير ويب جي ديٽا هٿن سان محفوظ ڪئي ويندي هئي. ان وقت ٽم بيرنيرسلي ڪيترائي ويب سرور جوڙيا جيڪي سرن جي ويب سرور ميزباني ۾ هئا.[4] پهريون اوزار جيڪو عالمگير ويب ۾ ڄاڻ کي ماڻهن جي مدد کان بغير ڳولڻ لاءِ استعمال ڪيو ويو ان کي آرڪي (انگريزي: Archie) چيو ويو.[5] آرڪي، آرڪائيو  (انگريزي: Archive) مان نڪتل لفظ آهي پر ان مان وي (انگريزي: V) کي ختم ڪيو ويو."
  },
  {
    "url": "https://sk.wikipedia.org/wiki/Web_search_engine",
    "title": "Web search engine – Wikipédia",
    "content": "Web search engine alebo WWW search engine (do slovenčiny prekladané ako webový/internetový prieskumový stroj, webový/internetový vyhľadávač, webový/internetový vyhľadávací nástroj, webový/internetový vyhľadávací prostriedok, webový/internetový vyhľadávací stroj, webová/internetová vyhľadávacia služba, webový/internetový vyhľadávací program, v bežnom jazyku len webový/internetový vyhľadávač) alebo skrátane search engine (slovenské názvy pozri v článku search engine) je search engine, ktorý vyhľadáva na WWW. V širšom zmysle zahŕňa aj web metasearch engines. Najznámejšíe web search engines sú Google, Yahoo, AltaVista a Bing. Search engine je zložený z týchto základných častí: V praxi funguje search engine tak, že používateľ zadá požiadavku a vyhľadávací program mu vypíše relevantné stránky, ktoré nájde z vlastnej databázy internetových zdrojov, vytvorenej na základe činnosti robota prehľadávajúceho hypertextovú štruktúru všetkých webových stránok. Vlastnú databázu zdrojov tvorí spider overovaním frekvencie používania, aktualizácie a miery vhodnosti ktorú vykonáva, naberajú stránky v jeho databáze na kredibilite, teda vyhľadávací program vyberá už len z kvalitných, hodnotných zdrojov. Výber je navyše rýchly, keďže úplné skenovanie webu sa vykonáva sústavne a nielen keď používateľ zadá požiadavku. Počas používania prieskumových strojov je veľmi dôležité správne voliť kľúčové slová, s ktorými pracuje tzn. na základe ktorých je v konečnom dôsledku aj spiderom prehľadaný celý WWW. Mali by byť úzko prepojené s témou, ktorú sa používateľ snaží vyhľadať a vhodné je používať i synonymá a booleovské operátory (&, +, -,...). Spider prechádza po už známych stránkach a cez ne hyperlinkami na ďalšie a ďalšie stránky, v čom prakticky spočíva celý systém,  akým je WWW vytvorený – ako obrovská sieť, spájajúca všetko so všetkým. Databáza search enginu je zase ako pomyselná kniha, z ktorej je používateľovi umožnené prečítať si určitý úsek, ktorý ho zaujíma – spider vytvorí zoznam stránok, ktoré pravidelne aktualizuje a obnovuje, databáza ich zapíše a vyhľadávací program ich cez požiadavky rozhrania používateľa zobrazí."
  },
  {
    "url": "https://sl.wikipedia.org/wiki/Spletni_iskalnik",
    "title": "Spletni iskalnik - Wikipedija, prosta enciklopedija",
    "content": "Spletni iskalnik (tudi internetni iskalnik) je namenjen iskanju informacij na  spletu in FTP strežniku, katerih iskalni izidi so običajno prikazani v obliki seznama. Informacije so lahko sestavljene iz  spletne strani, slik in drugih podakovnih oblik. Nekateri iskalniki prikažejo informacije s pomočjo  podatkovnega rudarjenja, ki so dostopne v  podatkovnih bazah ali  spletnih adresarjih. V nasprotju s spletnimi adresarji, ki jih vzdržujejo ljudje, spletni iskalniki delujejo s pomočjo  algoritmov oziroma kombinacijo algoritmov in človeškega vnosa. Iskanje informacij po spletnem prostoru lahko delimo tudi po geografskih razsežnostih: iskanje po svetu (Google), iskanje po posamezni državi (Najdi.si) ali pa lokalno iskanje (Raziskovalec.com). Pogosto so tudi spletni imeniki napačno imenovani iskalniki; od njih se razlikujejo po tem, da iščejo po vnaprej pripravljenih straneh s kratkim opisom. Spletni iskalniki namesto tega uporabljajo sezname vsebine, ki jih samodejno generirajo algoritmi za branje spletnih vsebin (t. i. pajki). Rezultate iskanja oboji predstavljajo v obliki seznama zadetkov, ki vsebuje povezave do najdenih spletnih strani, večpredstavnostnih datotek, lokacij na zemljevidu ipd. Skoraj vsi iskalniki in imeniki omogočajo tudi napredno iskanje informacij. Tako iskalnik Najdi.si, kot iskalnik Google.com omogočata izbiro naslednjih iskalnih parametrov: Januarja leta 1994 sta Jerry Yang in David Filo podiplomska študenta   elektrotehnike na  Univerzi Stanford ustanovila spletno stran z imenom Davidov in Jerryev vodnik po svetovnem spletu. To je bil imenik drugih  spletnih strani, organiziranih v hierarhijo, v nasprotju z iskalniki, ki indeksirajo strani. Aprila 1994, je bil Davidov in Jerryev vodnik po svetovnem spletu preimenovan v Yahoo. Ime Yahoo je kratica za »Še en hierarhično naravnan ponudnik upravljanja podatkov« (angleško Yet Another Hierarchical Officious Oracle). Domena Yahoo.com je bila ustanovljena 18. januarja leta 1995. Nato se marca leta 1995 vključila v posel in srečala z več deset kapitalisti  Silicijeve doline. Na kar so se aprila leta 1995 dogovorili  za financiranje Yahooja  z začetno naložbo v višini skoraj 2 milijona dolarjev.  Yahoo je bil tako zelo uspešna mednarodna nevladna organizacija, katera je imela aprila leta 1996 skupno 49 zaposlenih. Danes je Yahoo ena izmed vodilnih globalnih internetnih komunikacij, trgovin ter hkrati medijsko podjetje, katero ponuja celovito mrežo storitev za več kot 345 milijonov primerkov vsak mesec po vsem svetu. Kot prvi spletni navigacijski vodnik po spletu, je  www.yahoo.com  vodilni vodnik v smislu prometa, oglaševanja, gospodinjstva, kateri dosega poslovne uporabnike. Yahoo je številka 1 blagovnih znamk na internetu  in hkrati doseže največje občinstvo po vsem svetu. Podjetje ponuja spletno poslovanje in podjetniške storitve, zasnovane za povečanje  produktivnosti. Leta 2000 je Yahoo pričel uporabljati Google za rezultate iskanja. V naslednjih štirih letih so razvili svojo iskalno tehnologijo, katero so začeli uporabljati leta 2004. Yahoo je prav tako prenovil svoje poštne storitve, s čimer se je začelo  tekmovanje  z Google Gmail v letu 2007. Družba se je borila do 2008, z nekaterimi  velikimi odpuščanji. Istega leta meseca februarja so pri Microsoft Corporation naredili nenaročeno ponudbo za prevzem Yahoo-ja za 44.6 billionov dolarjev. Ponudbo so s strani Yahooja  nato uradno zavrnili za kar so trdili, da jih znatno podcenjujejo ter, da ta poteza ni bila v interesu njihovih delničarjev. Tri leta kasneje je imel Yahoo borzno kapitulacijo za 22.24 billionov. Januarja 2008 je soustanovitelja Jerrya Yanga nadomestil Carol Bartz.[1][2] Google je bil ustanovljen leta 1998, njegova ustanovitelja pa sta Larry Page in Sergey Brin, ki sta bila v tistem času študenta na  univerzi Stanford.\nPage in  Brin sta skupaj delala na iskalniku Back Rub od leta 1996, vendar pa sta dobila vzpodbudo od soustanovitelja  Yahooja David Fila  in se odločila, da leta 1998 ustanovita podjetje, katerega začetki so se pričeli v prijateljevi garaži. \nTakrat je bil Google še v  alfa fazi, število indeksiranih strani je bilo samo 25 milijonov, dnevnih iskanj pa je bilo 10,000. Popularnost iskalnika se je višala predvsem z ustnim izročilom ter z vračanjem obiskovalcev, saj so bili ti zadovoljni z rezultati iskanj. Google je napravil velik korak naprej leta 2000, ko je zamenjal Inktomi kot ponudnika dodatnega iskanja na  Yahooju. Za tem je ponudil iskanje še  AOLu,  Netscapu,  Freeserveu ter  BBCju. To je Googlu dalo izjemno pokritost iskanj. Posledično mu je narastel ugled in kmalu je postal eden najbolj zanesljivih in natančnih iskalnikov.\nKljub prekinitvi sodelovanja z  Yahoojem leta 2004 je Google še povečal svoj vpliv na  internetu. Globalno dominacijo so povečali z razvojem lokalnih verzij iskalnika. Aktivno je razvijal tudi različna iskanja kot so iskanje novic, slik, izdelkov  (Froogle) in lokalna iskanja. Usmeril se je v to, da postane glavni ponudnik računalniških storitev in iz prestola vrže Microsoft. V ta namen so razvili produkte kot so e-poštno storitev gmail, Google earth, Google talk (VoIP storitev), Google base in Google book search.\nGoogle je vsekakor postal sinonim za iskanje, poleg tega pa ima tudi mesto v slovarju za glagol »to google«. Širjenje in integracija različnih Googlovih storitev ga je naredila dominantnega v  spletnem trgu. Za mnoge strani pa je Google glavni vir obiskovalcev in je tako glavna tarča za rangiranje, saj ob dobri poziciji na iskalniku dobiš občutno več obiska na spletno stran.[3] Bing (predhodno poimenovan »Live Search, Windows Live Search, MSN Search«) je trenutni Microsoft-ov spletni iskalnik (oglaševan kot motor odločitve). Predstavil ga je Mikrosoftov glavni izvršni direktor 28. maja leta 2009, na konferenci Vse digitalne stvari v San Diegu. V celoti se je na spletu pojavil 3. junija leta 2009, predhodno verzijo so pa predstavili 1. junija leta 2009, na kar je v prvih tednih Bing uspešno pridobival tržni delež. Pomembne spremembe vključujejo zapisovanje iskalnih poizvedb v realnem času, tako kot so vnesene in tako je bila dodana lista sorodnih iskanj, ki bazira na semantični oziroma pomenski  tehnologiji iz  Powerseta, ki ga je Microsoft kupil leta 2008. Leta 2009, natančneje 29. julija sta Microsoft in Yahoo oznanila dogovor, s katerim bo Bing poganjal Yahoojev iskalnik. MSN iskalnik je bil prvič lansiran jeseni leta 1998 in je uporabljal iskalne zadetke iz  Inktomija. V začetku leta 1999 je MSN iskalnik lansiral različico, ki je prikazovala oglase iz Looksmarta, kateri so bili pomešani skupaj z zadetki iz  Inkomija, razen kratkega obdobja leta 1999, ko so namesto teh uporabljali zadetke iz  AltaViste. Od takrat je Microsoft nadgradil MSN iskalnik in s tem omogočil lasten vgrajen iskalnik rezultatov in indeks, ki se posodablja tedensko ali celo dnevno. Nadgradnja se je začela kot beta program novembra leta 2004 (na podlagi več letnih raziskav) in prišla iz bete februarja leta 2005. Prvi javni beta Windows Live Search je bil predstavljen  8. marca leta 2006, z dokončno sprostitvijo 11. septembra leta 2006 ter s tem nadomestil MSN iskalnik.  Kot prizadevanje za  ustvariti novo identiteto iskanja Microsoft-ovih storitev, je bil Live Search uradno nadomeščen z Bing-om 3. junij leta 2009. Tako sta se Microsoft in Yahoo dogovorila za 10- letno sodelovanje, v katerem bo Yahoo iskalnik nadomeščen z Bingom.[4][5] Spletni iskalniki so dokaj kompleksni programi. Če na kratko povzamemo, so sestavljeni iz treh delov: Spletni roboti oz. pajki so neke vrste izvidniki za spletne iskalnike, z namenom iskanja in odkrivanja spletnih strani na  internetu. Ko  spletno stran najdejo, se vklopijo indekserji oz. kazalniki, ki indeksirajo najdeno  spletno mesto. Tehnično  spletni roboti delujejo tako, da pošljejo spletnim  strežnikom ukaz oz. željo za spletno stran, ki jo dotični strežnik gosti, jo posnamejo in pošljejo kazalniku v nadaljnjo obravnavo. Pajki tako tudi iščejo različne spletne povezave ( HTMLjeva nadbesedila in tako sprotno pridobivajo nove  spletne naslove za obdelavo.   Administratorji se teh nezaželenih robotov lahko ubranijo s posebnimi  programi oz. ukazi (kot so robots.txt), v katerem je HTML koda, ki robotu prepove oz. mu prepreči pridobitev povratne informacije o  spletnem mestu. Drugače pa lahko vsak administrator prijavi svojo spletno stran preko iskalnika, kjer nato iskalnikov robot pregleda listo dodanih spletnih strani in jih tako indeksira. Indekserji oz. kazalniki od  spletnih robotov prejmejo celotne spletne strani v obdelavo, kjer indeksirajo oz. shranijo vsako besedo, ki se pojavi na obdelanem  spletnem mestu. Besede se shranijo v obratnem vrstnem redu abecedno. Vsak indeks ima eno besedo, seznam dokumentov  kjer se dotična beseda pojavi in v nekaterih primerih tudi lokacijo v tekstu, kjer se ta beseda nahaja. Za boljše, natančnejše in hitrejše iskanje, spletni iskalniki eliminirajo besede kot so »in«, »za«, »pri«, »je«, katere poimenujejo »odvečne besede« oziroma angleško: stop words. Te besede so nepomembne pri iskanju, zato jih lahko varno prezrejo v primeru iskanja. Iskalni procesor je najbolj sofisticiran sistem znotraj spletnega iskalnika. Tehnično vsebuje nekaj nivojev, kot so uporabniški vmesnik za iskanje (polje za vpis iskane besede), nadaljnje podprogram, ki oceni vpisano iskalno besedo in jo primerja z bazo kazalnika in relevantnimi dokumenti in jo nato prikaže v vmesniku, ki uporabniku izpiše predlagane  spletne strani. Polje za vpis iskalne besede in uporabniški vmesnik za prikaz najdenih rezultatov se od enega spletnega iskalnika do drugega razlikujejo predvsem v točnosti in dodatnih funkcijah (angleško advanced search options), ki s pomočjo drugih nivojev še dodatno filtrirajo iskane spletne besede in tako lahko ponudijo še bolj točne rezultate. Največja razlika med načini iskanja v različnih spletnih iskalnikih se kaže v postopkih računanja relevantnosti iskane besede. Večina jih računa na podlagi  statistične obdelave besed, drugi jih filtrirajo na podlagi drugih spletnih povezav, ki kažejo na iskano besedo in tako na iskano spletno stran. Ti postopki so zapisani v  algoritmih. Administratorji spletnih iskalnikov vseskozi nadgrajujejo in spreminjajo algoritme, ki izvajajo preračunavanje relevantnosti besed in tako jim omogočajo še večjo natančnost.[6] Ta članek o internetu je škrbina. Pomagajte Wikipediji in ga  razširite."
  },
  {
    "url": "https://ckb.wikipedia.org/wiki/%D8%A8%D8%B2%D9%88%DB%8E%D9%86%DB%95%D8%B1%DB%8C_%DA%AF%DB%95%DA%95%D8%A7%D9%86%DB%8C_%D9%88%DB%8E%D8%A8",
    "title": "بزوێنەری گەڕانی وێب - ویکیپیدیا، ئینسایکڵۆپیدیای ئازاد",
    "content": "بزوێنەری گەڕانی وێب جۆرێکە لە بزوێنەری گەڕان (بە ئینگلیزی: Search engine) ئامرازێکە گەڵاڵەدارێژی کراوە بۆ گەڕان بە دوای زانیاریدا لەسەر تەونی بەربڵاوی جیھانی (World Wide Web). ئاکامی گەڕانەکان زۆرتر بە شێوەی لیستەیەک پیشان دەدرێت و دەتوانێت پەڕەی وێب، وێنە یان ھەر چەشنە پەڕگەیەکی تری لە خۆ گرتبێت. ژمارەیەک لە بەناوبانگترین گەڕانچییکانی وێب ئەمانەن:\nگووگڵ، یاھوو، MSN، ئاڵتاڤیستا، و دەک دەک گۆ. ئەمەی دوایی، دەک دەک گۆ، بانگەشەی ئەوە دەکەن کە ئەوان زانیاریی کەسەکان نافرۆشن و ڕێگا نادەن کۆمپانیاکان ڕێکلام بخەنە ناو ئەنجامەکانی گەڕانەوە.[١]"
  },
  {
    "url": "https://sr.wikipedia.org/wiki/%D0%92%D0%B5%D0%B1-%D0%BF%D1%80%D0%B5%D1%82%D1%80%D0%B0%D0%B6%D0%B8%D0%B2%D0%B0%D1%87",
    "title": "Веб-претраживач — Википедија",
    "content": "Претраживач веба (енгл. web search engine) представља интернет сервис, чија је сврха тражење информација на вебу, и то углавном задавањем кључних ријечи, а много рјеђе одабиром понуђених ставки. Исход претраге се најчешће приказује као списак веб-сајтова који садрже тражену информацију, уз могућност да се веб-странице које су одговор на упит посјете са страна претраживача. Веб је развио Тим Бернерс-Ли и његове колеге, 1990. године. За нешто више од двије деценије, постао је највећи извор информација у историји човјечанства. Процјењује се да је укупан број докумената и записа у базама података, стотине милијарди.[1] До краја 2005. године, већ је било преко милијарду корисника интернета широм свијета. Проналажење информација на интернету је постало битан дио свакодневних животних активности. У ствари, претраживање је друга најпопуларнија активност на вебу, иза е-поште, са преко 550 милиона претрага сваки дан. Веб се састоји од површинског и дубинског (такође скривени или невидљиви веб). Свака страница на површинском вебу има логичну адресу која се назива веб-адреса (енгл. Uniforme Resource Locator - URL). Веб-адреса странице омогућава њено директно учитавање. Супротно томе, дубински веб садржи странице које није могуће директно учитати као и записе у базама података који су складиштени у системима база података. Сматра се да је дубински веб 100 пута већи од површинског.[1] Алатке које се користе за проналажење информација на вебу зову се претраживачи. Вјерује се да је више од милион претраживача оперативно на вебу. Претраживаче је могуће класификовати на основу типа података које претражују. Претраживачи који претражују текстуалне документе зовемо претраживачи докумената, док претраживаче који претражују структурисане податке који се чувају у базама података зовемо претраживачи база података. Многи популарни претраживачи као што су Google и Yahoo су претраживачи докумената, док се многи претраживачи е-трговина као што је Amazon.com, сматрају претраживачима база података. Интегрисани претраживач Википедије је такође примјер претраживача базе података. Претраживачи докумената обично имају простији интерфејс са текстуалним пољем гдје корисници уносе свој упит који обично чине кључне ријечи које одражавају потребе корисника за одређеним информацијама. С друге стране, многи претраживачи база података омогућавају корисницима да врше специфичне и сложеније упите. Постоје и претраживачи који су специјализовани за претрагу мултимедијалних датотека (аудио и видео записа и слика). Већина претраживача покривају само мали дио веба. Да би се повећала покривеност једног система претраге, могуће је комбиновати више претраживача одједном. Системи за претрагу који користе друге претраживаче за обављање претраге и комбинују их са својим резултатима, називају се метапретраживачи. Архитектура различитих претраживача веба може знатно варирати, међутим типичан претраживач докумената се обично састоји од слиједеће четири компоненте: претраживача локације веба (web crawler), индексера (indexer), индекса базе података и машине за упите (query engine). Претраживач локације веба, такође познат као програм трагач (web spider) или веб-робот, пролази кроз веб у потрази за веб-страницама слиједећи њихове URL адресе. Индексер је задужен за анализу текста сваке пронађене веб-странице и вађење кључних ријечи на основу којих се онда прави индексна база података свих анализираних веб-страница. Када корисник уради упит, машина за упите претражује индексну базу података у потрази за страницама које одговарају кључним ријечима које су предмет корисниковог упита. Претраживач веб-сајтова (web crawler) је рачунарски програм који преузима веб-странице са веб-сервера. URL сваке веб-странице идентификује њену локацију на вебу. Обзиром на постојање URL адресе, свака веб-страница се може преузети са веб-сервера употребом протокола за пренос хипертекста (HTTP). Полазећи од једне почетне URL адресе, претраживач локација веба непрестано преузима веб-странице базирајући се на њиховим URL адресама и вади URL адресе из оних већ преузетих, тако да је могуће преузети нове. Овај процес се завршава када је неки услов за заустављање програма испуњен. Неки од могућих услова за његово заустављање су: (1) нема више нових URL адреса на локацији или (2) унапријед дефинисани број, односно списак веб-страница је већ преузет. Пошто претраживач веб-локација може бити у интеракцији са разноликим самосталним веб-серверима, битно је да буде пројектован тако да га је лако прилагодити новим захтјевима. Како би се убрзао процес претраживања, могуће је употријебити више претраживача локација веба. Они могу бити два различита типа, централизовани и дистрибуирани.[2] Централизовани претраживачи локација веба се налазе на истој локацији са које се покрећу паралелно и са више машина.[3] Дистрибуирани претраживачи локација веба се налазе на више различитих локација на интернету и контролишу се путем једног централног координатора, другим ријечима сваки претраживач локација веба преузима само веб-странице које су му географски близу. Најзначајнија предност дистрибуираних претраживача локација веба тиче се смањења трошкова комуникација који су резултат њихове активности. Међутим централизовани претраживачи су лакши за имплементацију и контролу него дистрибуирани. Раст и константне промјене на вебу, стварају потребу да претраживачи локација веба регуларно врше нова претраживања као и да одржавају индексну базу података ажурном. Међутим, сувише често претраживање веба би довело до значајног трошења ресурса, али и потешкоће у раду веб-сервера на којима се тражене веб-странице налазе. Стога је потребно употријебити једну стратегију инкременталног претраживања. Једна од њих је да се само претражују веб-странице чији се садржај или URL адреса промијенила од посљедње претраге. Друга стратегија је да се употријебе претраживачи локација веба који имају предефинисану област претраге, или предефинисани скуп тема за претрагу. Ови посљедњи се могу искористити за креирање специјализованих претраживача локација веба који су једино заинтересовани за веб-странице одређене тематике. Конвенционални претраживачи веб-локација се могу употријебити само за претраживање површинског веба. Посебни претраживачи локација веба се пројектују за претрагу информација које се налазе у дубинском вебу. Пошто су информације које се налазе у дубинском вебу обично скривене иза разних интерфејса претраге, претраживачи локација дубинског веба су пројектовани тако да прикупљају податке вршећи упите у интерфејсу претраге и преузму повратне резултате. Након што су веб-странице преузете на мјесто претраживача, оне се обрађују у формату који је подесан за ефективну и ефикасну употребу са претраживачима. Садржаји странице могу бити представљени ријечима које се на њој налазе. Несадржајне ријечи као што су „је“ или „ли“ се обично не користе за представљање садржаја. Неријетко, ријечи се конвертују у њихов коријен употребом одговарајућег програма како би се олакшало погађање различитих варијација исте ријечи. На примјер, „рачун“ је коријен ријечи „рачунати“ и „рачунарство“. Након уклањања несадржајних ријечи са странице и процеса извлачења коријена, преостале ријечи (такозвани индексни појмови) користе се за представљање странице у претрази. Реченице се такође могу препознати као засебни индексни појмови. Након тога, одлучује се о значају сваког појма у заступању садржаја странице приликом давања резултата претраге у претраживачу. Значај појма п на страници с у оквиру датог скупа С страница, може се одредити на више начина. Ако третирамо сваку страницу као текстуални документ, онда се значај п обично израчунава на бази двије статистике. Прва се односи на фреквентност појма (фп) у с, односно број пута појављивања појма п у страници с, а друга се односи на фреквентност документа (фд) у скупу С, односно број страница у скупу страница С у којима се среће појам п. Интуитивно гледано, што се у више страница налази појам п, то је више битан као заступник садржаја странице. Међутим, што се у више различитих страница појављује појам п, утолико је мање употребљив за диференцирање различитих страница једних од других. Као резултат, значај појма би требало да буде монолитна опадајућа функција његове фреквентности у различитим документима. Тренутно, већина веб-страница је форматирана у језику за маркирање хипертекста (HTML), који посједује скуп тагова као што су title и header. Те информације се могу користити за утјецање на значај појмова који представљају веб-странице. На примјер, појмови који се налазе у наслову једне веб-странице, истакнути масним или искошеним словима, врло вјероватно су значајнији за заступање једне веб-странице него они који се појављују у њеном садржају (body) и који су без посебног форматирања. Уобичајени упит претраживачу веба се састоји од неких кључних ријечи. Такав упит се такође може представити као скуп појмова са одређеним значајем. Степен поклапања између странице и упита, односно такозване сличности, може се мјерити појмовима које они међусобно дијеле. Једноставан приступ овом проблему је да се сабирају производи значаја који одговарају појмовима између упита и странице. Овај приступ даје као резултат веће сличности за странице које дијеле најважније појмове са самим упитом. Међутим, има тенденцију да даје предност дужим страницама над краћим. Овај проблем се обично ријешава тако што се горња сличност дијели са производом значаја упита и странице. Функција која израчунава ову врсту сличности, назива се косинус функција. Дужину сваке странице је овдје могуће израчунати унапријед и ускладиштити на претраживачу. Постоје многе методе за ранговање веб-страница за корисничке упите, а различити претраживачи их различито користе. На примјер, неке методе ранговања могу узети у обзир близину појмова који су предмет упита у некој страници. Као други примјер, претраживач може сачувати информације о броју приступа различитих корисника одређеној страници и искористити те информације за ранговање веб-страница које ће се приказати поводом будућих упита. На вебу постоји много популарних претраживача, али Гугл је сматран једним од најпопуларнијих. Главни разлог за то је његова метода ранговања страница која има способност да разликује најважније странице од мање важних чак и када се у свима њима исти број пута појављују појмови који су предмет упита. За одлучивање о значају сваке странице, Гугл користи информације о линковању међу њима, односно начин на који линкују једне на друге. Тако линк са странице A на страницу Б а који је поставио аутор странице A, служи као индикација да аутор странице A сматра да страница Б има неку вриједност. На читавом вебу, на страницу Б може линковати већи број других страница и ти линкови могу послужити за одлучивање о њеној свеукупној вриједности или значају. За дату страницу, PageRank је мјера њеног релативног значаја на вебу, и он се израчунава на бази информација о линковању.[4] Три главне идеје стоје иза дефинисања значаја и израчунавања PageRank-a: (1) Странице које су линковане са више страница су највјероватније најважније. Другим ријечима, значај странице треба да се успостави на основу њене популарности међу ауторима свих других веб-страница. (2) Странице које су линковане са најзначајнијих веб-страница највјероватније и саме имају посебан значај. (3) Странице које имају линкове на више страница имају мање утицаја на значај сваке линковане странице појединачно. Другим ријечима, ако страница има више подстраница, онда она једино може пренијети мањи дио свог значаја на сваку од њих. На основу ових схватања Гуглови оснивачи су развили метод за израчунавање значаја (PageRank) сваке странице на вебу.[4] PageRank веб-странице се може комбиновати са другим мјерама на бази садржаја за индикацију њеног свеукупног значаја у односу на дати упит. На примјер, за задати упит x, страница може бити рангована на основу пондерисаног збира њених сличности са упитом и њеног PageRank-а. Међу страницама са великим сличностима, овај метод ће дати предност онима које имају виши PageRank. За страницу се каже да је релевантна, уколико корисник који је извршио упит налази да је корисна. За задати упит корисника на фиксни скуп страница, скуп оних које су релевантне је такође фиксан. Добар систем претраге треба кориснику да врати висок степен релевантних страница као и да их рангује високо у повратним резултатима. Традиционално, ефективност система претраге се мјери преко два количника позната као одзив и прецизност. Код упита у скуп докумената x, одзив је проценат релевантних докумената који се на упит одзивају, а прецизност је проценат одзваних докумената који су за упит релевантни. Да би се оцијенила ефективност једног система претраге, врше се пробе низом упита. За сваки посебан упит, скуп релевантних докумената се идентификује унапријед. Код сваког пробног упита тражи се вриједност прецизности за сваку тачку одзива понаособ. Када се направи једна просјечна вриједност прецизности за сваку тачку одзива, онда се добије крива укупне просјечне прецизности система претраге која служи као мјера његове ефективности. Један систем претраге се сматра ефективнијим од другог, уколико је његова крива прецизности и одзива изнад криве другог. Код савршеног система претраге вриједности одзива и прецизности треба да буду једнаке броју 1. Другим ријечима, такав систем проналази тачан скуп релевантних докумената за сваки упит. У пракси, савршен учинак није остварљив из много разлога. На примјер, потребе корисника за одређеним информацијама обично не могу бити прецизно дефинисане пробним упитом, али ни сам садржај у документима, као ни у упитима, се не може у потпуности представити пондерисаним појмовима. Употреба одзива и прецизности за мјерење ефективности традиционалног система претраге текста, захтијева да је унапријед познат број свих релевантних докумената за сваки пробни упит. Међутим, ово није практично за оцјењивање и независно вредновање великих претраживача, будући да је немогуће знати број релевантних страница за сваки упит у претраживачу, уколико се све оне ручно не прегледају. Без познавања броја релевантних страница за сваки пробни упит, мјера одзива се не може израчунати. Као резултат овог практичног ограничења, претраживачи се често оцјењују на основу средње прецизности која се израчунава на основу најрелевантнијих одзваних страница к за скуп пробних упита за неки мали цијели број —рецимо 20, или на основу просјечне позиције прве релевантне странице која је повратни резултат у сваком појединачном пробном упиту.[5] Претраживачи веба су, у основи, експертски системи који имају за циљ стварање што више хеуристика способних за помоћ експертском систему у предвиђању шта је то што корисник тражи. Специјализовани претраживачи веба за сврху имају тражење информација у вези са специфичном облашћу."
  },
  {
    "url": "https://sh.wikipedia.org/wiki/Veb-pretra%C5%BEiva%C4%8D",
    "title": "Veb-pretraživač – Wikipedija / Википедија",
    "content": "Web search engine [wɛb sɜːt͡ʃ ˈɛndʒɪn] (od engl. web: mreža + search engine: stroj za pretraživanje), softverski sistem dizajniran za pretraživanje informacija na World Wide Webu. Katkad se za nj upotrebljavaju još nazivi web tražilica i web pretraživač, posljednje često istovremeno za web browser. Rezultati pretraživanja općenito su predstavljeni linijom rezultatâ, odnosno stranicama tražilice s rezultatima ili SERP-ovima (akr. od engl. search engine results page). Informacije mogu biti mješavina web stranica, slika i drugih tipova datoteka. Neke tražilice često kopaju podatke dostupne u bazama podataka ili otvorenim direktorijima. Za razliku od web direktorijâ, koje održavaju tek humani urednici, tražilice također održavaju informacije u realnom vremenu tako što pokreću algoritam na web crawleru. Najpoznatiji današnji web search engine jest Google Search, a od drugih su poznati Yahoo! Search i Bing. Same internetske tražilice postojale su prije debija weba u decembru 1990. Pretraživanje korisnika WHOIS postoji od 1982.,[1] a mnogomrežno pretraživanje korisnika Knowbot Information Service (KIS) prvi put je implementirano 1989.[2] Prva dobro dokumentirana tražilica koja je pretraživala sadržajne datoteke, prije svega datoteke FTP-a, bio je Archie, koji je debitirao 10. septembra 1990."
  }
]